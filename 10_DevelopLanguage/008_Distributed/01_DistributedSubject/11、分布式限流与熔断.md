###### 1. 什么是服务降级？
服务降级是一种系统容错策略，其核心思想是：当系统整体资源紧张或某个非核心服务出现故障时，为了保证核心服务的可用性，有策略地暂时停止或简化非核心服务的功能，或返回一个预定义的默认值（兜底数据）。服务降级是一种**主动的、有计划的**防御行为。
**关键要点**：
- **触发时机**：通常在大促前（预案降级）或监控到系统负载过高、响应时间飙升时（自动/手动触发）。
- **实现方式**：
    1. **返回默认值**：如商品详情页的商品推荐服务不可用，则返回一个空的推荐列表。
    2. **调用本地备用逻辑**：如调用第三方地图API失败，转而使用本地存储的粗略地理位置信息。
    3. **功能屏蔽**：如暂时关闭商品评价、积分兑换等非关键功能。
- **代码层面**：通常通过配置开关或注解来实现。例如，使用 `@HystrixCommand(fallbackMethod = “defaultMethod”)`或 Sentinel 的 `@SentinelResource(fallback = “fallbackHandler”)`来定义降级逻辑。
###### 2. 什么是服务熔断？
服务熔断是一种**被动的、自动的**故障隔离机制，其灵感来源于电路熔断器。当对某个目标服务的调用失败率（如慢调用比例、异常比例）达到预设的阈值时，熔断器会“跳闸”，进入**打开状态**。在打开状态下，所有对此服务的请求会在短时间内**快速失败**，不再发起真实的远程调用，从而防止故障蔓延、避免资源耗尽。经过一段时间的“冷却期”后，熔断器会进入“半开状态”，尝试放行少量请求探测目标服务是否恢复，若成功则关闭熔断器，恢复调用；若失败则继续保持打开状态。
**熔断器状态机**（以Hystrix为例）：
1. **CLOSED（关闭）**：正常状态，请求正常通过。
2. **OPEN（打开）**：失败率超过阈值，所有请求被快速失败，直接执行降级逻辑。
3. **HALF-OPEN（半开）**：经过一个`sleepWindowInMilliseconds`时间后进入此状态，允许有限数量的试探请求通过。若这些请求成功，则熔断器关闭；若失败，则再次打开。
###### 3. 熔断和降级的区别是什么？
- **核心目标**：
    - **熔断**：目标是**故障隔离和快速失败**，防止级联故障，保护调用方。它关注的是下游服务的**健康状况**。
    - **降级**：目标是**弃车保帅**，在系统资源不足时，保障核心功能的可用性。它关注的是当前系统的**整体资源状况**。
- **触发条件**：
    - **熔断**：由下游服务的**调用失败率**（异常、超时）自动触发。
    - **降级**：可由**系统负载**（CPU、线程池）手动或自动触发，也可作为熔断触发后的后续处理手段（即熔断后执行降级逻辑）。
- **作用层次**：
    - **熔断**：通常作用于**单个服务依赖**。
    - **降级**：可以作用于单个服务，也可以作用于**整个功能模块或页面**。
- **关系**：熔断通常是降级的一种**触发原因**和**实现手段**。当熔断器打开时，系统会执行预定的降级逻辑。
###### 4. Hystrix 的工作原理是什么？
Hystrix 通过 “**命令模式**” 将对外部服务的调用封装在 `HystrixCommand`或 `HystrixObservableCommand`对象中。其核心工作流程如下：
1. **构建命令与执行**：每次调用被封装为一个命令对象。通过 `execute()`（同步）或 `queue()`（异步）触发执行。
2. **检查熔断器**：执行前，首先检查对应熔断器的状态。如果为`OPEN`，则直接跳到第8步（执行降级）。
3. **检查线程池/信号量**：Hystrix 使用**舱壁隔离**策略。默认使用线程池隔离（也可配置为信号量）。检查线程池队列是否已满或信号量是否耗尽，若是，则拒绝执行并跳至第8步。
4. **执行目标方法**：在独立的线程（线程池隔离）或调用线程（信号量隔离）中执行实际的网络请求。
5. **计算健康度**：无论成功、失败、超时还是线程拒绝，Hystrix 都会向熔断器报告结果。熔断器会统计一个时间窗口（默认10秒）内的请求总数、失败数，计算错误百分比。
6. **判断是否熔断**：如果错误百分比超过阈值（默认50%），且窗口内请求总数达到最低要求（默认20），熔断器状态从`CLOSED`变为`OPEN`。
7. **Fallback处理**：当第2、3、4步出现失败（异常、超时、拒绝）时，执行命令中定义的 `getFallback()`方法进行服务降级。
8. **返回结果**：返回正常或降级后的结果。
**源码角度**：核心类是 `HystrixCommand`。其 `execute()`方法最终会调用 `queue()`返回一个 `Future`，然后调用 `Future.get()`。`AbstractCommand`的 `executeCommandAndObserve()`方法是执行链条的核心，它按顺序应用了熔断检查、线程池隔离、请求执行、指标统计等逻辑。熔断器的实现 `HystrixCircuitBreaker`中，`isOpen()`方法决定了是否允许请求通过。
###### 5. Sentinel 和 Hystrix 的区别是什么？
- **设计理念与维度**：
    - **Hystrix**：以**隔离**和**熔断**为中心，关注服务调用间的故障容错。其核心维度是“服务依赖”。
    - **Sentinel**：以**流量**为切入点，从流量控制、熔断降级、系统负载保护等多个维度来保障服务的稳定性。其核心维度是“资源”，资源可以是任何东西（一个URL、一个RPC服务、一个方法）。
- **隔离机制**：
    - **Hystrix**：主要采用**线程池隔离**，隔离性好但开销大（上下文切换、排队）。
    - **Sentinel**：默认采用**信号量隔离**（并发线程数控制），轻量级，开销小。也支持基于慢调用比例的熔断降级。
- **流量控制**：
    - **Hystrix**：通过线程池大小或信号量总数进行**限流**，策略相对单一。
    - **Sentinel**：提供丰富的**流量整形**策略，包括QPS/并发线程数限流、冷启动、匀速排队（漏桶算法）等。
- **实时监控与动态规则**：
    - **Hystrix**：监控数据通过`hystrix-metrics-event-stream`暴露，需要结合Dashboard查看，规则一般为静态配置。
    - **Sentinel**：提供开箱即用的**实时监控控制台**，支持**动态规则推送**（可集成Nacos、ZooKeeper等），变更规则无需重启应用。
- **扩展性**：
    - **Sentinel**​ 提供了开放的SPI接口，方便用户自定义规则管理、适配数据源等。Hystrix扩展相对复杂。
###### 6. 分布式限流如何实现？
分布式限流旨在对集群级别的总流量进行控制。核心挑战是**在分布式环境下实现计数器的原子性操作和一致性**。
**常用实现方案**：
1. **基于Redis + Lua脚本**：这是最常用的方案。将限流逻辑（如令牌桶、漏桶、固定窗口、滑动窗口）写成Lua脚本，利用Redis的单线程执行特性保证原子性。
    - **示例（滑动窗口限流）**：使用Redis的 `ZSET`有序集合，成员为请求的唯一标识（如UUID），分数为请求的时间戳（毫秒）。每次请求前，执行Lua脚本移除窗口外的旧记录，并统计当前窗口内请求数，若未超限则插入当前记录并设置过期时间。
2. **基于网关层限流**：在API网关（如Spring Cloud Gateway、Zuul、Nginx+Lua）进行统一限流。网关是所有流量的入口，易于实施集群级别的控制。
3. **基于中间件**：使用阿里云的AHAS、Sentinel集群限流等专业组件，它们内置了分布式限流能力。
4. **令牌桶算法的分布式实现**：需要一个中心化的令牌桶服务。所有服务实例向该服务申请令牌。此服务本身需要高可用。
###### 7. 单机限流和分布式限流的区别是什么？
- **控制范围**：
    - **单机限流**：仅能限制当前JVM进程内的流量，无法知晓其他实例的请求情况。适用于为单个服务实例设置保护阈值。
    - **分布式限流**：限制的是集群、服务或资源的**全局总流量**。需要跨实例协调计数器。
- **实现复杂度**：
    - **单机限流**：实现简单，使用本地内存的原子类（如 `AtomicLong`）、`RateLimiter`（Guava）或 `Semaphore`即可。
    - **分布式限流**：实现复杂，需要引入外部存储（如Redis）和分布式协调机制，并处理网络延迟、数据一致性等问题。
- **精确性**：
    - **单机限流**：精确控制本机流量。
    - **分布式限流**：由于网络同步延迟，可能存在**瞬时流量毛刺**（在时间窗口切换的瞬间，各实例的计数器未完全同步，导致总量略超限）。
- **性能开销**：
    - **单机限流**：几乎没有额外开销。
    - **分布式限流**：每次请求都可能需要一次网络RPC（访问Redis），带来额外的延迟和负载。
###### 8. 如何设计一个限流系统？
一个健壮的限流系统设计需要包含以下层面：
1. **需求分析与指标定义**：
    - **限流对象**：API接口、用户ID、IP地址、还是整个服务？
    - **限流维度**：QPS（每秒请求数）、并发线程数、还是总请求量？
    - **限流阈值**：阈值是多少？是否需要支持动态调整？
2. **算法选择**：
    - **固定窗口计数器**：实现简单，但存在窗口临界点突刺问题。
    - **滑动窗口计数器**：更平滑，能部分解决突刺，但内存占用稍高。**推荐用于大多数API限流**。
    - **令牌桶**：支持突发流量，允许一定程度的流量整形。
    - **漏桶**：以恒定速率处理请求，能严格限制流量速率，但不支持突发。
3. **架构设计**：
    - **嵌入客户端**：限流逻辑集成在每个应用实例中（如使用Sentinel客户端）。优点是性能好，延迟低；缺点是需要一个中心化的控制台/规则服务器来同步规则和计数器。
    - **网关层集中式**：在API网关统一限流。优点是易于管理、与业务解耦；缺点是网关可能成为瓶颈，且对于服务间调用（不经过网关）无法控制。
    - **混合模式**：核心的、粗粒度的限流放在网关；细粒度的、业务相关的限流（如对某个用户限频）放在客户端。
4. **关键技术实现**：
    - **原子操作**：必须保证计数增减的原子性。分布式环境下使用**Redis + Lua脚本**是标准方案。
    - **高性能**：尽量减少网络开销。考虑使用本地缓存+定期同步到中心的方式，或使用支持高性能计数器的存储（如Redis的 `INCR`命令）。
    - **降级与容错**：当限流服务（如Redis）不可用时，系统应具备降级策略（如直接放行或本地限流），避免因限流系统故障导致业务不可用。
5. **监控与运维**：
    - **实时监控**：实时展示被限流的资源、QPS、拒绝请求数等。
    - **动态规则配置**：规则应能通过控制台或配置中心（如Nacos）动态下发，实时生效。
    - **告警**：当触发限流时，应及时告警通知相关人员。