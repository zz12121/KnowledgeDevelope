###### 1. 如何设计一个高可用的分布式系统？
高可用性的核心目标是确保系统在面对各类故障时，仍能提供持续可用的服务。设计遵循“冗余”、“解耦”、“自愈”和“容错”原则。具体策略包括：
1. **消除单点故障**：系统中的任何组件（服务器、服务、网络设备、数据中心）都必须有备份。
    - **无状态服务多副本**：通过负载均衡器（如Nginx、云ELB）将流量分发到多个完全相同的服务实例。任一实例故障，流量被自动导向健康实例。在Kubernetes中，`Deployment`通过`ReplicaSet`确保指定数量的Pod副本始终运行。
    - **有状态服务集群化**：数据库、缓存等有状态服务采用主从复制、集群分片模式。例如，Redis使用哨兵模式实现自动故障转移；Kafka通过分区副本和ISR机制保证Broker失效时数据可用。
2. **实现快速故障转移与恢复**：
    - **健康检查**：负载均衡器或服务注册中心（如Eureka、Nacos）需定期对后端实例进行健康检查（TCP/HTTP），及时剔除不健康节点。在Kubernetes中，`kubelet`通过`livenessProbe`和`readinessProbe`决定Pod的生命周期。
    - **服务发现与注册**：服务实例启动时向注册中心注册，下线时注销。客户端通过注册中心动态获取可用实例列表，而非硬编码IP。Eureka客户端通过周期性心跳维持注册，并在服务端采用自我保护模式应对网络分区。
3. **应用层容错设计**：
    - **客户端负载均衡与重试**：客户端（如通过Spring Cloud LoadBalancer）应具备重试机制，当某次请求失败时，自动重试其他实例。需配合**退避策略**（如指数退避）和**熔断器**防止雪崩。
    - **熔断与降级**：使用Hystrix或Sentinel实现熔断。当依赖服务失败率达到阈值，熔断器打开，直接执行降级逻辑（如返回缓存数据、默认值），保护自身和下游资源。熔断器在半开状态尝试放行部分请求以探测恢复情况。
4. **数据持久化与备份**：
    - **数据冗余**：采用多副本策略（如MySQL主从、Redis AOF+RDB、HDFS多副本）。
    - **备份与恢复**：定期全量与增量备份，并演练恢复流程。
    - **最终一致性设计**：对于跨服务数据，采用最终一致性模型（如通过消息队列异步同步），避免分布式事务带来的复杂性和性能瓶颈。
5. **架构解耦与异步通信**：
    - **消息队列**：使用Kafka、RocketMQ解耦服务。生产者不直接依赖消费者，即使消费者暂时不可用，消息也会被持久化，待恢复后处理。
    - **事件驱动**：状态变更以事件形式发布，其他服务订阅并异步处理，提高系统整体韧性。
**源码角度：以Eureka服务发现为例**，其高可用体现在客户端缓存和服务端集群。`DiscoveryClient`在初始化时会从Eureka Server拉取全量注册表并缓存到本地（`client.fetchRegistry`），后续通过增量更新维持同步。即使所有Eureka Server短暂不可用，客户端仍能依靠本地缓存进行服务调用。服务端之间通过异步复制（`PeerEurekaNodes`）同步注册信息，形成高可用集群。
###### 2. 如何设计一个可扩展的分布式系统？
可扩展性指系统能够通过增加资源来平滑地提升其处理能力，主要分为**垂直扩展**（升级单机性能）和**水平扩展**（增加机器数量）。分布式系统追求水平扩展。
1. **无状态设计**：
    - **核心原则**：服务实例不保存与会话或请求相关的本地状态。任何状态（如用户Session）都应存储在外部的共享存储中（如Redis）。
    - **好处**：请求可以被路由到任意一个实例，使得通过简单地增加实例数量来实现扩容成为可能。这是实现水平扩展的前提。
2. **水平分片**：
    - **数据分片**：将数据分散到多个节点。例如，数据库分库分表（如按用户ID哈希），Redis Cluster将16384个槽分配到不同节点，Elasticsearch将索引分为多个分片。
    - **计算分片**：将不同的业务逻辑或流量分配到不同的服务集群。例如，将用户服务和商品服务部署为独立的、可独立扩展的微服务。
3. **松耦合与微服务架构**：
    - **按业务边界拆分服务**：每个微服务独立开发、部署、扩展。使用API网关统一入口，服务间通过轻量级协议（如HTTP/gRPC）通信。
    - **独立数据存储**：每个服务拥有自己的私有数据库，避免数据库成为耦合点。
4. **弹性伸缩**：
    - **自动伸缩**：基于监控指标（CPU、内存、QPS）自动增减实例。在Kubernetes中，使用`Horizontal Pod Autoscaler`根据CPU利用率等指标自动调整Pod副本数。
    - **云原生**：利用云平台的弹性，在负载高峰时自动创建新实例。
5. **缓存与读写分离**：
    - **多层缓存**：使用本地缓存（Caffeine）、分布式缓存（Redis）减轻数据库压力，直接提升读吞吐量。
    - **读写分离**：数据库主库负责写，多个从库负责读，扩展读能力。
6. **异步化与消息队列**：
    - **非核心流程异步化**：将耗时操作（如发送邮件、生成报表）通过消息队列异步处理，主流程快速返回，提升系统整体吞吐量。
    - **流量削峰**：面对突发流量，消息队列作为缓冲区，保护下游系统不被冲垮。
**设计模式**：**AKF扩展立方**是指导可扩展性设计的经典模型，包含三个维度：X轴（水平复制）、Y轴（功能拆分）、Z轴（数据分片）。一个成熟的系统往往是三个维度扩展的结合。
###### 3. 什么是服务拆分的原则？
服务拆分是微服务架构设计的核心，目标是建立高内聚、低耦合、自治的独立服务单元。主要原则包括：
1. **单一职责与高内聚**：一个服务应只负责一个明确的业务能力或领域边界内的功能。相关性强、变更原因相同的功能应放在同一个服务内。这借鉴了**领域驱动设计中的限界上下文**。例如，“订单”和“支付”虽然相关，但属于不同的业务上下文，应拆分为订单服务和支付服务。
2. **自治性**：
    - **独立部署与扩展**：每个服务应能独立部署、升级和水平扩展，不影响其他服务。
    - **独立数据管理**：服务拥有自己的私有数据库，数据库schema不直接暴露给其他服务。服务间通过API通信，而非共享数据库，这是解耦的关键。
    - **独立技术栈**：在团队和技术条件允许下，不同服务可采用最适合其业务场景的技术栈。
3. **松耦合**：
    - **异步通信优先**：服务间通过发布/订阅事件进行异步通信，能有效解耦。例如，订单创建后发布“订单已创建”事件，库存服务和积分服务订阅该事件并异步处理，订单服务无需等待它们完成。
    - **避免级联故障**：通过熔断、降级、超时控制等模式，防止一个服务的故障蔓延到整个系统。
4. **基于业务能力而非技术层级**：传统的分层架构（如Web层、Service层、DAO层）不适合作为拆分的依据。拆分应围绕业务领域（如用户、商品、风控）进行，而不是技术职能。
5. **渐进式拆分**：不要试图一开始就设计出完美的拆分方案。应从单体应用开始，随着业务复杂度和团队规模的增长，识别出耦合度高、变更频繁的模块，逐步拆分。
**拆分过细的代价**：服务数量激增会带来部署、测试、监控、分布式事务、网络延迟等方面的复杂度显著上升。需要在自治性和复杂度之间取得平衡。
###### 4. 如何避免分布式系统中的单点故障？
避免单点故障意味着系统中不存在任何“一旦失效，则整个系统或核心功能不可用”的组件。策略覆盖从基础设施到应用逻辑的各个层面。
1. **负载均衡与集群化（计算层）**：
    - **前端**：使用多个负载均衡器实例（如Nginx、F5），通过Keepalived+VIP实现主备，或直接使用云厂商的负载均衡服务（SLB/ELB）。
    - **应用服务**：部署至少两个以上实例，通过前述的负载均衡器分发流量。使用服务注册与发现（Eureka/Nacos）动态管理实例。
2. **数据存储层（有状态服务）**：
    - **主从复制与故障转移**：MySQL配置主从复制，通过MHA或Orchestrator等工具实现主库故障时的自动切换。Redis使用哨兵模式或Cluster模式。
    - **分布式共识与多副本**：ZooKeeper、etcd等协调服务自身就以集群模式运行，基于Raft/ZAB协议在多个节点间同步数据，容忍少数节点故障。
    - **多活架构**：在更高要求下，可设计同城双活或异地多活的数据中心。
3. **中间件与依赖服务**：
    - **消息队列**：Kafka、RocketMQ均采用多Broker集群，分区有多副本，Leader失效后Follower可选举为新Leader。
    - **配置中心/注册中心**：Nacos、Apollo服务端均以集群部署，共享同一个元数据库（MySQL集群），客户端具有本地缓存，即使服务端全挂，应用也能依靠缓存运行。
4. **设计模式**：
    - **重试与退避**：客户端对失败请求进行重试，并配合随机退避（如指数退避）避免加重下游负担。
    - **熔断与降级**：如前所述，防止依赖的单个服务故障导致自身雪崩。
    - **冗余链路与多AZ部署**：关键服务跨多个可用区（Availability Zone）部署，即使一个机房故障，流量可切至其他机房。
**源码示例：Redis Sentinel故障转移**
当Sentinel集群认定主节点客观下线后，会通过Raft算法选举出一个Leader Sentinel来执行故障转移。`sentinelFailoverStateMachine`函数驱动状态机，其核心步骤包括：在从节点中选取一个最优的作为新主节点（`sentinelSelectSlave`），向该从节点发送 `SLAVEOF NO ONE`命令提升为主，并通知其他从节点复制新主节点。
###### 5. 什么是异地多活架构？
异地多活是指在相隔较远的多个地理区域（城市）部署独立的数据中心和完整的应用服务，每个区域都能同时处理用户请求，并提供服务。它是比“主备”、“冷备”、“双活”更高级的高可用和容灾架构。
**核心特征**：
- **多活**：多个数据中心都处于“活跃”状态，同时对外提供服务，而不仅仅是备用。
- **地理位置分散**：数据中心部署在不同城市，以抵御地域性重大灾难。
- **数据同步**：用户数据在多个数据中心之间双向或单向同步，保证数据最终一致性。
- **流量路由**：通过全局负载均衡（如DNS解析、HTTP重定向、Anycast）将用户流量导向最近或最合适的机房。
**技术挑战与解决方案**：
1. **数据一致性**：这是最大挑战。强一致性（如分布式事务）在跨地域延迟下性能极差，因此通常采用**最终一致性**。
    - **同步双写**：写入时同步写多个中心，延迟高，任一中心故障则写入失败。
    - **异步复制**：主流方案。通过消息队列或数据库日志捕获变更，异步复制到其他中心。存在短暂的数据延迟。
    - **冲突解决**：异步复制下，同一数据可能在两个中心被并发修改，产生冲突。常用解决方案有：**“最后写入获胜”**（基于时间戳）、**业务规则合并**（如购物车商品合并）或使用**CRDT**数据结构。
2. **流量调度与路由**：
    - **基于DNS**：根据用户IP解析到最近机房的VIP。切换慢（受TTL限制）。
    - **基于HTTP 302重定向**：更灵活，可结合实时容量和健康状态调度。
    - **客户端SDK**：客户端内置逻辑，根据延迟、负载选择最优机房入口。
3. **业务设计改造**：
    - **用户分区**：最常用的方案。将用户按ID哈希或其他规则固定路由到某个主场机房。该用户的所有读写操作都发生在主场，然后数据异步复制到其他机房。其他机房可读备份数据，但写操作需路由回主场。这大大降低了数据冲突的可能性。
    - **全局唯一ID**：必须使用能包含数据中心信息的分布式ID生成算法（如Snowflake算法中嵌入数据中心ID位），避免多中心ID冲突。
**与同城双活的区别**：同城双活数据中心距离近（通常<100km），网络延迟极低（<2ms），可采用共享存储或高速网络同步数据，更容易实现强一致性或近似强一致性。异地多活则面临更高的延迟（几十到几百毫秒），必须接受最终一致性。
###### 6. 如何设计一个秒杀系统？
秒杀系统的核心矛盾是**瞬间超高并发访问与有限的商品库存**。设计目标是：**保障系统在高并发下的稳定可用、防止超卖、保证公平性（或防黄牛）**。
1. **架构原则：分层过滤、逐层限流**
    - **前端优化**：
        - **静态化**：活动页面、商品详情页完全静态化（HTML+CDN），极大减轻后端压力。
        - **按钮控制**：通过JS在倒计时结束后才点亮按钮，并点击后立即置灰，防止用户重复提交。
        - **验证码/答题**：在提交订单前引入验证码或简单问题，分散请求峰值，增加机器抢购难度。
2. **网关层拦截**：
    - **限流**：在API网关层进行严格的限流，对非关键请求（如商品详情查询）和恶意请求进行拦截，只放行极少量秒杀请求到后端。
    - **防刷**：基于用户ID/IP进行频次控制。
3. **服务层设计**：
    - **独立部署**：将秒杀系统独立出来，避免影响主站正常业务。使用独立的域名、服务器集群。
    - **业务简化**：秒杀流程极简，通常只有“校验-扣库存-创建订单”几步，支付可异步进行。
    - **缓存预热**：将秒杀商品库存提前加载到Redis中，所有库存查询和扣减操作都在内存中进行。
4. **库存扣减（防超卖核心）**：
    - **Redis原子操作**：将库存数量存储在Redis中，使用 `DECR`或 `INCRBY`命令进行扣减。这些操作是原子的，可以保证不超卖。扣减前需要判断库存是否大于0。
    - **Lua脚本**：为了将“判断库存”和“扣减库存”组合成一个原子操作，通常使用Redis Lua脚本执行。
    ```lua
    -- 秒杀扣库存Lua脚本示例
    local stock_key = KEYS[1]
    local user_key = KEYS[2]
    local stock = tonumber(redis.call(‘GET’, stock_key))
    if stock <= 0 then
        return 0 -- 库存不足
    end
    if redis.call(‘SISMEMBER’, user_key, ARGV[1]) == 1 then
        return 2 -- 已经参与过
    end
    redis.call(‘DECR’, stock_key)
    redis.call(‘SADD’, user_key, ARGV[1])
    return 1 -- 成功
    ```
    - **异步扣减数据库**：Redis中成功扣减库存后，发送消息到MQ（如RocketMQ）。订单服务消费消息，进行创建订单、扣减数据库库存等后续操作。即使数据库操作慢，也不会影响秒杀接口的响应。
5. **队列泄洪**：
    - 即使经过层层限流，到达核心秒杀服务的请求仍可能超过其处理能力。此时可以将通过资格校验的请求放入一个内存队列（如Disruptor）或分布式MQ中，后端服务以固定的、可控的速度从队列中取出请求处理。这起到了平滑流量的作用。
6. **降级与熔断**：
    - 明确核心路径（校验库存、扣减Redis库存），非核心功能（如风控检查、物流计算）可暂时降级。
    - 对依赖的下游服务（如用户服务、风控服务）做好熔断。
**总结**：秒杀设计本质是将**同步的、瞬时的、高并发的购买请求，转化为异步的、串行的、可控的队列处理过程**。通过“Redis预扣库存 + 消息队列异步落库”是业内的标准解法。
###### 7. 如何设计一个分布式爬虫系统？
一个健壮的分布式爬虫系统需要解决URL去重、调度、并发控制、反爬应对和数据存储等问题。
1. **系统架构**：
    - **URL管理器**：负责管理待抓取URL队列和已抓取URL集合。这是核心，需要分布式。
    - **调度器**：从URL管理器中获取URL，并分配给下游的爬虫节点。
    - **爬虫节点**：执行实际的网页下载、解析和数据处理工作。可以水平扩展。
    - **数据存储**：存储原始网页、解析后的结构化数据。
    - **监控与管理台**：监控爬虫状态、速率、异常，并管理任务。
2. **核心问题与解决方案**：
    - **URL去重**：
        - **布隆过滤器**：内存占用极小，判断“可能存在”或“一定不存在”，适合海量URL判重。但存在误判率，且不支持删除。可使用Redis的 `SETBIT`和 `GETBIT`命令实现分布式布隆过滤器。
        - **分布式缓存/数据库**：将URL的MD5或SHA-1哈希值存入Redis的Set或专用去重库（如RocksDB）。准确性高，但存储成本也高。
    - **分布式URL调度**：
        - **主从模式**：一个Master节点负责URL调度和去重，多个Worker节点执行抓取。Master可能成为瓶颈。
        - **对等模式**：每个爬虫节点既是调度器也是工作者。可以使用消息队列（如Kafka）作为URL队列，所有节点共同消费。URL去重则需要一个共享的存储（如Redis）来协同。
    - **并发控制与礼貌爬取**：
        - **延时策略**：在每个爬虫节点或调度器中，针对不同域名设置爬取间隔（如 `robots.txt`中的 `Crawl-delay`），遵守网站规则。
        - **连接池管理**：限制对单个目标站点的并发连接数。
    - **反爬虫策略应对**：
        - **User-Agent轮换**：使用池化的常见浏览器UA。
        - **IP代理池**：使用大量代理IP轮换请求，防止IP被封。
        - **请求频率控制**：动态调整对特定站点的请求频率。
        - **解析JavaScript**：对于SPA（单页应用），需使用无头浏览器（如Puppeteer、Selenium）来执行JS渲染页面。
    - **数据存储与扩展性**：
        - 原始网页可存储到分布式文件系统（如HDFS）或对象存储（如S3）。
        - 结构化数据可存入Elasticsearch（用于搜索）或HBase/数据库。
**设计示例（基于消息队列）**：
3. URL种子被放入初始URL队列（Kafka Topic）。
4. 多个爬虫节点（消费者）从队列拉取URL。
5. 爬虫节点抓取页面前，先向“去重服务”（一个Redis集群）查询该URL是否已处理。若未处理，则执行抓取。
6. 抓取成功后，将新解析出的URLs写入URL队列，将原始内容和解析结果分别写入“原始存储”和“数据存储”。
7. 同时，将已处理的URL标记写入“去重服务”。
    此架构无中心Master，易于水平扩展。
###### 8. 如何设计一个分布式任务调度系统？
分布式任务调度系统需要解决**任务的精准触发、负载均衡、高可用、故障转移和可视化管控**。
1. **核心架构模式**：
    - **中心式调度**：一个中心调度器（可能主备）负责所有任务的触发和派发。优点是逻辑简单、状态集中；缺点是中心节点可能成为性能和单点瓶颈。例如早期的Quartz集群。
    - **去中心化调度**：没有中心调度器，每个执行器节点都有调度能力，通过选举或分布式协调服务（如ZooKeeper）来分配任务。扩展性更好。例如Elastic-Job-Lite和PowerJob。
2. **关键组件设计**：
    - **调度器**：负责任务时间的解析和触发。在中心式架构中独立部署；在去中心化架构中嵌入在每个执行器节点。
    - **执行器**：负责接收调度指令并执行具体的任务逻辑（如一个Java类的 `execute`方法）。通常以Agent形式部署在业务机器上。
    - **注册中心**：用于执行器的自动注册与发现，以及调度器集群的Leader选举。常用ZooKeeper、etcd或Nacos。
    - **任务存储**：存储任务元数据（如CRON表达式、处理器、参数）、执行日志和状态。使用关系型数据库（MySQL）或分布式存储。
3. **核心技术挑战与解决方案**：
    - **任务触发不重复不遗漏**：
        - **数据库行锁**（中心式）：Quartz集群采用此方式。触发前，调度器节点尝试锁定数据库中的任务记录，抢到锁的节点获得触发权。缺点是数据库压力大。
        - **分布式锁**（中心式/去中心化）：调度器通过竞争ZooKeeper临时节点或Redis锁来决定谁触发。
        - **分片广播**（去中心化）：这是更优雅的方案。任务被触发时，所有在线的执行器同时被通知，每个执行器根据自身的分片序号（由注册中心协调分配）去处理属于自己的那部分数据。例如，处理10000条数据，10个执行器，每个处理1000条。
    - **负载均衡**：调度器根据执行器的负载（CPU、内存）或轮询策略，将任务动态分配给负载较轻的执行器。
    - **故障转移与弹性**：
        - **心跳检测**：执行器定期向注册中心发送心跳。调度器监控心跳，将失联执行器上的任务重新调度到其他健康节点。
        - **任务失败重试**：任务执行失败后，系统应能根据策略（如立即重试、间隔重试）自动重试。
    - **可视化与管理**：提供Web控制台，用于任务CRUD、手动触发、暂停/恢复、查看执行日志和监控报警。
**源码示例：XXL-Job的调度中心逻辑**
XXL-Job采用“中心调度”模式。调度线程 `JobScheduleHelper`周期性扫描数据库中的任务表。当任务触发时间到达，调度器并不会直接执行，而是将一条“触发日志”插入数据库，并通过RPC调用将任务信息推送到指定的执行器。执行器接收到请求后，在自己的线程池中执行具体任务，并将结果回调给调度器。这种“调度与执行分离”的架构，使得调度中心更轻量，专注于触发和分发。其故障转移通过数据库锁和超时机制实现：如果某个调度器在触发任务后崩溃，其他调度器会在扫描中发现该次触发未完成（超时），则会自动接过并重新触发。