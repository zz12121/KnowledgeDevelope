```
技术自由圈^
```
# 牛逼的职业发展之路

40 岁老架构尼恩用一张图揭秘: Java 工程师的高端职业发展路径，走向食物链顶端的之路

链接：https://www.processon.com/view/link/618a2b62e0b34d73f7eb3cd


```
技术自由圈
```
# 史上最全：价值 10 W 的架构师知识图谱

此图梳理于尼恩的多个 3 高生产项目：多个亿级人民币的大型 SAAS 平台和智慧城市项目

链接：https://www.processon.com/view/link/60fb9421637689719d


```
技术自由圈
```
# 牛逼的架构师哲学

40 岁老架构师尼恩对自己的 20 年的开发、架构经验总结

链接：https://www.processon.com/view/link/616f801963768961e9d9aec


```
技术自由圈^
```
# 牛逼的 3 高架构知识宇宙

尼恩 3 高架构知识宇宙，帮助大家穿透 3 高架构，走向技术自由，远离中年危机

链接：https://www.processon.com/view/link/635097d2e0b34d40be778ab


```
技术自由圈
```
# 尼恩 Java 高并发三部曲（卷 1 加强版）

老版本：《Java 高并发核心编程卷 1 ：NIO、Netty、Redis、ZooKeeper》（已经过时，不建
议购买）

新版本：《Java 高并发核心编程卷 1 **加强版** ：NIO、Netty、Redis、ZooKeeper》

 由浅入深地剖析了高并发 IO 的底层原理。

 图文并茂的介绍了 TCP、HTTP、WebSocket 协议的核心原理。

 细致深入地揭秘了 Reactor 高性能模式。

 全面介绍了 Netty 框架，并完成单体 IM、分布式 IM 的实战设计。

 详尽地介绍了 ZooKeeper、Redis 的使用，以帮助提升高并发、可扩展能力

详情：https://www.cnblogs.com/crazymakercircle/p/16868827.html


```
技术自由圈
```
# 尼恩 Java 高并发三部曲（卷 2 加强版）

老版本：《Java 高并发核心编程卷 2 ：多线程、锁、JMM、JUC、高并发设计模式》
（已经过时，不建议购买）

新版本：《Java 高并发核心编程卷 2 **加强版** ：多线程、锁、JMM、JUC、高并发设计模式》

 由浅入深地剖析了 Java 多线程、线程池的底层原理。

 总结了 IO 密集型、CPU 密集型线程池的线程数预估算法。

 图文并茂的介绍了 Java 内置锁、JUC 显式锁的核心原理。

 细致深入地揭秘了 JMM 内存模型。

 全面介绍了 JUC 框架的设计模式与核心原理，并完成其高核心组件的实战介绍。

 详尽地介绍了高并发设计模式的使用，以帮助提升高并发、可扩展能力

详情参阅：https://www.cnblogs.com/crazymakercircle/p/16868827.html


```
技术自由圈
```
# 尼恩 Java 高并发三部曲（卷 3 加强版）

老版本：《SpringCloud Nginx 高并发核心编程》（已经过时，不建议购买）

新版本：《Java 高并发核心编程卷 3 **加强版** ：亿级用户 Web 应用架构与实战》

 在当今的面试场景中， 3 高知识是大家面试必备的核心知识，本书基于亿级用户 3 高 Web 应用

```
的架构分析理论，为大家对 3 高架构系统做一个系统化和清晰化的介绍。
```
 从 Java 静态代理、动态代理模式入手，抽丝剥茧地解读了 Spring Cloud 全家桶中 RPC 核心原

```
理和执行过程，这是高级Java工程师面试必备的基础知识。
```
 从 Reactor 反应器模式入手，抽丝剥茧地解读了 Nginx 核心思想和各配置项的底层知识和原理，

```
这是高级Java工程师、架构师面试必备的基础知识。
```
 从观察者模式入手，抽丝剥茧地解读了 RxJava、Hystrix 的核心思想和使用方法，这也是高级

```
Java工程师、架构师面试必备的基础知识。
```
详情：https://www.cnblogs.com/crazymakercircle/p/16868827.html


```
技术自由圈
```
# 尼恩 Java 面试宝典

40 个专题（卷王专供+ 史上最全 + 2023 面试必备）
详情：https://www.cnblogs.com/crazymakercircle/p/13917138.html


## SpringCloud Alibaba 学习圣经， 10 万字实

## 现微服务自由

#### 本书封面


#### 40 岁老架构师尼恩的掏心窝：

现在 **拿到 offer 超级难** ，甚至连面试电话，一个都搞不到。

尼恩的技术社群中（50+），很多小伙伴凭借 “左手云原生+右手大数据 +SpringCloud Alibaba 微服务
“三大绝活，拿到了 offer，并且是非常优质的 offer，据说 **年终奖都足足 18 个月** ，非常令人羡慕。

问题是：“左手云原生+右手大数据 +SpringCloud Alibaba 微服务“ **内容非常多，实操的环境非常复杂，
底层原理很深** 。

**米饭要一口一口的吃，不能急** 。在这里，尼恩从架构师视角出发，左手云原生+右手大数据
+SpringCloud Alibaba 微服务核心原理做一个宏观的介绍。

```
由于内容确实太多， 所以写多个pdf 电子书：
```
```
(1) 《 Docker 学习圣经 》PDF （V1已经完成）
```
```
(2) 《 SpringCloud Alibaba 微服务 学习圣经 》 PDF （V2已经完成）
```
```
(3) 《 K8S 学习圣经 》PDF （V1已经完成）
```
```
(4) 《 flink + hbase 学习圣经 》PDF （planning ......）
```
以上学习圣经，并且后续会持续升级，从 V 1 版本一直迭代发布。就像咱们的《尼恩 Java 面试宝典》一
样，已经迭代到 V 62 啦。

40 岁老架构师尼恩的掏心窝： 通过一系列的学习圣经，带大家穿透“左手云原生+右手大数据
+SpringCloud Alibaba 微服务“ ，实现技术自由，走向颠覆人生，让大家不迷路。

以上学习圣经的基础知识是尼恩的《高并发三部曲》，建议在看学习圣经之前，一定把尼恩的 Java 高
并发三部曲过一遍，切记，切记。

#### 版本升级说明

在 40 岁老架构师尼恩的读者社群（50+）中，大量的小伙伴是架构师、高级开发，大家都有丰富的开
发、架构经验。

在开发过程中，一般情况下，大家都是用现有的开发框架。

导致的一个严重问题是：很少有小伙伴能从 0 开始搭建一套 SpringCloud 微服务脚手架（SpringCloud+
Dubbo + Docker + Jenkins）。然而，零基础搭建一套 SpringCloud 微服务脚手架（SpringCloud+
Dubbo + Docker + Jenkins）， **这个实操对大家来说，至关重要** 。

尼恩一直在找一个契机，给大家梳理一个《零基础搭建一套 SpringCloud 微服务脚手架》博客。提升大
家的实操能力，动手能力。

直到今天、契机终于来了。

从 2020 年开始，尼恩一直在写一本微服务架构和开发领域的 **至尊宝典《SpringCloud 学习圣经》** ，全
量的博客加起来全网阅读量在 100 W+。在这么大访问量的激励下，尼恩一直在对这本《SpringCloud
学习圣经》进行迭代。

而且由于尼恩一人有精力有限，在尼恩的《技术自由圈》高并发研究社群中，不断吸取有志之士的加
入。咱们社群中一位资深的、华中科技大学硕士、有着 10 年开发和架构经验、并且管理一个 20 人团队的
架构师 Andy 加入了咱们技术迭代、技术研究的队伍。


他来给大家提供了一个优秀的微服务基础架构实操案例，《零基础搭建一套 SpringCloud 微服务脚手
架》。

这，就是本文。当然，本文也收入了咱们的 **10 W 字至尊宝典《SpringCloud alibaba 学习圣经》** 最新
升级版，《SpringCloud alibaba 学习圣经》宝典从此升级到了 V 3 版本。

最新的 PDF 文档，可以通过 **公众号技术自由圈** 领取。

#### 本书目录

**SpringCloud Alibaba 学习圣经， 10 万字实现微服务自由**
本书封面
40 岁老架构师尼恩的掏心窝：
版本升级说明
本书目录
**零基础搭建一套 SpringCloud 微服务脚手架（SpringCloud+ Dubbo + Docker + Jenkins）**
1 、本文学习内容和目标
2 、设计微服务框架的基本原则
3 、搭建基础设施
1. 安装 Java 开发环境
2. 安装和配置 Maven
3. 安装和配置 Git
4. 安装集成开发环境（IDE）
5. 创建项目和配置构建工具
4 、定义微服务接口和协议
6. 引入 JPA 依赖
7. 示例表结构
8. application. yml 配置
9. 定义学生实体类
10. 定义学生信息的持久化接口
11. 定义学生信息的控制器接口
5 、实现服务注册与发现
12. 安装和配置 Nacos
13. 引入 Nacos 相关依赖
14. 系统配置变更
15. 示例代码
6 、实现服务调用和负载均衡
7 、实现服务监控和日志管理
16. Spring Boot Admin 服务监控
17. ELK 日志监控
ELK 安装
微服务应用接入
8 、保护服务安全
9 、实现持续集成和部署
18. Jenkins
19. Docker
20. 配置步骤
10 、小结
**一键导入 SpringCloud 开发环境 （地表最强）**
环境准备
一键导入 OR 自己折腾
随书源码 crazy-springcloud 脚手架涉以及基础中间件


**微服务分布式系统的架构 12 次演进**
微服务分布式系统的几个基础概念
架构演进 1 ：单机架构
架构演进 2 ：引入缓存架构
架构演进 3 ：接入层引入反向代理实现负载均衡
架构演进 4 ：数据库读写分离
架构演进 5 ：数据库按业务分库
架构演进 6 ：使用 LVS 或 F 5 接入层负载均衡
架构演进 7 ：通过 DNS 轮询实现机房间的负载均衡
架构演进 8 ：引入 NoSQL 数据库和搜索引擎等技术
架构演进 9 ：大应用拆分为微服务
架构演进 10 ：引入企业服务总线 ESB 对微服务进行编排
架构演进 11 ：引入容器化技术实现动态扩容和缩容
架构演进 12 ：以云平台承载系统
架构演进的涉及的核心知识
**SpringCloud netflix 入门**
SpringCloud 开发脚手架
启动 Eureka Server 注册中心
启动 Config 配置中心
config-server 服务
微服务入门案例
uaa-provider 微服务提供者
uaa-provider 实现一个 Rest 接口
uaa-provider 的运行结果
demo-provider 完成 RPC 远程调用
REST 服务的本地代理接口
通过 REST 服务的本地代理接口，进行 RPC 调用
启动 demo-provider
通过 swagger 执行 RPC 操作
**SpringCloud Eureka 服务注册
SpringCloud Config 统一配置
Nacos 服务注册+ 统一配置**
1 、Nacos 优势
1.1 与 eureka 对比
1.2 与 springcloud config 对比
三大优势：
2 、Spring Cloud Alibaba 套件
Spring Cloud Alibaba 套件和 Spring Cloud Netflix 套件类比
3 、Nacos 的架构和安装
3.1 Nacos 的架构
3.2 Nacos Server 的下载和安装
4 、Nacos Server 的运行
4.1 两种模式
4.2 standalone 模式
4.3 cluster 模式
cluster 模式需要依赖 MySQL，然后改两个配置文件：
4.4 Nacos Server 的配置数据是存在哪里呢？
5 、实战 1 ：使用 Nacos 作为注册中心
实战的工程
5.1 如何使用 Nacos Client 组件
首先引入 Spring Cloud Alibaba 的 BOM
5.2 演示的模块结构
5.3 provider 微服务
step 1：在 provider 和 consumer 的 pom 添加以下依赖：
step 2：启动类
step 3：服务提供者的 Rest 服务接口
step 4：配置文件
step 5：启动之后，通过 swagger UI 访问：


5.4 Consumer 微服务演示 RPC 远程调用
消费者的 controller 类
消费者配置文件
通过 swagger UI 访问消费者：
5.5 涉及到的演示地址：
5.6 Nacos Console
6 、实战 2 ：使用 Nacos 作为配置中心
6.1 基本概念
1 ）Profile
2 ）Data ID
3 ）Group
6.2 通过 Nacos 的 console 去增加配置
1 ）nacos-config-demo-dev. yaml
2 ）nacos-config-demo-sit. yaml
6.3 使用 Nacos Config Client 组件
1 ）加载 nacos config 的客户端依赖：
启动类
控制类：
2 ）bootstrap 配置文件
6.4 测试结果
6.4 可以端如何与服务端的配置文件相互对应
7 、配置的隔离
8 、nacos 集群搭建
IP 规划
集群的使用
**Nacos 高可用架构与实操**
客户端高可用
客户端高可用的方式一：配置多个 nacos-server
Nacos Java Client 通用参数
客户端高可用的方式二：本地缓存文件 Failover 机制
本地缓存文件 Failover 机制
客户端 Naming 通用参数
Nacos 两种健康检查模式
agent 上报模式
服务端主动检测
临时实例
注册实例支持 ephemeral 字段
临时实例和持久化实例区别
Nacos Server 运行模式
Nacos CP/AP 模式设定
Nacos CP/AP 模式切换
AP/CP 的配套一致性协议
AP 模式下的 distro 协议
CP 模式下的 raft 协议
集群内部的特殊的心跳同步服务
集群部署模式高可用
节点数量
多可用区部署
部署模式
高可用 nacos 的部署架构
高可用 nacos 的部署实操
总结
**SpringCloud Feign 实现 RPC 远程调用
SpringCloud + Dubbo 实现 RPC 远程调用**
大背景：全链路异步化的大趋势来了
SpringCloud + Dubbo 完成 RPC 异步
Dubbo 3 应用的宏观架构
Dubbo 3 应用架构的核心组件


SpringBoot 整合 Dubbo 3.0 基础准备
SpringCloud+Nacos+Dubbo 3.
版本说明
项目结构介绍
1 、dubbo 的依赖的坐标
2 、注册中心的依赖的坐标
SpringBoot 整合 Dubbo 3.0 大致步骤
模块结构
Dubbo 微服务注册发现的相关配置
命名空间隔离
微服务 yml 配置
common-service 模块
服务提供者实操：dubbo-provider 服务
pom 依赖
服务实现类
dubbo 和 Feign 的一个不同
Provider 的 Dubbo+Nacos 配置文件
启动类加上@EnableDubbo 注解
启动、体验 Provider
在 Nacos 查看 Dubbo 服务的注册情况
服务消费者实操：dubbo-consumer 服务
consumer 模块
消费者实现类
消费者 Dubbo+Nacos 配置文件
启动类加上@EnableDubbo 注解
启动、体验 Consumer
在 Nacos 查看 Dubbo 服务的注册情况
Feign+Dubbo 性能的对比测试
Dubbo 比 Feign 高 10 倍以上的本质
Dubbo 与 SpringCloud 的通信 Openfeign 的区别
1 、协议支持方面
2 、通信性能方面
3 、线程模型方面
SpringCloud + Dubbo RPC 的集成价值
**hystrix 服务保护
Sentinel 服务保护**
sentinel 基本概念
服务雪崩效应
1 、什么是 Sentinel:
**Sentinel 具有以下特征:**
Sentinel 主要特性：
Sentinel 的使用
**Sentinel 中的管理控制台**
1 ）获取 Sentinel 控制台
2 ）sentinel 服务启动
**客户端能接入控制台**
Sentinel 与 Hystrix 的区别
2 、使用 Sentinel 来进行熔断与限流
2.1 定义资源
资源注解@SentinelResource
@SentinelResource 注解
fallback 函数签名和位置要求：
defaultFallback 函数签名要求：
**2.2 定义规则**
3 、sentinel 熔断降级
3.1 什么是熔断降级
3.2 熔断降级规则
3.3 几种降级策略


3.4 熔断降级代码实现
3.5 控制台降级规则
3.6 与 Hystrix 的熔断对比：
4 、Sentinel 流控（限流）
基本的参数
流控的几种 strategy：
4.1 直接失败模式
使用 API 进行资源定义
代码限流规则
网页限流规则配置
测试
4.2 关联模式
使用注解进行资源定义
代码配置关联限流规则
网页限流规则配置
测试
4.3 Warm up（预热）模式
使用注解定义资源
代码限流规则
网页限流规则配置
通过 jmeter 进行测试
4.4 排队等待模式
示例
使用注解定义资源
代码限流规则
网页限流规则配置
通过 jmeter 进行测试
4.5 热点规则 (ParamFlowRule)
自定义资源
限流规则代码：
网页限流规则配置
5 、Sentinel 系统保护
系统保护的目的
系统保护规则的应用
网页限流规则配置
6 、黑白名单规则
访问控制规则 (AuthorityRule)
7 、如何定义资源
方式一：主流框架的默认适配
方式二：抛出异常的方式定义资源
方式三：返回布尔值方式定义资源
方式四：注解方式定义资源
方式五：异步调用支持
8 、核心组件
Resource
Context
Context 的创建与销毁
Entry
DefaultNode
StatisticNode
9 、插槽 Slot
NodeSelectorSlot
调用链树
构造树干
创建 context
创建 Entry
退出 Entry
构造叶子节点


保存子节点
ClusterBuilderSlot
StatistcSlot
SystemSlot
AuthoritySlot
FlowSlot
DegradeSlot
DefaultProcessorSlotChain
slot 总结
10 、sentinel 滑动窗口实现原理
10.1 基本原理
10.2 sentinel 使用滑动窗口都统计啥
10.3 滑动窗口源码实现
10.3.1 MetricBucket
10.3.2 WindowWrap
10.3.3 LeapArray
**Zuul 微服务网关
Webflux 响应式编程**
WebFlux 学习前言
WebFlux 增删改查完整实战 demo
Dao 层 （又称 repository 层）
entity（又称 PO 对象）
Dao 实现类
Service 服务层
Controller 控制层
Mono
Flux
使用配置模式进行 WebFlux 接口开发
处理器类 Handler
路由配置
WebFlux 集成 Swagger
maven 依赖
swagger 配置
WebFlux 测试
配置模式的 WebFlux Rest 接口测试
注解模式的 WebFlux Rest 接口测试
swagger 增加界面
配置大全
静态资源配置
WebFluxSecurity 配置
WebSession 配置
文件上传配置
WebFlux 执行流程
WebFlux 学习提示
**Spring Cloud Gateway 微服务网关**
1 、SpringCloud Gateway 简介
1.1 本文姊妹篇《Flux 和 Mono 、reactor 实战 （史上最全）》
1.2 SpringCloud Gateway 特征
1.3 SpringCloud Gateway 和架构
1 ）SpringCloud Zuul 的 IO 模型
2 ）Webflux 服务器
3 ）Spring Cloud Gateway 的处理流程
2 、路由配置方式
2.1 基础 URI 路由配置方式
2.2 基于代码的路由配置方式
2.3 和注册中心相结合的路由配置方式
3 、路由匹配规则
说明：


3.1 Predicate 断言条件 (转发规则) 介绍
1 ）通过请求参数匹配
2 ）通过 Header 属性匹配
3 ）通过 Cookie 匹配
4 ）通过 Host 匹配
5 ）通过请求方式匹配
6 ）通过请求路径匹配
7 ）通过请求 ip 地址进行匹配
8 ）组合使用
3.2 过滤器规则（Filter）
过滤器规则（Filter）
PrefixPath
RedirectTo
RemoveRequestHeader
RemoveResponseHeader
RemoveRequestParameter
RewritePath
SetPath
SetRequestHeader
SetStatus
StripPrefix
RequestSize
Default-filters
3.3 通过代码进行配置
3.2 实现熔断降级
4 、高级配置
4.1 分布式限流
4.2 健康检查配置
maven 依赖
配置文件
4.3 统一配置跨域请求：
5 、整合 Nacos
maven 依赖
服务发现配置：从 Nacos 获取微服务提供者清单
nacos 实现动态配置
服务发现路由 predicates 和 filters 的自定义定义
为注册中心路由配置断言和过滤器
6 、整合 Swagger 聚合微服务系统 API 文档
maven 依赖
配置文件
效果：
7 、Gatway 网关的过滤器开发
7.1 过滤器的执行次序
7.2 定义全局过滤器
7.3 定义局部过滤器
8 、整合 Sentinel 完成流控和降级
maven 依赖
配置文件
限流规则通用配置
限流规则设置
网关限流参数
**SpringBoot Admin 进行微服务实例的监控**
使用 SpringBoot Admin 进行日志的记录
1 、SpringBoot Admin 简介
2 、使用 SpringBoot Admin 监控服务
2.1 导入依赖
2.2 配置 yml
2.3 集成 spring security


2.4 启动器类
2.5、测试
3 、actuator 启用和暴露端点
3.1 启用端点
3.2 暴露端点
4 、微服务 Provider 改造
4.1 导入依赖
4.2 配置 yml
使用 context-path
加上 spring security 密码
5 、admin 实现在线日志查看
5.1、添加 jar 包
5.2 在 application. yml 平级文件夹中添加 logback-spring. xml 配置文件
5.3 log. path 如何使用环境变量呢？
5.4 actuator 的配置
测试结果
1. 不暴露端点测试
2. 正常情况
6 、admin 与 Nacos（或 Eureka）结合的好处
**ELK 日志平台（elasticsearch +logstash+kibana）原理和实操**
ELK 的关系
ELK 优点
简单的 ELK 日志平台
ELK 改进之引入 Filebeat
ELK 的应用场景
ELK 的不足
es 的资源占用
Elasticsearch 概述
logstash 概述
logstash 作用：
logstash 的架构：
Input (输入）：
Filter (过滤器）
Output (输出）：
Logstash 的角色与不足
filebeat 介绍
filebeat 和 beats 的关系
Filebeat 是如何工作的
Filebeat 下载页面
Filebeat 文件夹结构
Filebeat 启动命令
配置 inputs
Log input
配置项
管理多行消息
配置 Logstash output
一键安装 es+logstash+ kibana
对应的镜像版本
docker 编码文件
访问 kibana
读取 filebeat-输出到 es 集群
在 kibana 显示的效果
使用 filebeat 发送日志
制作 filebeat 镜像
制作基础的 unbantu 镜像
推送镜像到 dockerhub
制作 filebeat 镜像
dockerfile


推送镜像到 dockerhub
example-application 微服务的 filebeat 配置：
filebeat. yml 的参考配置：
input. yml 配置：
修改 dockerfile
一键发布
启动之后
message-dispatcher 微服务的日志
查看日志索引
logstash 详解
stash 第一个事件
Logstash 的核心流程的三个环节
logstash 数值类型
logstash 条件判断
logstash 比较运算符
数据输入环节
stdin
file
syslogs
beats
kafka
数据处理环节
grok 解析文本并构造
date 日期解析
mutate 字段转换
covert 类型转换
split
merge
rename
remove_field：移除字段
join
geoip
ruby
urldecode
kv
useragent
数据输出
stdout
file
kafka
elasticseach
Kibana 查看应用日志
1 查看应用日志
2 如何搜索日志
3 如何查看指定时间的应用日志
4 如何定位错误日志
5 如何展开显示日志
es 的安全认证
配置 elk 的 ElastAlert 预警插件
**Prometheus+Grafana 检测预警**
什么是性能可观测
系统监控的核心指标
系统性能指标
资源性能指标
什么是 prometheus
prometheus 的运行原理
prometheus 主要特点
什么是 Grafana


Prometheus 的体系结构
Prometheus+Grafana 分层架构
Promcthcus 体系涉及的组件
如何收集度量值
指标类型
计数器
仪表盘
直方图
Summary
指标摘要及聚合
指标摘要
指标聚合
一键安装 prometheus
bridge 网络管理
创建库
docker 编排文件
一键安装 prometheus 的脚本
进入 prometheus
进入 grafana
Prometheus+Grafana 监控 SpringBoot 项目 JVM 信息
SpringBoot 项目配置 JVM 采集
Prometheus 配置
配置 grafana 监控 Linux 系统
使用 Exporter 收集指标
inux 直接安装 node_exporter
使用 Docker 容器安装 node_exporter
创建一个任务定时扫描暴露的指标信息
创建仪表盘 grafna
导入 Dashboard
选择数据源为 Prometheus
配置 grafana 监控 SpringBoot 应用
主要步骤
找 jvm 的 dashboard
JVM Quarkus 面板
Prometheus 数据模型
time-series 时间序列值
Sample 样本值
metrics name 指标名称
label 标签
Notation (符号)
TSDB 时序数据库
度量指标类型
Counter (计数器) 类型
Gauge (计量器、测量器)
Histogram (柱状图、直方图)
Summary
Summary 和 Histogram 的区分
学习 PromQL
数据模型
PromQL 入门
HTTP API
告警和通知
配置告警规则
使用 Alertmanager 发送告警通知
服务发现
为什么需要服务发现
prometheus 目前支持的服务发现类型
基于文件的服务发现方式


file_sd_configs
基于 consul 的服务发现
什么是基于 consul 的服务发现
Prometheus 配置
基于 eureka 的服务发现
eureka 客户端暴露出 prometheus 端口
prometheus 配置文件
基于 nacos 的服务发现
docker 编排文件
生产的配置文件
修改 prometheus 配置文件
修改 springboot 项目配置文件
**全方位 Springcloud 性能调优**
Servlet 容器优化
Zuul 配置优化
Feign 配置优化
hystrix 配置优化
ribbon 优化
**高质量实操：SpringCloud 高并发实战案例**
1 、超高并发 10 Wqps 秒杀实操
2 、超高并发 100 Wqps 车联网实操
3 、N 多其他的超高并发实操项目
**技术自由的实现路径：**
尼恩 N 篇硬核架构文章，帮你实现架构自由：
实现你的响应式自由：
实现你的 spring cloud 自由：
实现你的 linux 自由：
实现你的网络自由：
实现你的分布式锁自由：
实现你的王者组件自由：
实现你的面试题自由：
**参考文献**

## 零基础搭建一套 SpringCloud 微服务脚手架

## （SpringCloud+ Dubbo + Docker +

## Jenkins）

#### 1 、本文学习内容和目标

微服务架构通过将复杂的单体应用拆分为一组小型、自治的服务，为构建灵活、可扩展的应用提供了一
种新的方式。

微服务框架具有以下优势：

```
模块化和可扩展性：微服务框架将应用程序拆分成多个独立的服务，每个服务可以独立开发、部署
和扩展，提高了系统的灵活性和可伸缩性。
技术多样性：微服务框架支持使用不同的技术栈和编程语言来构建不同的服务，使团队可以根据具
体需求选择最合适的技术。
高可用性和容错性：微服务框架通过服务注册与发现、负载均衡和容错机制，提供了高可用性和容
错性，保证了系统的稳定性和可靠性。
```

```
独立部署和快速迭代：每个服务可以独立部署，使团队可以快速迭代和发布新功能，提高了开发和
交付的效率。
```
在构建微服务系统时，选择一个适合的框架可以加速开发过程并提高系统的稳定性和可维护性。

本文旨在通过手把手教程，引导读者从零开始搭建一套 Java 微服务框架。

我们将使用 Spring Cloud 作为基础框架，并结合 Nacos 作为服务注册中心，Spring Cloud Gateway 作为
API 网关，以及 Feign 作为服务之间的通信方式。

此外，我们还将探讨如何使用 Docker 容器化和 Jenkins 进行持续集成和部署，以构建一个完整的微服务架
构。

通过本文的学习，读者将掌握以下技能：

```
设计和搭建微服务框架的基本原则
配置和使用Spring Cloud、Nacos、Spring Cloud Gateway和Feign等关键组件
实现服务注册与发现、服务调用和负载均衡、服务监控和日志管理、服务安全和认证授权等核心功
能
应用Docker容器化和Jenkins进行持续集成和部署
```
在开始构建自己的 Java 微服务框架之前，让我们先了解下设计微服务框架要遵守的一些基本原则。

#### 2 、设计微服务框架的基本原则

在设计微服务框架时，我们需要遵循一些基本原则，以确保系统的可扩展性、可维护性和可靠性。以下
是设计微服务框架的基本原则：

1. 解耦和独立性：

微服务架构的核心概念之一是服务的解耦和独立性。每个微服务应该具有清晰的边界，它们可以独立开
发、部署和扩展。在设计框架时，要保证各个微服务之间的解耦，使其可以独立演化而不会对其他服务
产生过多的影响。

2. 可伸缩性和容错性：

微服务架构的另一个重要目标是实现可伸缩性和容错性。框架应该能够根据负载的增加或减少，自动扩
展或缩减服务实例的数量。同时，要考虑到服务的容错能力，当某个服务出现故障时，框架应该能够自
动将请求路由到其他可用的服务实例上。

3. 简化开发和部署过程：

微服务框架应该能够简化开发和部署的过程。提供一套标准化的开发模式和工具链，使开发人员可以快
速构建和部署微服务。自动化的构建、测试和部署流程能够提高开发效率，减少人为错误。

4. 兼容性和可扩展性：

框架应该具备良好的兼容性和可扩展性，能够与其他技术和组件进行集成。例如，能够无缝地与现有的
数据存储、消息队列、认证授权系统等进行整合。此外，框架本身也应该是可扩展的，可以根据需求灵
活地添加新的功能模块。

设计微服务框架时，需要综合考虑这些原则，并根据实际情况进行权衡和取舍。合理的框架设计能够提
高开发效率、降低系统复杂度，并为微服务架构的可持续发展打下坚实的基础。在后续的章节中，我们
将深入探讨如何应用这些原则，结合 Spring Cloud、Nacos、Spring Cloud Gateway、Feign 等组件，构
建一套高效可靠的 Java 微服务框架。

#### 3 、搭建基础设施


在搭建微服务框架之前，我们需要配置一些基础设施，包括 Java 开发环境、构建工具、版本控制和集成
开发环境（IDE）。下面是一些常用的工具和配置：

###### 1. 安装 Java 开发环境

**工具介绍：**

首先，确保你的系统上已经安装了 Java 开发工具包，Java 开发环境包括 Java Development Kit（JDK），
它是开发和运行 Java 应用程序所必需的工具包。

你可以从 Oracle 官方网站或 OpenJDK 项目中下载并安装最新版本的 JDK。

安装完成后，设置 JAVA_HOME 环境变量，指向 JDK 的安装目录。

**下载地址：**

```
Oracle JDK: https://www.oracle.com/java/technologies/javase-jdk11-downloads.html
OpenJDK: https://adoptopenjdk.net/
```
**安装方式：**

```
1. 根据你的操作系统选择合适的Java发行版。
2. 下载安装程序并运行。
3. 按照安装向导的指引完成安装。
4. 配置JAVA_HOME环境变量：
Windows：在系统变量中新建一个名为JAVA_HOME的变量，值为JDK的安装路径（例如：
D:\program\Java\jdk1.8.0_202）。
macOS/Linux：在终端中编辑~/.bashrc或~/.bash_profile文件，并添加以下行：
export JAVA_HOME=/path/to/jdk。然后运行source ~/.bashrc或source
~/.bash_profile命令使配置生效。
```
**操作示例：**

```
1. 打开命令行终端。
2. 执行以下命令验证Java安装是否成功：
```
```
3. 如果成功安装，会显示Java版本信息。
```
###### 2. 安装和配置 Maven

**工具介绍：**

Apache Maven 是一款流行的构建工具，用于管理 Java 项目的依赖和构建过程。你可以从 Apache Maven
官方网站下载 Maven，并按照官方文档的指引进行安装和配置。配置完成后，确保你可以在命令行中使
用 mvn 命令。

**下载地址：**

Maven 官方网站：https://maven.apache.org/download.cgi

**安装方式：**

```
1. 下载适用于你的操作系统的Maven二进制发行版（ZIP或tar.gz格式）。
2. 解压下载的文件到你选择的目录。
3. 配置MAVEN_HOME环境变量：
```
```
1 java -version
```

```
Windows：在系统变量中新建一个名为MAVEN_HOME的变量，值为Maven的安装路径（例
如：C:\apache-maven-3.8.3）。
macOS/Linux：在终端中编辑~/.bashrc或~/.bash_profile文件，并添加以下行：
export MAVEN_HOME=/path/to/maven。然后运行source ~/.bashrc或source
~/.bash_profile命令使配置生效。
4. 将Maven的bin目录添加到系统的PATH环境变量中。
```
**操作示例：**

```
1. 打开命令行终端。
2. 执行以下命令验证Maven安装是否成功：
```
```
3. 如果成功安装，会显示Maven版本信息。
```
###### 3. 安装和配置 Git

**工具介绍：**

Git 是一款分布式版本控制系统，常用于协作开发和代码管理。你可以从 Git 官方网站下载并安装 Git 客户
端。安装完成后，通过命令行验证 Git 是否正确安装，并设置你的用户名和邮箱。

**下载地址：**

Git 官方网站：https://git-scm.com/downloads

**安装方式：**

```
1. 下载适用于你的操作系统的Git安装程序。
2. 运行安装程序并按照安装向导的指引进行安装。
3. 在安装过程中选择合适的选项，例如选择安装位置和默认编辑器。
```
**操作示例：**

```
1. 打开命令行终端。
2. 配置用户名和邮箱：
```
```
3. 执行以下命令验证Git安装是否成功：
```
```
4. 如果成功安装，会显示Git版本信息。
```
###### 4. 安装集成开发环境（IDE）

**工具介绍：**

推荐使用 IntelliJ IDEA 作为开发微服务的集成开发环境，IntelliJ IDEA 是一款强大的 Java 集成开发环境，提
供了丰富的功能和工具来开发 Java 应用程序。你可以从 JetBrains 官方网站下载并安装 IntelliJ IDEA 的社区
版或旗舰版，一般社区版能满足基本的开发需要。安装完成后，打开 IntelliJ IDEA，并根据需要进行相应
的配置，例如选择主题、安装必要的插件等。

**下载地址：**

```
1 mvn -v
```
```
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"
```
```
1
2
```
```
1 git --version
```

IntelliJ IDEA 官方网站：https://www.jetbrains.com/idea/download/

**安装方式：**

```
1. 下载适用于你的操作系统的IntelliJ IDEA安装程序。
2. 运行安装程序并按照安装向导的指引进行安装。
3. 在安装过程中选择合适的选项，例如选择安装位置、启动器图标等。
```
**操作示例：**

```
1. 打开安装后的IntelliJ IDEA。
2. 配置IntelliJ IDEA的插件和主题等个性化设置。
```
###### 5. 创建项目和配置构建工具

```
1. 打开IntelliJ IDEA。
2. 在欢迎界面中选择"Create New Project"或点击菜单栏的"File" -> "New" -> "Project"。
3. 使用 Spring Initializr 快速生成项目结构，
输入名称，存储位置，jdk等信息，点击下一步
```
```
选择spring boot版本，选择所需依赖，点击创建
```

```
项目创建后，可以在pom.xml文件里继续配置依赖和构件项。
pom.xml示例如下：
```
```
<dependencies>
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
<version>2.7.11</version>
</dependency>
```
```
<dependency>
<groupId>com.h2database</groupId>
<artifactId>h2</artifactId>
<scope>runtime</scope>
</dependency>
<dependency>
<groupId>org.projectlombok</groupId>
<artifactId>lombok</artifactId>
<optional>true</optional>
</dependency>
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-test</artifactId>
<scope>test</scope>
</dependency>
</dependencies>
```
```
<dependencyManagement>
<dependencies>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-dependencies</artifactId>
<version>${spring-cloud.version}</version>
```
```
1 2 3 4 5 6 7 8 9
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34


```
<type>pom</type>
<scope>import</scope>
</dependency>
<dependency>
<groupId>com.alibaba.cloud</groupId>
<artifactId>spring-cloud-alibaba-dependencies</artifactId>
<version>2021.1</version>
<type>pom</type>
<scope>import</scope>
</dependency>
</dependencies>
</dependencyManagement>
```
```
<build>
<finalName>${project.artifactId}</finalName>
<plugins>
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-resources-plugin</artifactId>
<version>3.0.1</version>
<configuration>
<encoding>UTF-8</encoding>
</configuration>
<executions>
<execution>
<id>attach-sources</id>
</execution>
</executions>
</plugin>
<!--test case plugin-->
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-surefire-plugin</artifactId>
<version>3.0.0-M5</version>
<configuration>
<parallel>methods</parallel>
<threadCount> 10 </threadCount>
<argLine>-Dfile.encoding=UTF-8</argLine>
<skipTests>true</skipTests>
</configuration>
</plugin>
<plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-assembly-plugin</artifactId>
<version>3.3.0</version>
<configuration>
<descriptorRefs>
<descriptorRef>jar-with-dependencies</descriptorRef>
</descriptorRefs>
</configuration>
<executions>
<execution>
<id>make-assembly</id>
<phase>package</phase>
<goals>
<goal>single</goal>
</goals>
</execution>
```
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92


以上是安装和配置 Java、Maven、Git 和 IntelliJ IDEA 的详细步骤。

请按照这些步骤进行操作，确保这些工具在你的开发环境中正确安装和配置。

接下来我们就可以继续进行下一步，开始搭建 Java 微服务框架。

#### 4 、定义微服务接口和协议

在设计微服务框架时，定义微服务接口和协议是非常重要的一步。

接下来以学生信息为例，结合 JPA（Java Persistence API），提供相关的增删改查的代码示例。

JPA（Java Persistence API）是 Java EE 的一部分，是一种用于对象持久化的规范。

它提供了一种以面向对象的方式进行数据库操作的方式，通过简化数据库访问和数据对象之间的映射，
提高了开发效率。

JPA 使用注解来描述实体类和数据库表之间的映射关系，可以轻松地进行增删改查等常见数据库操作。它
支持各种关系型数据库，并提供了事务管理、缓存等功能。

在我们的示例中，我们使用 JPA 来定义学生实体类，并通过注解来映射学生信息表的结构。这样，我们可
以通过简单的代码操作来实现对学生信息的持久化和查询。

###### 1. 引入 JPA 依赖

在开始之前，我们需要在项目的 pom. xml 文件中添加 JPA 的依赖。JPA 是 Java Persistence API 的缩写，它
是 Java EE 中持久化操作的标准规范，用于简化数据库操作和实体对象的映射关系。

请在项目的 pom. xml 文件中添加以下依赖：

```
</executions>
</plugin>
```
```
<!--default package plugin-->
<plugin>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-maven-plugin</artifactId>
<executions>
<execution>
<goals>
<goal>repackage</goal>
</goals>
</execution>
</executions>
<configuration>
<fork>true</fork>
<jvmArguments>-Dfile.encoding=UTF-8</jvmArguments>
<addResources>true</addResources>
<classifier>exec</classifier>
</configuration>
</plugin>
</plugins>
</build>
```
```
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
```

这些依赖将启用 JPA 和 Hibernate 作为我们的持久化框架，并使用 H 2 数据库作为示例数据库。你可以根据
自己的需求替换为其他数据库。

###### 2. 示例表结构

在我们的示例中，我们将使用一个名为"student"的表来存储学生信息。该表包含三个列：id（主键），
name 和 age。

请确保在你的数据库中创建了名为"student"的表，以及相应的列。以下是表的 DDL 示例：

###### 3. application. yml 配置

在项目中，我们需要配置 application. yml 文件来连接数据库和配置 JPA。

请在 src/main/resources 目录下创建一个名为 application. yml 的文件，并添加以下内容：

```
<dependencies>
<!-- JPA -->
<dependency>
<groupId>javax.persistence</groupId>
<artifactId>javax.persistence-api</artifactId>
<version>2.2</version>
</dependency>
```
```
<!-- Hibernate JPA 实现 -->
<dependency>
<groupId>org.hibernate</groupId>
<artifactId>hibernate-core</artifactId>
<version>5.6.0.Final</version>
</dependency>
```
```
<!-- 数据库驱动 -->
<dependency>
<groupId>com.h2database</groupId>
<artifactId>h2</artifactId>
<version>1.4.200</version>
</dependency>
```
```
<!-- Spring Boot Data JPA -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
```
```
<!-- 其他依赖... -->
```
```
</dependencies>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
```
```
CREATE TABLE student (
id INT PRIMARY KEY,
name VARCHAR( 50 ),
age INT
);
```
```
1
2
3
4
5
```
```
server:
port: 8180
```
```
1
2
```

以上配置使用了 H 2 内存数据库，并使用了默认的 sa 用户和 password 密码进行连接。同时，我们启用了
Hibernate 的 DDL 自动更新功能，并配置 JPA 显示 SQL 语句。

###### 4. 定义学生实体类

首先，定义一个学生实体类，用于表示学生的信息。可以包含学生的 ID、姓名、年龄等属性。

###### 5. 定义学生信息的持久化接口

使用 JPA 来管理学生信息的持久化操作。定义一个学生信息的持久化接口，提供增删改查等方法。

```
servlet:
context-path: /
spring:
application:
name: students-service
datasource:
url: jdbc:h2:mem:test
username: sa
password: password
driver-class-name: org.h2.Driver
jpa:
hibernate:
ddl-auto: create
show-sql: true
```
```
3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```
```
import lombok.Data;
```
```
import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.GenerationType;
import javax.persistence.Id;
```
```
@Entity
@Data
public class Student {
@Id
@GeneratedValue(strategy = GenerationType.AUTO)
private Long id;
private String name;
private int age;
```
```
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```
```
import org.springframework.data.jpa.repository.JpaRepository;
```
```
public interface StudentRepository extends JpaRepository<Student, Long> {
// 可以添加自定义的查询方法，如根据姓名查询学生信息等
}
```
```
1
2
3
4
5
```

###### 6. 定义学生信息的控制器接口

在微服务框架中，通过 HTTP 协议暴露学生信息的 API 接口供外部调用。定义一个学生信息的控制器接
口，提供增删改查等 API 方法。

通过以上代码示例，我们定义了一个学生信息的实体类、学生信息的持久化接口和学生信息的控制器接
口。

这样就可以通过 API 来进行学生信息的增删改查操作。

在接下来的步骤中，我们将结合 Spring Cloud、Nacos、Spring Cloud Gateway、Feign、Docker 和
Jenkins 等组件，搭建一个完整的 Java 微服务框架。

```
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;
```
```
import java.util.List;
```
```
@RestController
@RequestMapping("/students")
public class StudentController {
@Autowired
private StudentRepository studentRepository;
```
```
@GetMapping
public List<Student> getAllStudents() {
return studentRepository.findAll();
}
```
```
@PostMapping
public Student createStudent(@RequestBody Student student) {
return studentRepository.save(student);
}
```
```
@GetMapping("/{id}")
public Student getStudentById(@PathVariable("id") Long id) {
return studentRepository.findById(id).orElse(null);
}
```
```
@PutMapping("/{id}")
public Student updateStudent(@PathVariable("id") Long id, @RequestBody
Student updatedStudent) {
Student existingStudent =
studentRepository.findById(id).orElse(null);
if (existingStudent != null) {
existingStudent.setName(updatedStudent.getName());
existingStudent.setAge(updatedStudent.getAge());
return studentRepository.save(existingStudent);
}
return null;
}
```
```
@DeleteMapping("/{id}")
public void deleteStudent(@PathVariable("id") Long id) {
studentRepository.deleteById(id);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
```
```
29
```
```
30
31
32
33
34
35
36
37
38
39
40
41
42
```

#### 5 、实现服务注册与发现

服务注册与发现是微服务架构中非常重要的一部分，它允许我们动态地注册和发现微服务实例，从而实
现微服务之间的通信。在这一节中，我们将详细介绍服务注册与发现的工作原理和实现方式。

```
1. 服务注册
```
服务注册是指将微服务实例的信息（例如主机名、端口号、服务名称等）注册到服务注册中心，使得其
他服务可以发现和调用它。下面是服务注册的步骤：

```
微服务启动时，它会向服务注册中心发送注册请求，提供自己的信息。
服务注册中心收到注册请求后，将微服务的信息保存起来，并为其生成一个唯一的标识符（例如服
务ID）。
其他微服务可以通过服务注册中心查询和获取已注册的微服务实例列表。
```
```
2. 服务发现
```
服务发现是指在需要调用其他微服务时，通过服务注册中心来获取可用的微服务实例信息。下面是服务
发现的步骤：

```
微服务需要调用其他服务时，它向服务注册中心发送发现请求，指定需要调用的服务名称。
服务注册中心根据服务名称查询已注册的微服务实例列表，并返回给调用方。
调用方根据负载均衡策略选择一个可用的微服务实例进行调用。
```
```
3. 实现方式
```
服务注册与发现可以采用不同的实现方式，常见的有以下几种：

```
基于服务注册中心的实现：使用独立的服务注册中心，例如Netflix Eureka、Consul或Nacos。微
服务在启动时将自己注册到注册中心，其他微服务可以通过查询注册中心获取可用的微服务实例信
息。
基于DNS的实现：每个微服务使用自己的主机名和端口号，其他微服务通过域名解析来发现和调用
它们。这种方式通常用于较小规模的微服务架构。
基于边车代理的实现：使用边车代理（例如Zuul或Spring Cloud Gateway）来代理所有的微服务请
求，并在代理层进行服务发现和负载均衡。
```
在实现服务注册与发现时，我们通常会使用专门的框架和工具来简化开发和管理。例如，结合 Spring
Cloud 框架和 Nacos 注册中心，我们可以通过使用@EnableDiscoveryClient 注解启用服务注册与发现

功能，并通过配置中心来配置服务注册中心的地址。然后，我们可以通过注入 DiscoveryClient 来获取

已注册的微服务实例列表，并根据需要进行调用。

通过服务注册与发现，我们可以实现微服务架构的弹性、可扩展和高可用性，使得微服务之间的通信更
加灵活和可靠。

接下来，我们将使用 Nacos 作为服务注册与发现的中间件，并进行安装和配置。

Nacos（Naming and Configuration Service）是一个开源的服务注册与发现中间件，由阿里巴巴集团
开发和维护。它提供了服务注册、发现、配置管理和动态配置更新的功能，可以帮助我们构建弹性可伸
缩的微服务架构。

Nacos 支持主流的服务注册和发现协议，如 Eureka、Consul 和 Nacos 自身的服务注册协议。它还提供了
灵活的配置管理功能，支持动态配置刷新，可以帮助我们实现微服务的配置中心。


###### 1. 安装和配置 Nacos

首先，我们需要安装 Nacos Server。请按照以下步骤进行操作：

**步骤 1 ：下载 Nacos Server**

你可以从 Nacos 的官方 GitHub 仓库下载最新版本的 Nacos Server。

下载地址：https://github.com/alibaba/nacos/releases

**步骤 2 ：解压 Nacos Server**

将下载的 Nacos 压缩文件解压到你选择的目录中。

**步骤 3 ：启动 Nacos Server**

进入解压后的 Nacos 目录，执行以下命令启动 Nacos Server：

Nacos Server 将在默认端口（ 8848 ）启动，并以单机模式运行。

**步骤 4 ：访问 Nacos 控制台**

在浏览器中访问以下地址，进入 Nacos 控制台：

###### 2. 引入 Nacos 相关依赖

在开始之前，我们需要在项目的 pom. xml 文件中添加 Nacos 相关依赖。请添加以下依赖：

```
PS D:\program\nacos\bin> .\startup.cmd -m standalone
"nacos is starting with standalone"
```
```
,--.
,--.'|
,--,: : | Nacos 2 .2.2
,`--.'`|  ' : ,---. Running in stand
alone mode, All function modules
| : : | |  ' ,'\ .--.--. Port: 8848
: | \ | : ,--.--. ,---. / / | / /  ' Pid: 16948
| : ' '; | / \ / \. ; ,. :| : /`./ Console:
http://10.23.48.43:8848/nacos/index.html
' ' ;. ;.--. .-. | / / '' | |: :| : ;_
| | | \ | \__\/:. ..  ' / ' | .; : \ \  `. https://nacos.io
' : | ; .' ," .--.; |' ; :__| : | `----. \
| | '`--' / / ,. |' | '.'|\ \ / / /`--' /
' : | ; : .' \ : : `----' '--'. /
; |.' | , .-./\ \ / `--'---'
'---' `--`---' `----'
```
```
2023 -05-17 11 :20:55,045 INFO Tomcat initialized with port(s): 8848 (http)
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
12
13
14
15
16
17
18
19
```
```
1 http://10.23.48.43:8848/nacos
```
```
xmlCopy code<dependencies>
<!-- Nacos Discovery -->
<dependency>
<groupId>com.alibaba.cloud</groupId>
<artifactId>spring-cloud-starter-alibaba-nacos-
discovery</artifactId>
```
```
1
2
3
4
5
```

以上依赖将引入 Nacos 服务注册与发现的功能。

###### 3. 系统配置变更

现在，我们需要进行一些系统配置变更，以便我们的微服务可以与 Nacos 进行交互。请在 bootstrap. yml
文件中添加以下配置：

以上配置指定了 Nacos Server 的地址和端口。

###### 4. 示例代码

接下来，我们将编写示例代码，实现服务注册与发现的功能。请添加以下代码到 Spring Boot 应用程序中
的启动类上方：

在以上代码中，我们使用了@EnableDiscoveryClient 注解来启用服务注册与发现功能。

现在，你的微服务将能够与 Nacos 进行通信，实现服务的注册和发现。

```
<version>2021.1</version>
</dependency>
```
```
<!-- spring cloud，基于spring cloud才会读取bootstrap.yml -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-bootstrap</artifactId>
</dependency>
```
```
<!-- 其他依赖... -->
</dependencies>
```
```
6
7
8
9
10
11
12
13
14
15
16
```
```
spring:
cloud:
nacos:
discovery:
server-addr: 10.23.48.43: 8848
```
```
1
2
3
4
5
```
```
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
```
```
@EnableDiscoveryClient
@SpringBootApplication
public class StudentsServiceApplication {
```
```
public static void main(String[] args) {
SpringApplication.run(StudentsServiceApplication.class, args);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```

在接下来的步骤中，我们将继续编写代码来实现其他功能，如服务网关、服务间通信等。

#### 6 、实现服务调用和负载均衡

在微服务架构中，服务之间的调用是非常常见的场景。为了简化服务调用的过程并实现负载均衡，我们
可以使用 Feign 与 Nacos 结合来实现。本节将详细介绍服务调用的概念以及如何使用 Feign 与 Nacos 来实
现服务调用和负载均衡。

```
1. 服务调用的概念
```
服务调用是指一个微服务向另一个微服务发起请求，获取所需的数据或执行特定的操作。在微服务架构
中，服务调用可以跨越多个微服务实例，因此需要一种机制来管理和处理服务之间的通信。

```
2. 使用Feign与Nacos实现服务调用和负载均衡
```
```
Feign是一个声明式的Web服务客户端，它可以与多种服务注册中心集成，包括Nacos。通过使用
Feign，我们可以通过简单的接口定义来调用其他微服务，并且不需要手动编写具体的HTTP请求代
码。
Nacos作为服务注册中心，提供了服务发现和负载均衡的功能。它可以自动维护微服务实例列表，
并根据负载均衡策略选择合适的实例进行调用。
```
下面是使用 Feign 与 Nacos 结合实现服务调用和负载均衡的步骤：

```
步骤 1: 引入依赖
```
在项目的 pom. xml 文件中添加 Feign 相关的依赖：

```
步骤 2: 启用Feign客户端
```
在启动类上添加@EnableFeignClients 注解，启用 Feign 客户端：

```
<dependencies>
<!-- Feign -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-loadbalancer</artifactId>
</dependency>
```
```
<!-- 其他依赖... -->
</dependencies>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```

```
步骤 3: 创建Feign客户端接口
```
创建一个 Feign 客户端接口，使用@FeignClient 注解指定要调用的微服务名称和相关配置：

```
步骤 4: 使用Feign客户端进行服务调用
```
通过注入 Feign 客户端接口，并调用相应的方法来进行服务调用：

在上面的示例中，通过 serviceClient.getStudentById () 来调用目标微服务的接口。

通过以上步骤，我们可以使用 Feign 与 Nacos 结合来实现服务调用和负载均衡。Feign 将负责处理底层的
HTTP 通信细节，而 Nacos 将负责维护微服务实例列表和选择合适的实例进行调用，从而实现了服务调用
和负载均衡的功能。

```
import org.springframework.cloud.openfeign.EnableFeignClients;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
```
```
@EnableDiscoveryClient
@EnableFeignClients
@SpringBootApplication
public class StudentsServiceApplication {
```
```
public static void main(String[] args) {
SpringApplication.run(StudentsServiceApplication.class, args);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.GetMapping;
```
```
@FeignClient(name = "students-service")
public interface StudentFeignClient {
@GetMapping("/students/{id}")
Student getStudentById(@PathVariable("id") Long id);
}
```
```
1 2 3 4 5 6 7 8
```
```
import com.crazymaker.students.entity.Student;
import com.crazymaker.students.feign.StudentFeignClient;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
```
```
@Service
public class StudentService {
private StudentFeignClient feignClient;
```
```
@Autowired
public StudentService(StudentFeignClient feignClient) {
this.feignClient = feignClient;
}
```
```
public Student getStudentById(Long id) {
return feignClient.getStudentById(id);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
```

#### 7 、实现服务监控和日志管理

服务监控是指对微服务架构中的各个服务进行实时监控和管理，以确保系统的稳定性和可靠性。通过监
控服务的运行状态、性能指标和异常情况，可以及时发现问题并采取相应措施，以提高系统的可用性和
响应能力。

日志管理是指对微服务架构中生成的日志进行收集、存储、分析和展示的过程。日志是系统运行的重要
记录，通过对日志进行管理和分析，可以了解系统的运行状况、故障信息和异常情况，以便于问题排查
和系统优化。

服务监控和日志管理对于微服务架构具有重要的必要性和优势：

```
实时监控和管理：通过服务监控，可以实时监测服务的运行状态、性能指标和异常情况，及时发现
问题并采取措施，保证系统的稳定性和可靠性。
故障排查和问题定位：通过日志管理，可以收集和分析系统生成的日志信息，帮助快速定位问题、
排查故障，提高故障处理的效率。
性能优化和系统优化：通过监控和分析服务的性能指标，可以发现性能瓶颈和优化空间，以提升系
统的性能和响应能力。
数据分析和业务洞察：通过对日志进行分析和挖掘，可以获得有价值的业务洞察，帮助优化业务流
程和决策制定。
```
下面我们通过分别搭建和配置 Spring Boot Admin 和 ELK 服务来实现性能监控和日志管理：

###### 1. Spring Boot Admin 服务监控

Spring Boot Admin 是一个开源的服务监控和管理工具，它提供了一个 Web 界面，用于监控和管理基于

Spring Boot 的应用程序。通过 Spring Boot Admin，可以实时监控和管理应用程序的运行状态、健康状
况、性能指标等，并提供了强大的可视化和告警功能。

安装和配置 Spring Boot Admin 的步骤如下：

```
1. 按上文步骤新建一个Spring Boot项目， 添加Spring Boot Admin的依赖
在Spring Boot应用程序的pom.xml文件中添加Spring Boot Admin的依赖：
```
```
2. 配置bootstrap.yml和application.yml文件
```
```
<dependency>
<groupId>de.codecentric</groupId>
<artifactId>spring-boot-admin-starter-server</artifactId>
<version>3.0.3</version>
</dependency>
```
```
1
2
3
4
5
```
```
spring:
cloud:
nacos:
discovery:
server-addr: 10.23.48.43: 8848
```
```
1
2
3
4
5
```
```
server:
port: 8181
servlet:
context-path: /
spring:
application:
name: admin-service
```
```
1 2 3 4 5 6 7
```

```
3. 启动Spring Boot Admin Server
```
启动应用程序, 点击 [http://10.23.48.43:8181/applications](http://10.23.48.43:8181/applications) 访问监控页面

```
4. 微服务项目配置Spring Boot Admin
```
1 ）pom. xml 里添加 admin 依赖

2 ）在 Spring Boot 应用程序的配置文件（如 application. yml）中，配置 Spring Boot Admin Server 的相
关信息：

```
5. 重新启动应用程序
```
重新启动应用程序, 它将自动注册到 Spring Boot Admin Server。

```
@EnableDiscoveryClient
@SpringBootApplication
@EnableAdminServer
public class AdminServiceApplication {
```
```
public static void main(String[] args) {
SpringApplication.run(AdminServiceApplication.class, args);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
<dependency>
<groupId>de.codecentric</groupId>
<artifactId>spring-boot-admin-starter-server</artifactId>
<version>3.0.3</version>
</dependency>
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
spring:
boot:
admin:
client:
url: http://10.23.48.43: 8181 # Spring Boot Admin Server的地址
```
```
management:
endpoints:
web:
exposure:
include: '*'
endpoint:
health:
show-details: ALWAYS
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
```

通过以上配置，我们可以实现 Spring Boot Admin 与 Nacos 的结合，使得应用程序能够通过 Spring Boot
Admin 进行监控和管理。

###### 2. ELK 日志监控

ELK 是一个流行的日志管理和分析解决方案，由 Elasticsearch、Logstash 和 Kibana 三个项目组成，常用

于日志收集和分析。

日志主要包括系统日志、应用程序日志和安全日志。运维和开发人员可以通过日志了解服务器运行过程
中发生的错误及错误产生的原因。定期分析日志可以了解服务器的运行情况、性能、安全性等。

每台服务器或应用程序都会产生日志，如果每次都登录这些服务器查看日志并分析会耗费大量时间，而
且效率低下，这时我们就需要思考如何将日志汇总起来统一查看。日志集中管理之后又会产生新的问
题，日志量太大，日志统计和检索又成为新的问题，如何能实现高性能的检索统计呢？ELK 能完美解决
我们的问题。

```
Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分
片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。
Logstash是一个完全开源的工具，他可以对你的日志进行收集、分析，并将其存储供以后使用。
kibana 也是一个开源和免费的工具，它可以为 Logstash 和 Elasticsearch 提供的日志分析友好的
Web 界面，可以汇总、分析和搜索重要数据日志。
```
关于 ELK 的原理和实操，强烈推荐：

**ELK 日志平台（elasticsearch +logstash+kibana）原理和实操（史上最全）：**

**https://www.cnblogs.com/crazymakercircle/p/16732034.html**

###### ELK 安装

本文这里为了方便演示，仅演示利用 docker 容器在本机部署 ELK，在实际生产环境下，推荐使用多台
linux 服务器，安装 ELK 集群。

首先新建 elk 目录，在目录下新建相关文件夹：


docker-compose. yml 内容如下：

```
version: '3.2'
```
```
services:
elasticsearch:
image: elasticsearch:7.17.4
volumes:
```
- ./es/plugins:/usr/share/elasticsearch/plugins #插件文件挂载
- ./es/data:/usr/share/elasticsearch/data #数据文件挂载
-
./es/elasticsearch. yml:/usr/share/elasticsearch/config/elasticsearch. yml #配
置
ports:
- '9200:9200'
- '9300:9300'
container_name: elasticsearch
environment:
- 'cluster. name=elasticsearch' #设置集群名称为elasticsearch
- 'discovery. type=single-node' #以单一节点模式启动
- 'ES_JAVA_OPTS=-Xms 1024 m -Xmx 1024 m' #设置使用jvm内存大小
networks:
- elk
logstash:
image: logstash: 7.17.4
container_name: logstash
volumes:
-
'./logstash/logstash. conf:/usr/share/logstash/pipeline/logstash. conf'
ports:
- '5044:5044'
- '50000:50000/tcp'
- '50000:5000/udp'
- '9600:9600'
environment:
LS_JAVA_OPTS: -Xms 1024 m -Xmx 1024 m
TZ: Asia/Shanghai
MONITORING_ENABLED: false
links:
- elasticsearch: es #可以用es这个域名访问elasticsearch服务
networks:
- elk
depends_on:
- elasticsearch

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
```
```
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
```

在 es 目录下新建 data、plugins 文件夹，以及 elasticsearch. yml 文件，内容如下：

在 logstash 目录下新建 logstash. conf 文件, 内容如下：

在 kibana 目录下，创建 config 文件夹，并新建 kibana. yml 文件，内容如下：

```
kibana:
image: kibana:7.17.4
container_name: kibana
volumes:
```
- ./kibana/config/kibana. yml:/usr/share/kibana/config/kibana. yml
ports:
- '5601:5601'
links:
- elasticsearch: es #可以用es这个域名访问elasticsearch服务
environment:
- ELASTICSEARCH_URL=http://elasticsearch:9200 #设置访问
elasticsearch 的地址
- 'elasticsearch. hosts=http://es:9200' #设置访问elasticsearch的地
址
- I 18 N_LOCALE=zh-CN
networks:
- elk
depends_on:
- elasticsearch
networks:
elk:
name: elk
driver:
bridge

```
40
41
42
43
44
45
46
47
48
49
50
```
```
51
```
```
52
53
54
55
56
57
58
59
60
61
```
```
cluster.name: elasticsearch # 集群名称
node.name: node-1 # 节点名称
network.host: 0.0.0.0 # 监听地址
http.port: 9200 # 监听端口
http.cors.enabled: true
http.cors.allow-origin: "*"
```
```
1 2 3 4 5 6
```
```
input {
tcp {
port => 5000
codec => json
}
}
```
```
filter {
# 进行过滤和转换规则的配置
}
```
```
output {
elasticsearch {
hosts => ["es:9200"] # Elasticsearch的地址
index => "my-application-%{+YYYY.MM.dd}" # 索引名称，可按日期划分
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```

启动：

###### 微服务应用接入

```
在应用系统中，将日志输出到Logstash的监听端口。以下是一个Logback配置的示例logback-
spring.xml：
```
```
引入logstash依赖
```
```
使用Kibana访问Elasticsearch，配置索引模式、定义可视化仪表板和图表等，以展示和分析日志数
据。
```
```
# Default Kibana configuration for docker target
server.host: '0.0.0.0'
server.shutdownTimeout: '5s'
elasticsearch.hosts: ['http://elasticsearch:9200']
monitoring.ui.container.elasticsearch.enabled: true
server.port: 5601 # 监听端口
```
```
1 2 3 4 5 6
```
```
1 docker-compose -f docker-compose.yml up -d
```
```
<configuration>
<appender name="logstash"
class="net.logstash.logback.appender.LogstashTcpSocketAppender">
<destination>10.23.48.43:5000</destination>
<encoder class="net.logstash.logback.encoder.LogstashEncoder" />
</appender>
```
```
<root level="info">
<appender-ref ref="logstash" />
</root>
</configuration>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
<dependency>
<groupId>net.logstash.logback</groupId>
<artifactId>logstash-logback-encoder</artifactId>
<version>7.2</version>
</dependency>
```
```
1
2
3
4
5
```

通过以上步骤，我们可以搭建起 ELK Stack，并将应用系统接入到 ELK Stack 中进行日志的收集、存储、
分析和可视化展示。这样，我们可以方便地对日志进行搜索、过滤、聚合和可视化，帮助我们快速定位
问题、监控系统的运行状况，并提供数据支持进行性能优化和故障排查。

通过结合这些开源中间件，我们可以实现对微服务的全面监控和日志管理，帮助我们及时发现问题、优
化性能，并提供可视化的展示和分析工具，从而提升微服务架构的可靠性和稳定性。

#### 8 、保护服务安全

在微服务架构中，服务安全保护是至关重要的，它们可以保护微服务免受未经授权的访问，并确保只有
经过身份验证和授权的用户才能访问受保护的资源。

Spring Cloud Gateway 是一个基于 Spring Framework 5、Project Reactor 和 Spring Boot 2 构建的轻量
级网关服务，用于构建和管理微服务架构中的 API 网关。

API 网关在微服务架构中安全方面扮演着重要的角色，它作为系统的入口，它可以提供以下安全功能：

```
1. 访问控制：API网关可以对传入的请求进行访问控制，确保只有经过身份验证和授权的用户能够访
问受保护的资源。它可以验证请求中的身份验证令牌或证书，并根据配置的权限规则进行访问控
制。
```

```
2. 安全认证和授权：API网关可以与认证和授权中间件（如OAuth2）集成，实现对微服务的安全认证
和授权。它可以验证请求中的访问令牌，并将认证和授权信息传递给后端的微服务，以确保只有具
有足够权限的用户能够访问特定的资源。
3. 保护后端服务：API网关可以隐藏后端的微服务架构，只暴露必要的接口给外部客户端，从而降低
了被恶意攻击的风险。它可以阻止未经授权的请求直接访问后端服务，并提供请求的限流和缓冲功
能，以保护后端服务免受过载或恶意攻击。
4. 安全审计和日志记录：API网关可以记录请求和响应的详细信息，包括访问时间、来源IP、请求内
容等，以便进行安全审计和故障排查。它可以将日志记录到集中的日志管理系统中，方便监控和分
析。
5. 攻击防护：API网关可以实施一些安全防护措施，如防止跨站脚本攻击（XSS）、跨站请求伪造
（CSRF）和注入攻击等。它可以对传入的请求进行验证和过滤，以识别和阻止潜在的恶意行为。
```
综上所述，API 网关在安全方面具有重要的作用。它可以提供访问控制、安全认证和授权、保护后端服
务、安全审计和日志记录以及攻击防护等功能，帮助确保微服务架构的安全性和可靠性。

通过合理配置和使用适当的安全机制，API 网关可以成为微服务架构中的首道防线，保护系统免受潜在的
安全威胁。

下面是搭建和配置 Spring Cloud Gateway 的步骤示例：

首先创建一个 Spring Boot 应用，在项目的 pom. xml 文件中添加相应的依赖配置。

接下来，在 application. yml 文件中配置 Spring Cloud Gateway 和 Nacos 的相关信息。

```
<dependencies>
<!-- Spring Cloud Gateway -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>
```
```
<!-- Spring Cloud Nacos Discovery -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-alibaba-nacos-
discovery</artifactId>
</dependency>
</dependencies>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
12
13
```
```
# Spring Cloud Gateway配置
server:
port: 8182
```
```
spring:
cloud:
gateway:
discovery:
locator:
enabled: true
lower-case-service-id: true
```
```
# Nacos配置
spring:
cloud:
nacos:
discovery:
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```

以上配置中，需要将 nacos. server-addr 替换为实际的 Nacos Server 地址。

然后，创建一个启动类并添加@EnableDiscoveryClient 注解，启用服务发现功能。

最后，通过配置路由规则，将请求转发到后端的微服务。可以使用@Bean 注解在配置类中定义路由规

则，或者使用配置文件中的 spring. cloud. gateway. routes 属性进行配置。

通过以上配置，Spring Cloud Gateway 将根据路由规则将/sample/**路径的请求转发到名为 sample-

service 的后端微服务。

请注意，示例中的 sample-service 是一个示意的后端微服务名称，需要根据实际情况替换为真实的微

服务名称。

```
server-addr: ${nacos.server-addr} # Nacos Server地址
```
```
# 网关路由配置
spring:
cloud:
gateway:
routes:
```
- id: sample-service # 路由 ID
uri: lb://student-service # 后端服务名
predicates:
- Path=/student/** # 匹配的路径
filters:
- StripPrefix=1 # 去除前缀

```
18
19
20
21
22
23
24
25
26
27
28
29
30
```
```
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
```
```
@SpringBootApplication
@EnableDiscoveryClient
public class GatewayApplication {
```
```
public static void main(String[] args) {
SpringApplication.run(GatewayApplication.class, args);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
```
```
@Configuration
public class GatewayConfig {
```
```
@Bean
public RouteLocator customRouteLocator(RouteLocatorBuilder builder) {
return builder.routes()
.route("student-service", r -> r.path("/student/**")
.uri("lb://student-service"))
.build();
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```

这样，结合 Spring Cloud Gateway 和 Nacos 的示例就完成了。启动应用后，它将根据配置的路由规则将
请求转发到相应的后端微服务，并通过 Nacos 进行服务的注册和发现。

Spring Cloud Gateway 作为微服务架构的入口，除了提供了路由转发，还提供了负载均衡、过滤器、安
全认证、请求限流和监控等功能，

关于服务保护，本圣经后面有专栏介绍。当然，有兴趣的小伙伴可以阅读

**SpringCloud gateway （史上最全） - 疯狂创客圈** ：https://www.cnblogs.com/crazymakercircle/p/1
1704077. html

进一步学习和掌握相关知识。

#### 9 、实现持续集成和部署

持续集成（Continuous Integration）是一种软件开发实践，旨在频繁地将代码集成到主干版本控制系
统中。持续部署（Continuous Deployment）是持续集成的延伸，自动将通过持续集成构建的可部署软
件包发布到生产环境。

以下是一种基本的实现方式：

```
1. 版本控制：使用版本控制系统（如Git）管理代码，确保团队成员可以协同开发，并且每个更改都
有明确的记录。
2. 自动化构建：使用构建工具（如Maven）配置构建脚本，定义项目的编译、打包、测试等步骤。
3. 持续集成服务器：使用持续集成服务器（如Jenkins）来触发构建，并执行自动化构建过程。
4. 自动化测试：编写并执行自动化测试脚本，包括单元测试、集成测试和端到端测试等，以确保代码
质量和功能稳定性。
5. 自动化部署：使用容器化技术（如Docker）打包应用程序，并将其部署到预先定义的环境中，如开
发环境、测试环境和生产环境。
```
接下来，我们将分别介绍 Jenkins 和 Docker，然后提供详细的配置步骤来实现持续集成和部署。

###### 1. Jenkins

Jenkins 是一个开源的持续集成工具，它提供了一系列功能和插件，用于自动化构建、测试和部署软件。
以下是 Jenkins 的基本概念和特点：

```
作业（Job）：Jenkins中的最小单位，代表一个构建或部署任务。
构建（Build）：Jenkins执行的一次构建过程，包括编译代码、运行测试、生成构建报告等。
插件（Plugin）：Jenkins提供了丰富的插件生态系统，用于扩展其功能，例如集成不同的版本控制
系统、构建工具和部署平台等。
```
###### 2. Docker

Docker 是一个开源的容器化平台，它可以将应用程序及其依赖项打包到容器中，提供了一致、可重复和
可移植的部署环境。以下是 Docker 的基本概念和特点：

```
镜像（Image）：Docker容器的基础组件，包含了一个完整的操作系统和应用程序的运行环境。
容器（Container）：基于镜像创建的运行实例，每个容器都是相互
```
隔离的，拥有自己的文件系统、进程空间和网络接口。

```
仓库（Repository）：用于存储和分享镜像的中央注册表，Docker Hub是最常用的公共仓库
```

###### 3. 配置步骤

```
1. 微服务应用新增Dockerfile文件
```
```
2. 安装和配置Jenkins：
```
```
下载Jenkins：访问Jenkins官方网站（Jenkins download and deployment）下载适合您操作系统
的Jenkins安装包。
安装Jenkins：按照官方文档提供的步骤，安装Jenkins并完成初始化配置。
安装插件：通过Jenkins的插件管理界面，安装必要的插件，如Git插件、Maven插件、docker插件
等。
安装过程可参考 https://zhuanlan.zhihu.com/p/566398364
```
```
3. 配置Jenkins构建任务：
```
```
创建新任务：在Jenkins主界面，选择"新建任务"，输入任务名称，并选择自由风格的软件项目。
```
```
配置源代码管理：在任务配置界面，选择Git作为源代码管理工具，提供代码仓库的URL和凭据。
```
```
FROM openjdk:8-jre
RUN mkdir /app
COPY students-service-exec.jar /app/app.jar
ENTRYPOINT ["java", "-Djava.security.egd=file:/dev/./urandom", "-jar",
"/app/app.jar"]
EXPOSE 8180
```
```
1
2
3
4
```
```
5
```

```
配置构建步骤：在构建步骤中，可以定义构建脚本，如编译代码、运行测试、打包构建等。
```
4. 安装和配置 Docker：

```
安装Docker：根据您的操作系统，参考Docker官方文档（https://docs.docker.com/get-
docker/）进行Docker的安装。
配置Docker构建环境：在Jenkins任务配置界面，选择"添加构建步骤"，选择"Execute
shell"或"Execute Windows batch command"，然后在脚本中使用Docker命令构建和发布镜像。
```

构建成功后，我们可以在 docker 桌面端看到 students-service 服务已启动，

通过以上步骤，您可以使用 Jenkins 和 Docker 实现持续集成和部署。

Jenkins 将根据配置的触发器自动执行构建任务，通过 Docker 构建和发布镜像，实现持续集成和部署的流
程。

您可以根据具体的项目需求和环境配置 Jenkins 任务和 Docker 构建脚本，以实现自动化的软件交付流程。

#### 10 、小结

在本文中，我们首先讨论了微服务框架的基本原则，然后通过安装和配置 Java、Maven、Git 和 IDEA 等工
具，为搭建基础设施做好准备。接着，我们定义了微服务接口和协议，并使用 JPA 提供了增删改查的代码
示例。通过集成 Nacos，我们实现了服务注册与发现，以及使用 Feign 实现服务调用和负载均衡。

我们还介绍了 Spring Boot Admin 和 Elasticsearch，用于实现服务监控和日志管理。最后，我们使用
Spring Cloud Gateway 实现了服务安全保护，并通过 Jenkins 和 Docker 实现了持续集成和部署。

通过这个搭建过程，我们达到了构建一个完整的 Java 微服务框架的目标。


```
序号 硬件 要求
```
```
1 CPU 至少 2 核
```
```
2 内存 至少16G
```
```
3 硬盘 至少100G磁盘空间
```
我们建立了基础设施，定义了接口和协议，实现了服务注册与发现、服务调用和负载均衡、服务监控和
日志管理、服务安全和认证授权，并实现了持续集成和部署。

搭建过程存在的不足和改进的方向如下：

```
1. 性能监控和报警：在服务监控和日志管理部分，我们可以进一步改进，引入Prometheus和
Grafana来实现更全面的性能监控和报警功能。Prometheus是一个开源的监控系统，可以收集和
存储各种指标数据，并提供强大的查询和报警功能。Grafana是一个数据可视化工具，可以将
Prometheus收集的数据以图表的形式展示出来，帮助我们更直观地了解系统的性能状况。通过使
用Prometheus和Grafana，我们可以实时监控系统的指标，并设置报警规则，及时发现并解决潜
在的性能问题。
2. 为了保障微服务系统的稳定性和可靠性，你还需要考虑服务的熔断、限流、降级等策略。可以使用
Hystrix 或者 Resilience4j 等库来实现这些功能。
3. 容器化部署：在持续集成和部署部分，我们可以进一步改进，引入Kubernetes（K8s）来实现容器
化部署。Kubernetes是一个开源的容器编排平台，可以简化和自动化应用程序的部署、扩展和管
理。通过使用Kubernetes，我们可以将微服务框架中的各个服务容器化，并通过K8s进行部署、伸
缩和管理，提供更高效、可靠和弹性的部署方案。
4. 自动化和流程改进：在持续集成和部署部分，我们可以进一步改进和优化自动化流程。通过使用更
先进的CI/CD工具和技术，如Jenkins Pipeline、GitOps等，可以实现更高度的自动化和流程改进。
例如，可以通过编写自动化的流水线脚本，实现代码的构建、测试、打包和部署的自动化，并与版
本控制系统（如Git）进行集成，实现代码的自动触发和持续交付。
```
通过上述改进，我们可以进一步提升微服务框架的性能监控、报警能力，以及部署和管理的效率和可靠
性。这将有助于更好地满足不断变化的业务需求，并提升系统的可靠性和稳定性。

## 一键导入 SpringCloud 开发环境 （地表最

## 强）

_SpringCloud + docker_ 学习环境非常复杂，尼恩搞这个前前后后起码 **折腾了一周** **_,_** **应该还不止** ，

其中，很多头疼的工作，包括 _linux_ 内核升级、磁盘扩容等等，苦不堪言。

现在把这个环境，以虚拟机 box 镜像的方式，导出来直接给大家，

大家一键导入后，直接享受 SpringCloud + docker 的实操，可以说，爽到不要不要的。

以上软件和尼恩个人的虚拟机 _box_ 镜像，可以找尼恩获取。

#### 环境准备

硬件总体要求，可以参考尼恩的本地硬件情况：

**1 硬件要求。**

本文硬件总体要求如下表：


```
软件 版本
```
```
Win win10以上
```
```
virtual box 6 以上
```
```
vagrant 2 以上
```
```
工欲善其事 必先利其器
```
```
地表最强 开发环境： vagrant+java+springcloud+redis+zookeeper镜像下载(&制作详解)
```
```
地表最强 热部署：java SpringBoot SpringCloud 热部署 热加载 热调试
```
```
地表最强 发请求工具（再见吧， PostMan ）：IDEA HTTP Client（史上最全）
```
```
地表最强 PPT 小工具： 屌炸天，像写代码一样写PPT
```
```
无编程不创客，无编程不创客，一大波编程高手正在疯狂创客圈交流、学习中! 找组织，GO
```
**2 本地虚拟机环境**

#### 一键导入 OR 自己折腾

大家一键导入尼恩的虚拟机环境，里边 zookeeper、nacos、docker、k 8 s 等组件都已经预装，直接享受
SpringCloud + docker 的实操，可以说，爽到不要不要的。

当然，如果想 **自己折腾** ，也可以按照的步骤来哈。

```
注：本文以 PDF 持续更新，最新尼恩 架构笔记、面试题 的PDF文件，请从下面的链接获取：语雀
或者 码云
```
#### 随书源码 crazy-springcloud 脚手架涉以及基础中间件

本 PDF 的随书源码仓库，就是尼恩的《 **Java 高并发核心编程卷 3 加强版，老版本为 SpringCloud
Nginx 高并发核心编程** 》一书的源码

也是尼恩的一个微服务开发脚手架 crazy-springcloud，其大致的模块和功能具体如下：

```
crazymaker-server -- 根项目
│ ├─cloud-center -- 微服务的基础设施中心
│ │ ├─cloud-eureka -- 注册中心
│ │ ├─cloud-config -- 配置中心
│ │ ├─cloud-zuul -- 网关服务
│ │ ├─cloud-zipkin -- 监控中心
│ ├─crazymaker-base -- 公共基础依赖模块
│ │ ├─base-common -- 普通的公共依赖，如 utils 类的公共方法
│ │ ├─base-redis -- 公共的 redis 操作模块
│ │ ├─base-zookeeper -- 公共的 zookeeper 操作模块
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```

基于 crazy-springcloud 脚手架（其他的脚手架也类似）的微服务开发和自验证过程中，涉及到的基础
中间件大致如下：

（ 1 ）ZooKeeper （虚拟机中已经预装）

ZooKeeper 是一个分布式的、开放源码的分布式协调应用程序，是大数据框架 Hadoop 和 Hbase 的重
要组件。在分布式应用中，它能够高可用地提供很多保障数据一致性的基础能力：分布式锁、选主、分
布式命名服务等。

在 crazy-springcloud 脚手架中，高性能分布式 ID 生成器用到了 ZooKeeper。有关其原理和使用，请参
见《Netty Zookeeper Redis 高并发实战》一书。

（ 2 ）Redis （虚拟机中已经预装）

Redis 是一个高性能的缓存数据库。在高并发的场景下，Redis 可以对关系数据库起到很好的缓冲作用；
在提高系统的并发能力和响应速度方面，Redis 举足轻重和至关重要。crazy-springcloud 脚手架的分布
式 Session 用到了 Redis。有关 Redis 的原理和使用，还是请参见《Netty Zookeeper Redis 高并发实
战》一书。

（ 3 ）Eureka （虚拟机中已经预装）

Eureka 是 Netflix 开发的服务注册和发现框架，本身是一个 REST 服务提供者，主要用于定位运行在
AWS（Amazon 云）的中间层服务，以达到负载均衡和中间层服务故障转移的目的。SpringCloud 将它
集成在其子项目 spring-cloud-netflix 中，以实现 SpringCloud 的服务注册和发现功能。

（ 4 ）SpringCloud Config （虚拟机中已经预装）

SpringCloud Config 是 SpringCloud 全家桶中最早的配置中心，虽然在生产场景中，很多的企业已经使
用 Nacos 或者 Consul 整合型的配置中心替代了独立的配置中心，但是 Config 依然适用于 SpringCloud
项目，通过简单的配置即可使用。

（ 5 ） Zuul

Zuul 是 Netflix 开源网关，可以和 Eureka、Ribbon、Hystrix 等组件配合使用，SpringCloud 对 Zuul 进
行了整合与增强，使用其作为微服务集群的内部网关，负责对给集群内部各个 provider 服务提供者进行
RPC 路由和请求过滤。

```
│ │ ├─base-session -- 分布式 session 模块
│ │ ├─base-auth -- 基于 JWT + SpringSecurity 的用户凭证与认证模块
│ │ ├─base-runtime -- 各 provider 的运行时公共依赖，装配的一些通用 Spring IOC
Bean 实例
│ ├─crazymaker-uaa --业务模块: 用户认证与授权
│ │ ├─uaa-api -- 用户 DTO、Constants 等
│ │ ├─uaa-client -- 用户服务的 Feign 远程客户端
│ │ ├─uaa-provider -- 用户认证与权限的实现，包含controller 层、service层、dao
层的代码实现
│ ├─crazymaker-seckill --业务模块: 秒杀练习
│ │ ├─seckill-api -- 秒杀 DTO、Constants 等
│ │ ├─seckill-client -- 秒杀服务的 Feign 远程调用模块
│ │ ├─seckill-provider -- 秒杀服务核心实现，包含controller层、service层、dao
层的代码实现
│ ├─crazymaker-demo --业务模块: 练习演示
│ │ ├─demo-api -- 演示模块的 DTO、Constants 等
│ │ ├─demo-client -- 演示模块的 Feign 远程调用模块
│ │ ├─demo-provider -- 演示模块的核心实现，包含controller层、service层、dao层
的代码实现
```
```
11
12
13
```
```
14
15
16
17
```
```
18
19
20
21
```
```
22
23
24
25
```

```
中间件 链接地址
```
```
Linux Redis 安装（带视频） Linux Redis 安装（带视频）
```
```
Linux Zookeeper 安装（带视频） Linux Zookeeper 安装, 带视频
```
```
....完整的开发环境的准备工作，-> 请去疯狂创客圈 博客园 总入口
```
```
。。。。
```
以上中间件的端口配置，以及部分安装和使用视频，大致如下表所示。

```
注：本文以 PDF 持续更新，最新尼恩 架构笔记、面试题 的PDF文件，请从下面的链接获取：语雀
或者 码云
```
ok，是不是很简单

```
源码请参见 《 Java 高并发核心编程 卷 3 加强版，老版本为 SpringCloud Nginx 高并发核心编
程 》一书源码
虽然SpringCloud 入门很简单，但是原理很复杂， 而且掌握SpringCloud 原理，是成为核心工厂
师的必备知识
```
## 微服务分布式系统的架构 12 次演进

在进入实操之前，咱们来点微服务分布式系统的架构演进。

在尼恩的《 **Java 高并发核心编程卷 3 加强版，老版本为 SpringCloud Nginx 高并发核心编程** 》一书
中，对亿级流量的系统架构，做了一个内部的分析。

本文《SpringCloud Alibaba 学习圣经》与之相配合，介绍一下微服务分布式系统的架构演进。

《Java 高并发核心编程卷 3 加强版》的亿级流量的系统架构，大家更加要好好看看，可以结合起来
看。

#### 微服务分布式系统的几个基础概念

在介绍架构之前，为了避免部分读者对架构设计中的一些概念不了解，下面对几个微服务分布式系统中
最基础的概念进行介绍。

**1 ）什么是分布式？**

系统中的多个模块在不同服务器上部署，即可称为分布式系统，如 Tomcat 和数据库分别部署在不同的服
务器上，或两个相同功能的 Tomcat 分别部署在不同服务器上。

**2 ）什么是高可用？**

系统中部分节点失效时，其他节点能够接替它继续提供服务，则可认为系统具有高可用性。


**3 ）什么是集群？**

一个特定领域的软件部署在多台服务器上并作为一个整体提供一类服务，这个整体称为集群。

如 Zookeeper 中的 Master 和 Slave 分别部署在多台服务器上，共同组成一个整体提供集中配置服务。

在常见的集群中，客户端往往能够连接任意一个节点获得服务，并且当集群中一个节点掉线时，其他节
点往往能够自动的接替它继续提供服务，这时候说明集群具有高可用性。

**4 ）什么是负载均衡？**

请求发送到系统时，通过某些方式把请求均匀分发到多个节点上，使系统中每个节点能够均匀的处理请
求负载，则可认为系统是负载均衡的。

**5 ）什么是正向代理和反向代理？**

系统内部要访问外部网络时，统一通过一个代理服务器把请求转发出去，在外部网络看来就是代理服务
器发起的访问，此时代理服务器实现的是正向代理；

当外部请求进入系统时，代理服务器把该请求转发到系统中的某台服务器上，对外部请求来说，与之交
互的只有代理服务器，此时代理服务器实现的是反向代理。

简单来说，正向代理是代理服务器代替系统内部来访问外部网络的过程，反向代理是外部请求访问系统
时通过代理服务器转发到内部服务器的过程。

```
注：本文以 PDF 持续更新，最新尼恩 架构笔记、面试题 的PDF文件，请从下面的链接获取：语雀
或者 码云
```
#### 架构演进 1 ：单机架构

在网站最初时，应用数量与用户数都较少，可以把 Java 应用（主要是 Tomcat 承载）和数据库部署在同一
台服务器上。

浏览器往发起请求时，首先经过 DNS 服务器（域名系统）把域名转换为实际 IP 地址 10.102.4.1，浏览器
转而访问该 IP 对应的 Tomcat。

具体的架构图，如下：


**架构瓶颈：**

随着用户数的增长，Tomcat 和数据库之间竞争资源，单机性能不足以支撑业务。

#### 架构演进 2 ：引入缓存架构

随着吞吐量的提升，不得不引入引入缓存架构。通过缓存能把绝大多数请求在读写数据库前拦截掉，大
大降低数据库压力。

含: 本地缓存和分布式缓存

在 Tomcat 同服务器上或同 JVM 中增加本地缓存，并在外部增加分布式缓存，缓存热门商品信息或热门商
品的 html 页面等。

**涉及的技术包括：**

使用 caffeine 作为本地缓存，使用 Redis 作为分布式缓存

**架构瓶颈：**

（ 1 ）这里会涉及缓存穿透/击穿、缓存雪崩、热点数据集中失效等问题

（ 2 ）这里会涉及缓存一致性的问题

（ 3 ）这里会涉及 hotkey 的问题


有关上面问题的解决方案：

请参见尼恩的《100 Wqps 三级缓存组件实操》+ 《100 Wqps Caffeine 底层源码、架构实操实操》

#### 架构演进 3 ：接入层引入反向代理实现负载均衡

接下来，缓存抗住了大部分的访问请求，服务层 Tomcat 上还是吞吐量低，响应逐渐变慢，需要进行架构
演进。

在多台服务器上分别部署 Tomcat，使用反向代理软件（Nginx）把请求均匀分发到每个 Tomcat 中。

此处假设 Tomcat 最多支持 100 个并发，Nginx 最多支持 50000 个并发，那么理论上 Nginx 把请求分发到
500 个 Tomcat 上，就能抗住 50000 个并发。

**其中涉及的技术包括：** Nginx、HAProxy，

两者都是工作在网络第七层的反向代理软件，主要支持 http 协议，还会涉及 session 共享、文件上传下载
的问题。

#### 架构演进 4 ：数据库读写分离

反向代理使服务层的并发量大大增加，但并发量的增长也意味着： 更多请求会穿透到数据库，数据库最
终成为瓶颈。

**数据库如何高并发？**

简单的方案：把数据库划分为读库和写库，读库可以有多个，


**读库和写库之间，如何实现数据一致性？**

简单的方案：可以通过 DB 的同步机制，把写库的数据同步到读库，对于需要查询最新写入数据场景，可
通过在缓存中多写一份，通过缓存获得最新数据。

**数据库读写分离的架构如下：**

其中涉及的技术包括：shardingjdbc ，它是数据库中间件，可通过它来组织数据库的分离读写和分库分
表，客户端通过它来访问下层数据库，还会涉及数据同步，数据一致性的问题。

有关上面分库分表解决方案：

请参见尼恩的《10 Wqps 日志平台实操》，对分库分表方案，做了非常详解的架构介绍，并且在实操维
度，对其中的核心的组件分布式 ID，结合雪花 id 源码，百度 id 源码，shardingjdbc id 源码，做了深入骨
髓的介绍。

#### 架构演进 5 ：数据库按业务分库

业务逐渐变多，不同业务之间的访问量差距较大，不同业务直接竞争数据库，相互影响性能。

随着用户数的增长，单机的写库会逐渐会达到性能瓶颈。

把不同业务的数据保存到不同的数据库中，使业务之间的资源竞争降低，对于访问量大的业务，可以部
署更多的服务器来支撑。


有关上面分库分表解决方案：

请参见尼恩的《10 Wqps 日志平台实操》，对分库分表方案，做了非常详解的架构介绍，并且在实操维
度，对其中的核心的组件分布式 ID，结合雪花 id 源码，百度 id 源码，shardingjdbc id 源码，做了深入骨
髓的介绍。

#### 架构演进 6 ：使用 LVS 或 F 5 接入层负载均衡

随着吞吐量大于 5 W，接入层 Nginx 扛不住了

由于瓶颈在 Nginx，因此无法 LVS 或 F 5 来实现多个 Nginx 的负载均衡。


图中的 LVS 和 F 5 是工作在网络第四层的负载均衡解决方案，区别是：

(1 ) LVS 是软件，运行在操作系统内核态，可对 TCP 请求或更高层级的网络协议进行转发，因此支持的协
议更丰富，并且性能也远高于 Nginx，可假设单机的 LVS 可支持几十万个并发的请求转发；

(2 ) F 5 是一种负载均衡硬件，与 LVS 提供的能力类似，性能比 LVS 更高，但价格昂贵。

如果不是财大气粗的 guoqi，推荐使用 LVS。

由于 LVS 是单机版的软件，若 LVS 所在服务器宕机则会导致整个后端系统都无法访问，因此需要有备用节
点。

LVS 如何高可用呢？

可使用 keepalived 软件模拟出虚拟 IP，然后把虚拟 IP 绑定到多台 LVS 服务器上，浏览器访问虚拟 IP 时，会
被路由器重定向到真实的 LVS 服务器

当主 LVS 服务器宕机时，keepalived 软件会自动更新路由器中的路由表，把虚拟 IP 重定向到另外一台正常
的 LVS 服务器，从而达到 LVS 服务器高可用的效果。

#### 架构演进 7 ：通过 DNS 轮询实现机房间的负载均衡

由于 LVS 也是单机的，随着并发数增长到几十万时，LVS 服务器最终会达到瓶颈，此时用户数达到千万甚
至上亿级别，用户分布在不同的地区，与服务器机房距离不同，导致了访问的延迟会明显不同。


此时，可以使用 DNS 进行负载均衡：在 DNS 服务器中可配置一个域名对应多个 IP 地址，每个 IP 地址对应
到不同的机房里的虚拟 IP。

当用户访问 taobao 时，DNS 服务器会使用轮询策略或其他策略，来选择某个 IP 供用户访问。

此方式能实现机房间的负载均衡

至此，系统可做到机房级别的水平扩展，千万级到亿级的并发量都可通过增加机房来解决，系统入口处
的请求并发量不再是问题。

问题是，光用 DNS 进行简单的 LVS 负载均衡，是不够的。

**所以呢？大部分的大厂应用，都是采用智能 DNS + 接入层流量二次路由的模式，具体的案例，可以来
找尼恩进行交流，这里不做展开。主要是内容太多啦。**

#### 架构演进 8 ：引入 NoSQL 数据库和搜索引擎等技术

随着数据的丰富程度和业务的发展，检索、分析等需求越来越丰富，单单依靠数据库无法解决如此丰富
的需求。

当数据库中的数据多到一定规模时，数据库就不适用于复杂的查询了，往往只能满足普通查询的场景。

对于统计报表场景，在数据量大时不一定能跑出结果，而且在跑复杂查询时会导致其他查询变慢

对于全文检索、可变数据结构等场景，数据库天生不适用，使用 elasticsearch 分布式搜索引擎解决。

如对于海量文件存储，可通过分布式文件系统 hbase 解决


对于全文检索场景，可通过搜索引擎如 ElasticSearch 解决，对于多维分析场景，可通过 Kylin 或 Druid 等
方案解决。

当然，引入更多组件同时会提高系统的复杂度，不同的组件保存的数据需要同步，需要考虑一致性的问
题，需要有更多的运维手段来管理这些组件等。

**接下来尼恩会讲云原生+大数据的架构，就是介绍的这套方案。**

#### 架构演进 9 ：大应用拆分为微服务

引入更多组件解决了丰富的需求，业务维度能够极大扩充，随之而来的是一个应用中包含了太多的业务
代码，业务的升级迭代、部署维护变得困难，效率低下。

解决方式是，进行业务的解耦。按照业务板块来划分应用代码，使单个应用的职责更清晰，相互之间可
以做到独立升级迭代。

不同的业务，可以解耦成不同的微服务。

这样的服务就是所谓的微服务，应用和服务之间通过 HTTP、TCP 或 RPC 请求等多种方式来访问公共服
务，每个单独的服务都可以由单独的团队来管理。

此外，可以通过 Dubbo、SpringCloud 等框架实现服务治理、限流、熔断、降级等功能，提高服务的稳
定性和可用性。

这时候应用之间可能会涉及到一些公共配置，可以通过分布式配置中心 Nacos 来解决。


#### 架构演进 10 ：引入企业服务总线 ESB 对微服务进行编排

由于不同服务之间存在共用的模块，由微服务单独管理会导致相同代码存在多份，导致公共功能升级时
全部应用代码都要跟着升级。

不同微服务的接口访问方式不同，微服务代码需要适配多种访问方式才能使用，此外，微服务访问微服
务，微服务之间也可能相互访问，调用链将会变得非常复杂，逻辑变得混乱。

在微服务的基础上，以应用为单位，进行微服务的分组，并且引入企业服务总线 ESB，对微服务进行编
排，形成应用。


通过 ESB 统一进行访问协议转换，应用统一通过 ESB 来访问后端服务，服务与服务之间也通过 ESB 来相互
调用，以此降低系统的耦合程度。

这种微服务编排为多个应用，公共服务单独抽取出来来管理，并使用企业消息总线来解除服务之间耦合
问题的架构。

**接下来尼恩会讲 ESB 架构，就是介绍的这套方案。**

#### 架构演进 11 ：引入容器化技术实现动态扩容和缩容

业务不断发展，应用和服务都会不断变多，应用和服务的部署变得复杂，同一台服务器上部署多个服务
还要解决运行环境冲突的问题

此外，对于如大促这类需要动态扩缩容的场景，需要水平扩展服务的性能，就需要在新增的服务上准备
运行环境，部署服务等，运维将变得十分困难。

目前最流行的容器化技术是 Docker，最流行的容器管理服务是 Kubernetes (K 8 S)，应用/服务可以打包为
Docker 镜像，通过 K 8 S 来动态分发和部署镜像。

Docker 镜像可理解为一个能运行你的应用/服务的最小的操作系统，里面放着应用/服务的运行代码，运
行环境根据实际的需要设置好。

把整个“操作系统”打包为一个镜像后，就可以分发到需要部署相关服务的机器上，直接启动 Docker 镜像
就可以把服务起起来，使服务的部署和运维变得简单。

有关 Docker + Kubernetes (K 8 S) 的内容，请参见尼恩的电子书：

由于内容确实太多，所以写多个 pdf 电子书：


(1) **《 Docker 学习圣经》PDF**

(2) **《 SpringCloud Alibaba 微服务学习圣经》** PDF

使用 Docker + Kubernetes (K 8 S) 后，在大促的之前，可以在现有的机器集群上划分出服务器来启动
Docker 镜像，增强服务的性能

大促过后就可以关闭镜像，对机器上的其他服务不造成影响。

#### 架构演进 12 ：以云平台承载系统

使用容器化技术后服务动态扩缩容问题得以解决，但是机器还是需要公司自身来管理，在非大促的时
候，还是需要闲置着大量的机器资源来应对大促，机器自身成本和运维成本都极高，资源利用率低。


系统可部署到公有云上，利用公有云的海量机器资源，解决动态硬件资源的问题

在大促的时间段里，在云平台中临时申请更多的资源， **结合 Docker 和 K 8 S 来快速部署服务** ，在大促结束
后释放资源，真正做到按需付费，资源利用率大大提高，同时大大降低了运维成本。

所谓的云平台，就是把海量机器资源，通过统一的资源管理，抽象为一个资源整体

在云平台上可按需动态申请硬件资源（如 CPU、内存、网络等），并且之上提供通用的操作系统，提供
常用的技术组件（如 Hadoop 技术栈，MPP 数据库等）供用户使用，甚至提供开发好的应用

用户不需要关心应用内部使用了什么技术，就能够解决需求（如音视频转码服务、邮件服务、个人博客
等）。

在云平台中会涉及如下几个概念：

IaaS：基础设施即服务。对应于上面所说的机器资源统一为资源整体，可动态申请硬件资源的层面；
PaaS：平台即服务。对应于上面所说的提供常用的技术组件方便系统的开发和维护；
SaaS：软件即服务。对应于上面所说的提供开发好的应用或服务，按功能或性能要求付费。

至此：以上所提到的从高并发访问问题，到服务的架构和系统实施的层面都有了各自的解决方案。

#### 架构演进的涉及的核心知识

通过以上架构的演进，可以看出：

（ 1 ）开发侧： 重点的知识体系是 SpringCloud + Nginx 的基础架构；

有关 SpringCloud + Nginx 的知识，请阅读本文《SpringCloud Alibaba 学习圣经》和与之相配合的
《Java 高并发核心编程卷 3 加强版》


《Java 高并发核心编程卷 3 加强版》

（ 2 ）运维侧： 重点的知识体系是 docker + k 8 s 的基础架构；

有关 docker + k 8 s 的知识，请阅读本文《docker 学习圣经》和与之相配合的《K 8 s 学习圣经》

**搞定这些，应对亿级流量，就具备了基础的知识底座。**

本文，聚焦 SpringCloud 的学习，主要是 SpringCloud Alibaba 的学习。

## SpringCloud netflix 入门

要了解 SpringCloud Alibaba ，先得了解 **SpringCloud netflix。**

为啥？ SpringCloud Alibaba 仅仅是在 SpringCloud netflix 的基础上，替换了部分组件。比如说注
册中心，比如 RPC 组件。

**所以，咱们得从 SpringCloud netflix 开始。**

SpringCloud Netflix 全家桶是 Pivotal 团队提供的一整套微服务开源解决方案，包括服务注册与发现、
配置中心、全链路监控、服务网关、负载均衡、断路器等组件，以上的组件主要通过对 NetFilx 的
NetFlix OSS 套件中的组件通过整合完成的，其中，比较重要的整合组件有:

（ 1 ）spring-cloud-netflix-Eureka 注册中心

（ 2 ）spring-cloud-netflix-hystrix RPC 保护组件

（ 3 ）spring-cloud-netflix-ribbon 客户端负载均衡组件

（ 4 ）spring-cloud-netflix-zuul 内部网关组件

（ 6 ）spring-cloud-config 配置中心

SpringCloud 全家桶技术栈除了对 NetFlix OSS 的开源组件做整合之外，还有整合了一些选型中立的开源
组件。比如，SpringCloud Zookeeper 组件整合了 Zookeeper，提供了另一种方式的服务发现和配置管
理。

SpringCloud 架构中的单体业务服务是基于 SpringBoot 应用进行启动和执行的。SpringBoot 是由
Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。
SpringCloud 利用 SpringBoot 是什么关系呢？

（ 1 ）首先 SpringCloud 利用 SpringBoot 开发便利性巧妙地简化了分布式系统基础设施的开发；

（ 2 ）其次 SpringBoot 专注于快速方便地开发单体微服务提供者，而 SpringCloud 解决的是各微服务提
供者之间的协调治理关系；

（ 3 ）第三 SpringBoot 可以离开 SpringCloud 独立使用开发项目，但是 SpringCloud 离不开
SpringBoot，其依赖 SpringBoot 而存在。

最终，SpringCloud 将 SpringBoot 开发的一个个单体微服务整合并管理起来，为各单体微服务提供配
置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等基础的分
布式协助能力。

#### SpringCloud 开发脚手架

无论是单体应用还是分布式应用，如果从零开始开发，都会涉及很多基础性的、重复性的工作需要做，
比如用户认证，比如 session 管理等等。有了开发脚手架，这块基础工作就可以省去，直接利用脚手架
提供的基础模块，然后按照脚手架的规范进行业务模块的开发即可。


笔者看了开源平台的不少开源的脚手架，发现很少是可以直接拿来做业务模块开发的，或者封装的过于
重量级而不好解耦，或者业务模块分包不清晰而不方便开发，所以，本着简洁和清晰的原则，笔者的发
起的疯狂创客圈社群推出了自己的微服务开发脚手架 crazy-springcloud，其大致的模块和功能具体如
下：

在业务模块如何分包的问题上，实际上大部分企业都有自己的统一规范。crazy-springcloud 脚手架从职
责清晰、方便维护、能快速导航代码的角度出发，将每一个业务模块，细分成以下三个子模块：

（ 1 ） {module}-api

此子模块定义了一些公共的 Constants 业务常量和 DTO 传输对象，该子模块既被业务模块内部依赖，
也可能被依赖该业务模块的外部模块所依赖；

（ 2 ） {module}-client

此子模块定义了一些被外部模块所依赖的 Feign 远程调用客户类，该子模块是专供外部的模块，不能被
内部的其他子模块所依赖；

（ 3 ） {module}-provider

此子模块是整个业务模块的核心，也是一个能够独立启动、运行的服务提供者（Application），该模块
包含涉及到业务逻辑的 controller 层、service 层、dao 层的完整代码实现。

crazy-springcloud 微服务开发脚手架在以下两方面进行了弱化：

```
crazymaker-server -- 根项目
│ ├─cloud-center -- 微服务的基础设施中心
│ │ ├─cloud-eureka -- 注册中心
│ │ ├─cloud-config -- 配置中心
│ │ ├─cloud-zuul -- 网关服务
│ │ ├─cloud-zipkin -- 监控中心
│ ├─crazymaker-base -- 公共基础依赖模块
│ │ ├─base-common -- 普通的公共依赖，如 utils 类的公共方法
│ │ ├─base-redis -- 公共的 redis 操作模块
│ │ ├─base-zookeeper -- 公共的 zookeeper 操作模块
│ │ ├─base-session -- 分布式 session 模块
│ │ ├─base-auth -- 基于 JWT + SpringSecurity 的用户凭证与认证模块
│ │ ├─base-runtime -- 各 provider 的运行时公共依赖，装配的一些通用 Spring IOC
Bean 实例
│ ├─crazymaker-uaa --业务模块: 用户认证与授权
│ │ ├─uaa-api -- 用户 DTO、Constants 等
│ │ ├─uaa-client -- 用户服务的 Feign 远程客户端
│ │ ├─uaa-provider -- 用户认证与权限的实现，包含controller 层、service层、dao
层的代码实现
│ ├─crazymaker-seckill --业务模块: 秒杀练习
│ │ ├─seckill-api -- 秒杀 DTO、Constants 等
│ │ ├─seckill-client -- 秒杀服务的 Feign 远程调用模块
│ │ ├─seckill-provider -- 秒杀服务核心实现，包含controller层、service层、dao
层的代码实现
│ ├─crazymaker-demo --业务模块: 练习演示
│ │ ├─demo-api -- 演示模块的 DTO、Constants 等
│ │ ├─demo-client -- 演示模块的 Feign 远程调用模块
│ │ ├─demo-provider -- 演示模块的核心实现，包含controller层、service层、dao层
的代码实现
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
14
15
16
17
```
```
18
19
20
21
```
```
22
23
24
25
```

（ 1 ）在部署方面对容器的介绍进行了弱化，没有使用 Docker 容器而是使用 Shell 脚本。有多方面的原
因：一是本脚手架初心是学习，使用 Shell 脚本而不是 Docker 去部署，方便大家学习 Shell 命令和脚
本；二是 Java 和 Docker 其实整合得很好，学习非常容易，可以稍加配置就能做到一键发布，找点资料
就可以掌握；三是部署和运维是一个专门的工作，生产环境的部署、甚至是整个自动化构建和部署的工
作，实际上属于运维的专项工作，由专门的运维岗位人员去完成，而部署的核心仍然是 Shell 脚本，所
以对于开发人员来说掌握 Shell 脚本才是重中之重。

（ 2 ）对监控软件的介绍进行了弱化。本书没有对链路监控、JVM 性能指标、断路器监控软件的使用做专
门介绍。有多方面的原因：一是监控的软件太多，如果介绍太全，篇幅又不够，介绍太少，大家又不一
定用到；二是监控软件的使用大多是一些软件的操作步骤和说明，原理性的内容比较少，使用视频的形
式会比文字形式知识传递的效果会更好。疯狂创客圈后续可能（但不一定）会推出一些微服务监控方面
的教学视频供大家参考，请大家关注社群博客。不论如何，只要掌握了 SpringCloud 核心原理，对那些
监控组件使用的掌握，对大家来说基本上都是一碟小菜。

#### 启动 Eureka Server 注册中心

Eureka 本身是 Netflix 开源的一款注册中心产品，并且 SpringCloud 提供了相应的集成封装，选择其作
为注册中心的讲解实例，是出于以下的原因：

（ 1 ）Eureka 在业界的应用十分广泛（尤其是国外），整个框架也经受住了 Netflix 严酷生产环境的考
验。

（ 2 ）除了 Eureka 注册中心，Netflix 的其他服务治理功能也十分强大，包括 Ribbon、Hystrix、
Feign、Zuul 等组件，结合到一起组成了一套完整的服务治理框架，使得服务的调用、路由也变得异常
容易。

那么，Netflix 和 SpringCloud 是什么关系呢？

Netflix 是一家互联网流媒体播放商，是美国视频巨头，访问量非常的大。也正是如此，Netflix 把整体的
系统迁移到了微服务架构。并且，Netflix 就把它的几乎整个微服务治理生态中的组件，都开源贡献给了
Java 社区，叫做 Netflix OSS。

SpringCloud 是 Spring 背后的 Pivotal 公司（由 EMC 和 VMware 联合成立的公司）在 2015 年推出的
开源产品，主要对 Netflix 开源组件的进一步封装，方便 Spring 开发人员构建微服务架构的应用。

SpringCloud Eureka 是 SpringCloud Netflix 微服务套件的一部分，基于 Netflix Eureka 做了二次封
装，主要负责完成微服务实例的自动化注册与发现，这也是微服务架构中最为核心和基础的功能。

Eureka 所治理的每一个微服务实例，被称之为 Provider Instance (提供者实例)。每一个 Provider
Instance 微服务实例包含一个 Eureka Client 客户端组件（相当于注册中心客户端组件），其主要的工
作为：

（ 1 ）向 Eureka Server 完成 Provider Instance 的注册、续约和下线等操作，主要的注册信息包括服务
名、机器 IP、端口号、域名等等。

（ 2 ）向 Eureka Server 获取 Provider Instance 清单，并且缓存在本地。

一般来说，Eureka Server 作为服务治理应用，会独立地部署和运行。一个 Eureka Server 注册中心应
用在新建的时候，首先需要在 pom. xml 文件中添加上 eureka-server 依赖库。

```
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
</dependency>
```
```
1
2
3
4
5
```

然后，需要在启动类中添加注解 @EnableEurekaServer，声明这个应用是一个 Eureka Server，启动类
的代码如下：

然后，在应用配置文件 application. yml 中，对 Eureka Server 的一些参数进行配置。一份基础的配置文
件大致如下：

以上的配置文件中，包含了三类配置项：作为服务注册中心的配置项（eureka. server.）、作为
_Provider_ 提供者的配置项（ _eureka. instance._ ）、作为注册中心客户端组件的配置项
（eureka. client.*），至于具体的原因，请参考《SpringCloud Nginx 高并发核心编程》一书。

配置完成后，通过运行启动类 EurekaServerApplication 就可以启动 Eureka Server，然后通过浏览器
访问 Eureka Server 的 **控制台界面** （其端口为 server. port 配置项的值），大致如下图所示。

```
package com.crazymaker.springcloud.cloud.center.eureka;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;
//在启动类中添加注解 @EnableEurekaServer
@EnableEurekaServer
@SpringBootApplication
public class EurekaServerApplication {
public static void main(String[] args) {
SpringApplication.run(EurekaServerApplication.class, args);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
server:
port: 7777
spring:
application:
name: eureka-server
eureka:
client:
register-with-eureka: false
fetch-registry: false
service-url:
#服务注册中心的配置内容，指定服务注册中心的位置
defaultZone:
${SCAFFOLD_EUREKA_ZONE_HOSTS:http://localhost:7777/eureka/}
instance:
hostname: ${EUREKA_ZONE_HOST:localhost}
server:
enable-self-preservation: true # 开启自我保护
eviction-interval-timer-in-ms: 60000 # 扫描失效服务的间隔时间（单位毫秒，默认是
60*1000）即 60 秒
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
13
14
15
16
17
```
```
18
```

图：Eureka Server 的控制台界面

```
注：本文以 PDF 持续更新，最新尼恩 架构笔记、面试题 的PDF文件，请从下面的链接获取：语雀
或者 码云
```
#### 启动 Config 配置中心

在采用分布式微服务架构的系统中，由于服务数量巨多，为了方便服务配置文件统一管理，所以需要分
布式配置中心组件。如果各个服务的配置分散管理，则，上线之后配置的如何保持一致，将会是一个很
头疼的问题。

所以，各个服务的配置定然需要集中管理。SpringCloud Config 配置中心是一个比较好的解决方案。使
用 SpringCloud Config 配置中心，涉及到两个部分：

（ 1 ）config-server 服务端配置；（需要独立运行）

（ 2 ）config-client 客户端配置。 （作为组件嵌入到 Provider 微服务提供者）

###### config-server 服务

通过 SpringCloud 构建一个 config-server 服务，大致需要三步。首先，在 pom. xml 中引入 spring-
cloud-config-server 依赖，大致如下：

其次，在所创建的 SpringBoot 的程序主类上，添加@EnableConfigServer 注解，开启 Config Server 服
务，代码如下：

```
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-config-server</artifactId>
</dependency>
```
```
1
2
3
4
```

第三步，设置属性文件的位置。SpringCloud Config 提供本地存储配置的方式。在 bootstrap 启动属性
文件中，设置属性 spring. profiles. active=native，并且设置属性文件所在的位置，大致如下：

配置说明：

（ 1 ）spring. profiles. active=native，表示读取本地配置，而不是从 git 读取配置。

（ 2 ）search-locations=classpath: config/ 表示查找文件的路径，在类路径的 config 下。

服务端的配置规则：在配置路径下，以 {label}/{application}-{profile}. properties 的命令规范，放置对
应的配置文件。上面实例，放置了以下配置文件：

分别对通用配置 common、数据库配置 db、缓存配置的相关属性，进行设置。Config 配置中心启动之
后，使用 [http://](http://) ${CONFIG-HOST}: ${CONFIG-PORT}/{application}/{profile}[/{label}] 的地址格式，可
以直接访问所加载好的配置属性。

例如，访问示例中的 [http://192.168.233.128:7788/crazymaker/redis/dev](http://192.168.233.128:7788/crazymaker/redis/dev) 地址，返回的配置信息如下
图所示。

```
@EnableConfigServer
@SpringBootApplication
public
class Application {
```
```
public static void main(String[] args) {
new SpringApplicationBuilder(Application.class).web(true).run(args);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
server:
port: 7788 #配置中心端口
spring:
application:
name: config-server # 服务名称
profiles:
active: native # 设置读取本地配置文件
cloud:
config:
server:
native:
searchLocations: classpath:config/  #申明本地配置文件的存放位置
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```

**特别说明：SpringCloud config-server 支持有多种配置方式，比如 Git，native，SVN 等** 。虽然官方
建议使用 Git 方式进行配置，这里没有重点介绍 Git 方式，而是使用了本地文件的方式。有三个原因：

（ 1 ） 对于学习或者一般的开发来说，本地的文件的配置方式更简化；

（ 2 ） **生产环境建议使用 Nacos，集成注册中心和配置中心，更加方便和简单** ；

#### 微服务入门案例

在本书的配套源码 crazy-springcloud 脚手架中，设计三个 Provider 服务提供者：uaa-provider （用户
账号与认证）、demo-provider （演示用途）、seckill-provider （秒杀服务），具体如下图所示。


图：本书的配套源码中的服务提供者

#### uaa-provider 微服务提供者

首先，一个 Provider 服务提供者至少需要以下两个组件包依赖：SpringBoot WEB 服务组件、Eureka
Client 客户端组件，大致如下：

SpringBoot WEB 服务组件用于提供 REST 接口服务，Eureka Client 客户端组件用于服务注册与发现。
从以上的 Maven 依赖可以看出，在 SpringCloud 技术体系中，一个 Provider 服务提供者首先是一个
SpringBoot 应用，所以，在学习 SpringCloud 微服务技术之前，必须具备一些基本的 SpringBoot 开发

知识。
然后，在 SpringBoot 应用的启动类上加上 @EnableDiscoveryClient 注解，用于启用 Eureka Client 客
户端组件，启动类的代码如下：

接下来，在 Provider 模块（或者项目）的 src/main/resources 的 bootstrap 启动属性文件中
（bootstrap. properties 或 bootstrap. yml），增加 Provider 实例相关的配置，具体如下：

```
<dependencies>
<!--SpringBoot WEB 服务组件 -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
</dependency>
```
```
<!-- Eureka Client 客户端组件 -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
</dependencies>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
package com.crazymaker.springcloud.user.info.start;
//...省略import
@SpringBootApplication
/*
* 启用 Eureka Client 客户端组件
*/
@EnableEurekaClient
public class UAACloudApplication
{
public static void main(String[] args)
{
SpringApplication.run(UAACloudApplication.class, args);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
```
```
spring:
application:
name: uaa-provider
```
```
server:
port: 7702
servlet:
```
```
1 2 3 4 5 6 7
```

在详细介绍上面的配置项之前，先启动一下 Provider 的启动类，控制台的日志大致如下：

如果看到上面的日志，表明 Provider 实例已经启动成功。可以进一步通过 Eureka Server 检查服务是否
注册成功：打开 Eureka Server 的控制台界面，可以看到 uua-provider 的一个实例已经成功注册，具体

如下图所示。

图：uua-provider 实例已经在成功注册到 Eureka Server

前面讲到，SpringCloud 中一个 Provider 实例身兼两者角色：Provider 服务提供者、注册中心客户
端。所以，在 Provider 的配置文件中，包含了两类配置： Provider 实例角色的相关配置、Eureka
Client 客户端角色的相关配置。
有关的 Provider 实例角色的相关配置，请参考《SpringCloud Nginx 高并发核心编程》一书。

###### uaa-provider 实现一个 Rest 接口

以 uaa-Provider 的获取用户信息接口为例，进行介绍，

这里实现一个获取用户信息的接口 /api/user/detail/v 1 ，该接口的具体的代码，在 uaa-Provider 模块
中，如下图所示：

```
context-path: /uaa-provider
eureka:
instance:
instance-id: ${spring.cloud.client.ip-address}:${server.port}
ip-address: ${spring.cloud.client.ip-address}
prefer-ip-address: true  #访问路径优先使用 IP地址
status-page-url-path: /${server.servlet.context-
path}${management.endpoints.web.base-path}/info
health-check-url-path: /${server.servlet.context-
path}${management.endpoints.web.base-path}/health
client:
egister-with-eureka: true  #注册到eureka服务器
fetch-registry: true #是否去注册中心获取其他服务
serviceUrl:
defaultZone: http://${EUREKA_ZONE_HOST:localhost}:7777/eureka/
```
```
8
9
10
11
12
13
14
```
```
15
```
```
16
17
18
19
20
```
```
...com.netflix.discovery.DiscoveryClient - DiscoveryClient_UAA-
PROVIDER/192.168.233.128:7702: registering service...
....
...com.netflix.discovery.DiscoveryClient - DiscoveryClient_UAA-
PROVIDER/192.168. 233.128:7702 - registration status: 204
```
```
1
```
```
2
3
```

具体的代码如下：

```
package com.crazymaker.springcloud.user.info.controller;
```
```
import com.alibaba.fastjson.JSONObject;
import com.crazymaker.springcloud.common.dto.UserDTO;
import com.crazymaker.springcloud.common.result.RestOut;
import
com.crazymaker.springcloud.user.info.service.impl.FrontUserEndSessionService
Impl;
import io.swagger.annotations.Api;
import io.swagger.annotations.ApiOperation;
import org.springframework.security.crypto.password.PasswordEncoder;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;
```
```
import javax.annotation.Resource;
```
```
@Api(value = "用户信息、基础学习DEMO", tags = {"用户信息、基础学习DEMO"})
@RestController
@RequestMapping("/api/user" )
public class UserController
{
```
```
@Resource
private FrontUserEndSessionServiceImpl userService;
/**
* 注入全局的加密器
*/
@Resource
PasswordEncoder passwordEncoder;
```
```
@GetMapping("/detail/v1" )
@ApiOperation(value = "获取用户信息" )
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
```

###### uaa-provider 的运行结果

```
public RestOut<UserDTO> getUser(@RequestParam(value = "userId", required
= true) Long userId)
{
UserDTO dto = userService.getUser(userId);
if (null == dto)
{
return RestOut.error("没有找到用户" );
}
return RestOut.success(dto).setRespMsg("操作成功" );
}
```
```
@GetMapping("/passwordEncoder/v1" )
@ApiOperation(value = "密码加密" )
public RestOut<String> passwordEncoder(
@RequestParam(value = "raw", required = true) String raw)
{
```
```
// passwordEncoder =
PasswordEncoderFactories.createDelegatingPasswordEncoder();
String encode = passwordEncoder.encode(raw);
return RestOut.success(encode);
}
}
```
```
34
```
```
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
```
```
52
53
54
55
```

获取用户信息：

```
注：本文以 PDF 持续更新，最新尼恩 架构笔记、面试题 的PDF文件，请从下面的链接获取：语雀
或者 码云
```
#### demo-provider 完成 RPC 远程调用

demo-provider 使用 Feign+Ribbon 进行 RPC 远程调用时，对每一个 Java 远程调用接口，Feign 都会
生成了一个 RPC 远程调用客户端实现类，只是，该实现类对于开发者来说是透明的，开发者感觉不到这
个类的存在。

需要在 Maven 的 pom 文件中，增加以下 Feign+Ribbon 集成模块的依赖：


客户端 RPC 实现类位于远程调用 Java 接口和服务提供者 Provider 之间，承担了以下职责：

（ 1 ）拼装 REST 请求：根据 Java 接口的参数，拼装目标 REST 接口的 URL；

（ 2 ）发送请求和获取结果：通过 Java HTTP 组件（如 HttpClient）调用服务提供者 Provider 的 REST
接口，并且获取 REST 响应；

（ 3 ）结果解码：解析 REST 接口的响应结果，封装成目标 POJO 对象（Java 接口的返回类型），并且返
回。

###### REST 服务的本地代理接口

该接口的具体的代码，在 **uaa-client 模块** 中，如下图所示：

具体的代码如下：

```
<!--导入 SpringCloud Ribbon -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-netflix-ribbon</artifactId>
</dependency>
```
```
<!--添加Feign依赖-->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
package com.crazymaker.springcloud.user.info.remote.client;
```
```
import com.crazymaker.springcloud.common.dto.UserDTO;
```
```
1
2
3
```

###### 通过 REST 服务的本地代理接口，进行 RPC 调用

进行 RPC 调用的具体的代码，在 **demo-provider** 模块中，如下图所示：

```
import com.crazymaker.springcloud.common.result.RestOut;
import com.crazymaker.springcloud.standard.config.FeignConfiguration;
import
com.crazymaker.springcloud.user.info.remote.fallback.UserClientFallback;
import
com.crazymaker.springcloud.user.info.remote.fallback.UserClientFallbackFacto
ry;
import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;
import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.RequestParam;
```
```
/**
* Feign 客户端接口
* @description: 用户信息 远程调用接口
* @date 2019年 7 月 22 日
*/
```
```
@FeignClient(value = "uaa-provider",
configuration = FeignConfiguration.class,
fallback = UserClientFallback.class,
// fallbackFactory = UserClientFallbackFactory.class,
path = "/uaa-provider/api/user")
public interface UserClient
{
/**
* 远程调用 RPC 方法：获取用户详细信息
* @param userId 用户 Id
* @return 用户详细信息
*/
@RequestMapping(value = "/detail/v1", method = RequestMethod.GET)
RestOut<UserDTO> detail(@RequestParam(value = "userId") Long userId);
}
```
```
4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
```
```
package com.crazymaker.springcloud.demo.controller;
```
```
import com.alibaba.fastjson.JSONObject;
import com.alibaba.fastjson.TypeReference;
import com.crazymaker.springcloud.common.dto.UserDTO;
import com.crazymaker.springcloud.common.result.RestOut;
import com.crazymaker.springcloud.common.util.JsonUtil;
import com.crazymaker.springcloud.user.info.remote.client.UserClient;
import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;
import io.swagger.annotations.Api;
import io.swagger.annotations.ApiOperation;
import org.springframework.boot.web.client.RestTemplateBuilder;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```

demo-provider 需要依赖 **uaa-client 模块**

###### 启动 demo-provider

```
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.client.RestTemplate;
```
```
import javax.annotation.Resource;
```
```
@RestController
@RequestMapping("/api/call/uaa/")
@Api(tags = "演示 uaa-provider 远程调用")
public class UaaRemoteCallController
{
```
```
//注入 @FeignClient注解配置 所配置的 客户端实例
@Resource
UserClient userClient;
```
```
@GetMapping("/user/detail/v2")
@ApiOperation(value = "Feign 远程调用")
public RestOut<JSONObject> remoteCallV2(
@RequestParam(value = "userId") Long userId)
{
RestOut<UserDTO> result = userClient.detail(userId);
JSONObject data = new JSONObject();
data.put("uaa-data", result);
return RestOut.success(data).setRespMsg("操作成功");
}
}
```
```
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
```

访问 swagger ui：

###### 通过 swagger 执行 RPC 操作


## SpringCloud Eureka 服务注册

Eureka 作为老牌 SpringCloud 注册中心，很多项目，仍然在使用，

另外，底层原理都是想通的，大家可以和 nacos 对比学习

Eureka 的详细介绍，请阅读《Java 高并发核心编程卷 3 加强版》

## SpringCloud Config 统一配置

SpringCloud Config 作为老牌 SpringCloud 配置中心，很多项目，仍然在使用，

另外，底层原理都是想通的，大家可以和 nacos 对比学习

SpringCloud Config 的详细介绍，请阅读《Java 高并发核心编程卷 3 加强版》

## Nacos 服务注册+ 统一配置


#### 1 、Nacos 优势

```
问题，既然有了Eureka ，为啥还要用Nacos？
```
而 Nacos 作为微服务核心的服务注册与发现中心，让大家在 Eureka 和 Consule 之外有了新的选择，开
箱即用，上手简洁，暂时也没发现有太大的坑。

###### 1.1 与 eureka 对比

```
1 eureka 2.0闭源码了。
```
```
2 从官网来看nacos 的注册的实例数是大于eureka的,
```
```
3 因为nacos使用的raft协议,nacos集群的一致性要远大于eureka集群.
```
分布式一致性协议 Raft，自 2013 年论文发表，之后就受到了技术领域的热捧，与其他的分布式一致性
算法比，Raft 相对比较简单并且易于实现，这也是 Raft 能异军突起的主要因素。

#### Raft 的数据一致性策略

```
Raft 协议强依赖 Leader 节点来确保集群数据一致性。即 client 发送过来的数据均先到达 Leader
节点，Leader 接收到数据后，先将数据标记为 uncommitted 状态，随后 Leader 开始向所有
Follower 复制数据并等待响应，在获得集群中大于 N/2 个 Follower 的已成功接收数据完毕的响应
后，Leader 将数据的状态标记为 committed，随后向 client 发送数据已接收确认，在向 client 发
送出已数据接收后，再向所有 Follower 节点发送通知表明该数据状态为committed。
```
###### 1.2 与 springcloud config 对比


**三大优势：**

```
springcloud config大部分场景结合git 使用, 动态变更还需要依赖Spring Cloud Bus 消息总线来通
过所有的客户端变化.
springcloud config不提供可视化界面
nacos config使用长连接更新配置, 一旦配置有变动后，通知Provider的过程非常的迅速, 从速度上
秒杀springcloud原来的config几条街,
```
#### 2 、Spring Cloud Alibaba 套件

目前 Spring Cloud Alibaba 主要有三个组件：

```
Nacos：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。
Sentinel：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定
性。
AliCloud OSS: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海
量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任
意类型的数据。
```
###### Spring Cloud Alibaba 套件和 Spring Cloud Netflix 套件类比

仔细看看各组件的功能描述，Spring Cloud Alibaba 套件和 Spring Cloud Netflix 套件大致的对应关系：

```
Nacos = Eureka/Consule + Config + Admin
Sentinel = Hystrix + Dashboard + Turbine
Dubbo = Ribbon + Feign
RocketMQ = RabbitMQ
Schedulerx = Quartz
AliCloud OSS、AliCloud SLS 这三个应该是独有的
```
链路跟踪（Sleuth、Zipkin）不知道会不会在 Sentinel 里

以上只是猜测，待我从坑里爬出来之后再回来更新。也欢迎大家一起交流探讨~

这里我就先试试 Nacos。

#### 3 、Nacos 的架构和安装

###### 3.1 Nacos 的架构


这是 Nacos 的架构图，可以看到它确实是融合了服务注册发现中心、配置中心、服务管理等功能，类似
于 Eureka/Consule + Config + Admin 的合体。

另外通过官方文档发现，Nacos 除了可以和 Spring Cloud 集成，还可以和 Spring、SpringBoot 进行集
成。

不过我们只关注于 Spring Cloud，别的就略过了，直接上手实战吧。

###### 3.2 Nacos Server 的下载和安装

在使用 Nacos 之前，需要先下载 Nacos 并启动 Nacos Server。

```
安装的参考教程：
```
```
https://www.cnblogs.com/crazymakercircle/p/11992539.html
```
#### 4 、Nacos Server 的运行

###### 4.1 两种模式

Nacos Server 有两种运行模式：

```
standalone
cluster
```
###### 4.2 standalone 模式

此模式一般用于 demo 和测试，不用改任何配置，直接敲以下命令执行

Windows 的话就是

然后从 [http://cdh1:8848/nacos/index.html](http://cdh1:8848/nacos/index.html) 进入控制台就能看到如下界面了

```
1 sh bin/startup.sh -m standalone
```
```
1 cmd bin/startup.cmd -m standalone
```

默认账号和密码为：nacos nacos

###### 4.3 cluster 模式

测试环境，可以先用 standalone 模式撸起来，享受 coding 的快感，但是，生产环境可以使用 cluster
模式。

**cluster 模式需要依赖 MySQL，然后改两个配置文件：**

大致如下：

1 ： cluster. conf，填入要运行 Nacos Server 机器的 ip

```
2. 修改NACOS_PATH/conf/application.properties，加入 MySQL 配置
```
创建一个名为 nacos_config 的 database，将 NACOS_PATH/conf/nacos-mysql. sql 中的表结构导入刚才
创建的库中，这几张表的用途就自己研究吧

###### 4.4 Nacos Server 的配置数据是存在哪里呢？

```
问题来了： Nacos Server 的配置数据是存在哪里呢？
```
我们没有对 Nacos Server 做任何配置，那么数据只有两个位置可以存储：

```
内存
本地数据库
```
如果我们现在重启刚刚在运行的 Nacos Server，会发现刚才加的 nacos. properties 配置还在，说明不
是内存存储的。

这时候我们打开 NACOS_PATH/data，会发现里边有个 derby-data 目录，我们的配置数据现在就存储在
这个库中。

```
Derby 是 Java 编写的数据库，属于 Apache 的一个开源项目
```
如果将数据源改为我们熟悉的 MySQL 呢？当然可以。

```
注意：不支持 MySQL 8.0 版本
```
这里有两个坑：

```
Nacos Server 的数据源是用 Derby 还是 MySQL 完全是由其运行模式决定的：
```
```
conf/cluster.conf
conf/application.properties
```
```
1
2
```
```
192.168.100.155
192.168.100.156
```
```
1
2
```
```
db.num= 1
db.url.0=jdbc:mysql://localhost:3306/nacos_config?
characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=t
rue
db.user=root
db.password=root
```
```
1
2
```
```
3
4
```

官方提供的 cluster. conf 示例如下

以上配置结束后，运行 Nacos Server 就能看到效果了。

#### 5 、实战 1 ：使用 Nacos 作为注册中心

###### 实战的工程

实战的工程的目录结构如下：

###### 5.1 如何使用 Nacos Client 组件

**首先引入 Spring Cloud Alibaba 的 BOM**

```
standalone 的话仅会使用 Derby，即使在 application.properties 里边配置 MySQL 也照样
无视；
cluster 模式会自动使用 MySQL，这时候如果没有 MySQL 的配置，是会报错的。
```
```
1
```
```
2
```
```
#it is ip
#example
10.10.109.214
11.16.128.34
11.16.128.36
```
```
1
2
3
4
5
```
```
<parent>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-parent</artifactId>
<version>2.0.4.RELEASE</version>
<relativePath/>
</parent>
<properties>
```
```
1 2 3 4 5 6 7
```

这里版本号有坑，文档上说和 Spring Boot 2.0. x 版本兼容，但是实测 2.0.6. RELEASE 报错

###### 5.2 演示的模块结构

服务注册中心和服务发现的服务端都是由 Nacos Server 来提供的，我们只需要提供 Service 向其注册就
好了。

这里模拟提供两个 service：provider 和 consumer

###### 5.3 provider 微服务

```
<spring-cloud.version>Finchley.SR2</spring-cloud.version>
<spring-cloud-alibaba.version>0.2.0.RELEASE</spring-cloud-
alibaba.version>
</properties>
<dependencyManagement>
<dependencies>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-alibaba-dependencies</artifactId>
<version>${spring-cloud-alibaba.version}</version>
<type>pom</type>
<scope>import</scope>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-dependencies</artifactId>
<version>${spring-cloud.version}</version>
<type>pom</type>
<scope>import</scope>
</dependency>
</dependencies>
</dependencyManagement>
```
```
8
9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
```
```
java.lang.NoClassDefFoundError:
org/springframework/core/env/EnvironmentCapable
```
```
1
```
```
alibaba
├── service-provider-demo
│ ├── pom.xml
│ └── src
└── sevice-consumer-demo
│ ├── pom.xml
│ └── src
└── pom.xml
```
```
1 2 3 4 5 6 7 8
```

**step 1：在 provider 和 consumer 的 pom 添加以下依赖：**

**step 2：启动类**

使用 Spring Cloud 的原生注解 @EnableDiscoveryClient 开启服务注册与发现

```
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
</dependency>
```
```
1 2 3 4 5 6 7 8
```
```
package com.crazymaker.cloud.nacos.demo.starter;
```
```
import com.crazymaker.springcloud.standard.context.SpringContextUtil;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.core.env.Environment;
import springfox.documentation.swagger2.annotations.EnableSwagger2;
```
```
import java.util.List;
```
```
@EnableSwagger2
@SpringBootApplication
@EnableDiscoveryClient
@Slf4j
public class ServiceProviderApplication {
public static void main(String[] args) {
ConfigurableApplicationContext applicationContext =
SpringApplication.run(ServiceProviderApplication.class, args);
```
```
Environment env = applicationContext.getEnvironment();
String port = env.getProperty("server.port");
String path = env.getProperty("server.servlet.context-path");
System.out.println("\n--------------------------------------\n\t" +
"Application is running! Access URLs:\n\t" +
"Local: \t\thttp://localhost:" + port + path+
"/index.html\n\t" +
"swagger-ui: \thttp://localhost:" + port + path + "/swagger-
ui.html\n\t" +
"----------------------------------------------------------
");
```
```
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
```
```
21
22
23
24
25
26
27
```
```
28
```
```
29
```
```
30
31
32
```

**step 3：服务提供者的 Rest 服务接口**

service-provider-demo 提供一个非常简单的 Rest 服务接口以供访问

**step 4：配置文件**

**step 5：启动之后，通过 swagger UI 访问：**

```
package com.crazymaker.cloud.nacos.demo.controller;
```
```
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.RestController;
```
```
@RestController
@RequestMapping("/echo")
public class EchoController {
//回显服务
@RequestMapping(value = "/{string}", method = RequestMethod.GET)
public String echo(@PathVariable String string) {
return "echo: " + string;
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```
```
spring:
application:
name: service-provider-demo
cloud:
nacos:
discovery:
server-addr: ${NACOS_SERVER:cdh1: 8848 }
server:
port: 18080
```
```
1 2 3 4 5 6 7 8 9
```

###### 5.4 Consumer 微服务演示 RPC 远程调用

在 NacosConsumerApplication 中集成 RestTemplate 和 Ribbon

**消费者的 controller 类**

```
@LoadBalanced
@Bean
public RestTemplate restTemplate() {
return new RestTemplate();
}
```
```
1
2
3
4
5
```
```
package com.crazymaker.cloud.nacos.demo.consumer.controller;
```
```
import com.crazymaker.cloud.nacos.demo.consumer.client.EchoClient;
import io.swagger.annotations.Api;
import io.swagger.annotations.ApiOperation;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.RestController;
```
```
import javax.annotation.Resource;
```
```
@RestController
@RequestMapping("/echo")
@Api(tags = "服务- 消费者")
public class EchoConsumerController {
```
```
//注入 @FeignClient 注解配置 所配置的 EchoClient 客户端Feign实例
@Resource
EchoClient echoClient;
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
```

**消费者配置文件**

**通过 swagger UI 访问消费者：**

访问远程的 echo API：

```
//回显服务
@ApiOperation(value = "消费回显服务接口")
@RequestMapping(value = "/{string}", method = RequestMethod.GET)
public String echoRemoteEcho(@PathVariable String string) {
return "provider echo is:" + echoClient.echo(string);
}
}
```
```
24
25
26
27
28
29
30
```
```
spring:
application:
name: sevice-consumer-demo
cloud:
nacos:
discovery:
server-addr: 127.0.0.1: 8848
server:
port: 18081
```
```
1 2 3 4 5 6 7 8 9
```

###### 5.5 涉及到的演示地址：

服务提供者 service-provider-demo：

[http://localhost:18080/provider/swagger-ui.html#/Echo_%E6%BC%94%E7%A4%BA](http://localhost:18080/provider/swagger-ui.html#/Echo_%E6%BC%94%E7%A4%BA)

服务消费者：

[http://localhost:18081/consumer/swagger-ui.html#/%E6%9C%8D%E5%8A%A1-](http://localhost:18081/consumer/swagger-ui.html#/%E6%9C%8D%E5%8A%A1-)
%20%E 6%B 6%88%E 8%B 4%B 9%E 8%80%85/echoRemoteEchoUsingGET

注册中心 Nacos：


[http://cdh1:8848/nacos/index.html#/serviceManagement?dataId=&group=&appName=&namesp](http://cdh1:8848/nacos/index.html#/serviceManagement?dataId=&group=&appName=&namesp)
ace=

###### 5.6 Nacos Console

这时候查看 Nacos Console 也能看到已注册的服务列表及其详情

#### 6 、实战 2 ：使用 Nacos 作为配置中心

###### 6.1 基本概念

**1 ）Profile**

Java 项目一般都会有多个 Profile 配置，用于区分开发环境，测试环境，准生产环境，生成环境等，每个
环境对应一个 properties 文件（或是 yml/yaml 文件），然后通过设置 spring. profiles. active 的值来决定
使用哪个配置文件。

例子：

Nacos Config 的作用就把这些文件的内容都移到一个统一的配置中心，即方便维护又支持实时修改后动
态刷新应用。

```
spring:
application:
name: sharding-jdbc-provider
jpa:
hibernate:
ddl-auto: none
dialect: org.hibernate.dialect.MySQL5InnoDBDialect
show-sql: true
profiles:
active: sharding-db-table  # 分库分表配置文件
#active: atomiclong-id # 自定义主键的配置文件
#active: replica-query # 读写分离配置文件
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```

**2 ）Data ID**

当使用 Nacos Config 后，Profile 的配置就存储到 Data ID 下，即一个 Profile 对应一个 Data ID

Data ID 的拼接格式：${prefix} - ${spring. profiles. active}. ${file-extension}

```
prefix 默认为 spring.application.name 的值，也可以通过配置项
spring.cloud.nacos.config.prefix 来配置
spring.profiles.active 取 spring.profiles.active 的值，即为当前环境对应的 profile
file-extension 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension
来配置
```
**3 ）Group**

Group 默认为 DEFAULT_GROUP，可以通过 spring. cloud. nacos. config. group 来配置，当配置项太多
或者有重名时，可以通过分组来方便管理

最后就和原来使用 springcloud 一样通过@RefreshScope 和@Value 注解即可

###### 6.2 通过 Nacos 的 console 去增加配置

这回首先要在 nacos 中配置相关的配置，打开 Nacos 配置界面，依次创建 2 个 Data ID

```
nacos-config-demo-dev.yaml 开发环境的配置
nacos-config-demo-test.yaml 测试环境的配置
```
**1 ）nacos-config-demo-dev. yaml**

内容如下图：


**2 ）nacos-config-demo-sit. yaml**

```
内容如下图：
```
###### 6.3 使用 Nacos Config Client 组件

问题 2 ：微服务 Provider 实例上，如何使用 Nacos Config Client 组件的有哪些步骤？

**1 ）加载 nacos config 的客户端依赖：**

**启动类**

```
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
<version>${nacos.version}</version>
</dependency>
```
```
1
2
3
4
5
```
```
package com.crazymaker.cloud.nacos.demo.consumer.starter;
```
```
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import
org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration;
import
org.springframework.boot.autoconfigure.data.redis.RedisRepositoriesAutoConfi
guration;
import
org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration;
import
org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAuto
Configuration;
```
```
1 2 3 4 5 6 7 8 9
```

```
import
org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration
;
import
org.springframework.boot.autoconfigure.security.servlet.SecurityAutoConfigur
ation;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
import org.springframework.cloud.openfeign.EnableFeignClients;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.core.env.Environment;
import springfox.documentation.swagger2.annotations.EnableSwagger2;
```
```
@EnableSwagger2
@EnableDiscoveryClient
@Slf4j
@SpringBootApplication(
scanBasePackages =
{
"com.crazymaker.cloud.nacos.demo",
"com.crazymaker.springcloud.standard"
},
exclude = {SecurityAutoConfiguration.class,
//排除db的自动配置
DataSourceAutoConfiguration.class,
DataSourceTransactionManagerAutoConfiguration.class,
HibernateJpaAutoConfiguration.class,
//排除redis的自动配置
RedisAutoConfiguration.class,
RedisRepositoriesAutoConfiguration.class})
//启动Feign
@EnableFeignClients(basePackages =
{"com.crazymaker.cloud.nacos.demo.consumer.client"})
public class ConfigDomeProviderApplication
{
public static void main(String[] args)
{
ConfigurableApplicationContext applicationContext = null;
try
{
applicationContext =
SpringApplication.run(ConfigDomeProviderApplication.class, args);
System.out.println("Server startup done.");
} catch (Exception e)
{
log.error("服务启动报错", e);
return;
}
```
```
Environment env = applicationContext.getEnvironment();
String port = env.getProperty("server.port");
String path = env.getProperty("server.servlet.context-path");
System.out.println("\n----------------------------------------------
------------\n\t" +
"Application is running! Access URLs:\n\t" +
"Local: \t\thttp://localhost:" + port + path +
"/index.html\n\t" +
"swagger-ui: \thttp://localhost:" + port + path + "/swagger-
ui.html\n\t" +
```
10

11

12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45

46
47
48
49
50
51
52
53
54
55
56

57
58

59


**控制类：**

**2 ）bootstrap 配置文件**

然后是在配置文件 (bootstrap. yml) 中加入以下的内容：

```
"----------------------------------------------------------
");
```
```
}
}
```
```
60
```
```
61
62
63
```
```
package com.crazymaker.cloud.nacos.demo.config.controller;
```
```
import io.swagger.annotations.Api;
import io.swagger.annotations.ApiOperation;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.RestController;
```
```
import javax.annotation.Resource;
```
```
@RestController
@RequestMapping("/config")
@Api(tags = "Nacos 配置中心演示")
public class ConfigGetController {
```
```
@Value("${foo.bar:empty}")
private String bar;
```
```
@Value("${spring.datasource.username:empty}")
private String dbusername;
```
```
//获取配置的内容
@ApiOperation(value = "获取配置的内容")
@RequestMapping(value = "/bar", method = RequestMethod.GET)
public String getBar() {
return "bar is :"+bar;
}
//获取配置的内容
@ApiOperation(value = "获取配置的db username")
@RequestMapping(value = "/dbusername", method = RequestMethod.GET)
public String getDbusername() {
return "db username is :"+bar;
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
```
```
spring:
application:
name: nacos-config-demo-provider
```
```
1
2
3
```

###### 6.4 测试结果

启动程序，通过 swagger ui 访问：

```
http://localhost:18083/config/swagger-ui.html#
```
执行结果如下：

```
profiles:
active:  dev
cloud:
nacos:
discovery:
server-addr: ${NACOS_SERVER:cdh1: 8848 }
config:
server-addr: ${NACOS_SERVER:cdh1: 8848 }
prefix: nacos-config-demo
group: DEFAULT_GROUP
file-extension: yaml
server:
port: 18083
servlet:
context-path: /config
```
```
4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
```

###### 6.4 可以端如何与服务端的配置文件相互对应

```
config.prefix 来对应主配置文件
使用spring.cloud.nacos.config.ext-config 选项来对应更多的文件
```
```
eg：
```
```
spring:
application:
name: nacos-config-demo-provider
profiles:
active: dev
cloud:
nacos:
discovery:
server-addr: ${NACOS_SERVER:cdh1: 8848 }
config:
server-addr: ${NACOS_SERVER:cdh1: 8848 }
prefix: nacos-config-demo
group: DEFAULT_GROUP
file-extension: yaml
ext-config:
```
- data-id: crazymaker-db-dev. yml
group: DEFAULT_GROUP
refresh: true
- data-id: crazymaker-redis-dev. yml
group: DEFAULT_GROUP
refresh: true
- data-id: crazymaker-common-dev. yml
group: DEFAULT_GROUP
refresh: true
- data-id: some. properties
group: DEFAULT_GROUP

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
```

启动程序，发现可以获取到其他 data-id 的配置 ,大家可以自行配置。

#### 7 、配置的隔离

在实际的应用中，存在着以下几种环境隔离的要求：

1 、开发环境、测试环境、准生产环境和生产环境需要隔离

2 、不同项目需要隔离

3 、同一项目，不同的模块需要隔离

可以通过三种方式来进行配置隔离：Nacos 的服务器、namespace 命名空间、group 分组，在
bootstrap. yml 文件中可以通过配置 Nacos 的 server-addr、namespace 和 group 来区分不同的配置信
息。

```
Nacos的服务器 spring.cloud.nacos.config.server-addr
Nacos的命名空间 spring.cloud.nacos.config.namespace，注意，这里使用命名空间的ID不是
名称
Nacos的分组 spring.cloud.nacos.config.group
```
#### 8 、nacos 集群搭建

如果我们要搭建集群的话，那么肯定是不能用内嵌的数据库，不然数据无法共享。所以，集群搭建的时
候我们需要将 Nacos 对接 Mysql 进行数据存储。

集群模式跟我们平时进行扩容是一样的，可以通过 Nginx 转发到多个节点，最前面挂一个域名即可，如
下图：

```
27 refresh: true
```

###### IP 规划

通常如果我们只是为了体验的话，直接在本地起动 3 个实例就可以了，没必要真的去搞三台服务器，下面
我们就以在本地的方式来搭建集群。将 Nacos 的解压包复制分成 3 份，分别是:

nacos
nacos 1
nacos 2

进入 nacos 的 conf 目录，编辑 application. properties 文件，增加数据库配置

复制代码同样的步骤进入 nacos 1 和 nacos 2 操作一遍，唯一需要修改的就是 application. properties 文件
中的 server. port，默认 nacos 的 server. port=8848，

我们在本地启动三个实例，那么端口肯定会冲突，所以其他 2 个实例的端口我们需要进行修改，比如
nacos 1 修改成 8847 ，nacos 2 修改成 8846 。

数据库配置信息好了后，我们需要将对应的数据库和表进行初始化，数据库脚本在 conf 目录下的 nacos-
mysql. sql 中，执行即可。

最后一步需要配置一份集群节点信息，配置文件在 conf 目录下的 cluster. conf. example 文件，我们进行
重命名成 cluster. conf。然后编辑 cluster. conf 文件，增加 3 个节点的信息，格式为 IP: PORT，三个目录都
一致即可。

启动的话直接到 bin 目录下，执行./startup. sh 就可以了，默认就是集群模式，不需要加任何参数。

###### 集群的使用

上面的集群，虽然可用，但仍不是真正的集群，我们一般不会这么用。nacos 集群的使用一般有 4 种方
式：

```
http://ip1:port/openAPI 直连ip模式，不同的节点，则需要修改ip才可以使用。
http://VIP:port/openAPI VIP模式高可用，客户端vip即可，VIP下面挂server真实ip，部署比较麻
烦，需要部署vip（keepalive）。
```
```
# 指定数据源为Mysql
spring.datasource.platform=mysql
```
```
# 数据库实例数量
db.num= 1
db.url.0=jdbc:mysql://localhost:3306/nacos?
characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=t
rue
db.user=root
db.password= 123456
```
```
1 2 3 4 5 6 7 8
```
```
127.0.0.1:8848
127.0.0.1:8847
127.0.0.1:8846
```
```
1
2
3
```

```
http://nacos.com:port/openAPI 域名模式，可读性好，而且换ip方便，在host文件配置本地域名
即可。
http://反向代理:port/openAPI 反向代理模式
```
```
这里介绍一下反向代理模式。
```
关于 Nginx 的安装和配置，本文就不进行讲解了，不会的可以自己去尝试下，反向代理模式核心配置如
下：

整体来说，nacos 的集群搭建方式还是挺简单的，没什么特别要注意的，最好是能通过域名的方式来进
行访问，另外数据库这块如果上生产环境，也需要考虑高可用问题，至少也得有个主从。

8648 的 nginx 提供的 nacos 服务接口，可以自定义。我们访问

[http://localhost:8648/nacos/#/clusterManagement?dataId=&group=&appName=&namespace=&s](http://localhost:8648/nacos/#/clusterManagement?dataId=&group=&appName=&namespace=&s)
erverId=

，就可以看到：

```
upstream nacos_server {
server 127.0.0.1:8848;
server 127.0.0.1:8847;
server 127.0.0.1:8846;
}
```
```
server {
listen 8648;
server_name localhost;
#charset koi8-r;
#access_log logs/host.access.log main;
location / {
proxy_pass http://nacos_server;
index index.html index.htm;
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```

我们可以简单测试一下，杀掉一个的 nacos ，看服务是否正常。后面，我们对微服务提供 nacos 服务的
时候，只要配置这个 nginx 端口就好了！！

## Nacos 高可用架构与实操

当我们在聊高可用时，我们在聊什么？

```
系统可用性达到 99.99%
在分布式系统中，部分节点宕机，依旧不影响系统整体运行
服务端集群化部署多个节点
```
Nacos 高可用，则是 Nacos 为了提升系统稳定性而采取的一系列手段。

Nacos 的高可用不仅仅存在于服务端，同时也存在于客户端，以及一些与可用性相关的功能特性中，这
些点组装起来，共同构成了 Nacos 的高可用。

#### 客户端高可用

先统一一下语义，在微服务架构中一般会有三个角色：

```
Consumer
Provider
Registry
```
以上的 registry 角色是 nacos-server，而 Consumer 角色和 Provider 角色都是 nacos-client。


###### 客户端高可用的方式一：配置多个 nacos-server

在生产环境，我们往往需要搭建 Nacos 集群，代码中，是这样配置的：

当其中一台 Nacos server 机器宕机时，为了不影响整体运行，客户端会存在重试机制。

```
server:
port: 8081
spring:
cloud:
nacos:
server-addr: 127.0.0.1:8848,127.0.0.1:8848,127.0.0.1: 8848
```
```
1 2 3 4 5 6
```
```
package com.alibaba.nacos.client.naming.net;
```
```
/**
* @author nkorange
*/
public class NamingProxy {
```
```
//api注册
```
```
public String reqAPI(String api, Map<String, String> params, String body,
List<String> servers, String method) throws NacosException {
```
```
params.put(CommonParams.NAMESPACE_ID, getNamespaceId());
```
```
if (CollectionUtils.isEmpty(servers) && StringUtils.isEmpty(nacosDomain)) {
throw new NacosException(NacosException.INVALID_PARAM, "no server
available");
}
```
```
NacosException exception = new NacosException();
```
```
if (servers != null && !servers.isEmpty()) {
```
```
Random random = new Random(System.currentTimeMillis());
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
12
13
14
15
16
```
```
17
18
19
20
21
22
23
```

```
参数名 含义 可选值 默认值
支持
版本
```
```
endpoint
连接Nacos Server指定的连
接点，可以参考文档
```
```
域名 空
>=
0.1.0
```
```
endpointPort
```
```
连接Nacos Server指定的连
接点端口，可以参考文档
合法端口号 空
```
```
>=
0.1.0
```
```
namespace 命名空间的ID 命名空间的ID
```
```
config模块为
空，naming
模块为public
```
```
>=
0.8.0
```
```
serverAddr
```
```
Nacos Server的地址列表，
这个值的优先级比endpoint
高
```
```
ip:port,ip:port,... 空
```
```
>=
0.1.0
```
```
JM.LOG.PATH(-
D)
```
```
客户端日志的目录 目录路径 用户根目录
>=
0.1.0
```
该可用性保证存在于 nacos-client 端。

###### Nacos Java Client 通用参数

###### 客户端高可用的方式二：本地缓存文件 Failover 机制

注册中心发生故障最坏的一个情况是整个 Server 端宕机，如果三个 Server 端都宕机了，怎么办呢？

```
这时候 Nacos 依旧有高可用机制做兜底。
```
```
int index = random.nextInt(servers.size());
```
```
//拿到地址列表，在请求成功之前逐个尝试，直到成功为止
```
```
for (int i = 0 ; i < servers.size(); i++) {
String server = servers.get(index);
try {
return callServer(api, params, body, server, method);
} catch (NacosException e) {
exception = e;
if (NAMING_LOGGER.isDebugEnabled()) {
NAMING_LOGGER.debug("request {} failed.", server, e);
}
}
index = (index + 1 ) % servers.size();
}
}
...
```
```
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
```

```
参数名 含义 可选值 默认值 支持版本
```
```
namingLoadCacheAtStart 启动时是否优先读取本地缓存 true/false false >=1.0.0
```
```
namingClientBeatThreadCount 客户端心跳的线程池大小 正整数 机器的CPU数的一半 >=1.0.0
```
```
namingPollingThreadCount
```
```
客户端定时轮询数
据更新的线程池大
小
```
```
正整数 机器的CPU数的一半 >=1.0.0
```
```
com.alibaba.nacos.naming.cache.dir(-D) 客户端缓存目录 目录路径 {user.home}/nacos/naming >=1.0.0
```
```
com.alibaba.nacos.naming.log.level(-D) Naming日志级别客户端的 info,error,warn等 info >=1.0.0
```
```
com.alibaba.nacos.client.naming.tls.enable(-
D) 是否打开HTTPS true/false false^
```
**本地缓存文件 Failover 机制**

一道经典的高可用的面试题：

```
当 springcloud 应用运行时，Nacos 注册中心宕机，会不会影响 RPC 调用。
```
这个题目大多数人，应该都不能回答出来.

Nacos 存在本地文件缓存机制，nacos-client 在接收到 nacos-server 的服务推送之后，会在内存中保存
一份，随后会落盘存储一份快照 snapshot 。有了这份快照，本地的 RPC 调用，还是能正常的进行。

```
关键是，这个本地文件缓存机制，默认是关闭的。
```
Nacos 注册中心宕机，Dubbo /springcloud 应用发生重启，会不会影响 RPC 调用。如果了解了 Nacos
的 Failover 机制，应当得到和上一题同样的回答：不会。

**客户端 Naming 通用参数**

snapshot 默认的存储路径为：{USER_HOME}/nacos/naming/ 中：

这份文件有两种价值，一是用来排查服务端是否正常推送了服务；二是当客户端加载服务时，如果无法
从服务端拉取到数据，会默认从本地文件中加载。

在生产环境，推荐开启该参数，以避免注册中心宕机后，导致服务不可用，在服务注册发现场景，可用
性和一致性 trade off 时，我们大多数时候会优先考虑可用性。

另外：{USER_HOME}/nacos/naming/{namespace} 下除了缓存文件之外还有一个 failover 文件夹，里
面存放着和 snapshot 一致的文件夹。

这是 Nacos 的另一个 failover 机制，snapshot 是按照某个历史时刻的服务快照恢复恢复，而 failover
中的服务可以人为修改，以应对一些极端场景。


该可用性保证存在于 nacos-client 端。

#### Nacos 两种健康检查模式

###### agent 上报模式

**客户端（注册在 nacos 上的其它微服务实例）健康检查。**

客户端通过心跳上报方式告知服务端 (nacos 注册中心) 健康状态；

默认心跳间隔 5 秒；

nacos 会在超过 15 秒未收到心跳后将实例设置为不健康状态；

超过 30 秒将实例删除；

###### 服务端主动检测

**服务端健康检查。**

nacos 主动探知客户端健康状态，默认间隔为 20 秒；

健康检查失败后实例会被标记为不健康，不会被立即删除。

###### 临时实例

**临时实例通过 agent 上报模式实现健康检查。**

Nacos 在 1.0.0 版本 instance 级别增加了一个 ephemeral 字段，该字段表示注册的实例是否是临时实

例还是持久化实例。

微服务注册为临时实例：

注意： 默认为临时实例，表示为临时实例。

**注册实例支持 ephemeral 字段**

如果是临时实例，则 instance 不会在 Nacos 服务端持久化存储，需要通过上报心跳的方式进行包活，

如果 instance 一段时间内没有上报心跳，则会被 Nacos 服务端摘除。

在被摘除后如果又开始上报心跳，则会重新将这个实例注册。

持久化实例则会持久化被 Nacos 服务端，此时即使注册实例的客户端进程不在，这个实例也不会从服务
端删除，只会将健康状态设为不健康。

```
# 默认true
spring:
cloud:
nacos:
discovery:
ephemeral: true
```
```
1 2 3 4 5 6
```

同一个服务下可以同时有临时实例和持久化实例，这意味着当这服务的所有实例进程不在时，会有部分
实例从服务上摘除，剩下的实例则会保留在服务下。

使用实例的 ephemeral 来判断，ephemeral 为 true 对应的是服务健康检查模式中的 client 模式, 为

```
false对应的是 server 模式。
```
Nacos 1.0.0 之前服务的健康检查模式有三种：client、server 和 none, 分别代表客户端上报、服务端探
测和取消健康检查。在控制台操作的位置如下所示：

在 Nacos 1.0.0 中将把这个配置去掉，改为使用实例的 ephemeral 来判断，ephemeral 为 true 对应的

是服务健康检查模式中的 client 模式, 为 false 对应的是 server 模式。

**临时实例和持久化实例区别**

临时和持久化的区别主要在健康检查失败后的表现，持久化实例健康检查失败后会被标记成不健康，而
临时实例会直接从列表中被删除。

这个特性比较适合那些需要应对流量突增，而弹性扩容的服务，当流量降下来后这些实例自己销毁自己
就可以了，不用再去 nacos 里手动调用注销实例。持久化以后，可以实时看到健康状态，便于做后续的
告警、扩容等一系列处理。


#### Nacos Server 运行模式

Server 的运行模式，是指 Nacos Server 可以运行在多种模式下，当前支持三种模式：

```
AP、
CP
MIXED 。
```
这里的运行模式，使用的是 CAP 理论里的 C、A 和 P 概念。

CAP 原则又称 CAP 定理，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用
性）、Partition tolerance（分区容错性），三者不可得兼。

一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同
一份最新的数据副本）

可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新
具备高可用性）

分区容忍性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一
致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。

CAP 原则的精髓就是要么 AP，要么 CP，要么 AC，但是不存在 CAP。如果在某个分布式系统中数据无副
本，那么系统必然满足强一致性条件，因为只有独一数据，不会出现数据不一致的情况，此时 C 和 P 两
要素具备，但是如果系统发生了网络分区状况或者宕机，必然导致某些数据不可以访问，此时可用性条
件就不能被满足，即在此情况下获得了 CP 系统，但是 CAP 不可同时满足。

基于 CAP 理论，在分布式系统中，数据的一致性、服务的可用性和网络分区容忍性只能三者选二。一般
来说分布式系统需要支持网络分区容忍性，那么就只能在 C 和 A 里选择一个作为系统支持的属性。C 的准
确定义应该是所有节点在同一时间看到的数据是一致的，而 A 的定义是所有的请求都会收到响应。

Nacos 支持 AP 和 CP 模式的切换，这意味着 Nacos 同时支持两者一致性协议。这样，Nacos 能够以一
个注册中心管理这些生态的服务。不过在 Nacos 中，AP 模式和 CP 模式的具体含义，还需要再说明下。

AP 模式为了服务的可能性而减弱了一致性，因此 AP 模式下只支持注册临时实例。AP 模式是在网络分区
下也能够注册实例。在 AP 模式下也不能编辑服务的元数据等非实例级别的数据，但是允许创建一个默认
配置的服务。同时注册实例前不需要进行创建服务的操作，因为这种模式下，服务其实降级成一个简单
的字符创标识，不在存储任何属性，会在注册实例的时候自动创建。

CP 模式下则支持注册持久化实例，此时则是以 Raft 协议为集群运行模式，因此网络分区下不能够注册
实例，在网络正常情况下，可以编辑服务器别的配置。改模式下注册实例之前必须先注册服务，如果服
务不存在，则会返回错误。

MIXED 模式可能是一种比较让人迷惑的模式，这种模式的设立主要是为了能够同时支持临时实例和持久
化实例的注册。这种模式下，注册实例之前必须创建服务，在服务已经存在的前提下，临时实例可以在
网络分区的情况下进行注册。

###### Nacos CP/AP 模式设定

使用如下请求进行 Server 运行模式的设定：

```
curl -X PUT
'$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP'
```
```
1
2
```

###### Nacos CP/AP 模式切换

Nacos 集群默认支持的是 CAP 原则中的 AP 原则.

但是 Nacos 集群可切换为 CP 原则，切换命令如下：

同时微服务的 bootstrap. properties 需配置如下选项指明注册为临时/永久实例
AP 模式不支持数据一致性，所以只支持服务注册的临时实例，CP 模式支持服务注册的永久实例，满足
配置文件的一致性

#### AP/CP 的配套一致性协议

介绍一致性模型之前，需要回顾 Nacos 中的两个概念：临时服务和持久化服务。

```
临时服务（Ephemeral）：临时服务健康检查失败后会从列表中删除，常用于服务注册发现场景。
持久化服务（Persistent）：持久化服务健康检查失败后会被标记成不健康，常用于 DNS 场景。
```
两种模式使用的是不同的一致性协议：

```
临时服务使用的是 Nacos 为服务注册发现场景定制化的私有协议 distro，其一致性模型是 AP；
而持久化服务使用的是 raft 协议，其一致性模型是 CP。
```
###### AP 模式下的 distro 协议

distro 协议的工作流程如下：

```
Nacos 启动时首先从其他远程节点同步全部数据。
Nacos 每个节点是平等的都可以处理写入请求，同时把新数据同步到其他节点。
每个节点只负责部分数据，定时发送自己负责数据的校验值到其他节点来保持数据一致性。
```
如图所示，每个节点负责一部分服务的写入。

```
curl -X PUT '$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?
entry=serverMode&value=CP'
```
```
1
```
```
#false为永久实例，true表示临时实例开启，注册为临时实例
spring.cloud.nacos.discovery.ephemeral=true
```
```
1
2
```

但每个节点都可以接收到写入请求，这时就存在两种情况：

```
当该节点接收到属于该节点负责的服务时，直接写入。
当该节点接收到不属于该节点负责的服务时，将在集群内部路由，转发给对应的节点，从而完成写
入。
```
读取操作则不需要路由，因为集群中的各个节点会同步服务状态，每个节点都会有一份最新的服务数
据。


而当节点发生宕机后，原本该节点负责的一部分服务的写入任务会转移到其他节点，从而保证 Nacos 集
群整体的可用性。

一个比较复杂的情况是，节点没有宕机，但是出现了网络分区，即下图所示：

这个情况会损害可用性，客户端会表现为有时候服务存在有时候服务不存在。

综上，Nacos 的 distro 一致性协议可以保证在大多数情况下，集群中的机器宕机后依旧不损害整体的可
用性。

Nacos 有两个一致性协议：distro 和 raft，distro 协议不会有脑裂问题。


###### CP 模式下的 raft 协议

```
此文还是聚焦于介绍nacos的高可用， raft协议，请参考尼恩的架构师视频。
```
#### 集群内部的特殊的心跳同步服务

心跳机制一般广泛存在于分布式通信领域，用于确认存活状态。

一般心跳请求和普通请求的设计是有差异的，心跳请求一般被设计的足够精简，这样在定时探测时可以
尽可能避免性能下降。

而在 Nacos 中，出于可用性的考虑，一个心跳报文包含了全部的服务信息，这样相比仅仅发送探测信息
降低了吞吐量，而提升了可用性，怎么理解呢？

考虑以下的两种场景：

```
nacos-server 节点全部宕机，服务数据全部丢失。nacos-server 即使恢复运作，也无法恢复出服
务，而心跳包含全部内容可以在心跳期间就恢复出服务，保证可用性。
nacos-server 出现网络分区。由于心跳可以创建服务，从而在极端网络故障下，依旧保证基础的可
用性。
```
调用 OpenApi 依次删除各个服务：

过 5 s 后刷新，服务又再次被注册了上来，符合我们对心跳注册服务的预期。

#### 集群部署模式高可用

最后给大家分享的 Nacos 高可用特性来自于其部署架构。

###### 节点数量

我们知道在生产集群中肯定不能以单机模式运行 Nacos。

那么第一个问题便是：我应该部署几台机器？

Nacos 有两个一致性协议：distro 和 raft，distro 协议不会有脑裂问题，所以理论来说，节点数大于等
于 2 即可；raft 协议的投票选举机制则建议是 2 n+1 个节点。

综合来看，选择 3 个节点是起码的，其次处于吞吐量和更吞吐量的考量，可以选择 5 个， 7 个，甚至 9
个节点的集群。

```
curl -X "DELETE mse-xxx-p.nacos-
ans.mse.aliyuncs.com:8848/nacos/v1/ns/service?
serviceName=providers: com. alibaba. edas. boot. EchoService: 1.0.0:DUBBO&groupName
=DEFAULT_GROUP"
```
```
1
```

###### 多可用区部署

组成集群的 Nacos 节点，应该尽可能考虑两个因素：

```
各个节点之间的网络时延不能很高，否则会影响数据同步。
各个节点所处机房、可用区应当尽可能分散，以避免单点故障。
```
```
以阿里云的 ECS 为例，选择同一个 Region 的不同可用区就是一个很好的实践。
```
###### 部署模式

生产环境，建议使用 k 8 s 部署或者阿里云的 ECS 部署。

考虑的中等公司，都会有运维团队，开发人员不需要参与。

所以，这里介绍的开发人员必须掌握的，docker 模式的部署。

###### 高可用 nacos 的部署架构


###### 高可用 nacos 的部署实操

```
实操这块，使用视频介绍更为清晰，请参考尼恩的架构师视频。
```
#### 总结

本文从多个角度出发，总结了一下 Nacos 是如何保障高可用的。

高可用特性绝不是靠服务端多部署几个节点就可以获得的，而是要结合客户端使用方式、服务端部署模
式、使用场景综合来考虑的一件事。

特别是在服务注册发现场景，Nacos 为可用性做了非常多的努力，而这些保障，ZooKeeper 是不一定有
的。在做注册中心选型时，可用性保障上，Nacos 绝对是优秀的。

## SpringCloud Feign 实现 RPC 远程调用

SpringCloud Feign 作为老牌 RPC 远程调用组件，很多项目，仍然在使用，

并且任然是面试的重点

另外，底层原理都是想通的，大家可以 Feign 和 dubbo 对比学习

SpringCloud Feign 的详细介绍，请阅读《Java 高并发核心编程卷 3 加强版》

## SpringCloud + Dubbo 实现 RPC 远程调用

#### 大背景：全链路异步化的大趋势来了

随着业务的发展，微服务应用的流量越来越大，使用到的资源也越来越多。

在微服务架构下，大量的应用都是 SpringCloud 分布式架构，这种架构总体上是 **全链路同步模式** 。

**全链路同步模式** 不仅造成了资源的极大浪费，并且在流量发生激增波动的时候，受制于系统资源而无法
快速的扩容。

全球后疫情时代，降本增效是大背景。如何降本增效？

可以通过技术升级， **全链路同步模式** ，升级为 **全链路异步模式** 。

先回顾一下全链路同步模式架构图


**全链路同步模式** ，如何升级为 **全链路异步模式** ，就是一个一个环节的异步化。

40 岁老架构师尼恩，持续深化自己的 3 高架构知识宇宙，当然首先要去完成一次牛逼的 **全链路异步模式
微服务实操，下面是尼恩的实操过程、效果、压测数据 (性能足足提升 10 倍多)。**

**全链路异步模式** 改造具体的内容，请参考尼恩的深度文章：全链路异步，让你的 SpringCloud 性能优化
10 倍+

并且，上面的文章，作为尼恩全链路异步的架构知识，收录在《尼恩 Java 面试宝典》V 46 版的架构专题
中

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从这里获取：码云
```
#### SpringCloud + Dubbo 完成 RPC 异步

高并发时代来了，各大项目有越来越强烈的诉求，全链路异步，是性能优化的一个杀手锏。

全链路异步核心环境，就是 RPC 的异步化。

使用 Dubbo 来替换 Feign，足足可以提升 10 倍性能。

所以，SpringCloud + Dubbo RPC 的集成是一个比较刚性的需求。

**有小伙伴查招聘网站，发现很多需要有 SpringCloud + Dubbo 的集成经验，刚好印证了这点。**

接下来，尼恩一步一步带着大家，来做一下 SpringCloud + Dubbo RPC 的集成实验+性能测试。

见证一下奇迹： 使用 Dubbo 提升 10 倍的性能。

#### Dubbo 3 应用的宏观架构

Dubbo 3 应用架构，如下图所示：


从上面的图中，整体的的 Dubbo 的 Rpc 框架中，核心的组件有：

```
config-center 配置中心，接下来使用 nacos
Consumer 消费端，业务服务，使用 Dubbo SDK 完成服务发现
Provider 服务提供端，业务服务，使用 Dubbo SDK 完成服务注册
Registry 注册中心配置中心，接下来使用 nacos
```
#### Dubbo 3 应用架构的核心组件

两大部分：

```
一个 Dubbo SDK
三中心
```
**Dubbo SDK**

Dubbo SDK 作为模块，被微服务所引入和依赖。跟随着微服务组件被部署在分布式集群各个位置，实现
各个微服务组件间的协作，主要是服务的注册、服务的发现

**三中心**

Dubbo 3 包含一些中心化组件，主要有 3 个，这包括：

```
注册中心
协调 Consumer 消费者与 Provider 服务提供者之间的地址注册与发现。
配置中心
存储 Dubbo 启动阶段的全局配置，保证配置的跨环境共享与全局一致性
负责服务治理规则（路由规则、动态配置等）的存储与推送。
元数据中心
接收 Provider 服务端上报的服务接口元数据，为 Admin 等控制台提供运维能力（如服务测试、
接口文档等）
作为服务发现机制的补充，提供额外的接口/方法级别配置信息的同步能力，相当于注册中心
的额外扩展。
```
以上三个中心，由 Nacos 组件承担，

所以在下面的实操中，无论 dubbo-Provider 还是 dubbo-consumer，配置文件中都是这么配置的：

```
1 dubbo:
```

#### SpringBoot 整合 Dubbo 3.0 基础准备

阿里早已把 dubbo 捐赠给了 Apache，现在 dubbo 由 Apache 在维护更新，dubbo 也已经成了 Apache 下的
顶级项目。

#### SpringCloud+Nacos+Dubbo 3.0

###### 版本说明

```
SpringCloud: Hoxton. SR 8
SpringCloudAlibaba: 2.2.3. RELEASE
SpringBoot: 2.3.4. RELEASE
Nacos: 2.0.3
Dubbo: 3.0.7
```
###### 项目结构介绍

**1 、dubbo 的依赖的坐标**

Maven 依赖的坐标是 Apache 官方最新的 3.0.4 坐标。

```
scan:
base-packages: com. crazymaker. cloud. dubbo
application:
name:  ${spring. application. name}
protocol:
name: dubbo
port: -1
registry:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
parameters:
namespace: dubbo
group: DUBBO_GROUP
config-center:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
group: DUBBO_GROUP
metadata-report:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
group: DUBBO_GROUP
```
```
2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
```

如果使用的老版本的 Dubbo，比如下面的这些依赖，现在需要去掉啦

**2 、注册中心的依赖的坐标**

老的项目采用 zookeeper 为注册中心，当然，咱们的电脑里的虚拟机，其实都已经安装好 zookeeper 服
务器，并已经启动。

所以，如果要使用 ZK，对于咱们技术自由圈（疯狂创客圈的新名字）的小伙伴来说，也是非常方便的。

但是 SpringCloud 项目上一般使用的注册中，不是 appolo，不是 eureka，而是 nacos。

现在 Dubbo 也支持 nacos，所以我们使用 Nacos 作为注册中心

```
<dependency>
<groupId>org. apache. dubbo</groupId>
<artifactId>dubbo-bom</artifactId>
<version>${dubbo. version}</version>
<type>pom</type>
<scope>import</scope>
</dependency>
```
```
1 2 3 4 5 6 7
```
```
<dependency>
<groupId>com. alibaba. spring. boot</groupId>
<artifactId>dubbo-spring-boot-starter</artifactId>
<version>2.0.0</version>
</dependency>
```
```
<dependency>
<groupId>io. dubbo. springboot</groupId>
<artifactId>spring-boot-starter-dubbo</artifactId>
<version>1.0.0</version>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```

这里有个版本兼容性的 bug，尼恩稍微花了一点点时间，才解决这个问题。

具体的话，视频中给大家详细说一下。

###### SpringBoot 整合 Dubbo 3.0 大致步骤

```
SpringBoot 项目创建
Dubbo 服务提供方实现
Dubbo 服务消费方实现
自定义 Filter 拦截所有消费请求
自定义 LoadBalance 完成特殊场景负载均衡
```
###### 模块结构

接下来演示下整合的模块结构，注意，是 Springcloud 项目的老项目升级，

不是一个全新的项目，很多老的代码，要复用的

所以，就是在原来的 crazy-SpringCloud 微服务项目上改造

但是这里仅仅演示 dubbo，所以，在原理的脚手架里边，增加了两个子模块，

**注意： 模块里边，并没有去掉原来的 SpringCloud OpenFeign 的 RPC 调用, 而是两种 RPC 调用共存。**

方便后续在业务中快速运用，完整结构如下：

```
1. consumer，服务消费方模块；
2. provider，服务提供方模块；
3. 统一定义服务接口和实体类，被其他工程模块引用；
```
consumer，服务消费方模块\ provider，服务提供方模块；如下图

```
<dependency>
<groupId>org. apache. dubbo</groupId>
<artifactId>dubbo-registry-nacos</artifactId>
<version>${dubbo. version}</version>
<exclusions>
<exclusion>
<groupId>com. alibaba. nacos</groupId>
<artifactId>nacos-client</artifactId>
</exclusion>
</exclusions>
```
```
<type>pom</type>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```

统一定义服务接口和实体类，被其他工程模块引用；所以，这个之前 fegin 的 RPC 接口，现在接着给
Dubbo 用。

###### Dubbo 微服务注册发现的相关配置

```
命名空间隔离
微服务注册中心配置
```

###### 命名空间隔离

在 nacos 的命名空间，用来做 dubbo 的命名空间隔离

首先创建 nacos 的命名空间

命名 id 是要用到的，这里需要填写，

主要，不要自动生产

#### 微服务 yml 配置

微服务 yml 配置， yml 配置 dubbo nacos 命名空间

```
dubbo:
scan:
base-packages: xxx
protocol:
name: dubbo
port: -1
# 注册中心配置
registry:
address: nacos://xxx
```
```
1 2 3 4 5 6 7 8 9
```

#### common-service 模块

服务接口进行 **利旧复用**

还是之前的 UserClient 老接口，open fegn 注解都不去掉

```
parameters:
namespace: dubbo
group: DUBBO_GROUP
```
```
dubbo:
application:
name:  ${spring. application. name}
protocol:
name: dubbo
port: -1
registry:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
parameters:
namespace: dubbo
group: DUBBO_GROUP
config-center:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
group: DUBBO_GROUP
metadata-report:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
group: DUBBO_GROUP
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
```
```
package com. crazymaker. springcloud. user. info. remote. client;
```
```
import com. crazymaker. springcloud. common. dto. UserDTO;
import com. crazymaker. springcloud. common. result. RestOut;
import com. crazymaker. springcloud. standard. config. FeignConfiguration;
import
com. crazymaker. springcloud. user. info. remote. fallback. UserClientFallbackFacto
ry;
import org. springframework. cloud. openfeign. FeignClient;
import org. springframework. web. bind. annotation. RequestMapping;
import org. springframework. web. bind. annotation. RequestMethod;
import org. springframework. web. bind. annotation. RequestParam;
```
```
/**
* Feign 客户端接口
* @description: 用户信息远程调用接口
* @date 2019 年 7 月 22 日
*/
```
```
@FeignClient (value = "uaa-provider",
configuration = FeignConfiguration. class,
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
```

#### 服务提供者实操：dubbo-provider 服务

###### pom 依赖

###### 服务实现类

```
// fallback = UserClientFallback. class,
fallbackFactory = UserClientFallbackFactory. class,
path = "/uaa-provider/api/user")
public interface UserClient
{
/**
* 远程调用 RPC 方法：获取用户详细信息
* @param userId 用户 Id
* @return 用户详细信息
*/
@RequestMapping (value = "/detail/v 1", method = RequestMethod. GET)
RestOut<UserDTO> detail (@RequestParam (value = "userId") Long userId);
```
```
@RequestMapping (value = "/hello/v 1", method = RequestMethod. GET)
public String hello (@RequestParam (value = "name") String name);
}
```
```
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
```
```
<!-- dubbo -->
<dependency>
<groupId>org. apache. dubbo</groupId>
<artifactId>dubbo</artifactId>
</dependency>
<dependency>
<groupId>org. apache. dubbo</groupId>
<artifactId>dubbo-registry-nacos</artifactId>
<exclusions>
<exclusion>
<artifactId>nacos-client</artifactId>
<groupId>com. alibaba. nacos</groupId>
</exclusion>
</exclusions>
</dependency>
```
```
<!-- dubbo starter -->
```
```
<dependency>
<groupId>com. alibaba. nacos</groupId>
<artifactId>nacos-client</artifactId>
<version>${nacos-client-verson}</version>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
```
```
package com. crazymaker. cloud. dubbo. demo. sevice;
```
```
import com. crazymaker. springcloud. common. dto. UserDTO;
import com. crazymaker. springcloud. common. result. RestOut;
import com. crazymaker. springcloud. user. info. remote. client. UserClient;
import org. apache. dubbo. config. annotation. DubboService;
```
```
1 2 3 4 5 6
```

###### dubbo 和 Feign 的一个不同

```
dubbo 的服务发布和暴露，直接在 service 层加上注解，就完成了
```
比如：上面的案例中，加上一个类级别的注解，就可以暴露这个服务接口 @DubboService

```
而 Feign 的服务接口发布，是在 @Controller 层完成的。
```
相对来说，比使用 Dubbo 更为笨重

###### Provider 的 Dubbo+Nacos 配置文件

```
import org. springframework. stereotype. Component;
```
```
@DubboService
@Component
```
```
public class UserClientDubboService implements UserClient {
@Override
public RestOut<UserDTO> detail (Long userId) {
UserDTO dto=new UserDTO ();
dto.setUserId (userId);
dto.setNickname (" dubbo rpc test");
return RestOut.success (dto). setRespMsg ("操作成功");
}
```
```
@Override
public String hello (String name) {
return "from dubbo provider : 你好，"+name;
}
}
```
```
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
```
```
spring:
security:
enabled: false
application:
name: dubbo-provider-demo
```
```
server:
port: 28088
servlet:
context-path: /dubbo-provider-demo
```
```
#### 暴露端点
management:
endpoints:
web:
base-path: "/actuator" # 配置 Endpoint 的基础路径
exposure:
include: '*' #在yaml 文件属于关键字，所以需要加引号
endpoint:
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
```

###### 启动类加上@EnableDubbo 注解

```
logfile:
# spring boot admin client 不配置日志文件路径（同时配置 logback-spring. xml 对
应的日志输出配置，否则无法输出日志），
# 控制台上的 Logging 模块下的 Logfile 会报错：Fetching logfile failed. Request
failed with status code 404
external-file: ${log_dubbo_provider_demo_path_full:C:/logs/dubbo-
provider-demo/logs/output. log}
enabled: true
health:
show-details: always
# 未配置/注释以下内容
# boot:
# admin:
# context-path: consumer
metrics:
tags:
application:  ${spring. application. name}
export:
prometheus:
enabled: true
step: 1 m
descriptions: true
```
```
dubbo:
scan:
base-packages: com. crazymaker. cloud. dubbo
application:
name:  ${spring. application. name}
protocol:
name: dubbo
port: -1
registry:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
parameters:
namespace: dubbo
group: DUBBO_GROUP
config-center:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
group: DUBBO_GROUP
metadata-report:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
group: DUBBO_GROUP
```
```
21
22
```
```
23
```
```
24
```
```
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
```
```
package com. crazymaker. cloud. dubbo. demo. starter;
```
```
import com. crazymaker. springcloud. common. util. IpUtil;
import lombok. extern. slf 4 j. Slf 4 j;
```
```
1
2
3
4
```

```
import org. apache. commons. lang 3. StringUtils;
import org. apache. dubbo. config. spring. context. annotation. EnableDubbo;
import org. springframework. boot. SpringApplication;
import org. springframework. boot. autoconfigure. SpringBootApplication;
import org. springframework. cloud. client. discovery. EnableDiscoveryClient;
import org. springframework. context. ConfigurableApplicationContext;
import org. springframework. core. env. Environment;
import springfox. documentation. swagger 2. annotations. EnableSwagger 2;
```
```
import java. net. Inet 4 Address;
import java. util. Optional;
```
```
@EnableSwagger 2
@EnableDiscoveryClient
@Slf 4 j
@EnableDubbo
```
```
@SpringBootApplication (scanBasePackages =
{
"com. crazymaker. cloud. dubbo. demo",
}
)
```
```
public class DubboProviderApplication {
public static void main (String[] args) {
try {
ConfigurableApplicationContext applicationContext =
SpringApplication.run (DubboProviderApplication. class, args);
```
```
Environment env = applicationContext.getEnvironment ();
String port = env.getProperty ("server. port");
String name = env.getProperty ("spring. application. name");
```
```
String path = env.getProperty ("server. servlet. context-path");
if (StringUtils.isBlank (path)) {
path = "";
}
Optional<Inet4Address> ip = IpUtil. getLocalIp 4 Address ();
```
```
log.info ("\n------------------------------------------------------
----\n\t" +
name.toUpperCase () + " is running! Access URLs:\n\t" +
"Local: \t\thttp://" + ip.get () + ": " + port + path +
"/\n\t" +
"swagger-ui: \thttp://" + ip.get () + ": " + port + path +
"/swagger-ui. html\n\t" +
"actuator: \thttp://" + ip.get () + ": " + port + path +
"/actuator/info\n\t" +
"-------------------------------------------------------
---");
```
```
} catch (Exception e) {
log.error ("服务启动报错", e);
```
```
}
}
}
```
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

33
34
35
36
37
38
39
40
41
42
43
44

45
46

47

48

49

50
51
52
53
54
55
56


###### 启动、体验 Provider

打包之后，在咱们优雅的虚拟机 centos 8 中，优雅的启动一下,

来一个优雅的启动命令

太爽................. 有个 vagrant+Centos 开发环境，开发 Java 应用，爽到不要不要的

关键是： 在这个虚拟机 box 文件中，尼恩给大家预装了 K 8 S 云原生环境，大家一键导入省了 N 多麻烦。
基于这个环境，下一步咱们就开始 “左手大数据、右手云原生” 的高端实操

继续看，屏幕的下面，下面还有 swagger 体验地址

使用这个地址，可以在浏览器开起 Swagger -UI 的界面

[http:///cdh1:28088/dubbo-provider-demo/swagger-ui.html](http:///cdh1:28088/dubbo-provider-demo/swagger-ui.html)

开启之后的效果，但是，这个和 dubbo 没有关系

因为 dubbo 的服务并没有在 swagger ui 展示

```
[ root@centos1 work]# cd dubbo-provider-demo-1.0-SNAPSHOT/bin/
[ root@centos1 bin]# sh ./deploy. sh start
PORT:28088
JVM:-server -Xms 512 m -Xmx 2048 m
nohup java -server -Xms 512 m -Xmx 2048 m -Dserver. port= 28088 -jar /work/dubbo-
provider-demo-1.0-SNAPSHOT/lib/dubbo-provider-demo-1.0-SNAPSHOT. jar
com. crazymaker. cloud. dubbo. demo. starter. DubboProviderApplication &
log file : /work/logs/dubbo-provider-demo-1.0-SNAPSHOT/output. log
```
```
1 2 3 4 5 6
```

###### 在 Nacos 查看 Dubbo 服务的注册情况

通过 nacos，可以看到 dubbo 的服务啦

具体的细节，咱们后面讲 dubbo 视频的时候，再说

这里先把性能优化起来

#### 服务消费者实操：dubbo-consumer 服务

###### consumer 模块


###### 消费者实现类

```
package com. crazymaker. cloud. dubbo. demo. consumer. controller;
```
```
import com. crazymaker. springcloud. common. dto. UserDTO;
import com. crazymaker. springcloud. common. result. RestOut;
import com. crazymaker. springcloud. user. info. remote. client. UserClient;
import io. swagger. annotations. Api;
import io. swagger. annotations. ApiOperation;
import org. apache. dubbo. config. annotation. DubboReference;
import org. springframework. web. bind. annotation. RequestMapping;
import org. springframework. web. bind. annotation. RequestMethod;
import org. springframework. web. bind. annotation. RequestParam;
import org. springframework. web. bind. annotation. RestController;
```
```
@RestController
@RequestMapping ("/user")
@Api (tags = "Dubbo-RPC 完成远程调用")
public class UserConsumerController {
```
```
//注入 @DubboReference 注解配置所配置的 EchoClient 客户端 Feign 实例
@DubboReference (loadbalance = "groupLoadBalance")
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
```

###### 消费者 Dubbo+Nacos 配置文件

```
private UserClient userService;
```
```
//用户详情
@ApiOperation (value = "用户详情接口")
@RequestMapping (value = "/detail/v 1", method = RequestMethod. GET)
RestOut<UserDTO> detail (@RequestParam (value = "userId", required =
true) Long userId) {
RestOut<UserDTO> ret = userService.detail (userId);
return ret;
}
}
```
```
22
23
24
25
26
27
```
```
28
29
30
31
```
```
spring:
security:
enabled: false
application:
name: dubbo-consumer-demo
#http://cdh1:18081/dubbo-consumer-demo/swagger-ui.html
```
```
server:
port: 18081
servlet:
context-path: /dubbo-consumer-demo
```
```
#### 暴露端点
management:
endpoints:
web:
base-path: "/actuator" # 配置 Endpoint 的基础路径
exposure:
include: '*' #在yaml 文件属于关键字，所以需要加引号
endpoint:
health:
show-details: always
metrics:
tags:
application: ${spring. application. name}
# boot:
# admin:
# context-path: consumer
```
```
dubbo:
scan:
base-packages: com. crazymaker. cloud. dubbo
application:
name:  ${spring. application. name}
protocol:
name: dubbo
port: -1
registry:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
```

###### 启动类加上@EnableDubbo 注解

```
parameters:
namespace: dubbo
group: DUBBO_GROUP
config-center:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
group: DUBBO_GROUP
metadata-report:
address: nacos://${NACOS_SERVER: cdh 1: 8848 }
username: nacos
password: nacos
group: DUBBO_GROUP
```
```
42
43
44
45
46
47
48
49
50
51
52
53
54
```
```
package com. crazymaker. cloud. dubbo. demo. consumer. starter;
```
```
import com. crazymaker. springcloud. common. util. IpUtil;
import lombok. extern. slf 4 j. Slf 4 j;
import org. apache. commons. lang 3. StringUtils;
import org. apache. dubbo. config. spring. context. annotation. EnableDubbo;
import org. springframework. boot. SpringApplication;
import
org. springframework. boot. actuate. autoconfigure. security. servlet. ManagementWe
bSecurityAutoConfiguration;
import org. springframework. boot. autoconfigure. SpringBootApplication;
import
org. springframework. boot. autoconfigure. data. redis. RedisAutoConfiguration;
import
org. springframework. boot. autoconfigure. data. redis. RedisRepositoriesAutoConfi
guration;
import
org. springframework. boot. autoconfigure. jdbc. DataSourceAutoConfiguration;
import
org. springframework. boot. autoconfigure. jdbc. DataSourceTransactionManagerAuto
Configuration;
import
org. springframework. boot. autoconfigure. orm. jpa. HibernateJpaAutoConfiguration
;
import
org. springframework. boot. autoconfigure. security. servlet. SecurityAutoConfigur
ation;
import org. springframework. cloud. client. discovery. EnableDiscoveryClient;
import org. springframework. cloud. openfeign. EnableFeignClients;
import org. springframework. context. ConfigurableApplicationContext;
import org. springframework. core. env. Environment;
import springfox. documentation. swagger 2. annotations. EnableSwagger 2;
```
```
import java. net. Inet 4 Address;
import java. util. Optional;
```
```
@EnableSwagger 2
@EnableDiscoveryClient
@Slf 4 j
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
```
```
12
```
```
13
```
```
14
```
```
15
```
```
16
17
18
19
20
21
22
23
24
25
26
27
```

```
@EnableDubbo
```
```
@SpringBootApplication (
scanBasePackages =
{
"com. crazymaker. cloud. dubbo. demo",
"com. crazymaker. springcloud. standard"
},
exclude = {SecurityAutoConfiguration. class,
//排除 db 的自动配置
DataSourceAutoConfiguration. class,
DataSourceTransactionManagerAutoConfiguration. class,
HibernateJpaAutoConfiguration. class,
SecurityAutoConfiguration. class,
ManagementWebSecurityAutoConfiguration. class,
//排除 redis 的自动配置
RedisAutoConfiguration. class,
RedisRepositoriesAutoConfiguration. class})
//启动 Feign
@EnableFeignClients (basePackages =
{"com. crazymaker. cloud. dubbo. demo. consumer. client"})
public class DubboConsumerApplication {
```
```
public static void main (String[] args) {
try {
ConfigurableApplicationContext applicationContext =
SpringApplication.run (DubboConsumerApplication. class, args);
```
```
Environment env = applicationContext.getEnvironment ();
String port = env.getProperty ("server. port");
String name = env.getProperty ("spring. application. name");
```
```
String path = env.getProperty ("server. servlet. context-path");
if (StringUtils.isBlank (path)) {
path = "";
}
Optional<Inet4Address> ip = IpUtil. getLocalIp 4 Address ();
```
```
log.info ("\n----------------------------------------------------
------\n\t" +
name.toUpperCase () + " is running! Access URLs:\n\t" +
"Local: \t\thttp://" + ip.get () + ": " + port + path +
"/\n\t" +
"swagger-ui: \thttp://" + ip.get () + ": " + port + path +
"/swagger-ui. html\n\t" +
"actuator: \thttp://" + ip.get () + ": " + port + path +
"/actuator/info\n\t" +
"-------------------------------------------------------
---");
```
```
} catch (Exception e) {
log.error ("服务启动报错", e);
```
```
}
}
}
```
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53

54
55
56
57
58
59
60
61
62
63
64
65

66
67

68

69

70

71
72
73
74
75
76
77


###### 启动、体验 Consumer

打包之后，在咱们优雅的虚拟机 centos 8 中，优雅的启动一下,

来一个优雅的启动命令

太爽................. 有个 vagrant+Centos 开发环境，开发 Java 应用，爽到不要不要的

关键是： 在这个虚拟机 box 文件中，尼恩给大家预装了 K 8 S 云原生环境，大家一键导入省了 N 多麻烦。

下一步，基于这个环境，咱们就开始 “左手大数据、右手云原生” 的高端实操

继续看，屏幕的下面，下面还有 swagger 体验地址

使用这个地址，可以在浏览器开起 Swagger -UI 的界面

[http:///10.0.2.15:18081/dubbo-consumer-demo/swagger-ui.html](http:///10.0.2.15:18081/dubbo-consumer-demo/swagger-ui.html)

注意，要修改一下 host，修改之后为

[http://cdh1:18081/dubbo-consumer-demo/swagger-ui.html](http://cdh1:18081/dubbo-consumer-demo/swagger-ui.html)

Feign 和 Dubbo 两大 RPC 调用并存的效果，如下：

```
[ root@centos1 bin]# cd ../../dubbo-consumer-demo-1.0-SNAPSHOT/bin/
[ root@centos1 bin]# sh ./deploy. sh start
PORT:18081
JVM:-server -Xms 512 m -Xmx 2048 m
nohup java -server -Xms 512 m -Xmx 2048 m -Dserver. port= 18081 -jar /work/dubbo-
consumer-demo-1.0-SNAPSHOT/lib/dubbo-consumer-demo-1.0-SNAPSHOT. jar
com. crazymaker. cloud. dubbo. demo. consumer. starter. DubboConsumerApplication &
nohup: appending output to 'nohup. out'
```
```
1 2 3 4 5 6
```

###### 在 Nacos 查看 Dubbo 服务的注册情况

通过 nacos，可以看到 dubbo 的服务的消费者啦

具体的细节，咱们后面讲 dubbo 视频的时候，再说

这里先把性能优化起来

#### Feign+Dubbo 性能的对比测试

然后进行了性能的对比验证

**dubbo 的压测数据**

```
wrk -t 8 -c 200 -d 30 s --latency http://cdh1:18081/dubbo-consumer-
demo/user/detail/v 1? userId= 1
```
```
[ root@centos1 src]# wrk -t 8 -c 200 -d 30 s --latency http://cdh1:18081/dubbo-
consumer-demo/user/detail/v 1? userId=1
Running 30 s test @ http://cdh1:18081/dubbo-consumer-demo/user/detail/v1?
userId= 1
```
```
1
```
```
2
3
```
```
4
```

**feign 的压测数据**

**从数据来看， dubbo rpc 是 feign rpc 性能 10 倍**

```
 8 threads and 200 connections
Thread Stats Avg Stdev Max +/- Stdev
Latency 30 .10 ms 45 .68 ms 644 .45 ms 95 .43%
Req/Sec 1 .12 k 465 .63 2 .36 k 66 .87%
Latency Distribution
50 % 18 .94 ms
75 % 28 .43 ms
90 % 46 .21 ms
99 % 283 .56 ms
 264316 requests in 30 .07 s, 148 .47 MB read
Requests/sec: 8788 .96
Transfer/sec: 4 .94 MB
```
```
5
6
7
8
9
10
11
12
13
14
15
16
```
```
wrk -t 8 -c 200 -d 30 s --latency http://cdh1:18081/dubbo-consumer-
demo/echo/variable/11
```
```
[ root@centos1 src]# wrk -t 8 -c 200 -d 30 s --latency http://cdh1:18081/dubbo-
consumer-demo/echo/variable/11
Running 30 s test @ http://cdh1:18081/dubbo-consumer-demo/echo/variable/11
 8 threads and 200 connections
Thread Stats Avg Stdev Max +/- Stdev
Latency 321 .50 ms 294 .59 ms 2 .00 s 61 .77%
Req/Sec 87 .18 43 .39 232 .00 67 .00%
Latency Distribution
50 % 309 .06 ms
75 % 503 .06 ms
90 % 687 .99 ms
99 % 1 .21 s
 20495 requests in 30 .10 s, 7 .64 MB read
Socket errors: connect 0 , read 0 , write 0 , timeout 49
Requests/sec: 680 .90
Transfer/sec: 259 .99 KB
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```

###### Dubbo 比 Feign 高 10 倍以上的本质

Dubbo 比 Feign 高 10 倍以上的本质，不是在于应用层的协议，和传输格式

面试的时候，太多的小伙伴被问到： Dubbo 比 Feign 性能高，说说原因。

小伙伴首先回答的是：

_Dubbo_ 是 _TCP_ 层的传输协议， _Feign_ 是应用层的 _HTTP_ 传输协议，所以性能低。

这个答案不是本质原因，HTTP 是有些冗余的头部报文，但是不至于差 10 倍。能差 0.5 倍，就已经顶天
了。

差 10 倍的原因：是同步链路与异步链路的区别。

或者说： _Dubbo_ 比 _Feign_ 高 _10_ 倍以上的本质，是 _RPC_ 调用异步化

RPC 调用主要的框架有：

特点是：

```
feign 是同步 IO 、阻塞模式的同步 RPC 框架
dubbo 是基于 Netty 的非阻塞 IO + Reactor 反应堆线程模型的异步 RPC 框架
```
异步 RPC 调用，等待 upstream 上游 response 返回时，线程不处于 block 状态

作为微服务架构中数据流量最大的一部分，RPC 调用异步化的收益巨大；

dubbo 的异步 RPC，仅仅 SpringCloud 微服务全链路异步的一个环节， SpringCloud 全链路异步有 5
个以上的环节。

有关微服务全链路异步高性能改造，请参考尼恩的深度文章：


全链路异步，让你的 SpringCloud 性能优化 10 倍+

高并发、云原生、大数据时代，很多组件都是全异步的、响应式的。

大家一定要掌握全链路异步的底层原理，掌握好响应式编程的底层原理和实操，为 “左手大数据、右手
云原生” 做好技术准备。

后面尼恩也会写一些列的博文，为大家“左手大数据、右手云原生” 做好技术储备。

具体请关注尼恩的疯狂创客圈社群（50+）。

###### Dubbo 与 SpringCloud 的通信 Openfeign 的区别

**1 、协议支持方面**

```
Feign 更加优雅简单。
Feign 是通过 REST API 实现的远程调用，基于 Http 传输协议，服务提供者需要对外暴露 Http 接口供
消费者调用，服务粒度是 http 接口级的。
很多文章说：通过短连接的方式进行通信，不适合高并发的访问。
尼恩纠错： Feign 并不是短链接，而是长连接，具体请阅读尼恩的《 Java 高并发核心编程卷 1 加
强版》
Dubbo 方式更灵活。
Dubbo 是通过 RPC 调用实现的远程调用，支持多传输协议 (Dubbo、Rmi、http、redis 等等)，可以
根据业务场景选择最佳的方式，非常灵活。
默认的 Dubbo 协议：利用 Netty，TCP 传输，单一、异步、长连接，适合数据量小、高并发和服务
提供者远远少于消费者的场景。Dubbo 通过 TCP 长连接的方式进行通信，服务粒度是方法级的。
```
**2 、通信性能方面**

```
Feign 基于 Http 传输协议，底层实现是 rest。在高并发场景下性能不够理想。
Dubbo 框架的通信协议采用 RPC 协议，属于传输层协议，提升了交互的性能，保持了长连接，高性
能。
```
**3 、线程模型方面**

```
Feign 使用的是阻塞式的线程模型，在请求和响应直之间， IO 线程是阻塞的，死等到超时。
Dubbo 使用的是异步非阻塞式的线程模型（Netty 的 Reactor 反应器），在请求和响应直之间， IO
线程是非阻塞的，有限的线程，可以处理大量的请求。
```
这点是性能的核心。

#### SpringCloud + Dubbo RPC 的集成价值

高并发时代来了，各大项目有越来越强烈的诉求，全链路异步，是性能优化的一个杀手锏。

使用 Dubbo 来替换 Feign，足足可以提升 10 倍性能。

所以，SpringCloud + Dubbo RPC 的集成是一个比较刚性的需求。

有小伙伴查招聘网站，发现很多需要有 SpringCloud + Dubbo 的集成经验，刚好印证了这点。


尼恩强烈建议大家，做一下 SpringCloud + Dubbo RPC 的集成，在同一个微服务下，同时使用了 Feign
+ Dubbo。

尼恩提示：很多小伙伴来找尼恩改简历，但是发现一点漂亮的技术亮点都没有，如果确实找不到亮点，
可以把这个实操做一下，写入简历，作为亮点。如果确实不清楚怎么写入简历，可以来找尼恩进行简历
指导。保证脱胎换骨、金光闪闪、天衣无缝。

## hystrix 服务保护

hystrix 作为老牌 SpringCloud 微服务保护的组件，很多项目仍然在使用，

另外，底层原理都是想通的，

大家可以和 sentinel 对比学习

hystrix 的详细介绍，请阅读《Java 高并发核心编程卷 3 加强版》

## Sentinel 服务保护

sentinel 是 SpringCloud 阿里巴巴的微服务保护组件，

学习的时候，sentinel 最好与 hystrix 对比学习，

#### sentinel 基本概念

开发的原因，需要对吞吐量（TPS）、QPS、并发数、响应时间（RT）几个概念做下了解，查自百度百
科，记录如下：


```
1. 响应时间 (RT)
```
响应时间是指系统对请求作出响应的时间。直观上看，这个指标与人对软件性能的主观感受是非常一致
的，因为它完整地记录了整个计算机系统处理请求的时间。由于一个系统通常会提供许多功能，而不同
功能的处理逻辑也千差万别，因而不同功能的响应时间也不尽相同，甚至同一功能在不同输入数据的情
况下响应时间也不相同。所以，在讨论一个系统的响应时间时，人们通常是指该系统所有功能的平均时
间或者所有功能的最大响应时间。当然，往往也需要对每个或每组功能讨论其平均响应时间和最大响应
时间。

对于单机的没有并发操作的应用系统而言，人们普遍认为响应时间是一个合理且准确的性能指标。需要
指出的是，响应时间的绝对值并不能直接反映软件的性能的高低，软件性能的高低实际上取决于用户对
该响应时间的接受程度。对于一个游戏软件来说，响应时间小于 100 毫秒应该是不错的，响应时间在 1 秒
左右可能属于勉强可以接受，如果响应时间达到 3 秒就完全难以接受了。而对于编译系统来说，完整编译
一个较大规模软件的源代码可能需要几十分钟甚至更长时间，但这些响应时间对于用户来说都是可以接
受的。

```
2. 吞吐量 (Throughput)
```
吞吐量是指系统在单位时间内处理请求的数量。对于无并发的应用系统而言，吞吐量与响应时间成严格
的反比关系，实际上此时吞吐量就是响应时间的倒数。前面已经说过，对于单用户的系统，响应时间
（或者系统响应时间和应用延迟时间）可以很好地度量系统的性能，但对于并发系统，通常需要用吞吐
量作为性能指标。

对于一个多用户的系统，如果只有一个用户使用时系统的平均响应时间是 t，当有你 n 个用户使用时，每
个用户看到的响应时间通常并不是 n×t，而往往比 n×t 小很多（当然，在某些特殊情况下也可能比 n×t 大，
甚至大很多）。这是因为处理每个请求需要用到很多资源，由于每个请求的处理过程中有许多不走难以

并发执行，这导致在具体的一个时间点，所占资源往往并不多。也就是说在处理单个请求时，在每个时
间点都可能有许多资源被闲置，当处理多个请求时，如果资源配置合理，每个用户看到的平均响应时间
并不随用户数的增加而线性增加。实际上，不同系统的平均响应时间随用户数增加而增长的速度也不大
相同，这也是采用吞吐量来度量并发系统的性能的主要原因。一般而言，吞吐量是一个比较通用的指
标，两个具有不同用户数和用户使用模式的系统，如果其最大吞吐量基本一致，则可以判断两个系统的
处理能力基本一致。

```
3. 并发用户数
```
并发用户数是指系统可以同时承载的正常使用系统功能的用户的数量。与吞吐量相比，并发用户数是一
个更直观但也更笼统的性能指标。实际上，并发用户数是一个非常不准确的指标，因为用户不同的使用
模式会导致不同用户在单位时间发出不同数量的请求。一网站系统为例，假设用户只有注册后才能使
用，但注册用户并不是每时每刻都在使用该网站，因此具体一个时刻只有部分注册用户同时在线，在线
用户就在浏览网站时会花很多时间阅读网站上的信息，因而具体一个时刻只有部分在线用户同时向系统
发出请求。这样，对于网站系统我们会有三个关于用户数的统计数字：注册用户数、在线用户数和同时
发请求用户数。由于注册用户可能长时间不登陆网站，使用注册用户数作为性能指标会造成很大的误
差。而在线用户数和同时发请求用户数都可以作为性能指标。相比而言，以在线用户作为性能指标更直
观些，而以同时发请求用户数作为性能指标更准确些。

```
4. QPS 每秒查询率 (Query Per Second)
```
每秒查询率 QPS 是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作
为域名系统服务器的机器的性能经常用每秒查询率来衡量。对应 fetches/sec，即每秒的响应请求数，也
即是最大吞吐能力。 （看来是类似于 TPS，只是应用于特定场景的吞吐量）

#### 服务雪崩效应


在微服务架构系统中通常会有多个服务，在服务调用中如果出现基础服务故障，可能会导致级联故障，
即一个服务不可用，可能导致所有调用它或间接调用它的服务都不可用，进而造成整个系统不可用的情
况，这种现象也被称为服务雪崩效应。

服务雪崩效应是一种因“服务提供者不可用”（原因）导致“服务调用者不可用”（结果），并将不可用逐渐
放大的现象。

服务雪崩效应示意如图所示，A 为服务提供者，B 为 A 的服务调用者，C 为 B 的服务调用者。

当服务 A 因为某些原因导致不可用时，会引起服务 B 的不可用，并将不可用放大到服务 C 进而导致整个系
统瘫痪，这样就形成了服务雪崩效应。

出现服务雪崩效应的原因如下：

硬件故障：如服务器宕机、机房断电、光纤被挖断等。

流量激增：如异常流量、重试加大流量等。

缓存穿透：一般发生在应用重启，所有缓存失效时，以及短时间内大量缓存失效时，因大量的缓存不命
中，使请求直击后端服务，造成服务提供者超负荷运行，引起服务不可用。

程序 bug：如程序逻辑导致死循环或者内存泄漏等。

如何解决服务器雪崩的方法有以下这些：

```
超时机制：在上游服务调用下游服务的时候，设置一个最大响应时间，如果超过这个时间，下游未
作出反应，就断开请求，释放掉线程。
限流机制：限流就是限制系统的输入和输出流量已达到保护系统的目的。为了保证系统的稳固运
行，一旦达到的需要限制的阈值，就需要限制流量并采取少量措施以完成限制流量的目的。
熔断机制：在互联网系统中，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系
统整体的可用性，可以暂时切断对下游服务的调用。这种牺牲局部，保全整体的措施就叫做熔断。
```

```
降级机制：降级是从系统功能优先级的角度考虑如何应对系统故障。服务降级指的是当服务器压力
剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以
保证核心任务的正常运行。降级其实就是为服务提供一个兜底方案，一旦服务无法正常调用，就使
用兜底方案。
```
Sentinel 为我们提供了多种的解决服务雪崩的方法：如超时机制、限流机制、熔断机制、降级机制等
等，后面会为大家进行介绍

#### 1 、什么是 Sentinel:

Sentinel 是阿里开源的项目，提供了流量控制、熔断降级、系统负载保护等多个维度来保障服务之间的
稳定性。
官网：https://github.com/alibaba/Sentinel/wiki

2012 年，Sentinel 诞生于阿里巴巴，其主要目标是流量控制。2013-2017 年，Sentinel 迅速发展，并成
为阿里巴巴所有微服务的基本组成部分。它已在 6000 多个应用程序中使用，涵盖了几乎所有核心电子商
务场景。 2018 年，Sentinel 演变为一个开源项目。 2020 年，Sentinel Golang 发布。

###### Sentinel 具有以下特征:

**丰富的应用场景** ：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即
突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用
等。

**完备的实时监控** ：Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机
器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。

**广泛的开源生态** ：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring
Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。

**完善的 SPI 扩展点** ：Sentinel 提供简单易用、完善的 SPI 扩展接口。您可以通过实现扩展接口来快
速地定制逻辑。例如定制规则管理、适配动态数据源等。

**Sentinel 的生态圈**


###### Sentinel 主要特性：

关于 Sentinel 与 Hystrix 的区别见：https://yq.aliyun.com/articles/633786/

到这已经学习 Sentinel 的基本的使用，在很多的特性和 Hystrix 有很多类似的功能。以下是 Sentinel 和
Hystrix 的对比。

###### Sentinel 的使用

**Sentinel 的使用可以分为两个部分:**

```
控制台（Dashboard）：控制台主要负责管理推送规则、监控、集群限流分配管理、机器发现等。
核心库（Java 客户端）：不依赖任何框架/库，能够运行于 Java 7 及以上的版本的运行时环境，同
时对 Dubbo / Spring Cloud 等框架也有较好的支持。
```
在这里我们看下控制台的使用

###### Sentinel 中的管理控制台

**1 ）获取 Sentinel 控制台**

您可以从 https://github.com/alibaba/Sentinel/releases 下载最新版本的控制台 jar 包。

您也可以从最新版本的源码自行构建 Sentinel 控制台：

```
下载控制台工程
使用以下命令将代码打包成一个 fat jar: mvn clean package
```

**2 ）sentinel 服务启动**

（ 1 ）java -jar sentinl+tab 键自动补全

开机启动：启动命令可以加入到启动的 rc. local 配置文件，之后做到开机启动

（ 2 ）启动 sentinel

除了流量控制以外，对调用链路中不稳定的资源进行熔断降级也是保障高可用的重要措施之一。

由于调用关系的复杂性，如果调用链路中的某个资源不稳定，最终会导致请求发生堆积。Sentinel 熔断
降级会在调用链路中某个资源出现不稳定状态时（例如调用超时或异常比例升高），对这个资源的调用
进行限制，让请求快速失败，避免影响到其它的资源而导致级联错误。当资源被降级后，在接下来的降
级时间窗口之内，对该资源的调用都自动熔断（默认行为是抛出 DegradeException）。

关于熔断降级的介绍见：Sentinel 熔断降级。

下面就使用基于注解的方式实现 Sentinel 的熔断降级的 demo。

```
注意 ：启动 Sentinel 控制台需要 JDK 版本为 1.8 及以上版本。
```
使用如下命令启动控制台：

其中 -Dserver. port=8849 用于指定 Sentinel 控制台端口为 8849 ，这个端口可以按需指定。

从 Sentinel 1.6.0 起，Sentinel 控制台引入基本的 **登录** 功能，默认用户名和密码都是 sentinel。可以

参考鉴权模块文档配置用户名和密码。

```
注：若您的应用为 Spring Boot 或 Spring Cloud 应用，您可以通过 Spring 配置文件来指定配置，
详情请参考 Spring Cloud Alibaba Sentinel 文档。
```
（ 1 ）获取 Sentinel 控制台

您可以从官方网站中下载最新版本的控制台 jar 包，下载地址如下：

```
java  -server -Xms 64 m -Xmx 256 m -Dserver. port= 8849 -
Dcsp. sentinel. dashboard. server=localhost: 8849 -Dproject. name=sentinel-
dashboard -jar /work/sentinel-dashboard-1.8.6. jar
```
```
1
```
```
/usr/bin/su  - root  -c "nohup java -server -Xms 64 m -Xmx 256 m -
Dserver. port=8849 -Dcsp. sentinel. dashboard. server=localhost: 8849 -
Dproject. name=sentinel-dashboard -jar /work/sentinel-dashboard-1.8.6. jar
2>&1 &"
```
```
1
```
```
nohup java  -server -Xms 64 m -Xmx 256 m -Dserver. port= 8849 -
Dcsp. sentinel. dashboard. server=localhost: 8849 -Dproject. name=sentinel-
dashboard -jar /work/sentinel-dashboard-1.8.6. jar &
```
```
1
```
```
https://github.com/alibaba/Sentinel/releases/download/1.6.3/sentinel-
dashboard-1.7.1. jar
```
```
1
```

（ 2 ）启动

使用如下命令启动控制台：

其中 - Dserver. port=8888 用于指定 Sentinel 控制台端口为 8888 。

从 Sentinel 1.6.0 起，Sentinel 控制台引入基本的登录功能，默认用户名和密码都是 sentinel 。可以参
考鉴权模块文档配置用户名和密码。

```
[ root@192 ~]# java -Dserver. port=8888 -
Dcsp. sentinel. dashboard. server=localhost: 8888 -Dproject. name=sentinel-
dashboard -jar sentinel-dashboard-1.8.6. jar
INFO: log base dir is: /root/logs/csp/
INFO: log name use pid is: false
```
. ____ _ __ _ _
/\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
\\/ ___)| |_)| | | | | || (_| | ) ) ) )
' |____| .__|_| |_|_| |_\__, | / / / /
=========|_|==============|___/=/_/_/_/
:: Spring Boot :: (v 2.0.5. RELEASE)

```
2020 -02-08 13 :07:29.316 INFO 114031 --- [ main]
c.a.c.s.dashboard. DashboardApplication : Starting DashboardApplication on
192 .168.180.137 with PID 114031 (/root/sentinel-dashboard-1.6.3. jar started
by root in /root)
2020 -02-08 13 :07:29.319 INFO 114031 --- [ main]
c.a.c.s.dashboard. DashboardApplication : No active profile set, falling
back to default profiles: default
2020 -02-08 13 :07:29.456 INFO 114031 --- [ main]
ConfigServletWebServerApplicationContext : Refreshing
org. springframework. boot. web. servlet. context. AnnotationConfigServletWebServe
rApplicationContext@59690aa4 : startup date [Sat Feb 08 13 :07:29 CST 2020 ];
root of context hierarchy
2020 -02-08 13 :07:33.783 INFO 114031 --- [ main]
o.s.b.w.embedded. tomcat. TomcatWebServer : Tomcat initialized with port (s):
8888 (http)
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
14
```
```
15
```
```
16
```

启动 Sentinel 控制台需要 JDK 版本为 1.8 及以上版本。

**查看机器列表以及健康情况**
默认情况下 Sentinel 会在客户端首次调用的时候进行初始化，开始向控制台发送心跳包。也可以配置
sentinel. eager=true ,取消 Sentinel 控制台懒加载。
打开浏览器即可展示 Sentinel 的管理控制台


###### 客户端能接入控制台

控制台启动后，客户端需要按照以下步骤接入到控制台。

父工程引入 alibaba 实现的 SpringCloud

子工程中引入 sentinel

（ 2 ）配置启动参数
在工程的 application. yml 中添加 Sentinel 控制台配置信息

这里的 spring. cloud. sentinel. transport. dashboard 配置控制台的请求路径。

```
<dependencyManagement>
<dependencies>
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-dependencies</artifactId>
<version>Greenwich. RELEASE</version>
<type>pom</type>
<scope>import</scope>
</dependency>
<dependency>
<groupId>com. alibaba. cloud</groupId>
<artifactId>spring-cloud-alibaba-dependencies</artifactId>
<version>2.1.0. RELEASE</version>
<type>pom</type>
<scope>import</scope>
</dependency>
</dependencies>
</dependencyManagement>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
```
```
<dependency>
<groupId>com. alibaba. cloud</groupId>
<artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
</dependency>
```
```
1
2
3
4
```
```
spring:
cloud:
sentinel:
transport:
dashboard: 192.168.180.137: 8849 #sentinel控制台的请求地址
```
```
1
2
3
4
5
```

###### Sentinel 与 Hystrix 的区别


**Items Sentinel Hystrix remark**

隔离策略

```
信号量隔离
（并发线程数
限流）（模拟
信号量）
```
```
线程池隔
离/信号
量隔离
```
```
Sentinel 不创建线程依赖 tomcat 或 jetty 容器的
线程池，存在的问题就是运行容器的线程数量
限制了 sentinel 设置值的上限可能设置不准。比
如 tomcat 线程池为 10 ，sentinel 设置 100 是没有
意义的，同时隔离性不好 hystrix 使用自己创建
的线程池，隔离性会更好
```
熔断降级策
略

```
基于响应时
间、异常比
率、异常数
```
```
基于异常
比率
```
```
快速失败的本质功能
```
实时统计实
现

```
滑动窗口
（LeapArray）
```
```
滑动窗口
（基于
RxJava）
```
动态规则配
置

```
支持多种数据
源
```
```
支持多种
数据源
```
扩展性多个扩展点

```
插件的形
式
```
基于注解的
支持
支持支持

限流

```
基于 QPS，支
持基于调用关
系的限流
```
```
有限的支
持（并发
线程数或
信号量大
小）
```
```
快速失败的本质功能
```
流量整形

```
支持预热模
式、匀速器模
式、预热排队
模式
```
```
不支持
（排队）
```
```
支持排队好吧
```
系统自适应
保护

```
支持（仅对
linux 生效）
不支持
```
```
所谓的自适应就是设置一个服务器最大允许处
理量的阈值。（有比没有强，但是要知道最大
负载量是多少。）
```
控制台

```
提供开箱即用
的控制台，可
配置规则、查
看秒级监控、
机器发现等
```
```
简单的监
控查看接
近实时数
据
```
```
控制台是非常有竞争力的功能，因为能集中配
置限制数据更方便，但是展示数据和实时性没
有 hystrix 直观。
```
配置持久化
ZooKeeper,
Apollo, Nacos

```
Git/svn/
本地文件
```
```
Sentinel 客户端采用直接链接持久化存储，应用
客户端引用了更多的依赖，同样的存储链接可
能有多个配置
```
动态配置支持支持

```
hystrix 可能需要手动触发，sentinel 增加了额外
的端口进行配置文件控制，应该也支持 spring
boot 动态配置
```
黑白名单支持不支持个人觉得这个功能用的不是很多


```
Items Sentinel Hystrix remark
```
```
springcloud
集成
高非常高 Spring boot 使用 hystrix 会更方便
```
```
整体优势
```
```
集中配置设置
及监控+更细的
控制规则
```
```
漂亮的界
面+接近
实时的统
计结果
```
```
集中配置可能更有吸引力，但是配置值是多少
以及让谁控制依然是很头疼的事情。运维控制
可能不知道哪个应该优先哪个不优先，应该调
整到多大。什么时候更适合使用 sentinel？个人
认为 docker 容器化部署之后 sentinel 可能更会
发挥作用，但是会有另外的竞品出现做选型。
```
**迁移方案**
Sentinel 官方提供了详细的由 Hystrix 迁移到 Sentinel 的方法

#### 2 、使用 Sentinel 来进行熔断与限流

Sentinel 可以简单的分为 Sentinel 核心库和 Dashboard。核心库不依赖 Dashboard，但是结合
Dashboard 可以取得最好的效果。
使用 Sentinel 来进行熔断保护，主要分为几个步骤:

```
1. 定义资源
```
```
资源：可以是任何东西，一个服务，服务里的方法，甚至是一段代码。
```
```
2. 定义规则
```
```
规则：Sentinel 支持以下几种规则：流量控制规则、熔断降级规则、系统保护规则、来源访
问控制规则
和热点参数规则。
```
```
3. 检验规则是否生效
```
Sentinel 的所有规则都可以在内存态中动态地查询及修改，修改之后立即生效. 先把可能需要保护的资
源定义好，之后再配置规则。


也可以理解为，只要有了资源，我们就可以在任何时候灵活地定义各种流量控制规则。在编码的时候，
只需要考虑这个代码是否需要保护，如果需要保护，就将之定义为一个资源。

###### 2.1 定义资源

**资源** 是 Sentinel 的关键概念。它可以是 Java 应用程序中的任何内容，例如，由应用程序提供的服务，
或由应用程序调用的其它应用提供的服务，RPC 接口方法，甚至可以是一段代码。

只要通过 Sentinel API 定义的代码，就是资源，能够被 Sentinel 保护起来。大部分情况下，可以使用方
法签名，URL，甚至服务名称作为资源名来标示资源。

把需要控制流量的代码用 Sentinel 的关键代码 SphU.entry ("资源名") 和 entry.exit () 包围起来即可。

实例代码：

在下面的例子中，用 try-with-resources 来定义资源。参考代码如下:

```
Entry entry = null;
try {
// 定义一个 sentinel 保护的资源，名称为 test-sentinel-api
entry = SphU.entry (resourceName);
// 模拟执行被保护的业务逻辑耗时
Thread.sleep ( 100 );
return a;
} catch (BlockException e) {
// 如果被保护的资源被限流或者降级了，就会抛出 BlockException
log.warn ("资源被限流或降级了", e);
return "资源被限流或降级了";
} catch (InterruptedException e) {
return "发生 InterruptedException";
} finally {
if (entry != null) {
entry.exit ();
}
```
```
ContextUtil.exit ();
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
```
```
public static void main (String[] args) {
// 配置规则.
initFlowRules ();
```
```
while (true) {
// 1.5.0 版本开始可以直接利用 try-with-resources 特性
try (Entry entry = SphU.entry ("HelloWorld")) {
// 被保护的逻辑
System.out.println ("hello world");
} catch (BlockException ex) {
// 处理被流控的逻辑
System.out.println ("blocked!");
}
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```

**资源注解@SentinelResource**

也可以使用 Sentinel 提供的注解@SentinelResource 来定义资源，实例如下:

**@SentinelResource 注解**

```
注意：注解方式埋点不支持 private 方法。
```
```
@SentinelResource 用于定义资源，并提供可选的异常处理和 fallback 配置项。
@SentinelResource 注解包含以下属性：
```
```
value：资源名称，必需项（不能为空）
entryType：entry 类型，可选项（默认为 EntryType. OUT）
blockHandler / blockHandlerClass:
```
blockHandler 对应处理 BlockException 的函数名称，可选项。blockHandler 函数访问范围需要是
public，返回类型需要与原方法相匹配，参数类型需要和原方法相匹配并且最后加一个额外的参数，类
型为 BlockException。blockHandler 函数默认需要和原方法在同一个类中。若希望使用其他类的函

数，则可以指定 blockHandlerClass 为对应的类的 Class 对象，注意对应的函数必需为 static 函数，否
则无法解析。

```
fallback /fallbackClass
fallback 函数名称，可选项，用于在抛出异常的时候提供 fallback 处理逻辑。fallback 函数可以针
对所有类型的异常（除了 exceptionsToIgnore 里面排除掉的异常类型）进行处理。
defaultFallback
（since 1.6.0）：默认的 fallback 函数名称，可选项，通常用于通用的 fallback 逻辑（即可以用于
很多服务或方法）。默认 fallback 函数可以针对所有类型的异常（除了 exceptionsToIgnore 里面排
除掉的异常类型）进行处理。若同时配置了 fallback 和 defaultFallback，则只有 fallback 会生
效。
```
**fallback 函数签名和位置要求：**

```
返回值类型必须与原函数返回值类型一致；
方法参数列表需要和原函数一致，或者可以额外多一个 Throwable 类型的参数用于接收对应的异
常。
fallback 函数默认需要和原方法在同一个类中。若希望使用其他类的函数，则可以指定
fallbackClass 为对应的类的 Class 对象，注意对应的函数必需为 static 函数，否则无法解析。
```
**defaultFallback 函数签名要求：**

```
返回值类型必须与原函数返回值类型一致；
方法参数列表需要为空，或者可以额外多一个 Throwable 类型的参数用于接收对应的异常。
defaultFallback 函数默认需要和原方法在同一个类中。若希望使用其他类的函数，则可以指定
fallbackClass 为对应的类的 Class 对象，注意对应的函数必需为 static 函数，否则无法解析。
```
```
@SentinelResource ("HelloWorld")
public void helloWorld () {
// 资源中的逻辑
System.out.println ("hello world");
}
```
```
1
2
3
4
5
```

```
exceptionsToIgnore（since 1.6.0）：用于指定哪些异常被排除掉，不会计入异常统计中，也不会
进入 fallback 逻辑中，而是会原样抛出。
```
###### 2.2 定义规则

规则主要有流控规则、熔断降级规则、系统规则、权限规则、热点参数规则等：

一段硬编码的方式定义流量控制规则如下：

加载规则：

#### 3 、sentinel 熔断降级

###### 3.1 什么是熔断降级

熔断降级对调用链路中不稳定的资源进行熔断降级是保障高可用的重要措施之一。

由于调用关系的复杂性，如果调用链路中的某个资源不稳定，最终会导致请求发生堆积。Sentinel 熔断
降级会在调用链路中某个资源出现不稳定状态时（例如调用超时或异常比例升高），对这个资源的调用
进行限制，让请求快速失败，避免影响到其它的资源而导致级联错误。当资源被降级后，在接下来的降
级时间窗口之内，对该资源的调用都自动熔断（默认行为是抛出 DegradeException）

###### 3.2 熔断降级规则

熔断降级规则包含下面几个重要的属性：

```
private void initSystemRule () {
List<SystemRule> rules = new ArrayList<>();
SystemRule rule = new SystemRule ();
rule.setHighestSystemLoad ( 10 );
rules.add (rule);
SystemRuleManager.loadRules (rules);
}
```
```
1 2 3 4 5 6 7
```
```
FlowRuleManager.loadRules (List<FlowRule> rules); // 修改流控规则
DegradeRuleManager.loadRules (List<DegradeRule> rules); // 修改降级规则
SystemRuleManager.loadRules (List<SystemRule> rules); // 修改系统规则
AuthorityRuleManager.loadRules (List<AuthorityRule> rules); // 修改授权规则
```
```
1
2
3
4
```

```
Field 说明
默认
值
```
```
resource 资源名，即规则的作用对象
```
```
grade 熔断策略，支持慢调用比例/异常比例/异常数策略
```
```
慢调
用比
例
```
```
count
慢调用比例模式下为慢调用临界 RT（超出该值计为慢调
用）；异常比例/异常数模式下为对应的阈值
```
```
timeWindow 熔断时长，单位为 s
```
```
minRequestAmount
```
```
熔断触发的最小请求数，请求数小于该值时即使异常比率超出
阈值也不会熔断（1.7.0 引入）
5
```
```
statIntervalMs
```
```
统计时长（单位为 ms），如 60*1000 代表分钟级（1.8.0 引
入）
```
```
1000
ms
```
```
slowRatioThreshold 慢调用比例阈值，仅慢调用比例模式有效（1.8.0 引入）
```
###### 3.3 几种降级策略

我们通常用以下几种降级策略：

```
平均响应时间 (DEGRADE_GRADE_RT)：
当资源的平均响应时间超过阈值（DegradeRule 中的 count，以 ms 为单位）之后，资源进入准降
级状态。如果接下来 1 s 内持续进入 5 个请求（即 QPS >= 5），它们的 RT 都持续超过这个阈值，
那么在接下的时间窗口（DegradeRule 中的 timeWindow，以 s 为单位）之内，对这个方法的调
用都会自动地熔断（抛出 DegradeException）。
```
```
注意 Sentinel 默认统计的 RT 上限是 4900 ms，超出此阈值的都会算作 4900 ms，若需要变
更此上限可以通过启动配置项 -Dcsp. sentinel. statistic. max. rt=xxx 来配置。
```
```
异常比例 (DEGRADE_GRADE_EXCEPTION_RATIO)：
当资源的每秒异常总数占通过量的比值超过阈值（DegradeRule 中的 count）之后，资源进入降级
状态，即在接下的时间窗口（DegradeRule 中的 timeWindow，以 s 为单位）之内，对这个方法
的调用都会自动地返回。
```
```
异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。
```
```
异常数 (DEGRADE_GRADE_EXCEPTION_COUNT)：
当资源近 1 分钟的异常数目超过阈值之后会进行熔断。
```
```
注意由于统计时间窗口是分钟级别的，若 timeWindow 小于 60 s，则结束熔断状态后仍可能
再进入熔断状态。
```

###### 3.4 熔断降级代码实现

可以通过调用 DegradeRuleManager.loadRules () 方法来用硬编码的方式定义流量控制规则。

```
具体源码，请参见疯狂创客圈 crazy-springcloud 源码工程
```
###### 3.5 控制台降级规则

配置

参数

```
@PostConstruct
public void initSentinelRule ()
{
//熔断规则： 5 s 内调用接口出现异常次数超过 5 的时候, 进行熔断
List<DegradeRule> degradeRules = new ArrayList<>();
DegradeRule rule = new DegradeRule ();
rule.setResource ("queryGoodsInfo");
rule.setCount ( 5 );
```
```
rule.setGrade (RuleConstant. DEGRADE_GRADE_EXCEPTION_COUNT);//熔断规则
rule.setTimeWindow ( 5 );
degradeRules.add (rule);
DegradeRuleManager.loadRules (degradeRules);
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
```

```
Field 说明默认值
```
```
resource 资源名，即限流规则的作用对象
```
```
count 阈值
```
```
grade 降级模式，根据 RT 降级还是根据异常比例降级 RT
```
```
timeWindow 降级的时间，单位为 s
```
###### 3.6 与 Hystrix 的熔断对比：

Hystrix 常用的线程池隔离会造成线程上下切换的 overhead 比较大；Hystrix 使用的信号量隔离对某个资
源调用的并发数进行控制，效果不错，但是无法对慢调用进行自动降级；

Sentinel 通过并发线程数的流量控制提供信号量隔离的功能；此外，Sentinel 支持的熔断降级维度更多，
可对多种指标进行流控、熔断，且提供了实时监控和控制面板，功能更为强大。

#### 4 、Sentinel 流控（限流）

流量控制 (Flow Control)，原理是监控应用流量的 QPS 或并发线程数等指标，当达到指定阈值时对流量进
行控制，避免系统被瞬时的流量高峰冲垮，保障应用高可用性。

通过流控规则来指定允许该资源通过的请求次数，例如下面的代码定义了资源 HelloWorld 每秒最多只
能通过 20 个请求。参考的规则定义如下：

一条限流规则主要由下面几个因素组成，我们可以组合这些元素来实现不同的限流效果：

```
resource：资源名，即限流规则的作用对象
count: 限流阈值
grade: 限流阈值类型（QPS 或并发线程数）
limitApp: 流控针对的调用来源，若为 default 则不区分调用来源
strategy: 调用关系限流策略
controlBehavior: 流量控制效果（直接拒绝、Warm Up、匀速排队）
```
```
private static void initFlowRules (){
List<FlowRule> rules = new ArrayList<>();
FlowRule rule = new FlowRule ();
rule.setResource ("HelloWorld");
rule.setGrade (RuleConstant. FLOW_GRADE_QPS);
// Set limit QPS to 20.
rule.setCount ( 20 );
rules.add (rule);
FlowRuleManager.loadRules (rules);
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```

###### 基本的参数

**资源名** ：唯一名称，默认请求路径

**针对来源** ：Sentinel 可以针对调用者进行限流，填写微服务名，默认为 default (不区分来源)

**阈值类型/单机阈值：**

```
1. QPS：每秒请求数，当前调用该 api 的 QPS 到达阈值的时候进行限流
2. 线程数：当调用该 api 的线程数到达阈值的时候，进行限流
```
**是否集群** ：是否为集群

###### 流控的几种 strategy ：

```
1. 直接：当 api 大达到限流条件时，直接限流
2. 关联：当关联的资源到达阈值，就限流自己
3. 链路：只记录指定路上的流量，指定资源从入口资源进来的流量，如果达到阈值，就进行限流，
api 级别的限流
```
###### 4.1 直接失败模式

**使用 API 进行资源定义**

```
/**
* 限流实现方式一: 抛出异常的方式定义资源
*
* @param orderId
* @return
*/
@ApiOperation (value = "纯代码限流")
@GetMapping ("/getOrder")
@ResponseBody
public String getOrder (@RequestParam (value = "orderId", required =
false) String orderId)
{
```
```
Entry entry = null;
// 资源名
String resourceName = "getOrder";
try
{
// entry 可以理解成入口登记
entry = SphU.entry (resourceName);
// 被保护的逻辑, 这里为订单查询接口
return "正常的业务逻辑 OrderInfo : " + orderId;
} catch (BlockException blockException)
{
// 接口被限流的时候, 会进入到这里
log.warn ("---getOrder 1 接口被限流了---, exception: ", blockException);
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
```

**代码限流规则**

**网页限流规则配置**

选择 QPS，直接，快速失败，单机阈值为 2 。

配置

```
return "接口限流, 返回空";
} finally
{
// SphU.entry (xxx) 需要与 entry.exit () 成对出现, 否则会导致调用链记录异常
if (entry != null)
{
entry.exit ();
}
}
}
```
```
26
27
28
29
30
31
32
33
34
35
```
```
//限流规则 QPS mode,
List<FlowRule> rules = new ArrayList<FlowRule>();
FlowRule rule 1 = new FlowRule ();
rule 1.setResource ("getOrder");
// QPS 控制在 2 以内
rule 1.setCount ( 2 );
// QPS 限流
rule 1.setGrade (RuleConstant. FLOW_GRADE_QPS);
rule 1.setLimitApp ("default");
rules.add (rule 1);
FlowRuleManager.loadRules (rules);
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```

```
Field 说明默认值
```
```
resource 资源名，资源名是限流规则的作用对象
```
```
count 限流阈值
```
```
grade 限流阈值类型，QPS 或线程数模式 QPS 模式
```
```
limitApp 流控针对的调用来源
default，代表不
区分调用来源
```
```
strategy
```
```
判断的根据是资源自身，还是根据其它关联资源
(refResource)，还是根据链路入口
根据资源本身
```
```
controlBehavior 流控效果（直接拒绝 / 排队等待 / 慢启动模式） 直接拒绝
```
参数

**测试**

频繁刷新请求， 1 秒访问 2 次请求，正常，超过设置的阈值，将报默认的错误。

再次的 1 秒访问 2 次请求，访问正常。超过 2 次，访问异常

###### 4.2 关联模式

调用关系包括调用方、被调用方；一个方法又可能会调用其它方法，形成一个调用链路的层次关系。
Sentinel 通过 NodeSelectorSlot 建立不同资源间的调用的关系，并且通过 ClusterBuilderSlot 记
录每个资源的实时统计信息。

当两个资源之间具有资源争抢或者依赖关系的时候，这两个资源便具有了关联。

比如对数据库同一个字段的读操作和写操作存在争抢，读的速度过高会影响写得速度，写的速度过高会
影响读的速度。如果放任读写操作争抢资源，则争抢本身带来的开销会降低整体的吞吐量。可使用关联
限流来避免具有关联关系的资源之间过度的争抢.

举例来说，read_db 和 write_db 这两个资源分别代表数据库读写，我们可以给 read_db 设置限流

规则来达到写优先的目的。具体的方法：

```
设置 `strategy` 为 `RuleConstant. STRATEGY_RELATE`
设置 `refResource` 为 `write_db`。
这样当写库操作过于频繁时，读数据的请求会被限流。
```
```
1
2
3
```

还有一个例子，电商的下订单和支付两个操作，需要优先保障支付，可以根据支付接口的流量阈
值，来对订单接口进行限制，从而保护支付的目的。

**使用注解进行资源定义**

添加 2 个请求

**代码配置关联限流规则**

```
@SentinelResource (value = "test 1", blockHandler = "exceptionHandler")
@GetMapping ("/test 1")
public String test 1 ()
{
log.info (Thread.currentThread (). getName () + "\t" + "... test 1");
return "-------hello baby，i am test 1";
}
```
```
// Block 异常处理函数，参数最后多一个 BlockException，其余与原函数一致.
public String exceptionHandler (BlockException ex)
{
// Do some log here.
ex.printStackTrace ();
log.info (Thread.currentThread (). getName () + "\t" +
"... exceptionHandler");
return String.format ("error: test 1 is not OK");
}
```
```
@SentinelResource (value = "test 1_ref")
@GetMapping ("/test 1_ref")
public String test 1_ref ()
{
log.info (Thread.currentThread (). getName () + "\t" + "... test 1_related");
return "-------hello baby，i am test 1_ref";
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```
```
16
17
18
19
20
21
22
23
24
25
```
```
// 关联模式流控 QPS 控制在 1 以内
String refResource = "test 1_ref";
FlowRule rRule = new FlowRule ("test 1")
.setCount ( 1 )  // QPS 控制在 1 以内
.setStrategy (RuleConstant. STRATEGY_RELATE)
.setRefResource (refResource);
```
```
rules.add (rRule);
FlowRuleManager.loadRules (rules);
```
```
1 2 3 4 5 6 7 8 9
```

**网页限流规则配置**

**测试**

选择 QPS，单机阈值为 1 ，选择关联，关联资源为/test_ref，这里用 Jmeter 模拟高并发，请求/test_ref。

在大批量线程高并发访问/test_ref，导致/test 失效了

链路类型的关联也类似，就不再演示了。多个请求调用同一微服务。


###### 4.3 Warm up（预热）模式

当流量突然增大的时候，我们常常会希望系统从空闲状态到繁忙状态的切换的时间长一些。即如果系统
在此之前长期处于空闲的状态，我们希望处理请求的数量是缓步的增多，经过预期的时间以后，到达系
统处理请求个数的最大值。Warm Up（冷启动，预热）模式就是为了实现这个目的的。

默认 coldFactor 为 3 ，即请求 QPS 从 threshold / 3 开始，经预热时长逐渐升至设定的 QPS 阈值。

**使用注解定义资源**

**代码限流规则**

**网页限流规则配置**

```
@SentinelResource (value = "testWarmUP", blockHandler =
"exceptionHandlerOfWarmUp")
@GetMapping ("/testWarmUP")
public String testWarmUP ()
{
log.info (Thread.currentThread (). getName () + "\t" + "... test 1");
return "-------hello baby，i am testWarmUP";
}
```
```
1 2 3 4 5 6 7
```
```
FlowRule warmUPRule = new FlowRule ();
warmUPRule.setResource ("testWarmUP");
warmUPRule.setCount ( 20 );
warmUPRule.setGrade (RuleConstant. FLOW_GRADE_QPS);
warmUPRule.setLimitApp ("default");
warmUPRule.setControlBehavior (RuleConstant. CONTROL_BEHAVIOR_WARM_UP);
warmUPRule.setWarmUpPeriodSec ( 10 );
```
```
1 2 3 4 5 6 7
```

先在单机阈值 10/3， 3 的时候，预热 10 秒后，慢慢将阈值升至 20 。刚开始刷/testWarmUP，会出现默认
错误，预热时间到了后，阈值增加，没超过阈值刷新，请求正常。

通常冷启动的过程系统允许通过的 QPS 曲线如下图所示：


如秒杀系统在开启瞬间，会有很多流量上来，很可能把系统打死，预热方式就是为了保护系统，可慢慢
的把流量放进来，慢慢的把阈值增长到设置的阈值。

**通过 jmeter 进行测试**

###### 4.4 排队等待模式

匀速排队（RuleConstant. CONTROL_BEHAVIOR_RATE_LIMITER）方式会严格控制请求通过的间隔时
间，也即是让请求以均匀的速度通过，对应的是漏桶算法。阈值必须设置为 QPS。

这种方式主要用于处理间隔性突发的流量，例如消息队列。想象一下这样的场景，在某一秒有大量的请
求到来，而接下来的几秒则处于空闲状态，我们希望系统能够在接下来的空闲期间逐渐处理这些请求，
而不是在第一秒直接拒绝多余的请求。

某瞬时来了大流量的请求, 而如果此时要处理所有请求，很可能会导致系统负载过高，影响稳定性。但其
实可能后面几秒之内都没有消息投递，若直接把多余的消息丢掉则没有充分利用系统处理消息的能力。
Sentinel 的 Rate Limiter 模式能在某一段时间间隔内以匀速方式处理这样的请求, 充分利用系统的处理能
力, 也就是削峰填谷, 保证资源的稳定性.

Sentinel 会以固定的间隔时间让请求通过, 访问资源。当请求到来的时候，如果当前请求距离上个通过的
请求通过的时间间隔不小于预设值，则让当前请求通过；否则，计算当前请求的预期通过时间，如果该
请求的预期通过时间小于规则预设的 timeout 时间，则该请求会等待直到预设时间到来通过；反之，则
马上抛出阻塞异常。

使用 Sentinel 的这种策略, 简单点说, 就是使用一个时间段 (比如 20 s 的时间) 处理某一瞬时产生的大量请求,
起到一个削峰填谷的作用, 从而充分利用系统的处理能力, 下图能很形象的展示这种场景: X 轴代表时间, Y
轴代表系统处理的请求.


**示例**

模拟 2 个用户同时并发的访问资源，发出 100 个请求,

如果设置 QPS 阈值为 1, 拒绝策略修改为 Rate Limiter 匀速
RuleConstant. CONTROL_BEHAVIOR_RATE_LIMITER 方式, 还需要设置 setMaxQueueingTimeMs (20 *
1000) 表示每一请求最长等待时间, 这里等待时间大一点, 以保证让所有请求都能正常通过;

假设这里设置的排队等待时间过小的话, 导致排队等待的请求超时而抛出异常 BlockException, 最终结果
可能是这 100 个并发请求中只有一个请求或几个才能正常通过, 所以使用这种模式得根据访问资源的耗时
时间决定排队等待时间. 按照目前这种设置, QPS 阈值为 10 的话, 每一个请求相当于是以匀速 100 ms 左右
通过.

**使用注解定义资源**

**代码限流规则**

```
@SentinelResource (value = "testLineUp",
blockHandler = "exceptionHandlerOftestLineUp")
@GetMapping ("/testLineUp")
public String testLineUp ()
{
log.info (Thread.currentThread (). getName () + "\t" + "... test 1");
return "-------hello baby，i am testLineUp";
}
```
```
1 2 3 4 5 6 7 8
```

**网页限流规则配置**

**通过 jmeter 进行测试**

```
FlowRule lineUpRule = new FlowRule ();
lineUpRule.setResource ("testLineUp");
lineUpRule.setCount ( 10 );
lineUpRule.setGrade (RuleConstant. FLOW_GRADE_QPS);
lineUpRule.setLimitApp ("default");
lineUpRule.setMaxQueueingTimeMs ( 20 * 1000 );
// CONTROL_BEHAVIOR_DEFAULT means requests more than threshold will be
rejected immediately.
// CONTROL_BEHAVIOR_DEFAULT 将超过阈值的流量立即拒绝掉.
lineUpRule.setControlBehavior (RuleConstant. CONTROL_BEHAVIOR_RATE_LIMITER);
rules.add (lineUpRule);
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```

###### 4.5 热点规则 (ParamFlowRule)

何为热点？热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数
据，并对其访问进行限制。比如：

```
商品 ID 为参数，统计一段时间内最常购买的商品 ID 并进行限制
用户 ID 为参数，针对一段时间内频繁访问的用户 ID 进行限制热点参数限流会统计传入参数中的
热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流。热点参数限流可
以看做是一种特殊的流量控制，仅对包含热点参数的资源调用生效。使用该规则需要引入依赖：
```
热点参数规则（ParamFlowRule）类似于流量控制规则（FlowRule）：


```
属性说明默认值
```
```
resource 资源名，必填
```
```
count 限流阈值，必填
```
```
grade 限流模式
QPS 模
式
```
```
durationInSec 统计窗口时间长度（单位为秒），1.6.0 版本开始支持 1 s
```
```
controlBehavior
```
```
流控效果（支持快速失败和匀速排队模式），1.6.0 版本开
始支持
```
```
快速失
败
```
```
maxQueueingTimeMs
```
```
最大排队等待时长（仅在匀速排队模式生效），1.6.0 版本
开始支持
0 ms
```
```
paramIdx
热点参数的索引，必填，对应 SphU.entry (xxx, args)
中的参数索引位置
```
```
paramFlowItemList
```
```
参数例外项，可以针对指定的参数值单独设置限流阈值，
不受前面 count 阈值的限制。仅支持基本类型和字符串类
型
```
```
clusterMode 是否是集群参数流控规则 false
```
```
clusterConfig 集群流控相关配置
```
**自定义资源**

**限流规则代码：**

可以通过 ParamFlowRuleManager 的 loadRules 方法更新热点参数规则，下面是官方实例：

```
@GetMapping ("/byHotKey")
@SentinelResource (value = "byHotKey",
blockHandler = "userAccessError")
public String test 4 (@RequestParam (value = "userId", required = false) String
userId,
@RequestParam (value = "goodId", required = false) int
goodId)
{
log.info (Thread.currentThread (). getName () + "\t" + "... byHotKey");
return "-----------by HotKey： UserId";
}
```
```
1 2 3 4 5 6 7 8 9
```

具体的限流代码如下：

**网页限流规则配置**

```
ParamFlowRule rule = new ParamFlowRule (resourceName)
.setParamIdx ( 0 )
.setCount ( 5 );
// 针对 int 类型的参数 PARAM_B，单独设置限流 QPS 阈值为 10 ，而不是全局的阈值 5.
ParamFlowItem item = new ParamFlowItem (). setObject (String.valueOf (PARAM_B))
.setClassType (int.class.getName ())
.setCount ( 10 );
rule.setParamFlowItemList (Collections.singletonList (item));
```
```
ParamFlowRuleManager.loadRules (Collections.singletonList (rule));
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
ParamFlowRule pRule = new ParamFlowRule ("byHotKey")
.setParamIdx ( 1 )
.setCount ( 1 );
// 针对参数值 1000 ，单独设置限流 QPS 阈值为 5 ，而不是全局的阈值 1.
ParamFlowItem item = new ParamFlowItem (). setObject (String.valueOf ( 1000 ))
.setClassType (int.class.getName ())
.setCount ( 5 );
pRule.setParamFlowItemList (Collections.singletonList (item));
```
```
ParamFlowRuleManager.loadRules (Collections.singletonList (pRule));
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```

#### 5 、Sentinel 系统保护

###### 系统保护的目的

在开始之前，我们先了解一下系统保护的目的：

```
保证系统不被拖垮
在系统稳定的前提下，保持系统的吞吐量
```
长期以来，系统保护的思路是根据硬指标，即系统的负载 (load 1) 来做系统过载保护。当系统负载高于
某个阈值，就禁止或者减少流量的进入；当 load 开始好转，则恢复流量的进入。这个思路给我们带来了
不可避免的两个问题：

```
load 是一个“结果”，如果根据 load 的情况来调节流量的通过率，那么就始终有延迟性。也就意味
着通过率的任何调整，都会过一段时间才能看到效果。当前通过率是使 load 恶化的一个动作，那
么也至少要过 1 秒之后才能观测到；同理，如果当前通过率调整是让 load 好转的一个动作，也需
要 1 秒之后才能继续调整，这样就浪费了系统的处理能力。所以我们看到的曲线，总是会有抖动。
恢复慢。想象一下这样的一个场景（真实），出现了这样一个问题，下游应用不可靠，导致应用
RT 很高，从而 load 到了一个很高的点。过了一段时间之后下游应用恢复了，应用 RT 也相应减
少。这个时候，其实应该大幅度增大流量的通过率；但是由于这个时候 load 仍然很高，通过率的
恢复仍然不高。
```

系统保护的目标是 **在系统不被拖垮的情况下，提高系统的吞吐率，而不是 load 一定要到低于某个阈
值** 。如果我们还是按照固有的思维，超过特定的 load 就禁止流量进入，系统 load 恢复就放开流量，这
样做的结果是无论我们怎么调参数，调比例，都是按照果来调节因，都无法取得良好的效果。

Sentinel 在系统自适应保护的做法是，用 load 1 作为启动自适应保护的因子，而允许通过的流量由处理
请求的能力，即请求的响应时间以及当前系统正在处理的请求速率来决定。

**系统保护规则的应用**

系统规则支持以下的模式：

```
Load 自适应 （仅对 Linux/Unix-like 机器生效）：系统的 load 1 作为启发指标，进行自适应系统保
护。当系统 load 1 超过设定的启发值，且系统当前的并发线程数超过估算的系统容量时才会触发系
统保护（BBR 阶段）。系统容量由系统的 maxQps * minRt 估算得出。设定参考值一般是 CPU
cores * 2.5。
CPU usage （1.5.0+ 版本）：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0），
比较灵敏。
平均 RT ：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。
并发线程数 ：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。
入口 QPS ：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。
```
系统保护规则是从应用级别的入口流量进行控制，从单台机器的 load、CPU 使用率、平均 RT、入口
QPS 和并发线程数等几个维度监控应用指标，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定

性。

系统保护规则是应用整体维度的，而不是资源维度的，并且 **仅对入口流量生效** 。入口流量指的是进入应
用的流量（EntryType. IN），比如 Web 服务或 Dubbo 服务端接收的请求，都属于入口流量。

系统规则的参数说明:

```
highestSystemLoad 最大的 load 1，参考值 -1 (不生效)
avgRt 所有入口流量的平均响应时间 -1 (不生效)
maxThread 入口流量的最大并发数 -1 (不生效)
qps 所有入口资源的 QPS -1 (不生效)
```
硬编码的方式定义流量控制规则如下：

```
List<SystemRule> srules = new ArrayList<>();
SystemRule srule = new SystemRule ();
srule.setAvgRt ( 3000 );
srules.add (srule);
SystemRuleManager.loadRules (srules);
```
```
1
2
3
4
5
```

**网页限流规则配置**

#### 6 、黑白名单规则

很多时候，我们需要根据调用方来限制资源是否通过，这时候可以使用 Sentinel 的访问控制（黑白名
单）的功能。黑白名单根据资源的请求来源（origin）限制资源是否通过，若配置白名单则只有请求来
源位于白名单内时才可通过；若配置黑名单则请求来源位于黑名单时不通过，其余的请求通过。

```
调用方信息通过 ContextUtil.enter (resourceName, origin) 方法中的 origin 参数传入。
```
###### 访问控制规则 (AuthorityRule)

授权规则，即黑白名单规则（AuthorityRule）非常简单，主要有以下配置项：

```
resource：资源名，即限流规则的作用对象
limitApp：对应的黑名单/白名单，不同 origin 用 , 分隔，如 appA, appB
strategy：限制模式，AUTHORITY_WHITE 为白名单模式，AUTHORITY_BLACK 为黑名单模式，默
认为白名单模式比如我们希望控制对资源 test 的访问设置白名单，只有来源为 appA 和 appB 的
请求才可通过，则可以配置如下白名单规则：
```
#### 7 、如何定义资源

Sentinel 可以简单的分为 Sentinel 核心库和 Dashboard。

核心库不依赖 Dashboard，可以通过代码手段定义资源。

我们说的资源，可以是任何东西，服务，服务里的方法，甚至是一段代码。

```
AuthorityRule rule = new AuthorityRule ();
rule.setResource ("test");
rule.setStrategy (RuleConstant. AUTHORITY_WHITE);
rule.setLimitApp ("appA, appB");
AuthorityRuleManager.loadRules (Collections.singletonList (rule));
```
```
1
2
3
4
5
```

使用 Sentinel 来进行资源保护，主要分为几个步骤:

```
1. 定义资源
2. 定义规则
3. 检验规则是否生效
```
先把可能需要保护的资源定义好（埋点），之后再配置规则。

也可以理解为，只要有了资源，我们就可以在任何时候灵活地定义各种流量控制规则。在编码的时候，
只需要考虑这个代码是否需要保护，如果需要保护，就将之定义为一个资源。

对于主流的框架，我们提供适配，只需要按照适配中的说明配置，Sentinel 就会默认定义提供的服务，
方法等为资源。

###### 方式一：主流框架的默认适配

为了减少开发的复杂程度，我们对大部分的主流框架，例如 Web Servlet、Dubbo、Spring Cloud、
gRPC、Spring WebFlux、Reactor 等都做了适配。您只需要引入对应的依赖即可方便地整合 Sentinel。
可以参见: 主流框架的适配。

###### 方式二：抛出异常的方式定义资源

```
SphU 包含了 try-catch 风格的 API。用这种方式，当资源发生了限流之后会抛出 BlockException。这
```
个时候可以捕捉异常，进行限流之后的逻辑处理。示例代码如下:

**特别地** ，若 entry 的时候传入了热点参数，那么 exit 的时候也一定要带上对应的参数（exit (count,

args)），否则可能会有统计错误。这个时候不能使用 try-with-resources 的方式。另外通过

```
Tracer.trace (ex) 来统计异常信息时，由于 try-with-resources 语法中 catch 调用顺序的问题，会导
```
致无法正确统计异常数，因此统计异常信息时也不能在 try-with-resources 的 catch 块中调用
Tracer.trace (ex)。

手动 exit 示例：

```
// 1.5.0 版本开始可以利用 try-with-resources 特性（使用有限制）
// 资源名可使用任意有业务语义的字符串，比如方法名、接口名或其它可唯一标识的字符串。
try (Entry entry = SphU.entry ("resourceName")) {
// 被保护的业务逻辑
// do something here...
} catch (BlockException ex) {
// 资源访问阻止，被限流或被降级
// 在此处进行相应的处理操作
}
```
```
1 2 3 4 5 6 7 8 9
```
```
Entry entry = null;
// 务必保证 finally 会被执行
try {
// 资源名可使用任意有业务语义的字符串，注意数目不能太多（超过 1 K），超出几千请作为参数
传入而不要直接作为资源名
// EntryType 代表流量类型（inbound/outbound），其中系统规则只对 IN 类型的埋点生
效
entry = SphU.entry ("自定义资源名");
// 被保护的业务逻辑
// do something...
} catch (BlockException ex) {
// 资源访问阻止，被限流或被降级
// 进行相应的处理操作
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```

```
参数名类型解释默认值
```
```
entryType EntryType
```
```
资源调用的流量类型，是入口流量
（EntryType. IN）还是出口流量
（EntryType. OUT），注意系统规则只对
IN 生效
```
```
EntryType. OUT
```
```
count int 本次资源调用请求的 token 数目 1
```
```
args Object[] 传入的参数，用于热点参数限流无
```
热点参数埋点示例：

```
SphU.entry () 的参数描述：
```
**注意** ：SphU.entry (xxx) 需要与 entry.exit () 方法成对出现，匹配调用，否则会导致调用链记录异

常，抛出 ErrorEntryFreeException 异常。常见的错误：

```
自定义埋点只调用 SphU.entry ()，没有调用 entry.exit ()
顺序错误，比如：entry 1 -> entry 2 -> exit 1 -> exit 2，应该为 entry 1 -> entry 2 ->
exit 2 -> exit 1
```
###### 方式三：返回布尔值方式定义资源

```
SphO 提供 if-else 风格的 API。用这种方式，当资源发生了限流之后会返回 false，这个时候可以根据
```
返回值，进行限流之后的逻辑处理。示例代码如下:

```
} catch (Exception ex) {
// 若需要配置降级规则，需要通过这种方式记录业务异常
Tracer.traceEntry (ex, entry);
} finally {
// 务必保证 exit，务必保证每个 entry 与 exit 配对
if (entry != null) {
entry.exit ();
}
}
```
```
12
13
14
15
16
17
18
19
20
```
```
Entry entry = null;
try {
// 若需要配置例外项，则传入的参数只支持基本类型。
// EntryType 代表流量类型，其中系统规则只对 IN 类型的埋点生效
// count 大多数情况都填 1 ，代表统计为一次调用。
entry = SphU.entry (resourceName, EntryType. IN, 1 , paramA, paramB);
// Your logic here.
} catch (BlockException ex) {
// Handle request rejection.
} finally {
// 注意：exit 的时候也一定要带上对应的参数，否则可能会有统计错误。
if (entry != null) {
entry.exit ( 1 , paramA, paramB);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```
```
// 资源名可使用任意有业务语义的字符串
if (SphO.entry ("自定义资源名")) {
```
```
1
2
```

**注意** ：SphO.entry (xxx) 需要与 SphO.exit () 方法成对出现，匹配调用，位置正确，否则会导致调用链记录

异常，抛出 ErrorEntryFreeException` 异常。

###### 方式四：注解方式定义资源

Sentinel 支持通过 @SentinelResource 注解定义资源并配置 blockHandler 和 fallback 函数来进

行限流之后的处理。示例：

注意 blockHandler 函数会在原方法被限流/降级/系统保护的时候调用，而 fallback 函数会针对所有
类型的异常。请注意 blockHandler 和 fallback 函数的形式要求，更多指引可以参见 Sentinel 注解

支持文档。

###### 方式五：异步调用支持

Sentinel 支持异步调用链路的统计。在异步调用中，需要通过 SphU.asyncEntry (xxx) 方法定义资

源，并通常需要在异步的回调函数中调用 exit 方法。以下是一个简单的示例：

```
// 务必保证 finally 会被执行
try {
/**
* 被保护的业务逻辑
*/
} finally {
SphO.exit ();
}
} else {
// 资源访问阻止，被限流或被降级
// 进行相应的处理操作
}
```
```
3 4 5 6 7 8 9
```
```
10
11
12
13
14
```
```
// 原本的业务方法.
@SentinelResource (blockHandler = "blockHandlerForGetUser")
public User getUserById (String id) {
throw new RuntimeException ("getUserById command failed");
}
```
```
// blockHandler 函数，原方法调用被限流/降级/系统保护的时候调用
public User blockHandlerForGetUser (String id, BlockException ex) {
return new User ("admin");
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
try {
AsyncEntry entry = SphU.asyncEntry (resourceName);
```
```
// 异步调用.
doAsync (userId, result -> {
try {
// 在此处处理异步调用的结果.
} finally {
// 在回调结束后 exit.
entry.exit ();
}
});
} catch (BlockException ex) {
// Request blocked.
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
```

```
SphU.asyncEntry (xxx) 不会影响当前（调用线程）的 Context，因此以下两个 entry 在调用链上是平
```
级关系（处于同一层），而不是嵌套关系：

若在异步回调中需要嵌套其它的资源调用（无论是 entry 还是 asyncEntry），只需要借助 Sentinel

提供的上下文切换功能，在对应的地方通过 ContextUtil.runOnContext (context, f) 进行 Context

变换，将对应资源调用处的 Context 切换为生成的异步 Context，即可维持正确的调用链路关系。示例
如下：

```
// Handle the exception (e.g. retry or fallback).
}
```
```
15
16
```
```
// 调用链类似于：
// -parent
// ---asyncResource
// ---syncResource
asyncEntry = SphU.asyncEntry (asyncResource);
entry = SphU.entry (normalResource);
```
```
1 2 3 4 5 6
```
```
public void handleResult (String result) {
Entry entry = null;
try {
entry = SphU.entry ("handleResultForAsync");
// Handle your result here.
} catch (BlockException ex) {
// Blocked for the result handler.
} finally {
if (entry != null) {
entry.exit ();
}
}
}
```
```
public void someAsync () {
try {
AsyncEntry entry = SphU.asyncEntry (resourceName);
```
```
// Asynchronous invocation.
doAsync (userId, result -> {
// 在异步回调中进行上下文变换，通过 AsyncEntry 的 getAsyncContext 方法
获取异步 Context
ContextUtil.runOnContext (entry.getAsyncContext (), () -> {
try {
// 此处嵌套正常的资源调用.
handleResult (result);
} finally {
entry.exit ();
}
});
});
} catch (BlockException ex) {
// Request blocked.
// Handle the exception (e.g. retry or fallback).
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
```
```
22
23
24
25
26
27
28
29
30
31
32
33
34
35
```

此时的调用链就类似于：

更详细的示例可以参考 Demo 中的 AsyncEntryDemo，里面包含了普通资源与异步资源之间的各种嵌套
示例。

#### 8 、核心组件

###### Resource

resource 是 sentinel 中最重要的一个概念，sentinel 通过资源来保护具体的业务代码或其他后方服务。
sentinel 把复杂的逻辑给屏蔽掉了，用户只需要为受保护的代码或服务定义一个资源，然后定义规则就
可以了，剩下的通通交给 sentinel 来处理了。并且资源和规则是解耦的，规则甚至可以在运行时动态修
改。定义完资源后，就可以通过在程序中埋点来保护你自己的服务了，埋点的方式有两种：

```
try-catch 方式（通过 SphU.entry (...)），当 catch 到 BlockException 时执行异常处理 (或
fallback)
if-else 方式（通过 SphO.entry (...)），当返回 false 时执行异常处理 (或 fallback)
```
以上这两种方式都是通过硬编码的形式定义资源然后进行资源埋点的，对业务代码的侵入太大，从 0.1.1
版本开始，sentinel 加入了注解的支持，可以通过注解来定义资源，具体的注解为：SentinelResource
。通过注解除了可以定义资源外，还可以指定 blockHandler 和 fallback 方法。

在 sentinel 中具体表示资源的类是：ResourceWrapper ，他是一个抽象的包装类，包装了资源的 Name
和 EntryType。他有两个实现类，分别是：StringResourceWrapper 和 MethodResourceWrapper。顾
名思义，StringResourceWrapper 是通过对一串字符串进行包装，是一个通用的资源包装类，
MethodResourceWrapper 是对方法调用的包装。

###### Context

Context 是对资源操作时的上下文环境，每个资源操作 (针对 Resource 进行的 entry/exit) 必须属于一个

Context，如果程序中未指定 Context，会创建 name 为"sentinel_default_context"的默认 Context。一
个 Context 生命周期内可能有多个资源操作，Context 生命周期内的最后一个资源 exit 时会清理该
Context，这也预示这整个 Context 生命周期的结束。Context 主要属性如下：

- parent
---asyncInvocation
-----handleResultForAsync

```
1
2
3
```

注意：一个 Context 生命期内 Context 只能初始化一次，因为是存到 ThreadLocal 中，并且只有在非 null
时才会进行初始化。

如果想在调用 SphU.entry () 或 SphO.entry () 前，自定义一个 context，则通过 ContextUtil.enter () 方法
来创建。context 是保存在 ThreadLocal 中的，每次执行的时候会优先到 ThreadLocal 中获取，为 null 时
会调用 MyContextUtil.myEnter (Constants. CONTEXT_DEFAULT_NAME, "",

resourceWrapper.getType ()) 创建一个 context。当 Entry 执行 exit 方法时，如果 entry 的 parent 节点为
null，表示是当前 Context 中最外层的 Entry 了，此时将 ThreadLocal 中的 context 清空。

**Context 的创建与销毁**

首先我们要清楚的一点就是，每次执行 entry () 方法，试图冲破一个资源时，都会生成一个上下文。这个
上下文中会保存着调用链的根节点和当前的入口。

Context 是通过 ContextUtil 创建的，具体的方法是 trueEntry，代码如下：

```
public class Context {
// context 名字，默认名字 "sentinel_default_context"
private final String name;
// context 入口节点，每个 context 必须有一个 entranceNode
private DefaultNode entranceNode;
// context 当前 entry，Context 生命周期中可能有多个 Entry，所有 curEntry 会有变化
private Entry curEntry;
// The origin of this context (usually indicate different invokers, e.g.
service consumer name or origin IP).
private String origin = "";
private final boolean async;
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
protected static Context trueEnter (String name, String origin) {
// 先从 ThreadLocal 中获取
Context context = contextHolder.get ();
if (context == null) {
// 如果 ThreadLocal 中获取不到 Context
// 则根据 name 从 map 中获取根节点，只要是相同的资源名，就能直接从 map 中获取到 node
Map<String, DefaultNode> localCacheNameMap = contextNameNodeMap;
DefaultNode node = localCacheNameMap.get (name);
if (node == null) {
// 省略部分代码
try {
LOCK.lock ();
node = contextNameNodeMap.get (name);
if (node == null) {
// 省略部分代码
// 创建一个新的入口节点
node = new EntranceNode (new StringResourceWrapper (name,
EntryType. IN), null);
Constants.ROOT.addChild (node);
// 省略部分代码
}
} finally {
LOCK.unlock ();
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```
```
18
19
20
21
22
23
```

上面的代码中我省略了部分代码，只保留了核心的部分。从源码中还是可以比较清晰的看出生成
Context 的过程：

```
1. 先从 ThreadLocal 中获取，如果能获取到直接返回，如果获取不到则继续第 2 步
2. 从一个 static 的 map 中根据上下文的名称获取，如果能获取到则直接返回，否则继续第 3 步
3. 加锁后进行一次 double check，如果还是没能从 map 中获取到，则创建一个 EntranceNode，并
把该 EntranceNode 添加到一个全局的 ROOT 节点中去，然后将该节点添加到 map 中去 (这部分代码
在上述代码中省略了)
4. 根据 EntranceNode 创建一个上下文，并将该上下文保存到 ThreadLocal 中去，下一个请求可以直
接获取
```
那保存在 ThreadLocal 中的上下文什么时候会清除呢？从代码中可以看到具体的清除工作在 ContextUtil
的 exit 方法中，当执行该方法时，会将保存在 ThreadLocal 中的 context 对象清除，具体的代码非常简
单，这里就不贴代码了。

那 ContextUtil. exit 方法什么时候会被调用呢？有两种情况：一是主动调用 ContextUtil. exit 的时候，二是
当一个入口 Entry 要退出，执行该 Entry 的 trueExit 方法的时候，此时会触发 ContextUtil. exit 的方法。但
是有一个前提，就是当前 Entry 的父 Entry 为 null 时，此时说明该 Entry 已经是最顶层的根节点了，可以清
除 context。

###### Entry

刚才在 Context 身影中也看到了 Entry 的出现，现在就谈谈 Entry。每次执行 SphU.entry () 或
SphO.entry () 都会返回一个 Entry，Entry 表示一次资源操作，内部会保存当前 invocation 信息。在一个
Context 生命周期中多次资源操作，也就是对应多个 Entry，这些 Entry 形成 parent/child 结构保存在
Entry 实例中，entry 类 CtEntry 结构如下：

```
}
// 创建一个新的 Context，并设置 Context 的根节点，即设置 EntranceNode
context = new Context (node, name);
context.setOrigin (origin);
// 将该 Context 保存到 ThreadLocal 中去
contextHolder.set (context);
}
return context;
}
```
```
24
25
26
27
28
29
30
31
32
```
```
class CtEntry extends Entry {
protected Entry parent = null;
protected Entry child = null;
```
```
protected ProcessorSlot<Object> chain;
protected Context context;
}
public abstract class Entry implements AutoCloseable {
private long createTime;
private Node curNode;
/**
* {@link Node} of the specific origin, Usually the origin is the Service
Consumer.
*/
private Node originNode;
private Throwable error; // 是否出现异常
protected ResourceWrapper resourceWrapper; // 资源信息
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
13
14
15
16
17
```

Entry 实例代码中出现了 Node，这个又是什么东东呢 : (，咱们接着往下看：

###### DefaultNode

Node（关于 _StatisticNode_ 的讨论放到下一小节）默认实现类 DefaultNode，该类还有一个子类
EntranceNode；context 有一个 entranceNode 属性，Entry 中有一个 curNode 属性。

```
EntranceNode ：该类的创建是在初始化 Context 时完成的（ContextUtil. trueEnter 方法），注意
该类是针对 Context 维度的，也就是一个 context 有且仅有一个 EntranceNode。
DefaultNode ：该类的创建是在 NodeSelectorSlot. entry 完成的，当不存在 context. name 对应的
DefaultNode 时会新建（new DefaultNode (resourceWrapper, null)，对应 resouce）并保存到本
地缓存（NodeSelectorSlot 中 private volatile Map<String, DefaultNode> map）；获取到
context. name 对应的 DefaultNode 后会将该 DefaultNode 设置到当前 context 的 curEntry. curNode
属性，也就是说，在 NodeSelectorSlot 中是一个 context 有且仅有一个 DefaultNode。
```
看到这里，你是不是有疑问？为什么一个 context 有且仅有一个 DefaultNode，我们的 resouece 跑哪去
了呢，其实，这里的一个 context 有且仅有一个 DefaultNode 是在 NodeSelectorSlot 范围内，
NodeSelectorSlot 是 ProcessorSlotChain 中的一环，获取 ProcessorSlotChain 是根据 Resource 维度来
的。总结为一句话就是： **针对同一个 Resource，多个 context 对应多个 DefaultNode；针对不同
Resource，(不管是否是同一个 context) 对应多个不同 DefaultNode** 。这还没看明白 : (，好吧，我不
bb 了，上图吧：

一个 Resouce 只有一个 clusterNode，多个 defaultNode 对应一个 clusterNode，如果
defaultNode. clusterNode 为 null，则在 ClusterBuilderSlot. entry 中会进行初始化。

同一个 Resource，对应同一个 ProcessorSlotChain，这块处理逻辑在 lookProcessChain 方法中，如
下：

```
public class DefaultNode extends StatisticNode {
private ResourceWrapper id;
/**
* The list of all child nodes.
* 子节点集合
*/
private volatile Set<Node> childList = new HashSet<>();
/**
* Associated cluster node.
*/
private ClusterNode clusterNode;
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
ProcessorSlot<Object> lookProcessChain (ResourceWrapper resourceWrapper) {
ProcessorSlotChain chain = chainMap.get (resourceWrapper);
if (chain == null) {
synchronized (LOCK) {
chain = chainMap.get (resourceWrapper);
if (chain == null) {
// Entry size limit.
if (chainMap.size () >= Constants. MAX_SLOT_CHAIN_SIZE) {
```
```
1 2 3 4 5 6 7 8
```

###### StatisticNode

StatisticNode 中保存了资源的实时统计数据（基于滑动时间窗口机制），通过这些统计数据，sentinel
才能进行限流、降级等一系列操作。StatisticNode 属性如下：

其中 MetricBucket. counters 数组大小为 MetricEvent 枚举值的个数，每个枚举对应一个统计项，比如
PASS 表示通过个数，限流可根据通过的个数和设置的限流规则配置 count 大小比较，得出是否触发限流
操作，所有枚举值如下：

```
return null;
}
```
```
chain = SlotChainProvider.newSlotChain ();
Map<ResourceWrapper, ProcessorSlotChain> newMap =
newHashMap<ResourceWrapper, ProcessorSlotChain>(
chainMap.size () + 1 );
newMap.putAll (chainMap);
newMap.put (resourceWrapper, chain);
chainMap = newMap;
}
}
}
return chain;
}
```
```
9
10
11
12
13
```
```
14
15
16
17
18
19
20
21
22
```
```
public class StatisticNode implements Node {
/**
* 秒级的滑动时间窗口（时间窗口单位 500 ms）
*/
private transient volatile Metric rollingCounterInSecond =
newArrayMetric (SampleCountProperty. SAMPLE_COUNT,
```
```
IntervalProperty. INTERVAL);
/**
* 分钟级的滑动时间窗口（时间窗口单位 1 s）
*/
private transient Metric rollingCounterInMinute = new ArrayMetric ( 60 , 60
* 1000 ,false);
/**
* The counter for thread count.
* 线程个数用户触发线程数流控
*/
private LongAdder curThreadNum = new LongAdder ();
}
public class ArrayMetric implements Metric {
private final LeapArray<MetricBucket> data;
}
public class MetricBucket {
// 保存统计值
private final LongAdder[] counters;
// 最小 rt
private volatile long minRt;
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
```

#### 9 、插槽 Slot

slot 是另一个 sentinel 中非常重要的概念，sentinel 的工作流程就是围绕着一个个插槽所组成的插槽链来
展开的。需要注意的是每个插槽都有自己的职责，他们各司其职完好的配合，通过一定的编排顺序，来
达到最终的限流降级的目的。默认的各个插槽之间的顺序是固定的，因为有的插槽需要依赖其他的插槽
计算出来的结果才能进行工作。

但是这并不意味着我们只能按照框架的定义来，sentinel 通过 SlotChainBuilder 作为 SPI 接口，使得
Slot Chain 具备了扩展的能力。我们可以通过实现 SlotsChainBuilder 接口加入自定义的 slot 并自定义
编排各个 slot 之间的顺序，从而可以给 sentinel 添加自定义的功能。

那 SlotChain 是在哪创建的呢？是在 CtSph.lookProcessChain () 方法中创建的，并且该方法会根据当前
请求的资源先去一个静态的 HashMap 中获取，如果获取不到才会创建，创建后会保存到 HashMap 中。
这就意味着，同一个资源会全局共享一个 SlotChain。默认生成 ProcessorSlotChain 为：

这里大概的介绍下每种 Slot 的功能职责：

```
NodeSelectorSlot 负责收集资源的路径，并将这些资源的调用路径，以树状结构存储起来，用
于根据调用路径来限流降级；
ClusterBuilderSlot 则用于存储资源的统计信息以及调用者信息，例如该资源的 RT, QPS,
thread count 等等，这些信息将用作为多维度限流，降级的依据；
StatisticsSlot 则用于记录，统计不同维度的 runtime 信息；
SystemSlot 则通过系统的状态，例如 load 1 等，来控制总的入口流量；
```
```
public enum MetricEvent {
PASS, // Normal pass.
BLOCK, // Normal block.
EXCEPTION,
SUCCESS,
RT,
OCCUPIED_PASS
}
```
```
1 2 3 4 5 6 7 8
```
```
// DefaultSlotChainBuilder
public ProcessorSlotChain build () {
ProcessorSlotChain chain = new DefaultProcessorSlotChain ();
chain.addLast (new NodeSelectorSlot ());
chain.addLast (new ClusterBuilderSlot ());
chain.addLast (new LogSlot ());
chain.addLast (new StatisticSlot ());
chain.addLast (new SystemSlot ());
chain.addLast (new AuthoritySlot ());
chain.addLast (new FlowSlot ());
chain.addLast (new DegradeSlot ());
```
```
return chain;
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```

```
AuthoritySlot 则根据黑白名单，来做黑白名单控制；
FlowSlot 则用于根据预设的限流规则，以及前面 slot 统计的状态，来进行限流；
DegradeSlot 则通过统计信息，以及预设的规则，来做熔断降级；
```
每个 Slot 执行完业务逻辑处理后，会调用 fireEntry () 方法，该方法将会触发下一个节点的 entry 方法，下
一个节点又会调用他的 fireEntry，以此类推直到最后一个 Slot，由此就形成了 sentinel 的责任链。

下面我们就来详细研究下这些 Slot 的原理。

###### NodeSelectorSlot

```
NodeSelectorSlot 是用来构造调用链的，具体的是将资源的调用路径，封装成一个一个的节点，再组
```
成一个树状的结构来形成一个完整的调用链，NodeSelectorSlot 是所有 Slot 中最关键也是最复杂的一

个 Slot，这里涉及到以下几个核心的概念：

```
Resource
```
资源是 Sentinel 的关键概念。它可以是 Java 应用程序中的任何内容，例如，由应用程序提供的服务，
或由应用程序调用的其它服务，甚至可以是一段代码。

只要通过 Sentinel API 定义的代码，就是资源，能够被 Sentinel 保护起来。大部分情况下，可以使用方
法签名，URL，甚至服务名称作为资源名来标示资源。

简单来说，资源就是 Sentinel 用来保护系统的一个媒介。源码中用来包装资源的类是：
com. alibaba. csp. sentinel. slotchain. ResourceWrapper，他有两个子类：
StringResourceWrapper 和 MethodResourceWrapper，通过名字就知道可以将一段字符串或一个方

法包装为一个资源。

打个比方，我有一个服务 A，请求非常多，经常会被陡增的流量冲垮，为了防止这种情况，简单的做
法，我们可以定义一个 Sentinel 的资源，通过该资源来对请求进行调整，使得允许通过的请求不会把服
务 A 搞崩溃。

每个资源的状态也是不同的，这取决于资源后端的服务，有的资源可能比较稳定，有的资源可能不太稳
定。那么在整个调用链中，Sentinel 需要对不稳定资源进行控制。当调用链路中某个资源出现不稳定，
例如，表现为 timeout，或者异常比例升高的时候，则对这个资源的调用进行限制，并让请求快速失
败，避免影响到其它的资源，最终导致雪崩的后果。

```
Context
```

上下文是一个用来保存调用链当前状态的元数据的类，每次进入一个资源时，就会创建一个上下文。 **相
同的资源名可能会创建多个上下文。** 一个 Context 中包含了三个核心的对象：

1 ）当前调用链的根节点：EntranceNode

2 ）当前的入口：Entry

3 ）当前入口所关联的节点：Node

上下文中只会保存一个当前正在处理的入口 Entry，另外还会保存调用链的根节点。 **需要注意的是，每次
进入一个新的资源时，都会创建一个新的上下文。**

```
Entry
```
每次调用 SphU #entry () 都会生成一个 Entry 入口，该入口中会保存了以下数据：入口的创建时间，当

前入口所关联的节点，当前入口所关联的调用源对应的节点。Entry 是一个抽象类，他只有一个实现类，
在 CtSph 中的一个静态类：CtEntry

```
Node
```
节点是用来保存某个资源的各种实时统计信息的，他是一个接口，通过访问节点，就可以获取到对应资
源的实时状态，以此为依据进行限流和降级操作。

可能看到这里，大家还是比较懵，这么多类到底有什么用，接下来就让我们更进一步，挖掘一下这些类
的作用，在这之前，我先给大家展示一下他们之间的关系，如下图所示：

这里把几种 Node 的作用先大概介绍下：


```
节点作用
```
```
StatisticNode 执行具体的资源统计操作
```
```
DefaultNode
```
```
该节点持有指定上下文中指定资源的统计信息，当在同一个上下文中多次调用
entry 方法时，该节点可能下会创建有一系列的子节点。另外每个 DefaultNode
中会关联一个 ClusterNode
```
```
ClusterNode
该节点中保存了资源的总体的运行时统计信息，包括 rt，线程数，qps 等等，相
同的资源会全局共享同一个 ClusterNode，不管他属于哪个上下文
```
```
EntranceNode
该节点表示一棵调用链树的入口节点，通过他可以获取调用链树中所有的子节
点
```
###### 调用链树

当在一个上下文中多次调用了 SphU #entry () 方法时，就会创建一棵调用链树。具体的代码在 entry 方法
中创建 CtEntry 对象时：

这里可能看代码没有那么直观，可以用一些图形来描述一下这个过程。

###### 构造树干

**创建 context**

context 的创建在上面已经分析过了，初始化的时候，context 中的 curEntry 属性是没有值的，如下图所
示：

```
CtEntry (ResourceWrapper resourceWrapper, ProcessorSlot<Object> chain,
Context context) {
super (resourceWrapper);
this. chain = chain;
this. context = context;
// 获取「上下文」中上一次的入口
parent = context.getCurEntry ();
if (parent != null) {
// 然后将当前入口设置为上一次入口的子节点
((CtEntry) parent). child = this;
}
// 设置「上下文」的当前入口为该类本身
context.setCurEntry (this);
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```

**创建 Entry**

每创建一个新的 Entry 对象时，都会重新设置 context 的 curEntry，并将 context 原来的 curEntry 设置为该
新 Entry 对象的父节点，如下图所示：

**退出 Entry**

某个 Entry 退出时，将会重新设置 context 的 curEntry，当该 Entry 是最顶层的一个入口时，将会把
ThreadLocal 中保存的 context 也清除掉，如下图所示：


###### 构造叶子节点

上面的过程是构造了一棵调用链的树，但是这棵树只有树干，没有叶子，那叶子节点是在什么时候创建
的呢？DefaultNode 就是叶子节点，在叶子节点中保存着目标资源在当前状态下的统计信息。通过分
析，我们知道了叶子节点是在 NodeSelectorSlot 的 entry 方法中创建的。具体的代码如下：

```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper, Object
obj, int count, Object... args) throws Throwable {
// 根据「上下文」的名称获取 DefaultNode
// 多线程环境下，每个线程都会创建一个 context，
// 只要资源名相同，则 context 的名称也相同，那么获取到的节点就相同
DefaultNode node = map.get (context.getName ());
if (node == null) {
synchronized (this) {
node = map.get (context.getName ());
if (node == null) {
// 如果当前「上下文」中没有该节点，则创建一个 DefaultNode 节点
node = Env.nodeBuilder.buildTreeNode (resourceWrapper, null);
// 省略部分代码
}
// 将当前 node 作为「上下文」的最后一个节点的子节点添加进去
// 如果 context 的 curEntry. parent. curNode 为 null，则添加到 entranceNode
中去
// 否则添加到 context 的 curEntry. parent. curNode 中去
((DefaultNode) context.getLastNode ()). addChild (node);
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```
```
17
18
19
```

上面的代码可以分解成下面这些步骤：

1 ）获取当前上下文对应的 DefaultNode，如果没有的话会为当前的调用新生成一个 DefaultNode 节点，
它的作用是对资源进行各种统计度量以便进行流控；

2 ）将新创建的 DefaultNode 节点，添加到 context 中，作为「entranceNode」或者
「curEntry. parent. curNode」的子节点；

3 ）将 DefaultNode 节点，添加到 context 中，作为「curEntry」的 curNode。

上面的第 2 步，不是每次都会执行。我们先看第 3 步，把当前 DefaultNode 设置为 context 的 curNode，实
际上是把当前节点赋值给 context 中 curEntry 的 curNode，用图形表示就是这样：

多次创建不同的 Entry，并且执行 NodeSelectorSlot 的 entry 方法后，就会变成这样一棵调用链树：

**PS：这里图中的 node 0，node 1，node 2 可能是相同的 node，因为在同一个 context 中从 map 中获取
的 node 是同一个，这里只是为了表述的更清楚所以用了不同的节点名。**

```
}
// 将该节点设置为「上下文」中的当前节点
// 实际是将当前节点赋值给 context 中 curEntry 的 curNode
// 在 Context 的 getLastNode 中会用到在此处设置的 curNode
context.setCurNode (node);
fireEntry (context, resourceWrapper, node, count, args);
}
```
```
20
21
22
23
24
25
26
27
```

**保存子节点**

上面已经分析了叶子节点的构造过程，叶子节点是保存在各个 Entry 的 curNode 属性中的。

我们知道 context 中只保存了入口节点和当前 Entry，那子节点是什么时候保存的呢，其实子节点就是上
面代码中的第 2 步中保存的。

下面我们来分析上面的第 2 步的情况：

第一次调用 NodeSelectorSlot 的 entry 方法时，map 中肯定是没有 DefaultNode 的，那就会进入第 2 步
中，创建一个 node，创建完成后会把该节点加入到 context 的 lastNode 的子节点中去。我们先看一下
context 的 getLastNode 方法：

代码中我们可以知道，lastNode 的值可能是 context 中的 entranceNode 也可能是
curEntry. parent. curNode，但是他们都是「DefaultNode」类型的节点，DefaultNode 的所有子节点是
保存在一个 HashSet 中的。

第一次调用 getLastNode 方法时，context 中 curEntry 是 null，因为 curEntry 是在第 3 步中才赋值的。所
以，lastNode 最初的值就是 context 的 entranceNode。那么将 node 添加到 entranceNode 的子节点中去
之后就变成了下面这样：

紧接着再进入一次，资源名不同，会再次生成一个新的 Entry，上面的图形就变成下图这样：

```
public Node getLastNode () {
// 如果 curEntry 不存在时，返回 entranceNode
// 否则返回 curEntry 的 lastNode，
// 需要注意的是 curEntry 的 lastNode 是获取的 parent 的 curNode，
// 如果每次进入的资源不同，就会每次都创建一个 CtEntry，则 parent 为 null，
// 所以 curEntry.getLastNode () 也为 null
if (curEntry != null && curEntry.getLastNode () != null) {
return curEntry.getLastNode ();
} else {
return entranceNode;
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```

此时再次调用 context 的 getLastNode 方法，因为此时 curEntry 的 parent 不再是 null 了，所以获取到的
lastNode 是 curEntry. parent. curNode，在上图中可以很方便的看出，这个节点就是 **node 0** 。那么把当
前节点 node 1 添加到 lastNode 的子节点中去，上面的图形就变成下图这样：

然后将当前 node 设置给 context 的 curNode，上面的图形就变成下图这样：


假如再创建一个 Entry，然后再进入一次不同的资源名，上面的图就变成下面这样：

至此 NodeSelectorSlot 的基本功能已经大致分析清楚了。

**PS：以上的分析是基于每次执行 SphU.entry (name) 时，资源名都是不一样的前提下。如果资源名都一
样的话，那么生成的 node 都相同，则只会再第一次把 node 加入到 entranceNode 的子节点中去，其他
的时候，只会创建一个新的 Entry，然后替换 context 中的 curEntry 的值。**

###### ClusterBuilderSlot

NodeSelectorSlot 的 entry 方法执行完之后，会调用 fireEntry 方法，此时会触发 ClusterBuilderSlot 的
entry 方法。

ClusterBuilderSlot 的 entry 方法比较简单，具体代码如下：

```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper,
DefaultNode node, int count, Object... args) throws Throwable {
if (clusterNode == null) {
synchronized (lock) {
if (clusterNode == null) {
// Create the cluster node.
clusterNode = Env.nodeBuilder.buildClusterNode ();
// 将 clusterNode 保存到全局的 map 中去
HashMap<ResourceWrapper, ClusterNode> newMap = new
HashMap<ResourceWrapper, ClusterNode>( 16 );
newMap.putAll (clusterNodeMap);
newMap.put (node.getId (), clusterNode);
```
```
clusterNodeMap = newMap;
}
}
}
// 将 clusterNode 塞到 DefaultNode 中去
node.setClusterNode (clusterNode);
```
```
// 省略部分代码
```
```
fireEntry (context, resourceWrapper, node, count, args);
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
```

NodeSelectorSlot 的职责比较简单，主要做了两件事：

一、为每个资源创建一个 clusterNode，然后把 clusterNode 塞到 DefaultNode 中去

二、将 clusterNode 保持到全局的 map 中去，用资源作为 map 的 key

**PS：一个资源只有一个 ClusterNode，但是可以有多个 DefaultNode**

###### StatistcSlot

StatisticSlot 负责来统计资源的实时状态，具体的代码如下：

```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper,
DefaultNode node, int count, Object... args) throws Throwable {
try {
// 触发下一个 Slot 的 entry 方法
fireEntry (context, resourceWrapper, node, count, args);
// 如果能通过 SlotChain 中后面的 Slot 的 entry 方法，说明没有被限流或降级
// 统计信息
node.increaseThreadNum ();
node.addPassRequest ();
// 省略部分代码
} catch (BlockException e) {
context.getCurEntry (). setError (e);
// Add block count.
node.increaseBlockedQps ();
// 省略部分代码
throw e;
} catch (Throwable e) {
context.getCurEntry (). setError (e);
// Should not happen
node.increaseExceptionQps ();
// 省略部分代码
throw e;
}
}
```
```
@Override
public void exit (Context context, ResourceWrapper resourceWrapper, int
count, Object... args) {
DefaultNode node = (DefaultNode) context.getCurNode ();
if (context.getCurEntry (). getError () == null) {
long rt = TimeUtil.currentTimeMillis () -
context.getCurEntry (). getCreateTime ();
if (rt > Constants. TIME_DROP_VALVE) {
rt = Constants. TIME_DROP_VALVE;
}
node.rt (rt);
// 省略部分代码
node.decreaseThreadNum ();
// 省略部分代码
}
fireExit (context, resourceWrapper, count);
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
```
```
28
29
30
```
```
31
32
33
34
35
36
37
38
39
40
```

代码分成了两部分，第一部分是 entry 方法，该方法首先会触发后续 slot 的 entry 方法，即 SystemSlot、
FlowSlot、DegradeSlot 等的规则，如果规则不通过，就会抛出 BlockException，则会在 node 中统计被
block 的数量。反之会在 node 中统计通过的请求数和线程数等信息。第二部分是在 exit 方法中，当退出该
Entry 入口时，会统计 rt 的时间，并减少线程数。

这些统计的实时数据会被后续的校验规则所使用，具体的统计方式是通过滑动窗口来实现的。

###### SystemSlot

SystemSlot 就是根据总的请求统计信息，来做流控，主要是防止系统被搞垮，具体的代码如下：

```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper,
DefaultNode node, int count,
boolean prioritized, Object... args) throws Throwable {
SystemRuleManager.checkSystem (resourceWrapper);
fireEntry (context, resourceWrapper, node, count, prioritized, args);
}
```
```
1 2 3 4 5 6
```
```
public static void checkSystem (ResourceWrapper resourceWrapper) throws
BlockException {
if (resourceWrapper == null) {
return;
}
// Ensure the checking switch is on.
if (! checkSystemStatus.get ()) {
return;
}
```
```
// for inbound traffic only
if (resourceWrapper.getEntryType () != EntryType. IN) {
return;
}
```
```
// total qps
double currentQps = Constants. ENTRY_NODE == null? 0.0 :
Constants. ENTRY_NODE.successQps ();
if (currentQps > qps) {
throw new SystemBlockException (resourceWrapper.getName (), "qps");
}
```
```
// total thread
int currentThread = Constants. ENTRY_NODE == null? 0 :
Constants. ENTRY_NODE.curThreadNum ();
if (currentThread > maxThread) {
throw new SystemBlockException (resourceWrapper.getName (), "thread");
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```
```
17
18
19
20
21
22
```
```
23
24
25
26
```

其中的 Constants. ENTRY_NODE 是一个全局的 ClusterNode，该节点的值是在 StatisticsSlot 中进行统计
的。

当前的统计值和系统配置的进行比较，各个维度超过范围抛 BlockException

###### AuthoritySlot

AuthoritySlot 做的事也比较简单，主要是根据黑白名单进行过滤，只要有一条规则校验不通过，就抛出
异常。

```
double rt = Constants. ENTRY_NODE == null? 0 :
Constants. ENTRY_NODE.avgRt ();
if (rt > maxRt) {
throw new SystemBlockException (resourceWrapper.getName (), "rt");
}
```
```
// BBR 算法
// load. BBR algorithm.
if (highestSystemLoadIsSet && getCurrentSystemAvgLoad () >
highestSystemLoad) {
if (! checkBbr (currentThread)) {
throw new SystemBlockException (resourceWrapper.getName (),
"load");
}
}
```
```
// cpu usage
if (highestCpuUsageIsSet && getCurrentCpuUsage () > highestCpuUsage) {
throw new SystemBlockException (resourceWrapper.getName (), "cpu");
}
}
```
```
27
```
```
28
29
30
31
32
33
34
```
```
35
36
```
```
37
38
39
40
41
42
43
44
```
```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper,
DefaultNode node, int count, boolean prioritized, Object... args)
throws Throwable {
checkBlackWhiteAuthority (resourceWrapper, context);
fireEntry (context, resourceWrapper, node, count, prioritized, args);
}
```
```
void checkBlackWhiteAuthority (ResourceWrapper resource, Context context)
throws AuthorityException {
// 通过监听来的规则集
Map<String, Set<AuthorityRule>> authorityRules =
AuthorityRuleManager.getAuthorityRules ();
```
```
if (authorityRules == null) {
return;
}
```
```
// 根据资源名称获取相应的规则
Set<AuthorityRule> rules = authorityRules.get (resource.getName ());
if (rules == null) {
return;
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
12
13
14
15
16
17
18
19
20
```

###### FlowSlot

FlowSlot 主要是根据前面统计好的信息，与设置的限流规则进行匹配校验，如果规则校验不通过则进行
限流，具体的代码如下：

###### DegradeSlot

DegradeSlot 主要是根据前面统计好的信息，与设置的降级规则进行匹配校验，如果规则校验不通过则
进行降级，具体的代码如下：

```
}
```
```
for (AuthorityRule rule : rules) {
// 黑名单白名单验证
// 只要有一条规则校验不通过，就抛出 AuthorityException
if (! AuthorityRuleChecker.passCheck (rule, context)) {
throw new AuthorityException (context.getOrigin (), rule);
}
}
}
```
```
21
22
23
24
25
26
27
28
29
30
```
```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper,
DefaultNode node, int count,
boolean prioritized, Object... args) throws Throwable {
checkFlow (resourceWrapper, context, node, count, prioritized);
```
```
fireEntry (context, resourceWrapper, node, count, prioritized, args);
}
```
```
void checkFlow (ResourceWrapper resource, Context context, DefaultNode node,
int count, boolean prioritized)
throws BlockException {
checker.checkFlow (ruleProvider, resource, context, node, count,
prioritized);
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
12
```
```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper,
DefaultNode node, int count,
boolean prioritized, Object... args) throws Throwable {
performChecking (context, resourceWrapper);
```
```
fireEntry (context, resourceWrapper, node, count, prioritized, args);
}
```
```
void performChecking (Context context, ResourceWrapper r) throws
BlockException {
List<CircuitBreaker> circuitBreakers =
DegradeRuleManager.getCircuitBreakers (r.getName ());
```
```
if (circuitBreakers == null || circuitBreakers.isEmpty ()) {
return;
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```

###### DefaultProcessorSlotChain

Chain 是链条的意思，从 build 的方法可看出，ProcessorSlotChain 是一个链表，里面添加了很多个
Slot。都是 ProcessorSlot 的子类。具体的实现需要到 DefaultProcessorSlotChain 中去看。

```
for (CircuitBreaker cb : circuitBreakers) {
if (! cb.tryPass (context)) {
throw new DegradeException (cb.getRule (). getLimitApp (),
cb.getRule ());
}
}
}
```
```
16
17
18
```
```
19
20
21
```
```
public class DefaultProcessorSlotChain extends ProcessorSlotChain {
```
```
AbstractLinkedProcessorSlot<?> first = new
AbstractLinkedProcessorSlot<Object>() {
```
```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper,
Object t, int count, boolean prioritized, Object... args)
throws Throwable {
super.fireEntry (context, resourceWrapper, t, count, prioritized,
args);
}
```
```
@Override
public void exit (Context context, ResourceWrapper resourceWrapper,
int count, Object... args) {
super.fireExit (context, resourceWrapper, count, args);
}
```
```
};
AbstractLinkedProcessorSlot<?> end = first;
```
```
@Override
public void addFirst (AbstractLinkedProcessorSlot<?> protocolProcessor) {
protocolProcessor.setNext (first.getNext ());
first.setNext (protocolProcessor);
if (end == first) {
end = protocolProcessor;
}
}
```
```
@Override
public void addLast (AbstractLinkedProcessorSlot<?> protocolProcessor) {
end.setNext (protocolProcessor);
end = protocolProcessor;
}
```
```
/**
* Same as {@link #addLast (AbstractLinkedProcessorSlot)}.
*
* @param next processor to be added.
*/
@Override
public void setNext (AbstractLinkedProcessorSlot<?> next) {
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
```

DefaultProcessorSlotChain 中有两个 AbstractLinkedProcessorSlot 类型的变量：first 和 end，这就是链
表的头结点和尾节点。

创建 DefaultProcessorSlotChain 对象时，首先创建了首节点，然后把首节点赋值给了尾节点，可以用下
图表示：

将第一个节点添加到链表中后，整个链表的结构变成了如下图这样：

```
addLast (next);
}
```
```
@Override
public AbstractLinkedProcessorSlot<?> getNext () {
return first.getNext ();
}
```
```
@Override
public void entry (Context context, ResourceWrapper resourceWrapper,
Object t, int count, boolean prioritized, Object... args)
throws Throwable {
first.transformEntry (context, resourceWrapper, t, count,
prioritized, args);
}
```
```
@Override
public void exit (Context context, ResourceWrapper resourceWrapper, int
count, Object... args) {
first.exit (context, resourceWrapper, count, args);
}
}
```
```
41
42
43
44
45
46
47
48
49
50
```
```
51
52
```
```
53
54
55
56
```
```
57
58
59
```

将所有的节点都加入到链表中后，整个链表的结构变成了如下图所示：

这样就将所有的 Slot 对象添加到了链表中去了，每一个 Slot 都是继承自 AbstractLinkedProcessorSlot。
而 AbstractLinkedProcessorSlot 是一种责任链的设计，每个对象中都有一个 next 属性，指向的是另一个
AbstractLinkedProcessorSlot 对象。其实责任链模式在很多框架中都有，比如 Netty 中是通过 pipeline 来
实现的。

知道了 SlotChain 是如何创建的了，那接下来就要看下是如何执行 Slot 的 entry 方法的了


从这里可以看到，从 fireEntry 方法中就开始传递执行 entry 了，这里会执行当前节点的下一个节点
transformEntry 方法，上面已经分析过了，transformEntry 方法会触发当前节点的 entry，也就是说
fireEntry 方法实际是触发了下一个节点的 entry 方法。

从最初的调用 Chain 的 entry () 方法，转变成了调用 SlotChain 中 Slot 的 entry () 方法。从
@SpiOrder (-10000) 知道，SlotChain 中的第一个 Slot 节点是 NodeSelectorSlot。

###### slot 总结

sentinel 的限流降级等功能，主要是通过一个 SlotChain 实现的。在链式插槽中，有 7 个核心的 Slot，这些
Slot 各司其职，可以分为以下几种类型：

一、进行资源调用路径构造的 NodeSelectorSlot 和 ClusterBuilderSlot

二、进行资源的实时状态统计的 StatisticsSlot

三、进行系统保护，限流，降级等规则校验的 SystemSlot、AuthoritySlot、FlowSlot、DegradeSlot

后面几个 Slot 依赖于前面几个 Slot 统计的结果。至此，每种 Slot 的功能已经基本分析清楚了。

#### 10 、sentinel 滑动窗口实现原理


###### 10.1 基本原理

滑动窗口可以先拆为滑动跟窗口两个词，先介绍下窗口，你可以这么理解，一段是时间就是窗口，比如

说我们可以把这个 1 s 认为是 1 个窗口

这个样子我们就能将 1 分钟就可以划分成 60 个窗口了，这个没毛病吧。如下图我们就分成了 60 个窗口

（这个多了我们就画 5 个表示一下）

比如现在处于第 1 秒上，那 1 s 那个窗口就是当前窗口，就如下图中红框表示。

好了，窗口就介绍完了，现在在来看下滑动，滑动很简单，比如说现在时间由第 1 秒变成了第 2 秒，就是
从当前这个窗口---->下一个窗口就可以了，这个时候下一个窗口就变成了当前窗口，之前那个当前窗口
就变成了上一个窗口，这个过程其实就是滑动。


好了，介绍完了滑动窗口，我们再来介绍下这个 sentinel 的滑动窗口的实现原理。
其实你要是理解了上面这个滑动窗口的意思，sentinel 实现原理就简单了。
先是介绍下窗口中里面都存储些啥。也就是上面这个小框框都有啥。

```
1. 它得有个开始时间吧，不然你怎么知道这个窗口是什么时候开始的
2. 还得有个窗口的长度吧，不然你咋知道窗口啥时候结束，通过这个开始时间+窗口长度=窗口结束时
间，就比如说上面的 1 s，间隔 1 s
3. 最后就是要在这个窗口里面统计的东西，你总不能白搞些窗口，搞些滑动吧。所以这里就存储了一
堆要统计的指标（qps，rt 等等）
```
说完了这一个小窗口里面的东西，就得来说说是怎么划分这个小窗口，怎么管理这些小窗口的了，也就
是我们的视野得往上提高一下了，不能总聚在这个小窗口上。

```
1. 要知道有多少个小窗口，在 sentinel 中也就是 sampleCount，比如说我们有 60 个窗口。
2. 还有就是 intervalInMs，这个 intervalInMs 是用来计算这个窗口长度的，intervalInMs/窗口数量=
窗口长度。也就是我给你 1 分钟，你给我分成 60 个窗口，这个时候窗口长度就是 1 s 了，那如果我给
你 1 s，你给我分 2 个窗口，这个时候窗口长度就是 500 毫秒了，这个 1 分钟，就是 intervalInMs。
3. 再就是存储这个窗口的容器（这里是数组），毕竟那么多窗口，还得提供计算当前时间窗口的方法
等等
```
最后我们就来看看这个当前时间窗口是怎么计算的。
咱们就拿 60 个窗口，这个 60 个窗口放在数组中，窗口长度是 1 s 来计算，看看当前时间戳的一个时间窗
口是是在数组中哪个位置。

比如说当前时间戳是 1609085401454 ms，算出秒 = 1609085401454 /1000（窗口长度）

我们再来计算下某个时间戳对应窗口的起始时间，还是以 1609085401454 来计算

这里 1609085401454%1000（窗口长度） 能算出来它的毫秒值，也就是 454 ，减去这个后就变成了
1609085401000

好了，sentinel 滑动窗口原理就介绍完成了。

```
1 在数组的位置 = 算出秒 %数组长度
```
```
1 窗口 startTime = 1609085401454 - 1609085401454%1000（窗口长度）=454
```

###### 10.2 sentinel 使用滑动窗口都统计啥

我们来介绍下使用这个滑动窗口都来统计啥

这是最基本的指标，然后通过这些指标，又可以计算出来比如说最大，最小，平均等等的一些指标。

###### 10.3 滑动窗口源码实现

我们先来看下这个窗口里面的统计指标的实现 MetricBucket

**10.3.1 MetricBucket**

这个 MetricBucket 是由 LongAdder 数组组成的，一个 LongAdder 就是一个 MetricEvent ，也就是第二小
节里面的 PASS ，BLOCK 等等。
我们稍微看下就可以了

可以看到它在实例化的时候创建一个 LongAdder 数据，个数就是那堆 event 的数量。这个 LongAdder 是
jdk 8 里面的原子操作类，你可以把它简单认为 AtomicLong。然后下面就是一堆 get 跟 add 的方法了，这
里我们就不看了。
接下来再来看看那这个窗口的实现 WindowWrap 类

```
public enum MetricEvent {
/**
* Normal pass.
*/
PASS,// 通过
/**
* Normal block.
*/
BLOCK,// 拒绝的
EXCEPTION,// 异常
SUCCESS,//成功
RT,// 耗时
/**
* Passed in future quota (pre-occupied, since 1.5.0).
*/
OCCUPIED_PASS
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```

**10.3.2 WindowWrap**

先来看下这个成员

窗口长度，窗口 startTime ，指标统计的都有了，下面的就没啥好看的了，我们再来看下的一个方法吧

就是判断某个时间戳是不是在这个窗口中
时间戳要大于等于窗口开始时间 && 小于这个结束时间。

接下来再来看下这个管理窗口的类 LeapArray

**10.3.3 LeapArray**

看下它的成员，窗口长度，样本数 sampleCount 也就是窗口个数， intervalInMs ，再就是窗口数组
看看它的构造方法

这个构造方法其实就是计算出来这个窗口长度，创建了窗口数组。


未完待续.....

## Zuul 微服务网关

Zuul 作为老牌 SpringCloud 微服务网关，很多项目仍然在使用，

另外，底层原理都是想通的，大家可以和 Spring Cloud Gateway 对比学习

Zuul 的详细介绍，请阅读《Java 高并发核心编程卷 3 加强版》

## Webflux 响应式编程

Spring Cloud Gateway 是一个响应式编程网关，是基于 Webflux 响应式框架开发的

学习 Spring Cloud Gateway，咱们得先学习 Webflux 响应式编程

大家做好死磕准备：

WebFlux 挺难的，基础的知识具体请参见尼恩的《响应式编程圣经》

《响应式圣经：10 W 字，实现 Spring 响应式编程自由》

更加进一步的学习，具体请参见尼恩的《全链路异步架构与实操》

#### WebFlux 学习前言

webmvc 和 webflux 作为 spring framework 的两个重要模块，代表了两个 **IO 模型** ，阻塞式和非阻塞式
的。

webmvc 是基于 servlet 的阻塞式模型（一般称为 oio），一个请求到达服务器后会单独分配一个线程去处
理请求，如果请求包含 IO 操作，线程在 IO 操作结束之前一直处于阻塞等待状态，这样线程在等待 IO 操作
结束的时间就浪费了。

webflux 是基于 reactor 的非阻塞模型 (一般称为 nio)，同样，请求到达服务器后也会分配一个线程去处理
请求，如果请求包含 IO 操作，线程在 IO 操作结束之前不再是处于阻塞等待状态，而是去处理其他事情，
等到 IO 操作结束之后，再通知（得益于系统的机制）线程继续处理请求。

这样线程就有效地利用了 IO 操作所消耗的时间。

#### WebFlux 增删改查完整实战 demo


###### Dao 层 （又称 repository 层）

###### entity（又称 PO 对象）

新建 User 对象，代码如下：

###### Dao 实现类

@Repository 用于标注数据访问组件，即 DAO 组件。实现代码中使用名为 repository 的 Map 对象作
为内存数据存储，并对对象具体实现了具体业务逻辑。JpaUserRepositoryImpl 负责将 PO 持久层（数
据操作）相关的封装组织，完成新增、查询、删除等操作。

```
package com. crazymaker. springcloud. reactive. user. info. entity;
```
```
import com. crazymaker. springcloud. reactive. user. info. dto. User;
```
```
import javax. persistence. Column;
import javax. persistence. Entity;
import javax. persistence. GeneratedValue;
import javax. persistence. GenerationType;
import javax. persistence. Id;
import javax. persistence. Table;
```
```
@Entity
@Table (name = "t_user")
public final class UserEntity extends User
{
```
```
@Id
@Column (name = "id")
@GeneratedValue (strategy = GenerationType. IDENTITY)
@Override
public long getUserId ()
{
return super.getUserId ();
}
```
```
@Column (name = "name")
public String getName ()
{
return super.getName ();
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
```
```
package com. crazymaker. springcloud. reactive. user. info. dao. impl;
```
```
import com. crazymaker. springcloud. reactive. user. info. dto. User;
import org. springframework. stereotype. Repository;
```
```
1
2
3
4
5
```

###### Service 服务层

```
import javax. persistence. EntityManager;
import javax. persistence. PersistenceContext;
import javax. persistence. Query;
import javax. transaction. Transactional;
import java. util. List;
```
```
@Repository
@Transactional
public class JpaUserRepositoryImpl
{
```
```
@PersistenceContext
private EntityManager entityManager;
```
```
public Long insert (final User user)
{
entityManager.persist (user);
return user.getUserId ();
}
```
```
public void delete (final Long userId)
{
Query query = entityManager.createQuery ("DELETE FROM UserEntity o
WHERE o.userId = ?1");
query.setParameter ( 1 , userId);
query.executeUpdate ();
}
```
```
@SuppressWarnings ("unchecked")
public List<User> selectAll ()
{
return (List<User>) entityManager.createQuery ("SELECT o FROM
UserEntity o"). getResultList ();
}
```
```
@SuppressWarnings ("unchecked")
public User selectOne (final Long userId)
{
Query query = entityManager.createQuery ("SELECT o FROM UserEntity o
WHERE o.userId = ?1");
query.setParameter ( 1 , userId);
return (User) query.getSingleResult ();
}
}
```
```
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
```
```
30
31
32
33
34
35
36
37
```
```
38
39
40
41
42
43
```
```
44
45
46
47
```
```
package com. crazymaker. springcloud. reactive. user. info. service. impl;
```
```
import com. crazymaker. springcloud. common. util. BeanUtil;
import
com. crazymaker. springcloud. reactive. user. info. dao. impl. JpaUserRepositoryImpl
;
```
```
1
2
3
4
5
```

```
import com. crazymaker. springcloud. reactive. user. info. dto. User;
import com. crazymaker. springcloud. reactive. user. info. entity. UserEntity;
import lombok. extern. slf 4 j. Slf 4 j;
import org. springframework. stereotype. Service;
import org. springframework. transaction. annotation. Transactional;
```
```
import javax. annotation. Resource;
import java. util. List;
```
```
@Slf 4 j
@Service
@Transactional
public class JpaEntityServiceImpl
{
```
```
@Resource
private JpaUserRepositoryImpl userRepository;
```
```
@Transactional
//增加用户
public User addUser (User dto)
{
User userEntity = new UserEntity ();
userEntity.setUserId (dto.getUserId ());
userEntity.setName (dto.getName ());
userRepository.insert (userEntity);
BeanUtil.copyProperties (userEntity, dto);
return dto;
}
```
```
@Transactional
//删除用户
public User delUser (User dto)
{
userRepository.delete (dto.getUserId ());
return dto;
}
```
```
//查询全部用户
public List<User> selectAllUser ()
{
log.info ("方法 selectAllUser 被调用了");
```
```
return userRepository.selectAll ();
}
```
```
//查询一个用户
public User selectOne (final Long userId)
{
```
```
log.info ("方法 selectOne 被调用了");
```
```
return userRepository.selectOne (userId);
}
```
```
}
```
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63


###### Controller 控制层

Spring Boot WebFlux 也可以使用注解模式来进行 API 接口开发。

```
package com. crazymaker. springcloud. reactive. user. info. controller;
```
```
import com. crazymaker. springcloud. common. result. RestOut;
import com. crazymaker. springcloud. reactive. user. info. dto. User;
import
com. crazymaker. springcloud. reactive. user. info. service. impl. JpaEntityService
Impl;
import io. swagger. annotations. Api;
import io. swagger. annotations. ApiImplicitParam;
import io. swagger. annotations. ApiImplicitParams;
import io. swagger. annotations. ApiOperation;
import lombok. extern. slf 4 j. Slf 4 j;
import org. springframework. web. bind. annotation. DeleteMapping;
import org. springframework. web. bind. annotation. PostMapping;
import org. springframework. web. bind. annotation. RequestBody;
import org. springframework. web. bind. annotation. RequestMapping;
import org. springframework. web. bind. annotation. RequestParam;
import org. springframework. web. bind. annotation. RestController;
import reactor. core. publisher. Flux;
import reactor. core. publisher. Mono;
```
```
import javax. annotation. Resource;
```
```
/**
* Mono 和 Flux 适用于两个场景，即：
* Mono：实现发布者，并返回 0 或 1 个元素，即单对象。
* Flux：实现发布者，并返回 N 个元素，即 List 列表对象。
* 有人会问，这为啥不直接返回对象，比如返回 City/Long/List。
* 原因是，直接使用 Flux 和 Mono 是非阻塞写法，相当于回调方式。
* 利用函数式可以减少了回调，因此会看不到相关接口。这恰恰是 WebFlux 的好处：集合了非阻塞
+ 异步
*/
@Slf 4 j
@Api (value = "用户信息、基础学习 DEMO", tags = {"用户信息 DEMO"})
@RestController
@RequestMapping ("/api/user")
public class UserReactiveController
{
```
```
@ApiOperation (value = "回显测试", notes = "提示接口使用者注意事项",
httpMethod = "GET")
@RequestMapping (value = "/hello")
@ApiImplicitParams ({
@ApiImplicitParam (paramType = "query",
dataType="string", dataTypeClass = String. class, name = "name", value = "名
称", required = true)})
public Mono<RestOut<String>> hello (@RequestParam (name = "name")
String name)
{
log.info ("方法 hello 被调用了");
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
```
```
29
30
31
32
33
34
35
36
37
```
```
38
39
40
```
```
41
```
```
42
43
```

```
return Mono.just (RestOut.succeed ("hello " + name));
}
```
```
@Resource
JpaEntityServiceImpl jpaEntityService;
```
```
@PostMapping ("/add/v 1")
@ApiOperation (value = "插入用户" )
@ApiImplicitParams ({
// @ApiImplicitParam (paramType = "body",
dataType="java. lang. Long", name = "userId", required = false),
// @ApiImplicitParam (paramType = "body", dataType="用户",
name = "dto", required = true)
@ApiImplicitParam (paramType = "body", dataTypeClass =
User. class, dataType="User", name = "dto",  required = true),
})
// @ApiImplicitParam (paramType = "body",
dataType="com. crazymaker. springcloud. reactive. user. info. dto. User",
required = true)
public Mono<User> userAdd (@RequestBody User dto)
{
//命令式写法
// jpaEntityService.delUser (dto);
```
```
//响应式写法
return Mono.create (cityMonoSink ->
cityMonoSink.success (jpaEntityService.addUser (dto)));
}
```
```
@PostMapping ("/del/v 1")
@ApiOperation (value = "响应式的删除")
@ApiImplicitParams ({
@ApiImplicitParam (paramType = "body",
dataType="User", dataTypeClass = User. class, name = "dto",  required = true),
})
public Mono<User> userDel (@RequestBody User dto)
{
//命令式写法
```
```
// jpaEntityService.delUser (dto);
```
```
//响应式写法
```
```
return Mono.create (cityMonoSink ->
cityMonoSink.success (jpaEntityService.delUser (dto)));
}
```
```
@PostMapping ("/list/v 1")
@ApiOperation (value = "查询用户")
public Flux<User> listAllUser ()
{
log.info ("方法 listAllUser 被调用了");
```
```
//命令式写法改为响应式以下语句，需要在流中执行
```
44
45
46
47
48
49
50
51
52
53
54
55
56

57

58

59
60

61
62
63
64
65
66
67

68
69
70
71
72
73
74

75
76
77
78
79
80
81
82
83
84

85
86
87
88
89
90
91
92
93


从返回值可以看出，Mono 和 Flux 适用于两个场景，即：

```
Mono：实现发布者，并返回 0 或 1 个元素，即单对象
Flux：实现发布者，并返回 N 个元素，即 List 列表对象
```
有人会问，这为啥不直接返回对象，比如返回 City/Long/List。原因是，直接使用 Flux 和 Mono 是非阻
塞写法，相当于回调方式。利用函数式可以减少了回调，因此会看不到相关接口。这恰恰是 WebFlux 的
好处：集合了非阻塞 + 异步。

```
// List<User> list = jpaEntityService.selectAllUser ();
//响应式写法
Flux<User> userFlux =
Flux.fromIterable (jpaEntityService.selectAllUser ());
return userFlux;
}
```
```
@PostMapping ("/detail/v 1")
@ApiOperation (value = "响应式的查看")
@ApiImplicitParams ({
@ApiImplicitParam (paramType = "body", dataTypeClass =
User. class, dataType="User", name = "dto",  required = true),
})
public Mono<User> getUser (@RequestBody User dto)
{
log.info ("方法 getUser 被调用了");
```
```
//构造流
Mono<User> userMono =
Mono.justOrEmpty (jpaEntityService.selectOne (dto.getUserId ()));
return userMono;
}
```
```
@PostMapping ("/detail/v 2")
@ApiOperation (value = "命令式的查看")
@ApiImplicitParams ({
@ApiImplicitParam (paramType = "body",
dataType="User", dataTypeClass = User. class, name = "dto",  required =
true),
})  public RestOut<User> getUserV 2 (@RequestBody User dto)
{
log.info ("方法 getUserV 2 被调用了");
```
```
User user = jpaEntityService.selectOne (dto.getUserId ());
return RestOut.success (user);
}
```
```
}
```
```
94
95
96
```
```
97
98
99
100
101
102
103
```
```
104
105
106
107
108
109
110
```
```
111
112
113
114
115
116
117
```
```
118
119
120
121
122
123
124
125
126
```

###### Mono

Mono 是什么？ 官方描述如下：A Reactive Streams Publisher with basic rx operators that
completes successfully by emitting an element, or with an error.

Mono 是响应流 Publisher 具有基础 rx 操作符。可以成功发布元素或者错误。如图所示：

file

Mono 常用的方法有：

```
Mono.create ()：使用 MonoSink 来创建 Mono
Mono.justOrEmpty ()：从一个 Optional 对象或 null 对象中创建 Mono。
Mono.error ()：创建一个只包含错误消息的 Mono
Mono.never ()：创建一个不包含任何消息通知的 Mono
Mono.delay ()：在指定的延迟时间之后，创建一个 Mono，产生数字 0 作为唯一值
```
###### Flux

Flux 是什么？ 官方描述如下：A Reactive Streams Publisher with rx operators that emits 0 to N
elements, and then completes (successfully or with an error).

Flux 是响应流 Publisher 具有基础 rx 操作符。可以成功发布 0 到 N 个元素或者错误。Flux 其实是
Mono 的一个补充。如图所示：

file

所以要注意：如果知道 Publisher 是 0 或 1 个，则用 Mono。


Flux 最值得一提的是 fromIterable 方法。 fromIterable (Iterable<? extends T> it) 可以发布

Iterable 类型的元素。当然，Flux 也包含了基础的操作：map、merge、concat、flatMap、take，这
里就不展开介绍了。

#### 使用配置模式进行 WebFlux 接口开发

1 可以编写一个处理器类 Handler 代替 Controller ， Service 、dao 层保持不变。

2 配置请求的路由

#### 处理器类 Handler

处理器类 Handler 需要从请求解析参数，并且封装响应，代码如下：

```
package com. crazymaker. springcloud. reactive. user. info. config. handler;
```
```
import com. crazymaker. springcloud. common. exception. BusinessException;
import com. crazymaker. springcloud. reactive. user. info. dto. User;
import
com. crazymaker. springcloud. reactive. user. info. service. impl. JpaEntityServiceI
mpl;
import lombok. extern. slf 4 j. Slf 4 j;
import org. apache. commons. lang 3. StringUtils;
import org. springframework. stereotype. Component;
import org. springframework. web. reactive. function. server. ServerRequest;
import org. springframework. web. reactive. function. server. ServerResponse;
import reactor. core. publisher. Flux;
import reactor. core. publisher. Mono;
```
```
import javax. annotation. Resource;
```
```
import static org. springframework. http. MediaType. APPLICATION_JSON_UTF 8;
import static
org. springframework. web. reactive. function. server. ServerResponse. ok;
```
```
@Slf 4 j
@Component
public class UserReactiveHandler
{
```
```
@Resource
private JpaEntityServiceImpl jpaEntityService;
```
```
/**
* 得到所有用户
*
* @param request
* @return
*/
public Mono<ServerResponse> getAllUser (ServerRequest request)
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```
```
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
```

```
{
log.info ("方法 getAllUser 被调用了");
return ok (). contentType (APPLICATION_JSON_UTF 8)
.body (Flux.fromIterable (jpaEntityService.selectAllUser ()),
User. class);
}
```
```
/**
* 创建用户
*
* @param request
* @return
*/
public Mono<ServerResponse> createUser (ServerRequest request)
{
// 2.0.0 是可以工作, 但是 2.0.1 下面这个模式是会报异常
Mono<User> user = request.bodyToMono (User. class);
/**Mono 使用响应式的, 时候都是一个流, 是一个发布者, 任何时候都不能调用发布者的订阅
方法
也就是不能消费它, 最终的消费还是交给我们的 Springboot 来对它进行消费, 任何时候不
能调用它的
user.subscribe ();
不能调用 block
把异常放在统一的地方来处理
*/
```
```
return user.flatMap (dto ->
{
// 校验代码需要放在这里
if (StringUtils.isBlank (dto.getName ()))
{
throw new BusinessException ("用户名不能为空");
}
```
```
return ok (). contentType (APPLICATION_JSON_UTF 8)
.body (Mono.create (cityMonoSink ->
cityMonoSink.success (jpaEntityService.addUser (dto))), User. class);
});
}
```
```
/**
* 根据 id 删除用户
*
* @param request
* @return
*/
public Mono<ServerResponse> deleteUserById (ServerRequest request)
{
String id = request.pathVariable ("id");
// 校验代码需要放在这里
if (StringUtils.isBlank (id))
{
throw new BusinessException ("id 不能为空");
}
User dto = new User ();
dto.setUserId (Long.parseLong (id));
return ok (). contentType (APPLICATION_JSON_UTF 8)
```
36
37
38
39

40
41
42
43
44
45
46
47
48
49
50
51
52

53

54
55
56
57
58
59
60
61
62
63
64
65
66
67
68

69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88


#### 路由配置

```
.body (Mono.create (cityMonoSink ->
cityMonoSink.success (jpaEntityService.delUser (dto))), User. class);
}
```
```
}
```
```
89
```
```
90
91
92
```
```
package com. crazymaker. springcloud. reactive. user. info. config;
```
```
import
com. crazymaker. springcloud. reactive. user. info. config. handler. UserReactiveHan
dler;
import org. springframework. beans. factory. annotation. Value;
import org. springframework. context. annotation. Bean;
import org. springframework. context. annotation. Configuration;
import org. springframework. http. MediaType;
import org. springframework. http. server. reactive. ServerHttpRequest;
import org. springframework. web. reactive. function. server. RouterFunction;
import org. springframework. web. reactive. function. server. RouterFunctions;
import org. springframework. web. reactive. function. server. ServerResponse;
import org. springframework. web. server. WebFilter;
```
```
import static
org. springframework. web. reactive. function. server. RequestPredicates. DELETE;
import static
org. springframework. web. reactive. function. server. RequestPredicates. GET;
import static
org. springframework. web. reactive. function. server. RequestPredicates. POST;
import static
org. springframework. web. reactive. function. server. RequestPredicates. accept;
```
```
@Configuration
public class RoutersConfig
{
```
```
@Bean
RouterFunction<ServerResponse> routes (UserReactiveHandler handler)
{
```
```
// 下面的相当于类里面的 @RequestMapping
// 得到所有用户
return RouterFunctions.route (GET ("/user"), handler::getAllUser)
// 创建用户
```
```
.andRoute (POST ("/user"). and (accept (MediaType. APPLICATION_JSON_UTF 8)),
handler::createUser)
// 删除用户
.andRoute (DELETE ("/user/{id}"), handler::deleteUserById);
}
```
```
@Value ("${server. servlet. context-path}")
private String contextPath;
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
```
```
15
```
```
16
```
```
17
```
```
18
19
20
21
22
23
24
25
26
27
28
29
30
31
```
```
32
33
34
35
36
37
```

#### WebFlux 集成 Swagger

本文主要展示一下如何使用支持 WebFlux 的 Swagger

###### maven 依赖

```
swagger. version 目前是 3.0.0，Spring 5 引入了 WebFlux，而当前版本的 SpringFox
Swagger 2 (2.9.2) 还不支持 WebFlux，得使用 3.0.0 才支持
```
```
//处理上下文路径，没有上下文路径，此函数可以忽略
@Bean
public WebFilter contextPathWebFilter ()
{
return (exchange, chain) ->
{
ServerHttpRequest request = exchange.getRequest ();
```
```
String requestPath = request.getURI (). getPath ();
if (requestPath.startsWith (contextPath))
{
return chain.filter (
exchange.mutate ()
```
```
.request (request.mutate (). contextPath (contextPath). build ())
.build ());
}
return chain.filter (exchange);
};
}
}
```
```
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
```
```
53
54
55
56
57
58
```
```
<dependency>
<groupId>io. springfox</groupId>
<artifactId>springfox-swagger 2</artifactId>
<version>${swagger. version}</version>
</dependency>
<dependency>
<groupId>io. springfox</groupId>
<artifactId>springfox-spring-webflux</artifactId>
<version>${swagger. version}</version>
</dependency>
<dependency>
<groupId>io. springfox</groupId>
<artifactId>springfox-swagger-ui</artifactId>
<version>${swagger. version}</version>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```

###### swagger 配置

```
package com. crazymaker. springcloud. reactive. user. info. config;
```
```
import org. springframework. beans. factory. annotation. Value;
import org. springframework. context. annotation. Bean;
import org. springframework. context. annotation. Configuration;
import org. springframework. core. Ordered;
import org. springframework. core. annotation. Order;
import org. springframework. web. util. UriComponentsBuilder;
import springfox. documentation. PathProvider;
import springfox. documentation. builders. ApiInfoBuilder;
import springfox. documentation. builders. PathSelectors;
import springfox. documentation. builders. RequestHandlerSelectors;
import springfox. documentation. service. ApiInfo;
import springfox. documentation. service. Contact;
import springfox. documentation. spi. DocumentationType;
import springfox. documentation. spring. web. paths. DefaultPathProvider;
import springfox. documentation. spring. web. paths. Paths;
import springfox. documentation. spring. web. plugins. Docket;
import springfox. documentation. swagger 2. annotations. EnableSwagger 2 WebFlux;
```
```
@Configuration
@EnableSwagger 2 WebFlux
public class SwaggerConfig
{
```
```
@Bean
public Docket createRestApi ()
{
// return new Docket (DocumentationType. OAS_30)
return new Docket (DocumentationType. SWAGGER_2)
.apiInfo (apiInfo ())
.pathMapping (servletContextPath)  //注意 webflux 没有 context-
path 配置，如果不加这句话的话，接口测试时路径没有前缀
```
```
.select ()
```
```
.apis (RequestHandlerSelectors.basePackage ("com. crazymaker. springcloud. reacti
ve. user. info. controller"))
.paths (PathSelectors.any ())
.build ();
```
```
}
@Value ("${server. servlet. context-path}")
private String servletContextPath;
```
```
//构建 api 文档的详细信息函数
private ApiInfo apiInfo ()
{
return new ApiInfoBuilder ()
//页面标题
.title ("疯狂创客圈 springcloud + Nginx 高并发核心编程")
//描述
.description ("Zuul+Swagger 2 构建 RESTful APIs")
//条款地址
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
```
```
34
35
36
```
```
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
```

#### WebFlux 测试

###### 配置模式的 WebFlux Rest 接口测试

配置模式的 WebFlux Rest 接口只能使用 PostMan 测试，例子如下：

```
.termsOfServiceUrl ("https://www.cnblogs.com/crazymakercircle/")
.contact (new Contact ("疯狂创客圈",
"https://www.cnblogs.com/crazymakercircle/", ""))
.version ("1.0")
.build ();
}
```
```
/**
* 重写 PathProvider ,解决 context-path 重复问题
* @return
*/
@Order (Ordered. HIGHEST_PRECEDENCE)
@Bean
public PathProvider pathProvider () {
return new DefaultPathProvider () {
@Override
public String getOperationPath (String operationPath) {
operationPath =
operationPath.replaceFirst (servletContextPath, "/");
UriComponentsBuilder uriComponentsBuilder =
UriComponentsBuilder.fromPath ("/");
return
Paths.removeAdjacentForwardSlashes (uriComponentsBuilder.path (operationPath).
build (). toString ());
}
```
```
@Override
public String getResourceListingPath (String groupName, String
apiDeclaration) {
apiDeclaration = super.getResourceListingPath (groupName,
apiDeclaration);
return apiDeclaration;
}
};
}
}
```
```
53
```
```
54
```
```
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
```
```
70
```
```
71
```
```
72
73
74
75
```
```
76
```
```
77
78
79
80
81
```

注意，不能带上下文路径：

[http://192.168.68.1:7705/uaa-react-provider/user](http://192.168.68.1:7705/uaa-react-provider/user)

###### 注解模式的 WebFlux Rest 接口测试

###### swagger 增加界面

CRUD 其他的界面，略过

#### 配置大全


###### 静态资源配置

###### WebFluxSecurity 配置

```
@Configuration
@EnableWebFlux //使用注解@EnableWebFlux
public class WebFluxConfig implements WebFluxConfigurer { //继承
WebFluxConfigurer
//配置静态资源
@Override
public void addResourceHandlers (ResourceHandlerRegistry registry) {
registry.addResourceHandler ("/static/**")
.addResourceLocations ("classpath:/static/");
registry.addResourceHandler ("/file/**")
.addResourceLocations ("file: " +
System.getProperty ("user. dir") + File. separator + "file" + File. separator);
registry.addResourceHandler ("/swagger-ui. html**")
.addResourceLocations ("classpath:/META-INF/resources/");
registry.addResourceHandler ("/webjars/**")
.addResourceLocations ("classpath:/META-
INF/resources/webjars/");
}
//配置拦截器
//配置编解码
...
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
12
13
14
```
```
15
16
17
18
19
```
```
@Configuration
@EnableWebSecurity
public class WebMvcSecurityConfig extends WebSecurityConfigurerAdapter
implements
AuthenticationEntryPoint, //未验证回调
AuthenticationSuccessHandler, //验证成功回调
AuthenticationFailureHandler, //验证失败回调
LogoutSuccessHandler { //登出成功回调
```
```
@Override
public void commence (HttpServletRequest request, HttpServletResponse
response, AuthenticationException authException) throws IOException,
ServletException {
sendJson (response, new Response<>(HttpStatus.UNAUTHORIZED.value (),
"Unauthorized"));
}
```
```
@Override
public void onAuthenticationFailure (HttpServletRequest request,
HttpServletResponse response, AuthenticationException exception) throws
IOException, ServletException {
sendJson (response, new Response<>( 1 , "Incorrect"));
}
```
```
@Override
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
```
```
12
13
14
15
```
```
16
17
18
19
```

```
public void onAuthenticationSuccess (HttpServletRequest request,
HttpServletResponse response, Authentication authentication) throws
IOException, ServletException {
sendJson (response, new Response<>( 0 ,
authentication.getClass (). getSimpleName ()));
}
```
```
@Override
public void onLogoutSuccess (HttpServletRequest request,
HttpServletResponse response, Authentication authentication) throws
IOException, ServletException {
sendJson (response, new Response<>( 0 , "Success"));
}
```
```
@Override
protected void configure (HttpSecurity http) throws Exception {
http
.csrf ()
.disable ()
.authorizeRequests ()
.antMatchers ("/swagger*/**", "/webjars/**", "/v 2/api-docs")
.permitAll ()
.and ()
.authorizeRequests ()
.antMatchers ("/static/**", "/file/**")
.permitAll ()
.and ()
.authorizeRequests ()
.anyRequest ()
.authenticated ()
.and ()
.logout ()
.logoutUrl ("/user/logout") //虚拟路径，不是控制器定义的路径
.logoutSuccessHandler (this)
.permitAll ()
.and ()
.exceptionHandling ()
.authenticationEntryPoint (this)
.and ()
.formLogin ()
.usernameParameter ("username")
.passwordParameter ("password")
.loginProcessingUrl ("/user/login") //虚拟路径，不是控制器定义的
路径
.successForwardUrl ("/user/login") //是控制器定义的路径
.failureHandler (this)
.and ()
.httpBasic ()
.authenticationEntryPoint (this);
}
```
```
@Override
protected void configure (AuthenticationManagerBuilder auth) throws
Exception {
auth.userDetailsService (userDetailService);
}
```
20

21

22
23
24
25

26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57

58
59
60
61
62
63
64
65
66

67
68


webflux-验证依赖于用户数据服务，需定义实现 ReactiveUserDetailsService 的 Bean

```
@Configuration
@EnableWebFluxSecurity //使用注解@EnableWebFluxSecurity
public class WebFluxSecurityConfig implements
WebFilter, //拦截器
ServerLogoutSuccessHandler, //登出成功回调
ServerAuthenticationEntryPoint, //验证入口
ServerAuthenticationFailureHandler, //验证成功回调
ServerAuthenticationSuccessHandler { //验证失败回调
//实现接口的方法
@Override
public Mono<Void> filter (ServerWebExchange exchange, WebFilterChain
chain) {
//配置 webflux 的 context-path
ServerHttpRequest request = exchange.getRequest ();
if (request.getURI (). getPath (). startsWith (contextPath)) {
exchange =
exchange.mutate (). request (request.mutate (). contextPath (contextPath). build ())
.build ();
}
//把查询参数转移到 FormData 中，不然验证过滤器
（ServerFormLoginAuthenticationConverter）接受不到参数
if (exchange.getRequest (). getMethod () == HttpMethod. POST &&
exchange.getRequest (). getQueryParams (). size () > 0 ) {
ServerWebExchange finalExchange = exchange;
ServerWebExchange realExchange = new Decorator (exchange) {
@Override
public Mono<MultiValueMap<String, String>> getFormData () {
return super.getFormData (). map (new
Function<MultiValueMap<String, String>, MultiValueMap<String, String>>() {
@Override
public MultiValueMap<String, String>
apply (MultiValueMap<String, String> stringStringMultiValueMap) {
if (stringStringMultiValueMap.size () == 0 ) {
return
finalExchange.getRequest (). getQueryParams ();
} else {
return stringStringMultiValueMap;
}
}
});
}
};
return chain.filter (realExchange);
}
return chain.filter (exchange);
}
```
```
@Override
public Mono<Void> onLogoutSuccess (WebFilterExchange webFilterExchange,
Authentication authentication) {
return sendJson (webFilterExchange.getExchange (), new Response<>("登出
成功"));
}
```
```
@Override
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
12
13
14
15
```
```
16
17
```
```
18
```
```
19
20
21
22
23
```
```
24
25
```
```
26
27
```
```
28
29
30
31
32
33
34
35
36
37
38
39
40
41
```
```
42
```
```
43
44
45
```

```
public Mono<Void> commence (ServerWebExchange exchange,
AuthenticationException e) {
return sendJson (exchange, new Response<>
(HttpStatus.UNAUTHORIZED.value (), "未验证"));
}
```
```
@Override
public Mono<Void> onAuthenticationFailure (WebFilterExchange
webFilterExchange, AuthenticationException exception) {
return sendJson (webFilterExchange.getExchange (), new Response<>( 1 ,
"验证失败"));
}
```
```
@Override
public Mono<Void> onAuthenticationSuccess (WebFilterExchange
webFilterExchange, Authentication authentication) {
return webFilterExchange.getChain (). filter (
webFilterExchange.getExchange (). mutate ()
.request (t ->
t.method (HttpMethod. POST). path ("/user/login")) //转发到自定义控制器
.build ()
);
}
```
```
@Bean
public SecurityWebFilterChain
springSecurityFilterChain (ServerHttpSecurity http) {
http.addFilterAfter (this, SecurityWebFiltersOrder. FIRST)
.csrf (). disable ()
.authorizeExchange ()
.pathMatchers ("/swagger*/**", "/webjars/**", "/v 2/api-docs")
//swagger
.permitAll ()
.and ()
.authorizeExchange ()
.pathMatchers ("/static/**", "/file/**") //静态资源
.permitAll ()
.and ()
.authorizeExchange ()
.anyExchange ()
.authenticated ()
.and ()
.logout () //登出
.logoutUrl ("/user/logout")
.logoutSuccessHandler (this)
.and ()
.exceptionHandling () //未验证回调
.authenticationEntryPoint (this)
.and ()
.formLogin ()
.loginPage ("/user/login")
.authenticationFailureHandler (this) //验证失败回调
.authenticationSuccessHandler (this) //验证成功回调
.and ()
.httpBasic ()
.authenticationEntryPoint (this); //basic 验证，一般用于移
动端
return http.build ();
```
46

47

48
49
50
51

52

53
54
55
56

57
58
59

60
61
62
63
64
65

66
67
68
69

70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93

94


###### WebSession 配置

```
}
}
```
```
95
96
```
```
@Configuration
@EnableRedisWebSession (maxInactiveIntervalInSeconds = 60 ) //使用注解
@EnableRedisWebSession ，maxInactiveIntervalInSeconds 设置数据过期时间，
spring. session. timeout 不管用
public class RedisWebSessionConfig { //考虑到分布式系统，一般使用 redis 存储 session
```
```
@Bean
public LettuceConnectionFactory lettuceConnectionFactory () {
return new LettuceConnectionFactory ();
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
//单点登录使用 ReactiveRedisSessionRepository.getSessionRedisOperations (). scan
方法查询相同用户名的 session，删除其他 session 即可
public Mono<Map<String, String>> findByPrincipalName (String name) {
return
reactiveSessionRepository.getSessionRedisOperations (). scan (ScanOptions. scanO
ptions (). match (ReactiveRedisSessionRepository. DEFAULT_NAMESPACE +
":sessions:*"). build ())
.flatMap (new Function<String, Publisher<Tuple 2<String,
Map. Entry<Object, Object>>>>() {
@Override
public Publisher<Tuple2<String, Map.Entry<Object, Object>>>
apply (String s) {
return
reactiveSessionRepository.getSessionRedisOperations (). opsForHash (). entries (s
)
.map (new Function<Map.Entry<Object, Object>,
Tuple 2<String, Map.Entry<Object, Object>>>() {
@Override
public Tuple 2<String, Map.Entry<Object, Object>>
apply (Map. Entry<Object, Object> objectObjectEntry) {
return Tuples.of (s, objectObjectEntry);
}
});
}
})
.filter (new Predicate<Tuple2<String, Map.Entry<Object, Object>>>() {
@Override
public boolean test (Tuple 2<String, Map.Entry<Object, Object>>
rule) {
Map. Entry<Object, Object> t = rule. getT 2 ();
String key = "sessionAttr: " +
HttpSessionSecurityContextRepository. SPRING_SECURITY_CONTEXT_KEY;
if (key.equals (t.getKey ())) {
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
12
13
14
15
16
17
18
```
```
19
20
```
```
21
```

对标的 SpringWebMVC 配置

###### 文件上传配置

```
User sci = (User) ((SecurityContextImpl)
t.getValue ()). getAuthentication (). getPrincipal ();
return sci.getUsername (). equals (name);
}
return false;
}
})
.collectMap (new Function<Tuple2<String, Map.Entry<Object, Object>>,
String>() {
@Override
public String apply (Tuple 2<String, Map.Entry<Object, Object>>
rule) {
return name;
}
}, new Function<Tuple2<String, Map.Entry<Object, Object>>, String>()
{
@Override
public String apply (Tuple 2<String, Map.Entry<Object, Object>>
rule) {
return
rule. getT 1 (). replace (ReactiveRedisSessionRepository. DEFAULT_NAMESPACE +
":sessions: ", "");
}
});
}
```
```
22
```
```
23
24
25
26
27
28
```
```
29
30
```
```
31
32
33
```
```
34
35
```
```
36
```
```
37
38
39
```
```
@Configuration
@EnableRedisHttpSession //使用注解@EnableRedisHttpSession
public class RedisHttpSessionConfig { //考虑到分布式系统，一般使用 redis 存储 session
```
```
@Bean
public LettuceConnectionFactory redisConnectionFactory () {
return new LettuceConnectionFactory ();
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
//单点登录使用 FindByIndexNameSessionRepository 根据用户名查询 session，删除其他
session 即可
Map<String, Session> map =
findByIndexNameSessionRepository.findByPrincipalName (name);
```
```
1
```
```
2
```
```
//参数上传
//定义参数 bean
@Setter
@Getter
@ToString
```
```
1
2
3
4
5
```

#### WebFlux 执行流程

userAdd 方法代码如下：

由于返回的数据只有一个所以使用的是 Mono 作为返回数据，使用 Mono 类静态 create 方法创建 Mono 对
象，代码如下：

```
@ApiModel
public class QueryBean{
@ApiModelProperty (value = "普通参数", required = false, example = "")
private String query;
@ApiModelProperty (value = "文件参数", required = false, example = "")
private FilePart image; //强调，webflux 中使用 FilePart 作为接收文件的类型
}
//定义接口
@ApiOperation ("一个接口")
@PostMapping ("/path")
//这里需要使用@ApiImplicitParam 显示配置【文件参数】才能使 swagger 界面显示上传文件按钮
@ApiImplicitParams ({
@ApiImplicitParam (
paramType = "form", //表单参数
dataType = "__file", //最新版本使用__file 表示文件，以前用的是 file
name = "image", //和 QueryBean 里面的【文件参数 image】同名
value = "文件") //注释
})
public Mono<Response> bannerAddOrUpdate (QueryBean q) {
```
```
}
```
```
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
```
```
public Mono<User> userAdd (@RequestBody User dto)
{
//命令式写法
// jpaEntityService.delUser (dto);
```
```
//响应式写法
return Mono.create (cityMonoSink ->
cityMonoSink.success (jpaEntityService.addUser (dto)));
}
```
```
1 2 3 4 5 6 7 8
```
```
public abstract class Mono<T> implements Publisher<T> {
static final BiPredicate EQUALS_BIPREDICATE = Object:: equals;
```
```
public Mono () {
}
```
```
public static <T> Mono<T> create (Consumer<MonoSink<T>> callback) {
return onAssembly (new MonoCreate (callback));
}
...
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```

可以到 create 方法接收一个参数，参数是 Consumer 对象，通过 callback 可以看出，这里使用的是
callback 回调，下面看看 Consumer 接口的定义：

通过上面的代码可以看出，有两个方法，一个是默认的方法 andThen，还有一个 accept 方法，

Mono.create () 方法的参数需要一个实现类，实现 Consumer 接口；Mono. create 方法的参数指向的实例
对象, 就是要实现这个 accept 方法。

例子中，下面的 lambda 表达式，就是 accept 方法的实现，实参的类型为 Consumer<MonoSink<User>>

, accept 的实现为如下：

来来来，重复看一下，create 方法的实现：

在方法内部调用了 onAssembly 方法，参数是 MonoCreate 对象，然后我们看看 MonoCreate 类，代码如
下：

```
@FunctionalInterface
public interface Consumer<T> {
```
```
/**
* Performs this operation on the given argument.
*
* @param t the input argument
*/
void accept (T t);
```
```
/**
* Returns a composed {@code Consumer} that performs, in sequence, this
* operation followed by the {@code after} operation. If performing
either
* operation throws an exception, it is relayed to the caller of the
* composed operation. If performing this operation throws an
exception,
* the {@code after} operation will not be performed.
*
* @param after the operation to perform after this operation
* @return a composed {@code Consumer} that performs in sequence this
* operation followed by the {@code after} operation
* @throws NullPointerException if {@code after} is null
*/
default Consumer<T> andThen (Consumer<? super T> after) {
Objects.requireNonNull (after);
return (T t) -> { accept (t); after.accept (t); };
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
14
15
```
```
16
17
18
19
20
21
22
23
24
25
26
27
```
```
1 cityMonoSink -> cityMonoSink.success (jpaEntityService.addUser (dto))
```
```
public static <T> Mono<T> create (Consumer<MonoSink<T>> callback) {
return onAssembly (new MonoCreate (callback));
}
```
```
1
2
3
```

```
//
// Source code recreated from a .class file by IntelliJ IDEA
// (powered by Fernflower decompiler)
//
```
```
package reactor. core. publisher;
```
```
import java. util. Objects;
import java. util. concurrent. atomic. AtomicBoolean;
import java. util. concurrent. atomic. AtomicIntegerFieldUpdater;
import java. util. concurrent. atomic. AtomicReferenceFieldUpdater;
import java. util. function. Consumer;
import java. util. function. LongConsumer;
import reactor. core. CoreSubscriber;
import reactor. core. Disposable;
import reactor. core. Scannable. Attr;
import reactor. core. publisher. FluxCreate. SinkDisposable;
import reactor. util. annotation. Nullable;
import reactor. util. context. Context;
```
```
final class MonoCreate<T> extends Mono<T> {
final Consumer<MonoSink<T>> callback;
```
```
MonoCreate (Consumer<MonoSink<T>> callback) {
this. callback = callback;
}
```
```
public void subscribe (CoreSubscriber<? super T> actual) {
MonoCreate. DefaultMonoSink<T> emitter = new
MonoCreate.DefaultMonoSink (actual);
actual.onSubscribe (emitter);
```
```
try {
this.callback.accept (emitter);
} catch (Throwable var 4) {
emitter.error (Operators.onOperatorError (var 4,
actual.currentContext ()));
}
```
```
}
```
```
static final class DefaultMonoSink<T> extends AtomicBoolean implements
MonoSink<T>, InnerProducer<T> {
final CoreSubscriber<? super T> actual;
volatile Disposable disposable;
static final
AtomicReferenceFieldUpdater<MonoCreate.DefaultMonoSink, Disposable>
DISPOSABLE =
AtomicReferenceFieldUpdater.newUpdater (MonoCreate. DefaultMonoSink. class,
Disposable. class, "disposable");
volatile int state;
static final AtomicIntegerFieldUpdater<MonoCreate.DefaultMonoSink>
STATE =
AtomicIntegerFieldUpdater.newUpdater (MonoCreate. DefaultMonoSink. class,
"state");
volatile LongConsumer requestConsumer;
```
```
1 2 3 4 5 6 7 8 9
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

30
31
32
33
34
35

36
37
38
39
40

41
42
43

44
45

46


```
static final
AtomicReferenceFieldUpdater<MonoCreate.DefaultMonoSink, LongConsumer>
REQUEST_CONSUMER =
AtomicReferenceFieldUpdater.newUpdater (MonoCreate. DefaultMonoSink. class,
LongConsumer. class, "requestConsumer");
T value;
static final int NO_REQUEST_HAS_VALUE = 1 ;
static final int HAS_REQUEST_NO_VALUE = 2 ;
static final int HAS_REQUEST_HAS_VALUE = 3 ;
```
```
DefaultMonoSink (CoreSubscriber<? super T> actual) {
this. actual = actual;
}
```
```
public Context currentContext () {
return this.actual.currentContext ();
}
```
```
@Nullable
public Object scanUnsafe (Attr key) {
if (key != Attr. TERMINATED) {
return key == Attr. CANCELLED?
OperatorDisposables.isDisposed (this. disposable) : super.scanUnsafe (key);
} else {
return this. state == 3 || this. state == 1 ;
}
}
```
```
public void success () {
if (STATE.getAndSet (this, 3 ) != 3 ) {
try {
this.actual.onComplete ();
} finally {
this.disposeResource (false);
}
}
```
```
}
```
```
public void success (@Nullable T value) {
if (value == null) {
this.success ();
} else {
int s;
do {
s = this. state;
if (s == 3 || s == 1 ) {
Operators.onNextDropped (value,
this.actual.currentContext ());
return;
}
```
```
if (s == 2 ) {
if (STATE.compareAndSet (this, s, 3 )) {
try {
this.actual.onNext (value);
this.actual.onComplete ();
} finally {
```
47

48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64

65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89

90
91
92
93
94
95
96
97
98


```
this.disposeResource (false);
}
}
```
```
return;
}
```
```
this. value = value;
} while (! STATE.compareAndSet (this, s, 1 ));
```
```
}
}
```
```
public void error (Throwable e) {
if (STATE.getAndSet (this, 3 ) != 3 ) {
try {
this.actual.onError (e);
} finally {
this.disposeResource (false);
}
} else {
Operators.onOperatorError (e, this.actual.currentContext ());
}
```
```
}
```
```
public MonoSink<T> onRequest (LongConsumer consumer) {
Objects.requireNonNull (consumer, "onRequest");
if (! REQUEST_CONSUMER.compareAndSet (this, (Object) null,
consumer)) {
throw new IllegalStateException ("A consumer has already
been assigned to consume requests");
} else {
return this;
}
}
```
```
public CoreSubscriber<? super T> actual () {
return this. actual;
}
```
```
public MonoSink<T> onCancel (Disposable d) {
Objects.requireNonNull (d, "onCancel");
SinkDisposable sd = new SinkDisposable ((Disposable) null, d);
if (! DISPOSABLE.compareAndSet (this, (Object) null, sd)) {
Disposable c = this. disposable;
if (c instanceof SinkDisposable) {
SinkDisposable current = (SinkDisposable) c;
if (current. onCancel == null) {
current. onCancel = d;
} else {
d.dispose ();
}
}
}
```
```
return this;
}
```
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127

128

129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154


```
public MonoSink<T> onDispose (Disposable d) {
Objects.requireNonNull (d, "onDispose");
SinkDisposable sd = new SinkDisposable (d, (Disposable) null);
if (! DISPOSABLE.compareAndSet (this, (Object) null, sd)) {
Disposable c = this. disposable;
if (c instanceof SinkDisposable) {
SinkDisposable current = (SinkDisposable) c;
if (current. disposable == null) {
current. disposable = d;
} else {
d.dispose ();
}
}
}
```
```
return this;
}
```
```
public void request (long n) {
if (Operators.validate (n)) {
LongConsumer consumer = this. requestConsumer;
if (consumer != null) {
consumer.accept (n);
}
```
```
int s;
do {
s = this. state;
if (s == 2 || s == 3 ) {
return;
}
```
```
if (s == 1 ) {
if (STATE.compareAndSet (this, s, 3 )) {
try {
this.actual.onNext (this. value);
this.actual.onComplete ();
} finally {
this.disposeResource (false);
}
}
```
```
return;
}
} while (! STATE.compareAndSet (this, s, 2 ));
```
```
}
}
```
```
public void cancel () {
if (STATE.getAndSet (this, 3 ) != 3 ) {
this. value = null;
this.disposeResource (true);
}
```
```
}
```
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212


上面的代码比较多，我们主要关注下面两个函数：

通过上面的代码可以看出，一个是构造器，参数是 Consumer，里面进行操作保存了 Consumer 对象，
然后在 subscribe 方法里面有一句代码是 this.callback.accept (emitter), 就是在这里进行了接口的回调，
回调 Consumer 的 accept 方法，这个方法是在调用 Mono.create () 方法的时候实现了。然后在细看
subscribe 方法，这里面有一个 actual. onSubscribe 方法，通过方法名可以知道，这里是订阅了消息。
webflux 是基于 reactor 模型，基于事件消息和异步，这里也体现了一个异步。

Mono 和 Flux 的其他用法可以参照上面的源码流程自己看看，就不细说了。

#### WebFlux 学习提示

WebFlux 挺难的，基础的知识具体请参见尼恩的《响应式编程圣经》

```
void disposeResource (boolean isCancel) {
Disposable d = this. disposable;
if (d != OperatorDisposables. DISPOSED) {
d = (Disposable) DISPOSABLE.getAndSet (this,
OperatorDisposables. DISPOSED);
if (d != null && d != OperatorDisposables. DISPOSED) {
if (isCancel && d instanceof SinkDisposable) {
((SinkDisposable) d). cancel ();
}
```
```
d.dispose ();
}
}
```
```
}
}
}
```
```
213
214
215
216
```
```
217
218
219
220
221
222
223
224
225
226
227
228
```
```
MonoCreate (Consumer<MonoSink<T>> callback) {
this. callback = callback;
}
```
```
public void subscribe (CoreSubscriber<? super T> actual) {
MonoCreate. DefaultMonoSink<T> emitter = new
MonoCreate.DefaultMonoSink (actual);
actual.onSubscribe (emitter);
```
```
try {
this.callback.accept (emitter);
} catch (Throwable var 4) {
emitter.error (Operators.onOperatorError (var 4,
actual.currentContext ()));
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
13
14
```

《响应式圣经：10 W 字，实现 Spring 响应式编程自由》

更加进一步的学习，具体请参见尼恩的《全链路异步架构与实操》

## Spring Cloud Gateway 微服务网关

Zuul 作为老牌 SpringCloud 微服务网关，很多项目仍然在使用，

另外，底层原理都是想通的，大家可以和 Spring Cloud Gateway 对比学习

Zuul 的详细介绍，请阅读《Java 高并发核心编程卷 3 加强版》

了解了 WEBFlux 基础知识之后，接下来，咱们死磕 Spring Cloud Gateway 网关。

#### 1 、SpringCloud Gateway 简介

SpringCloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0
和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理
方式。

SpringCloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Zuul，在 Spring Cloud 2.0
以上版本中，没有对新版本的 Zuul 2.0 以上最新高性能版本进行集成，仍然还是使用的 Zuul 2.0 之前的非
Reactor 模式的老版本。而为了提升网关的性能，SpringCloud Gateway 是基于 WebFlux 框架实现的，而
WebFlux 框架底层则使用了高性能的 Reactor 模式通信框架 Netty。

Spring Cloud Gateway 的目标，不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的
功能，例如：安全，监控/指标，和限流。

**特别说明** ：
Spring Cloud Gateway 底层使用了高性能的通信框架 Netty。

Netty 是高性能中间件的通讯底座， rocketmq 、seata、nacos 、sentinel 、redission 、dubbo 等太
多、太多的的大名鼎鼎的中间件，无一例外都是基于 netty。

可以毫不夸张的说： **netty 是进入大厂、走向高端的必备技能** 。

要想深入了解 springcloud gateway ， **最好是掌握 netty 编程** 。

有关 netty 学习具体请参见机工社出版、尼恩的畅销书: 《 **Java 高并发核心编程卷 1** 》

###### 1.1 本文姊妹篇《Flux 和 Mono 、reactor 实战 （史上最全）》

另外，如果要掌握 springcloud gateway 的过滤器编程，或者掌握 springcloud gateway 开发，还必
须：
**具备 reactor 响应式编程的实战能力**

有关 reactor 响应式编程的实战，具体请参考本文姊妹篇：

《Flux 和 Mono 、reactor 实战 （史上最全）》


###### 1.2 SpringCloud Gateway 特征

SpringCloud 官方，对 SpringCloud Gateway 特征介绍如下：

（ 1 ）基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0

（ 2 ）集成 Hystrix 断路器

（ 3 ）集成 Spring Cloud DiscoveryClient

（ 4 ）Predicates 和 Filters 作用于特定路由，易于编写的 Predicates 和 Filters

（ 5 ）具备一些网关的高级功能：动态路由、限流、路径重写

从以上的特征来说，和 Zuul 的特征差别不大。SpringCloud Gateway 和 Zuul 主要的区别，还是在底层的
通信框架上。

简单说明一下上文中的三个术语：

**（ 1 ）Filter（过滤器）** ：

和 Zuul 的过滤器在概念上类似，可以使用它拦截和修改请求，并且对上游的响应，进行二次处理。过滤
器为 org. springframework. cloud. gateway. filter. GatewayFilter 类的实例。

**（ 2 ）Route（路由）** ：

网关配置的基本组成模块，和 Zuul 的路由配置模块类似。一个 **Route 模块** 由一个 ID，一个目标 URI，一
组断言和一组过滤器定义。如果断言为真，则路由匹配，目标 URI 会被访问。

**（ 3 ）Predicate（断言）** ：

这是一个 Java 8 的 Predicate，可以使用它来匹配来自 HTTP 请求的任何内容，例如 headers 或参数。
**断言的** 输入类型是一个 ServerWebExchange。

###### 1.3 SpringCloud Gateway 和架构

Spring 在 2017 年下半年迎来了 Webflux，Webflux 的出现填补了 Spring 在响应式编程上的空白，
Webflux 的响应式编程不仅仅是编程风格的改变，而且对于一系列的著名框架，都提供了响应式访问的
开发包，比如 Netty、Redis 等等。

SpringCloud Gateway 使用的 Webflux 中的 reactor-netty 响应式编程组件，底层使用了 Netty 通讯框
架。


**1 ）SpringCloud Zuul 的 IO 模型**

Springcloud 中所集成的 Zuul 版本，采用的是 Tomcat 容器，使用的是传统的 Servlet IO 处理模型。

大家知道，servlet 由 servlet container 进行生命周期管理。container 启动时构造 servlet 对象并调用
servlet init () 进行初始化；container 关闭时调用 servlet destory () 销毁 servlet；container 运行时接受请
求，并为每个请求分配一个线程（一般从线程池中获取空闲线程）然后调用 service ()。

弊端：servlet 是一个简单的网络 IO 模型，当请求进入 servlet container 时，servlet container 就会为其
绑定一个线程，在并发不高的场景下这种模型是适用的，但是一旦并发上升，线程数量就会上涨，而线
程资源代价是昂贵的（上线文切换，内存消耗大）严重影响请求的处理时间。在一些简单的业务场景
下，不希望为每个 request 分配一个线程，只需要 1 个或几个线程就能应对极大并发的请求，这种业务场
景下 servlet 模型没有优势。

所以 Springcloud Zuul 是基于 servlet 之上的一个阻塞式处理模型，即 spring 实现了处理所有 request 请
求的一个 servlet（DispatcherServlet），并由该 servlet 阻塞式处理处理。所以 Springcloud Zuul 无法摆
脱 servlet 模型的弊端。虽然 Zuul 2.0 开始，使用了 Netty，并且已经有了大规模 Zuul 2.0 集群部署的成熟
案例，但是，Springcloud 官方已经没有集成改版本的计划了。

**2 ）Webflux 服务器**

Webflux 模式替换了旧的 Servlet 线程模型。用少量的线程处理 request 和 response io 操作，这些线程称
为 Loop 线程，而业务交给响应式编程框架处理，响应式编程是非常灵活的，用户可以将业务中阻塞的操
作提交到响应式框架的 work 线程中执行，而不阻塞的操作依然可以在 Loop 线程中进行处理，大大提高
了 Loop 线程的利用率。官方结构图：


Webflux 虽然可以兼容多个底层的通信框架，但是一般情况下，底层使用的还是 Netty，毕竟，Netty 是
目前业界认可的最高性能的通信框架。而 Webflux 的 Loop 线程，正好就是著名的 Reactor 模式 IO 处理模
型的 Reactor 线程，如果使用的是高性能的通信框架 Netty，这就是 Netty 的 EventLoop 线程。

关于 Reactor 线程模型，和 Netty 通信框架的知识，是 Java 程序员的重要、必备的内功，个中的原理，具
体请参见尼恩编著的《Netty、Zookeeper、Redis 高并发实战》一书，这里不做过多的赘述。

**3 ）Spring Cloud Gateway 的处理流程**

客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配
的路由，将其发送到 Gateway Web Handler。Handler 再通过指定的过滤器链来将请求发送到我们实
际的服务执行业务逻辑，然后返回。过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前
（“pre”）或之后（“post”）执行业务逻辑。

#### 2 、路由配置方式

###### 2.1 基础 URI 路由配置方式

如果请求的目标地址，是单个的 URI 资源路径，配置文件示例如下：


各字段含义如下：

id：我们自定义的路由 ID，保持唯一

uri：目标服务地址

predicates：路由条件，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法
来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。

上面这段配置的意思是，配置了一个 id 为 url-proxy-1 的 URI 代理规则，路由的规则为：

当访问地址 [http://localhost:8080/csdn/1.jsp](http://localhost:8080/csdn/1.jsp) 时，

会路由到上游地址 https://blog.csdn.net/1.jsp 。

###### 2.2 基于代码的路由配置方式

转发功能同样可以通过代码来实现，我们可以在启动类 GateWayApplication 中添加方法
customRouteLocator () 来定制转发规则。

```
server:
port: 8080
spring:
application:
name: api-gateway
cloud:
gateway:
routes:
```
- id: url-proxy-1
uri: https://blog.csdn.net
predicates:
- Path=/csdn

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
package com. springcloud. gateway;
```
```
import org. springframework. boot. SpringApplication;
import org. springframework. boot. autoconfigure. SpringBootApplication;
import org. springframework. cloud. gateway. route. RouteLocator;
import org. springframework. cloud. gateway. route. builder. RouteLocatorBuilder;
import org. springframework. context. annotation. Bean;
```
```
@SpringBootApplication
public class GatewayApplication {
```
```
public static void main (String[] args) {
SpringApplication.run (GatewayApplication. class, args);
}
```
```
@Bean
public RouteLocator customRouteLocator (RouteLocatorBuilder builder) {
return builder.routes ()
.route ("path_route", r -> r.path ("/csdn")
.uri ("https://blog.csdn.net"))
.build ();
}
```
```
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
```

我们在 yaml 配置文件中注销掉相关路由的配置，重启服务，访问链接：http://localhost:8080/ csdn，
可以看到和上面一样的页面，证明我们测试成功。

上面两个示例中 uri 都是指向了我的 CSDN 博客，在实际项目使用中可以将 uri 指向对外提供服务的项目
地址，统一对外输出接口。

###### 2.3 和注册中心相结合的路由配置方式

在 uri 的 schema 协议部分为自定义的 lb: 类型，表示从微服务注册中心（如 Eureka）订阅服务，并且进行
服务的路由。

一个典型的示例如下：

注册中心相结合的路由配置方式，与单个 URI 的路由配置，区别其实很小，仅仅在于 URI 的 schema 协议
不同。单个 URI 的地址的 schema 协议，一般为 http 或者 https 协议。

#### 3 、路由匹配规则

Spring Cloud Gateway 的功能很强大，我们仅仅通过 Predicates 的设计就可以看出来，前面我们只是
使用了 predicates 进行了简单的条件匹配，其实 Spring Cloud Gataway 帮我们内置了很多 Predicates
功能。

```
server:
port: 8084
spring:
cloud:
gateway:
routes:
```
- id: seckill-provider-route
uri: lb://seckill-provider
predicates:
- Path=/seckill-provider/**
- id: message-provider-route
uri: lb://message-provider
predicates:
- Path=/message-provider/**

```
application:
name: cloud-gateway
```
```
eureka:
instance:
prefer-ip-address: true
client:
service-url:
defaultZone: http://localhost:8888/eureka/
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
```

```
Route（路
由）
```
```
路由是网关的基本单元，由 ID、URI、一组 Predicate、一组 Filter 组成，根据
Predicate 进行匹配转发。
```
```
Predicate
（谓语、断
言）
```
```
路由转发的判断条件，目前 SpringCloud Gateway 支持多种方式，常见如：
Path、Query、Method、Header 等，写法必须遵循 key=vlue 的形式
```
```
Filter（过
滤器）
过滤器是路由转发请求时所经过的过滤逻辑，可用于修改请求、响应内容
```
Spring Cloud Gateway 是通过 Spring WebFlux 的 HandlerMapping 做为底层支持来匹配到转发路
由，Spring Cloud Gateway 内置了很多 Predicates 工厂，这些 Predicates 工厂通过不同的 HTTP 请求
参数来匹配，多个 Predicates 工厂可以组合使用。

gateWay 的主要功能之一是转发请求，转发规则的定义主要包含三个部分

```
其中 Route 和 Predicate 必须同时申明
```
例子：

```
//通过配置文件配置
spring:
cloud:
gateway:
routes:
```
- id: gate_route
uri: [http://localhost:](http://localhost:) 9023
predicates:
## 当请求的路径为 gate、rule 开头的时，转发到http://localhost:9023服务器上
- Path=/gate/**,/rule/**
### 请求路径前加上/app
filters:
- PrefixPath=/app

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```

**说明：**

此文的配置，大部分在 demo 工程中已经验证，在写的过程中，可能有格式错误

如果遇到问题，可以找尼恩

###### 3.1 Predicate 断言条件 (转发规则) 介绍

Predicate 来源于 Java 8，是 Java 8 中引入的一个函数，Predicate 接受一个输入参数，返回一个布尔值
结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。可以用
于接口请求参数校验、判断新老数据是否有变化需要进行更新操作。

在 Spring Cloud Gateway 中 Spring 利用 Predicate 的特性实现了各种路由匹配规则，有通过
Header、请求参数等不同的条件来进行作为条件匹配到对应的路由。网上有一张图总结了 Spring
Cloud 内置的几种 Predicate 的实现。

说白了 Predicate 就是为了实现一组匹配规则，方便让请求过来找到对应的 Route 进行处理，接下来我
们接下 Spring Cloud GateWay 内置几种 Predicate 的使用。

```
转发规则（predicates），假设转发 uri 都设定为 http://localhost:9023
```

```
规则实例说明
```
```
Path - Path=/gate/ ,/rule/
```
```
## 当请求的路径为 gate、rule
开头的时，转发到http://localh
ost: 9023 服务器上
```
```
Before
```
- Before=2017-01-20 T17:42:47.789-
07:00[America/Denver]

```
在某个时间之前的请求才会被转
发到 http://localhost:9023服务
器上
```
```
After
```
- After=2017-01-20 T17:42:47.789-
07:00[America/Denver]

```
在某个时间之后的请求才会被转
发
```
```
Between
```
- Between=2017-01-20 T17:42:47.789-
07:00[America/Denver], 2017-01-
21 T17:42:47.789-07:00[America/Denver]

```
在某个时间段之间的才会被转发
```
```
Cookie - Cookie=chocolate, ch. p
```
```
名为 chocolate 的表单或者满足
正则 ch. p 的表单才会被匹配到进
行请求转发
```
```
Header - Header=X-Request-Id, \d+
```
```
携带参数 X-Request-Id 或者满足
\d+的请求头才会匹配
```
```
Host - Host=www.hd123.com
```
```
当主机名为www.hd123.com的
时候直接转发到http://localhos
t: 9023 服务器上
```
```
Method - Method=GET
```
```
只有 GET 方法才会匹配转发请
求，还可以限定 POST、PUT 等
请求方式
```
**1 ）通过请求参数匹配**

Query Route Predicate 支持传入两个参数，一个是属性名一个为属性值，属性值可以是正则表达式。

这样配置，只要请求中包含 smile 属性的参数即可匹配路由。

使用 curl 测试，命令行输入:

curl localhost: 8080? smile=x&id=2

经过测试发现只要请求汇总带有 smile 参数即会匹配路由，不带 smile 参数则不会匹配。

```
server:
port: 8080
spring:
cloud:
gateway:
routes:
```
- id: query_route
uri: https://example.org
predicates:
- Query=smile

```
1 2 3 4 5 6 7 8 9
```
```
10
```

还可以将 Query 的值以键值对的方式进行配置，这样在请求过来时会对属性值和正则进行匹配，匹配上
才会走路由。

这样只要当请求中包含 keep 属性并且参数值是以 pu 开头的长度为三位的字符串才会进行匹配和路由。

使用 curl 测试，命令行输入:

curl localhost: 8080? keep=pub

测试可以返回页面代码，将 keep 的属性值改为 pubx 再次访问就会报 404, 证明路由需要匹配正则表达
式才会进行路由。

**2 ）通过 Header 属性匹配**

Header Route Predicate 和 Cookie Route Predicate 一样，也是接收 2 个参数，一个 header 中属性
名称和一个正则表达式，这个属性值和正则表达式匹配则执行。

使用 curl 测试，命令行输入:

curl [http://localhost:8080](http://localhost:8080) -H "X-Request-Id: 88"

则返回页面代码证明匹配成功。将参数-H "X-Request-Id: 88"改为-H "X-Request-Id: spring"再次执行时
返回 404 证明没有匹配。

**3 ）通过 Cookie 匹配**

Cookie Route Predicate 可以接收两个参数，一个是 Cookie name ,一个是正则表达式，路由规则会通
过获取对应的 Cookie name 值和正则表达式去匹配，如果匹配上就会执行路由，如果没有匹配上则不
执行。

```
server:
port: 8080
spring:
cloud:
gateway:
routes:
```
- id: query_route
uri: https://example.org
predicates:
- Query=keep, pu.

```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
server:
port: 8080
spring:
cloud:
gateway:
routes:
```
- id: query_route
uri: https://example.org
predicates:
- Header=X-Request-Id, \d+

```
1 2 3 4 5 6 7 8 9
```
```
10
```

使用 curl 测试，命令行输入:

```
curl http://localhost:8080 --cookie "sessionId=test"
```
则会返回页面代码，如果去掉--cookie "sessionId=test"，后台汇报 404 错误。

**4 ）通过 Host 匹配**

Host Route Predicate 接收一组参数，一组匹配的域名列表，这个模板是一个 ant 分隔的模板，用. 号作
为分隔符。它通过参数中的主机地址作为匹配规则。

使用 curl 测试，命令行输入:

```
curl http://localhost:8080 -H "Host: http://www.baidu.com"
```
```
curl http://localhost:8080 -H "Host: md. baidu. com"
```
经测试以上两种 host 均可匹配到 host_route 路由，去掉 host 参数则会报 404 错误。

**5 ）通过请求方式匹配**

可以通过是 POST、GET、PUT、DELETE 等不同的请求方式来进行路由。

```
server:
port: 8080
spring:
application:
name: api-gateway
cloud:
gateway:
routes:
-id: gateway-service
uri: https://www.baidu.com
order: 0
predicates:
```
- Cookie=sessionId, test

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
server:
port: 8080
spring:
cloud:
gateway:
routes:
```
- id: query_route
uri: https://example.org
predicates:
- Host=**. baidu. com

```
1 2 3 4 5 6 7 8 9
```
```
10
```

使用 curl 测试，命令行输入:

# curl 默认是以 GET 的方式去请求

```
curl http://localhost:8080
```
测试返回页面代码，证明匹配到路由，我们再以 POST 的方式请求测试。

# curl 默认是以 GET 的方式去请求

```
curl -X POST http://localhost:8080
```
返回 404 没有找到，证明没有匹配上路由

**6 ）通过请求路径匹配**

Path Route Predicate 接收一个匹配路径的参数来判断是否走路由。

如果请求路径符合要求，则此路由将匹配，例如：/foo/1 或者 /foo/bar。

使用 curl 测试，命令行输入:

curl [http://localhost:8080/foo/1](http://localhost:8080/foo/1)

curl [http://localhost:8080/foo/xx](http://localhost:8080/foo/xx)

curl [http://localhost:8080/boo/xx](http://localhost:8080/boo/xx)

经过测试第一和第二条命令可以正常获取到页面返回值，最后一个命令报 404 ，证明路由是通过指定路
由来匹配。

```
server:
port: 8080
spring:
cloud:
gateway:
routes:
```
- id: query_route
uri: https://example.org
predicates:
- Method=GET

```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
server:
port: 8080
spring:
cloud:
gateway:
routes:
```
- id: query_route
uri: https://example.org
predicates:
-Path=/foo/{segment}

```
1 2 3 4 5 6 7 8 9
```
```
10
```

**7 ）通过请求 ip 地址进行匹配**

Predicate 也支持通过设置某个 ip 区间号段的请求才会路由，RemoteAddr Route Predicate 接受 cidr
符号 (IPv 4 或 IPv 6 ) 字符串的列表 (最小大小为 1)，例如 192.168.0.1/16 (其中 192.168.0.1 是 IP 地址， 16
是子网掩码)。

可以将此地址设置为本机的 ip 地址进行测试。

curl localhost:8080

如果请求的远程地址是 192.168.1.10，则此路由将匹配。

**8 ）组合使用**

各种 Predicates 同时存在于同一个路由时，请求必须同时满足所有的条件才被这个路由匹配。

一个请求满足多个路由的断言条件时，请求只会被首个成功匹配的路由转发

```
server:
port: 8080
spring:
cloud:
gateway:
routes:
```
- id: query_route
uri: https://example.org
predicates:
- RemoteAddr=192.168.1.1/24

```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
server:
port: 8080
spring:
application:
name: api-gateway
cloud:
gateway:
routes:
```
- id: gateway-service
uri: https://www.baidu.com
order: 0
predicates:
- Host=**. foo. org
- Path=/headers
- Method=GET
- Header=X-Request-Id, \d+
- Query=foo, ba.
- Query=baz
- Cookie=chocolate, ch. p

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
```

```
过滤规则实例说明
```
```
PrefixPath - PrefixPath=/app 在请求路径前加上 app
```
```
RewritePath
```
- RewritePath=/test,
/app/test

```
访问 localhost: 9022/test, 请求会转发到
localhost: 8001/app/test
```
```
SetPath SetPath=/app/{path}
```
```
通过模板设置路径，转发的规则时会在路
径前增加 app，{path}表示原请求路径
```
```
RedirectTo 重定向
```
```
RemoveRequestHeader 去掉某个请求头信息
```
###### 3.2 过滤器规则（Filter）

**过滤器规则（Filter）**

注：当配置多个 filter 时，优先定义的会被调用，剩余的 filter 将不会生效

**PrefixPath**

对所有的请求路径添加前缀：

访问/hello 的请求被发送到https://example.org/mypath/hello。

**RedirectTo**

重定向，配置包含重定向的返回码和地址：

```
spring:
cloud:
gateway:
routes:
```
- id: prefixpath_route
uri: https://example.org
filters:
- PrefixPath=/mypath

```
1 2 3 4 5 6 7 8 9
```
```
spring:
cloud:
gateway:
routes:
```
- id: prefixpath_route
uri: https://example.org
filters:
- RedirectTo=302, https://acme.org

```
1 2 3 4 5 6 7 8 9
```

**RemoveRequestHeader**

去掉某个请求头信息：

去掉请求头信息 X-Request-Foo

###### RemoveResponseHeader

去掉某个回执头信息：

**RemoveRequestParameter**

去掉某个请求参数信息：

**RewritePath**

改写路径：

```
spring:
cloud:
gateway:
routes:
```
- id: removerequestheader_route
uri: https://example.org
filters:
- RemoveRequestHeader=X-Request-Foo

```
1 2 3 4 5 6 7 8
```
```
spring:
cloud:
gateway:
routes:
```
- id: removerequestheader_route
uri: https://example.org
filters:
- RemoveResponseHeader=X-Request-Foo

```
1 2 3 4 5 6 7 8
```
```
spring:
cloud:
gateway:
routes:
```
- id: removerequestparameter_route
uri: https://example.org
filters:
- RemoveRequestParameter=red

```
1 2 3 4 5 6 7 8
```

/where/... 改成 test/...

使用代码改下路径

**SetPath**

设置请求路径，与 RewritePath 类似。

如/red/blue 的请求被转发到/blue。

```
spring:
cloud:
gateway:
routes:
```
- id: rewrite_filter
uri: [http://localhost:](http://localhost:) 8081
predicates:
- Path=/test/**
filters:
- RewritePath=/where (?<segment>/?.*), /test (?<segment>/?.*)

```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
RouteLocatorBuilder. Builder builder = routeLocatorBuilder.routes ();
builder
.route ("path_rote_at_guigu", r -> r.path ("/guonei")
.uri ("http://news.baidu.com/guonei"))
.route ("csdn_route", r -> r.path ("/csdn")
.uri ("https://blog.csdn.net"))
.route ("blog 3_rewrite_filter", r -> r.path ("/blog 3/**")
.filters (f -> f.rewritePath ("/blog 3/(?<segment>.*)", "/$\\
{segment}"))
.uri ("https://blog.csdn.net"))
.route ("rewritepath_route", r -> r.path ("/baidu/**")
.filters (f -> f.rewritePath ("/baidu/(?<segment>.*)", "/$\\
{segment}"))
.uri ("http://www.baidu.com"))
```
```
.build ();
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
12
13
14
```
```
spring:
cloud:
gateway:
routes:
```
- id: setpath_route
uri: https://example.org
predicates:
- Path=/red/{segment}
filters:
- SetPath=/{segment}

```
1 2 3 4 5 6 7 8 9
```
```
10
```

**SetRequestHeader**

设置请求头信息。

**SetStatus**

设置回执状态码。

**StripPrefix**

跳过指定路径。

请求/name/blue/red 会转发到/red。

**RequestSize**

请求大小。

```
spring:
cloud:
gateway:
routes:
```
- id: setrequestheader_route
uri: https://example.org
filters:
- SetRequestHeader=X-Request-Red, Blue

```
1 2 3 4 5 6 7 8
```
```
spring:
cloud:
gateway:
routes:
```
- id: setstatusint_route
uri: https://example.org
filters:
- SetStatus=401

```
1 2 3 4 5 6 7 8
```
```
spring:
cloud:
gateway:
routes:
```
- id: nameRoot
uri: https://nameservice
predicates:
- Path=/name/**
filters:
- StripPrefix=2

```
1 2 3 4 5 6 7 8 9
```
```
10
```

超过 5 M 的请求会返回 413 错误。

**Default-filters**

对所有请求添加过滤器。

###### 3.3 通过代码进行配置

通过代码进行配置，将路由规则设置为一个 Bean 即可：

```
spring:
cloud:
gateway:
routes:
```
- id: request_size_route
uri: [http://localhost:8080/upload](http://localhost:8080/upload)
predicates:
- Path=/upload
filters:
- name: RequestSize
args:
maxSize: 5000000

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
spring:
cloud:
gateway:
default-filters:
```
- AddResponseHeader=X-Response-Default-Red, Default-Blue
- PrefixPath=/httpbin

```
1 2 3 4 5 6
```
```
@Bean
public RouteLocator customRouteLocator (RouteLocatorBuilder builder) {
return builder.routes ()
.route ("path_route", r -> r.path ("/get")
.uri ("http://httpbin.org"))
.route ("host_route", r -> r.host ("*. myhost. org")
.uri ("http://httpbin.org"))
.route ("rewrite_route", r -> r.host ("*. rewrite. org")
.filters (f -> f.rewritePath ("/foo/(?<segment>.*)",
"/${segment}"))
.uri ("http://httpbin.org"))
.route ("hystrix_route", r -> r.host ("*. hystrix. org")
.filters (f -> f.hystrix (c -> c.setName ("slowcmd")))
.uri ("http://httpbin.org"))
.route ("hystrix_fallback_route", r ->
r.host ("*. hystrixfallback. org")
.filters (f -> f.hystrix (c ->
c.setName ("slowcmd"). setFallbackUri ("forward:/hystrixfallback")))
.uri ("http://httpbin.org"))
.route ("limit_route", r -> r
.host ("*. limited. org"). and (). path ("/anything/**")
.filters (f -> f.requestRateLimiter (c ->
c.setRateLimiter (redisRateLimiter ())))
.uri ("http://httpbin.org"))
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
```
```
15
```
```
16
17
18
19
```
```
20
```

###### 3.2 实现熔断降级

为什么要实现熔断降级？

在分布式系统中，网关作为流量的入口，因此会有大量的请求进入网关，向其他服务发起调用，其他服
务不可避免的会出现调用失败（超时、异常），失败时不能让请求堆积在网关上，需要快速失败并返回
给客户端，想要实现这个要求，就必须在网关上做熔断、降级操作。

为什么在网关上请求失败需要快速返回给客户端？

因为当一个客户端请求发生故障的时候，这个请求会一直堆积在网关上，当然只有一个这种请求，网关
肯定没有问题（如果一个请求就能造成整个系统瘫痪，那这个系统可以下架了），但是网关上堆积多了
就会给网关乃至整个服务都造成巨大的压力，甚至整个服务宕掉。因此要对一些服务和页面进行有策略
的降级，以此缓解服务器资源的的压力，以保证核心业务的正常运行，同时也保持了客户和大部分客户
的得到正确的相应，所以需要网关上请求失败需要快速返回给客户端。

这里的配置，使用了两个过滤器：

（ 1 ）过滤器 StripPrefix，作用是去掉请求路径的最前面 n 个部分截取掉。

```
.build ();
}
```
```
21
22
```
```
server. port: 8082
```
```
spring:
application:
name: gateway
redis:
host: localhost
port: 6379
password: 123456
cloud:
gateway:
routes:
```
- id: rateLimit_route
uri: [http://localhost:](http://localhost:) 8000
order: 0
predicates:
- Path=/test/**
filters:
- StripPrefix=1
- name: Hystrix
args:
name: fallbackCmdA
fallbackUri: forward:/fallbackA

```
hystrix. command. fallbackCmdA. execution. isolation. thread. timeoutInMillisecond
s: 5000
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
```

StripPrefix=1 就代表截取路径的个数为 1 ，比如前端过来请求/test/good/1/view，匹配成功后，路由到
后端的请求路径就会变成http://localhost:8888/good/1/view。

（ 2 ）过滤器 Hystrix，作用是通过 Hystrix 进行熔断降级

当上游的请求，进入了 Hystrix 熔断降级机制时，就会调用 fallbackUri 配置的降级地址。需要注意的是，
还需要单独设置 Hystrix 的 commandKey 的超时时间

fallbackUri 配置的降级地址的代码如下：

#### 4 、高级配置

###### 4.1 分布式限流

从某种意义上讲，令牌桶算法是对漏桶算法的一种改进，桶算法能够限制请求调用的速率，而令牌桶算
法能够在限制调用的平均速率的同时还允许一定程度的突发调用。在令牌桶算法中，存在一个桶，用来
存放固定数量的令牌。算法中存在一种机制，以一定的速率往桶中放令牌。每次请求调用需要先获取令
牌，只有拿到令牌，才有机会继续执行，否则选择选择等待可用的令牌、或者直接拒绝。放令牌这个动
作是持续不断的进行，如果桶中令牌数达到上限，就丢弃令牌，所以就存在这种情况，桶中一直有大量
的可用令牌，这时进来的请求就可以直接拿到令牌执行，比如设置 qps 为 100 ，那么限流器初始化完成一
秒后，桶中就已经有 100 个令牌了，这时服务还没完全启动好，等启动完成对外提供服务时，该限流器
可以抵挡瞬时的 100 个请求。所以，只有桶中没有令牌时，请求才会进行等待，最后相当于以一定的速

```
package org. gateway. controller;
```
```
import org. gateway. response. Response;
import org. springframework. web. bind. annotation. GetMapping;
import org. springframework. web. bind. annotation. RestController;
```
```
@RestController
public class FallbackController {
```
```
@GetMapping ("/fallbackA")
public Response fallbackA () {
Response response = new Response ();
response.setCode ("100");
response.setMessage ("服务暂时不可用");
return response;
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```

率执行。

在 Spring Cloud Gateway 中，有 Filter 过滤器，因此可以在“pre”类型的 Filter 中自行实现上述三种过滤
器。但是限流作为网关最基本的功能，Spring Cloud Gateway 官方就提供了
RequestRateLimiterGatewayFilterFactory 这个类，适用在 Redis 内的通过执行 Lua 脚本实现了令牌桶的
方式。具体实现逻辑在 RequestRateLimiterGatewayFilterFactory 类中，lua 脚本在如下图所示的文件夹


中：

首先在工程的 pom 文件中引入 gateway 的起步依赖和 redis 的 reactive 依赖，代码如下：

配置如下：

在上面的配置文件，指定程序的端口为 8081 ，配置了 redis 的信息，并配置了 RequestRateLimiter 的限
流过滤器，该过滤器需要配置三个参数：

```
burstCapacity，令牌桶总容量。
replenishRate，令牌桶每秒填充平均速率。
key-resolver，用于限流的键的解析器的 Bean 对象的名字。它使用 SpEL 表达式根据#
{@beanName}从 Spring 容器中获取 Bean 对象。
```
这里根据用户 ID 限流，请求路径中必须携带 userId 参数

```
server:
port: 8081
spring:
cloud:
gateway:
routes:
```
- id: limit_route
uri: [http://httpbin.org:80/get](http://httpbin.org:80/get)
predicates:
- After=2017-01-20 T17:42:47.789-07:00[America/Denver]
filters:
- name: RequestRateLimiter
args:
key-resolver: '#{@userKeyResolver}'
redis-rate-limiter. replenishRate: 1
redis-rate-limiter. burstCapacity: 3
application:
name: cloud-gateway
redis:
host: localhost
port: 6379
database: 0

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
```

KeyResolver 需要实现 resolve 方法，比如根据 userid 进行限流，则需要用 userid 去判断。实现完
KeyResolver 之后，需要将这个类的 Bean 注册到 Ioc 容器中。

如果需要根据 IP 限流，定义的获取限流 Key 的 bean 为：

通过 exchange 对象可以获取到请求信息，这边用了 HostName，如果你想根据用户来做限流的话这边可
以获取当前请求的用户 ID 或者用户名就可以了，比如：

如果需要根据接口的 URI 进行限流，则需要获取请求地址的 uri 作为限流 key，定义的 Bean 对象为：

通过 exchange 对象可以获取到请求信息，这边用了 HostName，如果你想根据用户来做限流的话这边可
以获取当前请求的用户 ID 或者用户名就可以了，比如：

如果需要根据接口的 URI 进行限流，则需要获取请求地址的 uri 作为限流 key，定义的 Bean 对象为：

###### 4.2 健康检查配置

admin-client、actuator 健康检查配置，为之后的功能提供支持，此部分比较简单，不再赘述，加入以
下 maven 依赖和配置

**maven 依赖**

```
@Bean
KeyResolver userKeyResolver () {
return exchange ->
Mono.just (exchange.getRequest (). getQueryParams (). getFirst ("user"));
}
```
```
1
2
3
```
```
4
```
```
@Bean
public KeyResolver ipKeyResolver () {
return exchange ->
Mono.just (exchange.getRequest (). getRemoteAddress (). getHostName ());
}
```
```
1
2
3
```
```
4
```
```
@Bean
KeyResolver apiKeyResolver () {
return exchange -> Mono.just (exchange.getRequest (). getPath (). value ());
}
```
```
1
2
3
4
```
```
@Bean
KeyResolver apiKeyResolver () {
return exchange -> Mono.just (exchange.getRequest (). getPath (). value ());
}
```
```
1
2
3
4
```

**配置文件**

若转发的目标地址为微服务中组件，不为具体 ip: port 形式的，应写成 lb://mas-openapi-service 形式，
目标地址会从注册中心直接拉取

###### 4.3 统一配置跨域请求：

现在的请求通过经过 gateWay 网关时，需要在网关统一配置跨域请求，需求所有请求通过

```
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
<groupId>de. codecentric</groupId>
<artifactId>spring-boot-admin-starter-client</artifactId>
<version>2.1.0</version>
</dependency>
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
spring:
application:
name: mas-cloud-gateway
boot:
admin:
client:
### 本地搭建的 admin-server
url: http://localhost: 8011
eureka:
client:
registerWithEureka: true
fetchRegistry: true
healthcheck:
enabled: true
serviceUrl:
defaultZone: http://localhost:6887/eureka/
enabled: true
feign:
sentinel:
enabled: true
management:
endpoints:
web:
exposure:
include: '*'
endpoint:
health:
show-details: ALWAYS
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
```
```
spring:
cloud:
gateway:
globalcors:
```
```
1
2
3
4
```

#### 5 、整合 Nacos

###### maven 依赖

```
cors-configurations:
'[/**]':
allowed-origins: "*"
allowed-headers: "*"
allow-credentials: true
allowed-methods:
```
- GET
- POST
- DELETE
- PUT
- OPTION

```
5
6
7
8
9
10
11
12
13
14
15
```
```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
https://maven.apache.org/xsd/maven-4.0.0.xsd">
<modelVersion>4.0.0</modelVersion>
<parent>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-parent</artifactId>
<version>2.1.9. RELEASE</version>
<relativePath/> <!-- lookup parent from repository -->
</parent>
<groupId>com. example</groupId>
<artifactId>nacos_gateway</artifactId>
<version>0.0.1-SNAPSHOT</version>
<packaging>war</packaging>
<name>nacos_gateway</name>
<description>Demo project for Spring Boot</description>
```
```
<properties>
<java.version>1.8</java.version>
<spring-cloud.version>Greenwich. SR 3</spring-cloud.version>
</properties>
```
```
<dependencies>
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-test</artifactId>
<scope>test</scope>
</dependency>
<!--gateway-->
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>
<!--nacos dicovery-->
<dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
```

需要注意在 Gateway 服务中的 pom. xml 文件中不要存在这个 jar

否则调用接口时会报以下错误因为 gateway 使用的是 webflux, 默认使用 netty, 所以从依赖中排除 tomcat
相关的依赖

```
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-alibaba-nacos-
discovery</artifactId>
</dependency>
</dependencies>
```
```
<dependencyManagement>
<dependencies>
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-dependencies</artifactId>
<version>${spring-cloud. version}</version>
<type>pom</type>
<scope>import</scope>
</dependency>
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-alibaba-dependencies</artifactId>
<version>0.2.2. RELEASE</version>
<type>pom</type>
<scope>import</scope>
</dependency>
</dependencies>
</dependencyManagement>
```
```
<build>
<plugins>
<plugin>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-maven-plugin</artifactId>
</plugin>
</plugins>
</build>
```
```
</project>
```
```
36
37
```
```
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
```
```
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-tomcat</artifactId>
<scope>provided</scope>
</dependency>
```
```
1
2
3
4
5
```

错误 2 是由于 spring-boot-starter-web 引起

###### 服务发现配置：从 Nacos 获取微服务提供者清单

```
java. lang. ClassCastException:
org. springframework. core. io. buffer. DefaultDataBufferFactory cannot be cast to
org. springframework. core. io. buffer. NettyDataBufferFactory
at
org. springframework. cloud. gateway. filter. NettyWriteResponseFilter. lambda$filt
er$1 (NettyWriteResponseFilter. java:82) ~[spring-cloud-gateway-core-
2.1.3. RELEASE. jar: 2.1.3. RELEASE]
at reactor.core.publisher.MonoDefer.subscribe (MonoDefer. java:44)
[reactor-core-3.2.12. RELEASE. jar: 3.2.12. RELEASE]
```
```
1
```
```
2
```
```
3
```
```
server:
port: 9999
```
```
spring:
application:
name: springcloud-gateway
profiles:
active: dev
cloud:
nacos:
discovery:
server-addr: ${NACOS_SERVER: cdh 1: 8848 }
config:
server-addr: ${NACOS_SERVER: cdh 1: 8848 }
prefix: springcloud-gateway
group: DEFAULT_GROUP
file-extension: yml
ext-config:
```
- data-id: crazymaker-db-dev. yml
group: DEFAULT_GROUP
refresh: true
- data-id: crazymaker-redis-dev. yml
group: DEFAULT_GROUP
refresh: true
- data-id: crazymaker-common-dev. yml
group: DEFAULT_GROUP
refresh: true
- data-id: some. properties
group: DEFAULT_GROUP
refresh: true
gateway:
enabled: true
discovery:
locator:
enabled: true  #开启从注册中心动态创建路由的功能 ，利用微服务名进行路由
lower-case-service-id: true
filters:
- args[name]: serviceId
name: Hystrix
predicates:
- args[pattern]: '"''/''+serviceId+''/**''"'
name: Path

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
```

```
routes:
```
- id: blog
uri: https://blog.csdn.net/
predicates:
- Path=/csdn
- id: blog 1
uri: https://blog.csdn.net/
predicates:
- Path=/blog 1/**
filters:
- RewritePath=/blog 1/(?<segment>.*), /$\{segment}
# 代理前
[http://192.168.68.1:9999/blog1/crazymakercircle/article/details/80208650](http://192.168.68.1:9999/blog1/crazymakercircle/article/details/80208650)
# 代理后
https://blog.csdn.net/crazymakercircle/article/details/80208650
- id: service_provider_demo_route
uri: lb://service-provider-demo
predicates:
- Path=/provider/**
- id: service_provider_demo_route_filter
uri: lb://service-provider-demo
predicates:
- Path=/filter/**
filters:
- RewritePath=/filter/(?<segment>.*), /provider/$\{segment}
- UserIdCheck
- id: service_consumer_demo_route
uri: lb://service-consumer-demo
predicates:
- Path=/consumer/**
- id: sentinel_demo_provider_route
uri: lb://sentinel-demo-provider
predicates:
- Path=/sentinel-demo/**
- id: uaa-provider_route
uri: lb://uaa-provider
predicates:
- Path=/uaa-provider/**
sentinel:
transport:
dashboard: cdh 1: 8849 #配置Sentinel dashboard 地址
port: 8719 #这里配置的是本地端口
eager: true
inetutils:
timeout-seconds: 10
preferred-networks: ${SCAFFOLD_PREFERRED_NETWORKS: 192.168.68.}
prefer-ip-address: true  #访问路径可以显示IP地址

```
ribbon:
eager-load:
enabled: true # 开启 Ribbon 的饥饿加载模式，启动时创建 RibbonClient
MaxAutoRetries: 1 # 同一台实例的最大重试次数，但是不包括首次调用，默认为 1 次
MaxAutoRetriesNextServer: 2 # 重试负载均衡其他实例的最大重试次数，不包括首次调
用，默认为 0 次
OkToRetryOnAllOperations: true  # 是否对所有操作都重试，默认 false
ServerListRefreshInterval: 2000 # 从注册中心刷新服务器列表信息的时间间隔，默认为
2000 毫秒，即 2 秒
```
43
44
45
46
47
48
49
50
51
52
53
54

55

56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94

95
96


```
retryableStatusCodes: 400,401,403,404,500,502,504
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. RetryRule #配置规则
重试
ConnectTimeout: 3000 #连接建立的超时时长 ，默认 1 秒
ReadTimeout: 3000 #处理请求的超时时间 ，默认为 1 秒
MaxTotalConnections: 1000 # 最大连接数
MaxConnectionsPerHost: 1000 # 每个 host 最大连接数
restclient:
enabled: true
```
```
hystrix:
threadpool:
default:
coreSize: 10 # 线程池核心线程数
maximumSize: 20 # 线程池最大线程数
allowMaximumSizeToDivergeFromCoreSize: true # 线程池最大线程数是否有效
keepAliveTimeMinutes: 10 # 设置可空闲时间，单位分钟
demo-provider:
coreSize: 20 # 线程池核心线程数
maximumSize: 100 # 线程池最大线程数
allowMaximumSizeToDivergeFromCoreSize: true # 线程池最大线程数是否有效
keepAliveTimeMinutes: 20 # 设置可空闲时间，单位分钟
propagate:
request-attribute:
enabled: true
command:
default:  #全局默认配置
execution:  #线程隔离相关配置
timeout:
enabled: true #是否给方法执行设置超时时间 ，默认为 true。一般我们不要改。
isolation:
strategy: THREAD  #配置请求隔离的方式 ，这里是默认的线程池方式。还有一种
信号量的方式 semaphore，使用比较少。
thread:
timeoutInMilliseconds: 100000 #方式执行的超时时间 ，默认为 1000 毫秒，
在实际场景中需要根据情况设置
interruptOnTimeout: true #发生超时时是否中断方法的执行 ，默认值为
true。不要改。
interruptOnCancel: false  #是否在方法执行被取消时中断方法 ，默认值为
false。没有实际意义，默认就好！
circuitBreaker: #熔断器相关配置
enabled: true #是否启动熔断器 ，默认为 true，false 表示不要引入 Hystrix。
requestVolumeThreshold: 20 #启用熔断器功能窗口时间内的最小请求数 ，假设
我们设置的窗口时间为 10 秒，
sleepWindowInMilliseconds: 5000 #此配置的作用是指定熔断器打开后多长时
间内允许一次请求尝试执行，官方默认配置为 5 秒。
errorThresholdPercentage: 50 #窗口时间内超过50 %的请求失败后就会打开熔断
器将后续请求快速失败掉, 默认配置为 50
metrics:
rollingStats:
timeInMilliseconds: 10000
numBuckets: 10
```
```
# 暴露监控端点
management:
endpoints:
web:
```
```
97
98
```
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128

129
130

131

132

133
134
135

136

137

138
139
140
141
142
143
144
145
146


###### nacos 实现动态配置

使用 nacos 实现动态路由，以上两种方式都是实现的静态配置路径，只能应对部分场景，接下来配置
nacos 实现动态配置以及配置的存储，由于 gateWay 并没有适配 nacos，需要自定义监听器：

```
exposure:
include: '*'
```
```
147
148
```
```
@Component
@Slf 4 j
public class NacosDynamicRouteService implements
ApplicationEventPublisherAware {
private String dataId = "gateway-router";
private String group = "DEFAULT_GROUP";
@Value ("${spring. cloud. nacos. config. server-addr}")
private String serverAddr;
@Autowired
private RouteDefinitionWriter routeDefinitionWriter;
private ApplicationEventPublisher applicationEventPublisher;
private static final List<String> ROUTE_LIST = new ArrayList<>();
@PostConstruct
public void dynamicRouteByNacosListener () {
try {
ConfigService configService =
NacosFactory.createConfigService (serverAddr);
configService.getConfig (dataId, group, 5000 );
configService.addListener (dataId, group, new Listener () {
@Override
public void receiveConfigInfo (String configInfo) {
clearRoute ();
try {
if (StringUtil.isNullOrEmpty (configInfo)) {//配置被删
除
return;
}
List<RouteDefinition> gatewayRouteDefinitions =
JSONObject.parseArray (configInfo, RouteDefinition. class);
for (RouteDefinition routeDefinition :
gatewayRouteDefinitions) {
addRoute (routeDefinition);
}
publish ();
} catch (Exception e) {
log.error ("receiveConfigInfo error" + e);
}
}
@Override
public Executor getExecutor () {
return null;
}
});
} catch (NacosException e) {
log.error ("dynamicRouteByNacosListener error" + e);
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```
```
16
17
18
19
20
21
22
```
```
23
24
25
```
```
26
```
```
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
```

在 nacos 中增加一个规则：

访问网关的路由规则，能看到刚刚加入的规则，访问 _ [http://localhost:9022/baidu](http://localhost:9022/baidu) _ 时请求直接被转发到百
度的首页了。

```
private void clearRoute () {
for (String id : ROUTE_LIST) {
this.routeDefinitionWriter.delete (Mono.just (id)). subscribe ();
}
ROUTE_LIST.clear ();
}
private void addRoute (RouteDefinition definition) {
try {
routeDefinitionWriter.save (Mono.just (definition)). subscribe ();
ROUTE_LIST.add (definition.getId ());
} catch (Exception e) {
log.error ("addRoute error" + e);
}
}
private void publish () {
this.applicationEventPublisher.publishEvent (new
RefreshRoutesEvent (this. routeDefinitionWriter));
}
@Override
public void setApplicationEventPublisher (ApplicationEventPublisher
applicationEventPublisher) {
this. applicationEventPublisher = applicationEventPublisher;
}
```
```
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
```
```
59
60
61
```
```
62
63
```
```
[{
"filters": [],
"id": "baidu_route",
"order": 0 ,
"predicates": [{
"args": {
"pattern": "/baidu"
},
"name": "Path"
}],
"uri": "https://www.baidu.com"
}]
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```

###### 服务发现路由 predicates 和 filters 的自定义定义

可以将网关配置为基于使用 DiscoveryClient 注册中心注册的服务发现路由。

要启用此功能，请设置 spring. cloud. gateway. discovery. locator. enabled=true，并确保
DiscoveryClient 实现位于 classpath 上并已启用（如 netflix eureka、consul 或 zookeeper）。

###### 为注册中心路由配置断言和过滤器

默认情况下，网关为通过 DiscoveryClient 创建的路由定义单个断言和过滤器。

**注意**

这个默认的过滤器，老版本没有，这就要命了

**尼恩这几天做推送中台架构实操的时候，升级了一下 springcloud gateway，这就要要了命了**

请求全部是 404

而且由于响应式编程不是太好调试，不过，尼恩不吃这套，喜欢深入敌后，进入源码后，大概找到了
**断言处理迭代** 的地方，看到了 10 多个过滤器

其中一个过滤器 rewrite，干了一件匪夷所思的事情，把后端服务的前缀给剔除了，如果前缀是
serviceId 的话

这就是咱们的问题所在：赶巧的是，咱后端微服务，需要路径前缀，并且，路径的前缀就是 serviceId

这下子，这个 springcloud gateway 的升级骚操作，把路劲前缀搞没了，当然路由不过去了， **害的我白
瞎了 2 小时**

```
默认情况下，网关为通过 DiscoveryClient 创建的路由定义单个断言和过滤器。
```
```
默认断言是使用/serviceId/**定义的 path 断言，其中 serviceId 是 DiscoveryClient 中服务的
ID。
默认过滤器是使用正则表达式 /serviceId/(?.*) 和替换的/${remaining}进行重写。这只是在请求
被发送到下游之前从路径中截取掉 service id 。
```
```
1
2
3
```
```
4
```

翻阅官方的文档，按照官方说明：

可以通过设置 spring. cloud. gateway. discovery. locator. predicates[x] and
spring. cloud. gateway. discovery. locator. filters[y] 来，去自定义 DiscoveryClient 路由使用的断言和过
滤器。

当做了定制以后，默认的就没了，如果你想要保留默认功能，需要手动加上默认断言和过滤器。

下面是这样一个例子。

**Example 69. application. properties**

于是，需要去掉这个 RewritePath 过滤器，下面为 nacos 定制客户端的过滤器

```
spring. cloud. gateway. discovery. locator. predicates[0]. name: Path
spring. cloud. gateway. discovery. locator. predicates[0]. args[pattern]:
"'/'+serviceId+'/**'"
spring. cloud. gateway. discovery. locator. predicates[1]. name: Host
spring. cloud. gateway. discovery. locator. predicates[1]. args[pattern]:
"'**. foo. com'"
spring. cloud. gateway. discovery. locator. filters[0]. name: Hystrix
spring. cloud. gateway. discovery. locator. filters[0]. args[name]: serviceId
spring. cloud. gateway. discovery. locator. filters[1]. name: RewritePath
spring. cloud. gateway. discovery. locator. filters[1]. args[regexp]: "'/' +
serviceId + '/(?<remaining>.*)'"
spring. cloud. gateway. discovery. locator. filters[1]. args[replacement]:
"'/${remaining}'"
```
```
1 2 3 4 5 6 7 8 9
```

本质上，要不要去掉 url 的前缀，配置文件的下边，完全有规则可以自己配置，

当然，官方希望为微服务 DiscoveryClient 路由增加一些过滤器，也是可以理解的

不管怎么说，咱们这个场景下，只能覆盖官方的默认的 discovery. filters 的配置啦，于是进行了上边的
修改

修改之后，重启，

再通过断点看过滤器，那个 rewrite 过滤器，没有了

终于不是 404 的错误了

```
看起来，springcloud 官方勤快的升级版本，也没有干什么有意义的事情，做的很多都是花瓶的
活儿
```
#### 6 、整合 Swagger 聚合微服务系统 API 文档

有关源码具体请参见疯狂创客圈的 crazy-springcloud 脚手架

```
gateway:
enabled: true
discovery:
locator:
enabled: true  #开启从注册中心动态创建路由的功能 ，利用微服务名进行路由
lower-case-service-id: true
filters:
```
- args[name]: serviceId
name: Hystrix
predicates:
- args[pattern]: '"''/''+serviceId+''/**''"'
name: Path

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```

###### maven 依赖

```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
http://maven.apache.org/xsd/maven-4.0.0.xsd">
<parent>
<artifactId>cloud-center-alibaba</artifactId>
<groupId>com. crazymaker. springcloud</groupId>
<version>1.0-SNAPSHOT</version>
</parent>
<modelVersion>4.0.0</modelVersion>
```
```
<groupId>com. crazymaker. springcloud</groupId>
<artifactId>springcloud-gateway-demo</artifactId>
<version>1.0-SNAPSHOT</version>
<name>springcloud-gateway-demo</name>
<packaging>jar</packaging>
```
```
<dependencies>
<!--gateway 网关依赖,内置webflux 依赖 -->
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>
<!--新增sentinel-->
<dependency>
<groupId>com. alibaba. csp</groupId>
<artifactId>sentinel-spring-cloud-gateway-adapter</artifactId>
</dependency>
<dependency>
<groupId>com. alibaba. csp</groupId>
<artifactId>sentinel-transport-simple-http</artifactId>
</dependency>
```
```
<!-- nacos服务注册发现依赖-->
<dependency>
<groupId>com. alibaba. cloud</groupId>
<artifactId>spring-cloud-starter-alibaba-nacos-
discovery</artifactId>
<exclusions>
<exclusion>
<groupId>com. google. guava</groupId>
<artifactId>guava</artifactId>
</exclusion>
</exclusions>
</dependency>
<dependency>
<groupId>mysql</groupId>
<artifactId>mysql-connector-java</artifactId>
<version>${mysql. connector. version}</version>
</dependency>
<!-- nacos配置服务依赖-->
<dependency>
<groupId>com. alibaba. cloud</groupId>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
```
```
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
```

```
<artifactId>spring-cloud-starter-alibaba-nacos-
config</artifactId>
<exclusions>
<exclusion>
<groupId>com. google. guava</groupId>
<artifactId>guava</artifactId>
</exclusion>
</exclusions>
</dependency>
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-netflix-hystrix</artifactId>
</dependency>
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```
```
<dependency>
<groupId>org. projectlombok</groupId>
<artifactId>lombok</artifactId>
<optional>true</optional>
</dependency>
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-test</artifactId>
<scope>test</scope>
</dependency>
```
```
<dependency>
<groupId>org. projectlombok</groupId>
<artifactId>lombok</artifactId>
<optional>true</optional>
</dependency>
```
```
<dependency>
<groupId>cn. hutool</groupId>
<artifactId>hutool-all</artifactId>
<version>${hutool. version}</version>
</dependency>
<dependency>
<groupId>io. springfox</groupId>
<artifactId>springfox-swagger 2</artifactId>
<version>${swagger. version}</version>
</dependency>
<dependency>
<groupId>io. springfox</groupId>
<artifactId>springfox-swagger-common</artifactId>
<version>${swagger. version}</version>
</dependency>
<dependency>
<groupId>com. github. xiaoymin</groupId>
<artifactId>swagger-bootstrap-ui</artifactId>
<version>${swagger-ui. version}</version>
</dependency>
<dependency>
```
```
54
```
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110


```
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-commons</artifactId>
</dependency>
</dependencies>
<build>
<plugins>
```
```
<plugin>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-maven-plugin</artifactId>
<configuration>
```
```
<mainClass>com. crazymaker. cloud. nacos. demo. gateway. starter. GatewayProvider
Application</mainClass>
</configuration>
<executions>
<execution>
<goals>
<goal>repackage</goal>
</goals>
</execution>
</executions>
</plugin>
```
```
<plugin>
<artifactId>maven-assembly-plugin</artifactId>
<version>2.4.1</version>
<configuration>
<descriptors>
```
```
<descriptor>src/main/assembly/assembly. xml</descriptor>
</descriptors>
</configuration>
<executions>
<execution>
<id>make-assembly</id>
<phase>package</phase>
<goals>
<goal>single</goal>
</goals>
</execution>
</executions>
</plugin>
```
```
<!-- 添加docker-maven插件 -->
```
```
<plugin>
<groupId>com. spotify</groupId>
<artifactId>docker-maven-plugin</artifactId>
<version>1.1.1</version>
<configuration>
```
```
<imageName>dockerlocal: 5000/${project. artifactId}:${project. version}
</imageName>
<baseImage>dockerlocal: 5000/java</baseImage>
<entryPoint>["java", "-jar",
"/${project. build. finalName}. jar"]</entryPoint>
```
111
112
113
114
115
116
117
118
119
120
121
122

123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139

140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160

161
162


###### 配置文件

```
<dockerDirectory>docker</dockerDirectory>
<resources>
<resource>
<targetPath>/</targetPath>
<directory>${project. build. directory}
</directory>
```
```
<include>${project. build. finalName}. jar</include>
</resource>
</resources>
</configuration>
</plugin>
```
```
</plugins>
</build>
</project>
```
```
163
164
165
166
167
```
```
168
```
```
169
170
171
172
173
174
175
176
```
```
package com. crazymaker. cloud. nacos. demo. gateway. config;
```
```
import lombok. AllArgsConstructor;
import org. springframework. cloud. gateway. config. GatewayProperties;
import org. springframework. cloud. gateway. route. RouteLocator;
import org. springframework. cloud. gateway. support. NameUtils;
import org. springframework. context. annotation. Primary;
import org. springframework. stereotype. Component;
import springfox. documentation. swagger. web. SwaggerResource;
import springfox. documentation. swagger. web. SwaggerResourcesProvider;
```
```
import java. util. ArrayList;
import java. util. List;
```
```
/**
* @ClassName SwaggerProvider
* @PackageName com. ruoyi. gateway. config
* @Description
* @Author daiz
* @Date 2019/8/16 10:04
* @Version 1.0
*/
@Component
@Primary
@AllArgsConstructor
public class SwaggerConfig implements SwaggerResourcesProvider
{
public static final String API_URI = "/v 2/api-docs";
```
```
private final RouteLocator routeLocator;
```
```
private final GatewayProperties gatewayProperties;
```
```
@Override
public List<SwaggerResource> get ()
{
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
```

###### 效果：

```
/**
* 网关应用名称, 不需要在网关的 swagger 上展示
*/
String appName = "springcloud-gateway";
```
```
List<SwaggerResource> resources = new ArrayList<>();
List<String> routes = new ArrayList<>();
// 取出 gateway 的 route
routeLocator.getRoutes (). subscribe (route ->
routes.add (route.getId ()));
// 结合配置的 route-路径 (Path)，和 route 过滤，只获取有效的 route 节点
// 打开下面注释可以自动扫描接入 gateway 的服务，为了演示，只扫描 system
// gatewayProperties.getRoutes (). stream (). filter (routeDefinition ->
// routes.contains (routeDefinition.getId ()))
gatewayProperties.getRoutes (). stream ()
.filter (route -> route.getUri (). getHost () != null)
.filter (route -> !appName.equals (route.getUri (). getHost ()))
.forEach (routeDefinition ->
routeDefinition.getPredicates (). stream ()
.filter (predicateDefinition ->
("Path"). equalsIgnoreCase (predicateDefinition.getName ()))
.forEach (predicateDefinition -> resources
```
```
.add (swaggerResource (routeDefinition.getId (), predicateDefinition.getArgs ()
.get (NameUtils. GENERATED_NAME_PREFIX
+ "0"). replace ("/**", API_URI)))));
return resources;
}
```
```
private SwaggerResource swaggerResource (String name, String location)
{
SwaggerResource swaggerResource = new SwaggerResource ();
swaggerResource.setName (name);
swaggerResource.setLocation (location);
swaggerResource.setSwaggerVersion ("2.0");
return swaggerResource;
}
}
```
```
37
38
39
40
41
42
43
44
45
```
```
46
47
48
49
50
51
52
53
```
```
54
```
```
55
56
```
```
57
```
```
58
59
60
61
62
63
64
65
66
67
68
69
```

#### 7 、Gatway 网关的过滤器开发

###### 7.1 过滤器的执行次序

Spring-Cloud-Gateway 基于过滤器实现，同 zuul 类似，有 **pre** 和 **post** 两种方式的 filter, 分别处理 **前置
逻辑** 和 **后置逻辑** 。客户端的请求先经过 **pre** 类型的 filter，然后将请求转发到具体的业务服务，收到业务
服务的响应之后，再经过 **post** 类型的 filter 处理，最后返回响应到客户端。

过滤器执行流程如下， **order 越大，优先级越低**

order 越大，优先级越低


分为全局过滤器和局部过滤器

```
全局过滤器：
```
```
对所有路由生效
```
```
2 、接口用时统计
```
```
局部过滤器：
```
```
对指定路由生效
```
###### 7.2 定义全局过滤器

实现 GlobalFilter 和 Ordered，重写相关方法，加入到 spring 容器管理即可，无需配置，全局过滤器对
所有的路由都有效。

全局过滤器举例：代码如下：

```
package com. crazymaker. cloud. nacos. demo. gateway. config;
```
```
import lombok. extern. slf 4 j. Slf 4 j;
import org. springframework. cloud. gateway. filter. GatewayFilterChain;
import org. springframework. cloud. gateway. filter. GlobalFilter;
import org. springframework. context. annotation. Bean;
import org. springframework. context. annotation. Configuration;
import org. springframework. core. Ordered;
import org. springframework. core. annotation. Order;
import org. springframework. web. server. ServerWebExchange;
import reactor. core. publisher. Mono;
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```

```
@Configuration
public class FilterConfig
{
```
```
@Bean
@Order (- 1 )
public GlobalFilter a ()
{
return new AFilter ();
}
```
```
@Bean
@Order ( 0 )
public GlobalFilter b ()
{
return new BFilter ();
}
```
```
@Bean
@Order ( 1 )
public GlobalFilter c ()
{
return new CFilter ();
}
```
```
@Slf 4 j
public class AFilter implements GlobalFilter, Ordered
{
```
```
@Override
public Mono<Void> filter (ServerWebExchange exchange,
GatewayFilterChain chain)
{
log.info ("AFilter 前置逻辑");
return chain.filter (exchange). then (Mono.fromRunnable (() ->
{
log.info ("AFilter 后置逻辑");
}));
}
```
```
// 值越小，优先级越高
// int HIGHEST_PRECEDENCE = -2147483648;
// int LOWEST_PRECEDENCE = 2147483647;
@Override
public int getOrder ()
{
return HIGHEST_PRECEDENCE + 100 ;
}
}
```
```
@Slf 4 j
public class BFilter implements GlobalFilter, Ordered
{
@Override
public Mono<Void> filter (ServerWebExchange exchange,
GatewayFilterChain chain)
```
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44

45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67


###### 7.3 定义局部过滤器

步骤：

```
1. 需要实现 GatewayFilter, Ordered，实现相关的方法
2. 加入到过滤器工厂，并且注册到 spring 容器中。
3. 在配置文件中进行配置，如果不配置则不启用此过滤器规则。
```
局部过滤器举例, 对请求头部的 user-id 进行校验，代码如下：

```
{
log.info ("BFilter 前置逻辑");
return chain.filter (exchange). then (Mono.fromRunnable (() ->
{
log.info ("BFilter 后置逻辑");
}));
}
```
```
// 值越小，优先级越高
// int HIGHEST_PRECEDENCE = -2147483648;
// int LOWEST_PRECEDENCE = 2147483647;
@Override
public int getOrder ()
{
return HIGHEST_PRECEDENCE + 200 ;
}
}
```
```
@Slf 4 j
public class CFilter implements GlobalFilter, Ordered
{
```
```
@Override
public Mono<Void> filter (ServerWebExchange exchange,
GatewayFilterChain chain)
{
log.info ("CFilter 前置逻辑");
return chain.filter (exchange). then (Mono.fromRunnable (() ->
{
log.info ("CFilter 后置逻辑");
}));
}
```
```
// 值越小，优先级越高
// int HIGHEST_PRECEDENCE = -2147483648;
// int LOWEST_PRECEDENCE = 2147483647;
@Override
public int getOrder ()
{
return HIGHEST_PRECEDENCE + 300 ;
}
}
}
```
```
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
```
```
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
```

1 、需要实现 GatewayFilter, Ordered，实现相关的方法

```
package com. crazymaker. cloud. nacos. demo. gateway. filter;
```
```
import lombok. extern. slf 4 j. Slf 4 j;
import org. apache. commons. lang 3. StringUtils;
import org. springframework. cloud. gateway. filter. GatewayFilter;
import org. springframework. cloud. gateway. filter. GatewayFilterChain;
import org. springframework. cloud. gateway. filter. GlobalFilter;
import org. springframework. core. Ordered;
import org. springframework. http. HttpStatus;
import org. springframework. stereotype. Component;
import org. springframework. web. server. ServerWebExchange;
import reactor. core. publisher. Mono;
```
```
//@Component
@Slf 4 j
public class UserIdCheckGateWayFilter implements GatewayFilter, Ordered
{
@Override
public Mono<Void> filter (ServerWebExchange exchange, GatewayFilterChain
chain)
{
String url =
exchange.getRequest (). getPath (). pathWithinApplication (). value ();
log.info ("请求 URL: " + url);
log.info ("method: " + exchange.getRequest (). getMethod ());
/* String secret =
exchange.getRequest (). getHeaders (). getFirst ("secret");
if (StringUtils.isBlank (secret))
{
return chain.filter (exchange);
}*/
//获取 param 请求参数
String uname =
exchange.getRequest (). getQueryParams (). getFirst ("uname");
//获取 header
String userId = exchange.getRequest (). getHeaders (). getFirst ("user-
id");
log.info ("userId：" + userId);
```
```
if (StringUtils.isBlank (userId))
{
log.info ("*****头部验证不通过，请在头部输入 user-id");
//终止请求，直接回应
exchange.getResponse (). setStatusCode (HttpStatus. NOT_ACCEPTABLE);
return exchange.getResponse (). setComplete ();
}
return chain.filter (exchange);
}
```
```
// 值越小，优先级越高
// int HIGHEST_PRECEDENCE = -2147483648;
// int LOWEST_PRECEDENCE = 2147483647;
@Override
public int getOrder ()
{
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
```
```
20
21
```
```
22
23
24
```
```
25
26
27
28
29
30
```
```
31
32
```
```
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
```

2 、加入到过滤器工厂，并且注册到 spring 容器中。

3 、在配置文件中进行配置，如果不配置则不启用此过滤器规则。

#### 8 、整合 Sentinel 完成流控和降级

```
return HIGHEST_PRECEDENCE;
}
}
```
```
51
52
53
```
```
package com. crazymaker. cloud. nacos. demo. gateway. config;
```
```
import
com. crazymaker. cloud. nacos. demo. gateway. filter. UserIdCheckGateWayFilter;
import org. springframework. cloud. gateway. filter. GatewayFilter;
import
org. springframework. cloud. gateway. filter. factory. AbstractGatewayFilterFactor
y;
import org. springframework. stereotype. Component;
```
```
@Component
public class UserIdCheckGatewayFilterFactory extends
AbstractGatewayFilterFactory<Object>
{
@Override
public GatewayFilter apply (Object config)
{
return new UserIdCheckGateWayFilter ();
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
12
13
14
15
16
17
```
- id: service_provider_demo_route_filter
uri: lb://service-provider-demo
predicates:
- Path=/filter/**
filters:
- RewritePath=/filter/(?<segment>.*), /provider/$\{segment}
- UserIdCheck

```
1 2 3 4 5 6 7
```

###### maven 依赖

使用 Sentinel 作为 gateWay 的限流、降级、系统保护工具

###### 配置文件

客户端配置：在配置文件中增加下列配置，dashboard 就可以轻松管理客户端了，还有一种方式是在启
动时加入

###### 限流规则通用配置

由于 sentinel 的工作原理其实借助于全局的 filter 进行请求拦截并计算出是否进行限流、熔断等操作的，
增加 SentinelGateWayFilter 配置

sentinel 不仅支持通过硬代码方式进行资源的申明，还能通过注解方式进行声明，为了让注解生效，还
需要配置切面类 SentinelResourceAspect

```
<!--alibaba 流量卫士-->
<dependency>
<groupId>com. alibaba. csp</groupId>
<artifactId>sentinel-core</artifactId>
<version>${sentinel. version}</version>
</dependency>
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
</dependency>
<dependency>
<groupId>com. alibaba. csp</groupId>
<artifactId>sentinel-spring-cloud-gateway-adapter</artifactId>
<version>1.7.1</version>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```
```
spring:
cloud:
sentinel:
transport:
## VM
##-Djava. net. preferIPv 4 Stack=true -
Dcsp. sentinel. dashboard. server=localhost: 8080 -Dcsp. sentinel. api. port=8666 -
Dproject. name=gateway -Dcsp. sentinel. app. type=1
dashboard: localhost: 8880
port: 8880
```
```
1 2 3 4 5 6 7 8
```
```
@Bean//拦截请求
@Order (Ordered. HIGHEST_PRECEDENCE)
public GlobalFilter sentinelGatewayFilter () {
return new SentinelGatewayFilter ();
}
```
```
1
2
3
4
5
```

sentinel 拦截包括了视图、静态资源等，需要配置 viewResolvers 以及拦截之后的异常，我们也可以自定
义抛出异常的提示

自定义异常提示：当发生限流、熔断异常时，会返回定义的提示信息。

不需要额外的配置，sentinel 就已经可以正常工作了

###### 限流规则设置

1 资源定义：定义 API 组

2 定义限流规则

```
具体请参见学习视频
```
```
@Bean
public SentinelResourceAspect sentinelResourceAspect () {
return new SentinelResourceAspect ();
}
```
```
1
2
3
4
```
```
public SentinelConfig (ObjectProvider<List<ViewResolver>>
viewResolversProvider,
ServerCodecConfigurer serverCodecConfigurer) {
this. viewResolvers =
viewResolversProvider.getIfAvailable (Collections::emptyList);
this. serverCodecConfigurer = serverCodecConfigurer;
}
```
```
@Bean//自定义异常
@Order (Ordered. HIGHEST_PRECEDENCE)
public ExceptionHandler sentinelGatewayBlockExceptionHandler () {
// Register the block exception handler for Spring Cloud Gateway.
return new ExceptionHandler (viewResolvers, serverCodecConfigurer);
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
/**
* 配置限流的异常处理器:SentinelGatewayBlockExceptionHandler
*/
@Bean
@Order (Ordered. HIGHEST_PRECEDENCE)
public SentinelGatewayBlockExceptionHandler
sentinelGatewayBlockExceptionHandler () {
return new SentinelGatewayBlockExceptionHandlerEX (viewResolvers,
serverCodecConfigurer);
}
```
```
1 2 3 4 5 6 7 8
```

###### 网关限流参数

其中网关限流规则 GatewayFlowRule 的字段解释如下：

```
resource：资源名称，可以是网关中的 route 名称或者用户自定义的 API 分组名称。
resourceMode：规则是针对 API Gateway 的 route（RESOURCE_MODE_ROUTE_ID）还是用户
在 Sentinel 中定义的 API 分组（RESOURCE_MODE_CUSTOM_API_NAME），默认是 route。
grade：限流指标维度，同限流规则的 grade 字段。
count：限流阈值
intervalSec：统计时间窗口，单位是秒，默认是 1 秒。
controlBehavior：流量整形的控制效果，同限流规则的 controlBehavior 字段，目前支持快速失
败和匀速排队两种模式，默认是快速失败。
burst：应对突发请求时额外允许的请求数目。
maxQueueingTimeoutMs：匀速排队模式下的最长排队时间，单位是毫秒，仅在匀速排队模式下
生效。
paramItem
参数限流配置。若不提供，则代表不针对参数进行限流，该网关规则将会被转换成普通流控规则；
否则会转换成热点规则。其中的字段：
parseStrategy：从请求中提取参数的策略，目前支持提取来源 IP
（PARAM_PARSE_STRATEGY_CLIENT_IP）、Host（PARAM_PARSE_STRATEGY_HOST）、
任意 Header（PARAM_PARSE_STRATEGY_HEADER）和任意 URL 参数
（PARAM_PARSE_STRATEGY_URL_PARAM）四种模式。
fieldName：若提取策略选择 Header 模式或 URL 参数模式，则需要指定对应的 header 名称
或 URL 参数名称。
pattern：参数值的匹配模式，只有匹配该模式的请求属性值会纳入统计和流控；若为空则统
计该请求属性的所有值。（1.6.2 版本开始支持）
matchStrategy：参数值的匹配策略，目前支持精确匹配
（PARAM_MATCH_STRATEGY_EXACT）、子串匹配
（PARAM_MATCH_STRATEGY_CONTAINS）和正则匹配
（PARAM_MATCH_STRATEGY_REGEX）。（1.6.2 版本开始支持）
```
用户可以通过 GatewayRuleManager.loadRules (rules) 手动加载网关规则，或通过
GatewayRuleManager. register 2 Property (property) 注册动态规则源动态推送（推荐方式）。

## SpringBoot Admin 进行微服务实例的监控

#### 使用 SpringBoot Admin 进行日志的记录

SpringBootAdmin 用来管理和监控 SpringBoot、SpringCloud 应用程序，它利用 spring-boot-starter-
actuator 提供的功能，将各个微服务的状态整合到一起，并提供良好的界面查看支持，并且能够动态的
修改实例日志级别。SpringBootAdmin 分为 server 端和 client 端，server 端可查看各个微服务的状态，
client 端将微服务注册到 server 端。

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从下面的链接获取：语雀
或者码云
```
#### 1 、SpringBoot Admin 简介


使用 SpringBoot Admin 进行日志的记录，可以很轻松的是实现对 SpringBoot、SpringCloud 项目运行状
态的监控。Spring Boot Admin 本身也是一个 Web 应用，每个 Spring Boot 应用程序都被视为客户端并注
册到管理服务器。

Spring Boot Admin 是一个开源社区项目，用于管理和监控 SpringBoot 应用程序。应用程序作为 Spring
Boot Admin Client 向为 Spring Boot Admin Server 注册（通过 HTTP）或使用 SpringCloud 注册中心（例
如 Eureka，Nacos）发现。 UI 是的 Vue. js 应用程序，展示 Spring Boot Admin Client 的 Actuator 端点上
的一些监控。常见的功能如下：

```
显示健康状况
显示详细信息，例如
JVM 和内存指标
micrometer. io 指标
数据源指标
缓存指标
显示内部信息
关注并下载日志文件
查看 JVM 系统和环境属性
查看 Spring Boot 配置属性
支持 Spring Cloud 的可发布/ env-和// refresh-endpoint
轻松的日志级别管理
与 JMX-beans 交互
查看线程转储
查看 http-traces
查看审核事件
查看 http 端点
查看预定的任务
查看和删除活动会话（使用 spring-session）
查看 Flyway / Liquibase 数据库迁移
下载 heapdump
状态更改通知（通过电子邮件，Slack，Hipchat 等）
状态更改的事件日志（非持久性）
```
Git 地址：https://github.com/codecentric/spring-boot-admin

文档地址：https://codecentric.github.io/spring-boot-admin/2.1.6/#getting-started

```
Spring Boot Admin 背后的数据采集是由 Spring Boot Actuator 端点提供。actuator 相关映射, 启动
时会自动加载
```
#### 2 、使用 SpringBoot Admin 监控服务

Admin-Server 并注册到 Nacos：创建一个 SpringBoot 项目，命名为 provider-monitor，引入如下
SpringCloud、Nacos、SpringBoot Admin 的依赖：


###### 2.1 导入依赖

版本需要配套，否则会抛出异常：

###### 2.2 配置 yml

```
<!-->spring-boot-admin-starter-server-->
<dependency>
<groupId>de. codecentric</groupId>
<artifactId>spring-boot-admin-starter-server</artifactId>
<version>${spring-boot-admin. version}</version>
</dependency>
```
```
<!--健康检查-->
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
</dependency>
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-security</artifactId>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
```
```
<spring-boot.version>2.0.8. RELEASE</spring-boot.version>
<spring-boot-admin.version>2.0.0</spring-boot-admin.version>
```
```
1
2
```
```
#### 暴露端点
management:
endpoints:
web:
base-path: "/actuator" # 配置 Endpoint 的基础路径
exposure:
include: '*' #在yaml 文件属于关键字，所以需要加引号
endpoint:
logfile:
# spring boot admin client 不配置日志文件路径（同时配置 logback-spring. xml 对
应的日志输出配置，否则无法输出日志），
# 控制台上的 Logging 模块下的 Logfile 会报错：Fetching logfile failed. Request
failed with status code 404
external-file: E:/logs/service-provider-demo/logs/output. log
enabled: true
health:
show-details: always
# 未配置/注释以下内容
# boot:
# admin:
# context-path: consumer
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
```
```
12
13
14
15
16
17
18
19
```

###### 2.3 集成 spring security

官方说明：

```
由于有多种方法可以解决分布式 Web 应用程序中的身份验证和授权，因此 Spring Boot Admin 不会
提供默认方法。默认情况下 spring-boot-admin-server-ui 提供登录页面和注销按钮。
```
我们这里采用 spring security 提供安全保障， **在 Spring Boot Admin Server 中统一配置**

Web 应用程序中的身份验证和授权有多种方法，因此 Spring Boot Admin 不提供默认方法。默认情况
下，spring-boot-admin-server-ui 提供登录页面和注销按钮。我们结合 Spring Security 实现需要用户名
和密码登录的安全认证。

sc-admin-server 工程的 pom 文件需要增加以下的依赖：

在 sc-admin-server 工的配置文件 application. yml 中配置 spring security 的用户名和密码，这时需要
在服务注册时带上 metadata-map 的信息，如下：

**@EnableWebSecurity** 注解以及 **WebSecurityConfigurerAdapter** 一起配合提供基于 web 的
security。继承了 WebSecurityConfigurerAdapter 之后，再加上几行代码，我们就能实现要求用户在进
入应用的任何 URL 之前都进行验证的功能，写一个配置类 SecuritySecureConfig 继承
WebSecurityConfigurerAdapter，配置如下：

```
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-security</artifactId>
</dependency>
```
```
1
2
3
4
```
```
spring:
security:
user:
name: "admin"
password: "admin"
```
```
eureka:
instance:
metadata-map:
user. name: ${spring. security. user. name}
user. password: ${spring. security. user. password}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
@Configuration
public class SecuritySecureConfig extends WebSecurityConfigurerAdapter {
```
```
private final String adminContextPath;
```
```
public SecuritySecureConfig (AdminServerProperties adminServerProperties)
{
this. adminContextPath = adminServerProperties.getContextPath ();
}
```
```
@Override
protected void configure (HttpSecurity http) throws Exception {
// @formatter:off
SavedRequestAwareAuthenticationSuccessHandler successHandler = new
SavedRequestAwareAuthenticationSuccessHandler ();
successHandler.setTargetUrlParameter ("redirectTo");
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
14
```

###### 2.4 启动器类

添加 _@EnableAdminServer_ 注解

**admin 会自己拉取 Eureka 上注册的 app 信息，主动去注册。这也是唯一区别之前入门中手动注册的
地方，就是 client 端不需要 admin-client 的依赖，也不需要配置 admin 地址了，一切全部由
admin-server 自己实现。这样的设计对环境变化很友好，不用改了 admin-server 后去改所有 app 的
配置了。**

###### 2.5、测试

```
successHandler.setDefaultTargetUrl (adminContextPath + "/");
```
```
http.authorizeRequests ()
//授予对所有静态资产和登录页面的公共访问权限。
.antMatchers (adminContextPath + "/assets/**"). permitAll ()
.antMatchers (adminContextPath + "/login"). permitAll ()
//必须对每个其他请求进行身份验证
.anyRequest (). authenticated ()
.and ()
//配置登录和注销
.formLogin (). loginPage (adminContextPath +
"/login"). successHandler (successHandler). and ()
.logout (). logoutUrl (adminContextPath + "/logout"). and ()
//启用 HTTP-Basic 支持。这是 Spring Boot Admin Client 注册所必需的
.httpBasic (). and ();
// @formatter:on
}
}
```
```
15
16
17
18
19
20
21
22
23
24
25
```
```
26
27
28
29
30
31
```
```
package com. crazymaker. springcloud. adminserver;
```
```
import de. codecentric. boot. admin. server. config. EnableAdminServer;
import org. springframework. boot. SpringApplication;
import org. springframework. boot. autoconfigure. SpringBootApplication;
import org. springframework. cloud. client. discovery. EnableDiscoveryClient;
```
```
@SpringBootApplication
@EnableAdminServer
@EnableDiscoveryClient
public class AdminServerApplication {
```
```
public static void main (String[] args) {
SpringApplication.run ( AdminServerApplication. class, args );
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从下面的链接获取：语雀
或者码云
```
#### 3 、actuator 启用和暴露端点

执行器端点使您可以监视应用程序并与之交互。Spring Boot 包含许多内置端点，您可以添加自己的端
点。例如，health 端点提供基本的应用程序运行状况信息。

每个端点都可以启用或禁用。这控制了是否创建了端点以及它的 bean 在应用程序上下文中是否存在。为
了可以远程访问，端点还必须通过 JMX 或 HTTP 公开。大多数应用程序选择 HTTP，其中终结点的 ID 和前
缀/actuator 映射到 URL。例如，默认情况下，health 端点映射到/actuator/health。

**端点列表：**


```
ID 描述
```
```
auditevents 公开当前应用程序的审核事件信息。需要一个 AuditEventRepositoryBean。
```
```
beans 显示应用程序中所有 Spring Bean 的完整列表。
```
```
caches 公开可用的缓存。
```
```
conditions 显示在配置和自动配置类上评估的条件以及它们匹配或不匹配的原因。
```
```
configprops 显示所有的整理列表@ConfigurationProperties。
```
```
env 公开 Spring 的属性 ConfigurableEnvironment。
```
```
flyway 显示已应用的所有 Flyway 数据库迁移。需要一个或多个 FlywayBean。
```
```
health 显示应用程序运行状况信息。
```
```
httptrace
```
```
显示 HTTP 跟踪信息（默认情况下，最近 100 个 HTTP 请求-响应交换）。需要
一个 HttpTraceRepositoryBean。
```
```
info 显示任意应用程序信息。
```
```
integrationgraph 显示 Spring Integration 图。需要对的依赖 spring-integration-core。
```
```
loggers 显示和修改应用程序中记录器的配置。
```
```
liquibase 显示已应用的所有 Liquibase 数据库迁移。需要一个或多个 LiquibaseBean。
```
```
metrics 显示当前应用程序的“指标”信息。
```
```
mappings 显示所有@RequestMapping 路径的整理列表。
```
```
scheduledtasks 显示应用程序中的计划任务。
```
```
sessions
允许从 Spring Session 支持的会话存储中检索和删除用户会话。需要使用
Spring Session 的基于 Servlet 的 Web 应用程序。
```
```
shutdown 使应用程序正常关闭。默认禁用。
```
```
threaddump 执行线程转储。
```
```
ID 描述
```
```
heapdump 返回 hprof 堆转储文件。
```
```
jolokia
```
```
通过 HTTP 公开 JMX bean（当 Jolokia 在类路径上时，不适用于 WebFlux）。需要
对的依赖 jolokia-core。
```
```
logfile
```
```
返回日志文件的内容（如果已设置 logging. file. name 或 logging. file. path 属性）。
支持使用 HTTP Range 标头来检索部分日志文件的内容。
```
```
conditions 显示在配置和自动配置类上评估的条件以及它们匹配或不匹配的原因。
```
```
prometheus
以 Prometheus 服务器可以抓取的格式公开指标。需要对的依赖 micrometer-
registry-prometheus。
```
如果您的应用程序是 Web 应用程序（Spring MVC，Spring WebFlux 或 Jersey），则可以使用以下附加端
点：


###### 3.1 启用端点

默认情况下，除了 shutdown 端点是关闭的，其它的都是启用的。

1 ）启用 shutdown 端点

2 ）关闭默认端点

3 ）启用 info 端点

```
禁用的端点将从应用程序上下文中完全删除。
如果只想更改公开端点的技术，请使用 include 和 exclude 属性。
```
###### 3.2 暴露端点

停止公开所有在 JMX 上公开的端点，只公开 info 和 health 两个端点，使用如下属性：

通过 HTTP 公开所有的端点，除了 env 和 beans 端点，使用如下的属性：

[端点参考资料]：Spring Boot Actuator

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从下面的链接获取：语雀
或者码云
```
#### 4 、微服务 Provider 改造

对前面的 service-consumer-demo、service-provider-demo 两个微服务进行改造，加入 spring-
boot-admin 的监控。

###### 4.1 导入依赖

```
1 management. endpoint.<id>. enabled
```
```
1 management. endpoint. shutdown. enabled=true
```
```
1 management. endpoints. enabled-by-default=false
```
```
1 management. endpoint. info. enabled=true
```
```
1 management. endpoints. jmx. exposure. include=health, info
```
```
management. endpoints. web. exposure. include=*
management. endpoints. web. exposure. exclude=env, beans
```
```
1
2
```
```
<!-- admin-client -->
<dependency>
<groupId>de. codecentric</groupId>
<artifactId>spring-boot-admin-starter-client</artifactId>
</dependency>
```
```
1
2
3
4
5
```

###### 4.2 配置 yml

###### 使用 context-path

很多时候项目由于模块比较多，通常会配置 servlet 的 context-path 属性增加一节路径加以区分不同服
务，如下

这时候 actuator 访问路径就是http://ip:port/activity/actuator SpringBoot Admin 是没法识别到的，
actuator 默认访问的还是http://ip:port/actuator，这时候需要我们进行配置 management 的 context-
path 属性，如下

```
# 服务名
spring:
application:
name: admin-client
# 指定 admin 监控中心地址
boot:
admin:
client:
url: http://localhost: 9009
# 服务端口
server:
port: 9010
```
```
# 定义日志文件输出路径 [注释后不显示本服务的 logfile 模块]
logging:
file: E:/data/adminTest/logs/output. log
```
```
# include 暴露端点
management:
endpoints:
web:
exposure:
include: '*'
# 在访问/actuator/health 时显示完整信息
endpoint:
health:
show-details: always
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
```
```
server:
port: 8005
servlet:
context-path: /activity
```
```
1
2
3
4
```
```
spring:
cloud:
nacos:
discovery:
server-addr: 192.168.1.36: 8848
metadata:
management:
context-path: ${server. servlet. context-path}/actuator
```
```
1 2 3 4 5 6 7 8 9
```

###### 加上 spring security 密码

**在 Spring Boot Admin Server 中统一配置** 采用 spring security 提供安全保障，客户端需要配置 security
账号密码，服务注册时带上 metadata-map 信息.

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从下面的链接获取：语雀
或者码云
```
#### 5 、admin 实现在线日志查看

具体操作：

###### 5.1、添加 jar 包

###### 5.2 在 application. yml 平级文件夹中添加 logback-spring. xml 配置

###### 文件

```
<dependency>
<groupId>org. slf 4 j</groupId>
<artifactId>slf 4 j-api</artifactId>
<version>1.7.25</version>
</dependency>
```
```
1
2
3
4
5
```
```
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
<property name="APP_Name" value="adminTest"/>
<contextName>${APP_Name}</contextName>
<!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径，请根据需求配置路径-->
```
```
<property name="LOG_HOME" value="E:/data/adminTest/logs"/>
```
```
<!-- 彩色日志依赖的渲染类 -->
<conversionRule conversionWord="clr"
converterClass="org. springframework. boot. logging. logback. ColorConverter"/>
<conversionRule conversionWord="wex"
```
```
converterClass="org. springframework. boot. logging. logback. WhitespaceThrowabl
eProxyConverter"/>
<conversionRule conversionWord="wEx"
```
```
converterClass="org. springframework. boot. logging. logback. ExtendedWhitespace
ThrowableProxyConverter"/>
<!-- 彩色日志格式 -->
<property name="CONSOLE_LOG_PATTERN"
value="adminTest >> ${CONSOLE_LOG_PATTERN:-%clr (%d{yyyy-MM-dd
HH:mm: ss. SSS}){faint} %clr (${LOG_LEVEL_PATTERN:-%5 p}) %clr (${PID:- })
{magenta} %clr (---){faint} %clr ([%15.15 t]){faint} %clr (%-40.40 logger{39})
{cyan} %clr (LN:%L){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>
```
```
<!-- 控制台输出 -->
<appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
12
13
```
```
14
15
16
```
```
17
18
19
```

```
<encoder
class="ch. qos. logback. classic. encoder. PatternLayoutEncoder">
<pattern>${CONSOLE_LOG_PATTERN}</pattern>
<charset>utf 8</charset>
</encoder>
</appender>
```
```
<!-- 按照每天生成日志文件 -->
<appender name="FILE"
class="ch. qos. logback. core. rolling. RollingFileAppender">
<file>${LOG_HOME}/output. log</file>
<rollingPolicy
class="ch. qos. logback. core. rolling. TimeBasedRollingPolicy">
<!--日志文件输出的文件名-->
<FileNamePattern>${LOG_HOME}/output-%d{yyyy-MM-
dd}. log</FileNamePattern>
<!--日志文件保留天数-->
<MaxHistory> 30 </MaxHistory>
</rollingPolicy>
```
```
<encoder
class="ch. qos. logback. classic. encoder. PatternLayoutEncoder">
<!--格式化输出：%d 表示日期，%thread 表示线程名，%-5 level：级别从左显示 5 个字符
宽度%msg：日志消息，%n 是换行符-->
<pattern>%d{yyyy-MM-dd HH:mm: ss. SSS} [%thread] %-5 level
%logger{50}:%L - %msg%n</pattern>
</encoder>
</appender>
```
```
<!-- show parameters for hibernate sql 专为 Hibernate 定制 -->
<logger name="org. hibernate. type. descriptor. sql. BasicBinder"
level="WARN"/>
<logger name="org. hibernate. type. descriptor. sql. BasicExtractor"
level="WARN"/>
<logger name="org.hibernate.SQL" level="WARN"/>
<logger name="org.hibernate.engine.QueryParameters" level="DEBUG"/>
<logger name="org.hibernate.engine.query.HQLQueryPlan" level="WARN"/>
```
```
<!--mybatis log configure-->
<logger name="com.apache.ibatis" level="WARN"/>
<logger name="java.sql.Connection" level="WARN"/>
<logger name="java.sql.Statement" level="WARN"/>
<logger name="java.sql.PreparedStatement" level="WARN"/>
<logger name="org.apache.shiro" level="WARN"/>
<logger name="springfox.documentation" level="WARN"/>
```
```
<!-- 日志输出级别，注意：如果不写<appender-ref ref="FILE" /> ，将导致
springboot Admin 找不到文件，无法查看日志 -->
<root level="INFO">
<appender-ref ref="STDOUT"/>
<appender-ref ref="FILE"/>
</root>
</configuration>
```
20

21
22
23
24
25
26
27

28
29

30
31

32
33
34
35
36

37

38

39
40
41
42
43

44

45
46
47
48
49
50
51
52
53
54
55
56
57

58
59
60
61
62
63


###### 5.3 log. path 如何使用环境变量呢？

某些情况下需要变量设置个默认值，以防出现比较恶心的 _IS_UNDEFINED 后缀（ log 4 j 不存在的变量会
留空）
只要使用" :- " 操作符即可 (冒号+减号)。

比如 **log. path** 没有定义, 使用该变量的地方就会变成** log. path_IS_UNDEFINED**，给他一个默认值

```
${log. path :-/var/logs/myapp }
```
###### 5.4 actuator 的配置

#### 测试结果

###### 1. 不暴露端点测试

```
#### 暴露端点
management:
endpoints:
web:
base-path: "/actuator" # 配置 Endpoint 的基础路径
exposure:
include: '*' #在yaml 文件属于关键字，所以需要加引号
endpoint:
logfile:
# spring boot admin client 不配置日志文件路径（同时配置 logback-spring. xml 对
应的日志输出配置，否则无法输出日志），
# 控制台上的 Logging 模块下的 Logfile 会报错：Fetching logfile failed. Request
failed with status code 404
external-file: E:/logs/service-provider-demo/logs/output. log
enabled: true
health:
show-details: always
# 未配置/注释以下内容
# boot:
# admin:
# context-path: consumer
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
```
```
12
13
14
15
16
17
18
19
20
```
```
# 未配置/注释以下内容
management:
endpoints:
web:
exposure:
include: "*"
123456
```
```
1 2 3 4 5 6 7 8
```

**监控中心后台除 Detials 外所有功能失效**

###### 2. 正常情况

控制台打印的项目前缀

访问 localhost[Ip 地址]: 端口号即可
本案例中访问 [http://localhost:9009](http://localhost:9009) 即可进入监控中心
_logfile_ 模块查看日志详情

可以通过 springbootAdmin 调整各个包下的日志等级，相当方便

#### 6 、admin 与 Nacos（或 Eureka）结合的好处

到这里就结束，可以直接启动。admin 会自己拉取 Nacos（或 Eureka）上注册的 Provider 信息，主动去
注册。


```
这也是唯一区别之前手动注册的地方，就是 Provider 端不需要配置 admin 地址了，一切全部由
admin-server 自己实现。
```
这样的，不用在改了 admin-server 之后，强制去改所有 Provider 的配置了。

## ELK 日志平台（elasticsearch

## +logstash+kibana）原理和实操

ELK 指的是 Elastic 公司下面 Elasticsearch、Logstash、Kibana 三大开源框架首字母大写简称。
Elasticsearch、Logstash、Kibana 三大开源框架首字母大写简称。

#### ELK 的关系

在 ELK 架构中，Elasticsearch、Logstash 和 Kibana 三款软件作用如下：
**1 、Elasticsearch**
Elasticsearch 是一个高度可扩展的全文搜索和分析引擎，基于 Apache Lucence（事实上，Lucence 也是
百度所采用的搜索引擎）构建，能够对大容量的数据进行接近实时的存储、搜索和分析操作。
**2 、Logstash**
Logstash 是一个数据收集引擎，它可以动态的从各种数据源搜集数据，并对数据进行过滤、分析和统一
格式等操作，并将输出结果存储到指定位置上。Logstash 支持普通的日志文件和自定义 Json 格式的日志
解析。
**3 、Kibana**
Kibana 是一个数据分析和可视化平台，通常与 Elasticsearch 配合使用，用于对其中的数据进行搜索、分
析，并且以统计图标的形式展示。

ELK 的架构如下所示：

如上图所示，Logstash 安装在各个设备上，用于收集日志信息，收集到的日志信息统一汇总到
Elasticsearch 上，然后由 Kibana 负责 web 端的展示。

其中，如果终端设备过多，会导致 Elasticsearch 过载的现象，此时，我们可以采用一台 Redis 设备作为消
息队列，以暂时缓存数据，避免 Elasticsearch 压力突发。


```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### ELK 优点

ELK 架构优点如下：
**1 、处理方式灵活。** Elasticsearch 是全文索引，具有强大的搜索能力。
**2 、配置相对简单。** Kibana 的配置非常简单，Elasticsearch 则全部使用 Json 接口，配置也不复杂，
Logstash 的配置使用模块的方式，配置也相对简单。
**3 、检索性能高。** ELK 架构通常可以达到百亿级数据的查询秒级响应。
**4 、集群线性扩展。** Elasticsearch 本身没有单点的概念，自动默认集群模式，Elasticsearch 和 Logstash
都可以灵活扩展。
**5 、页面美观。** Kibana 的前端设计美观，且操作简单。

**Logstash** : 从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到
ES。

**Elasticsearch** : 对大容量的数据进行接近实时的存储、搜索和分析操作。

**Kibana** ：数据分析和可视化平台。与 Elasticsearch 配合使用，对数据进行搜索、分析和以统计图表的
方式展示。

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### 简单的 ELK 日志平台

刚来公司的时候，我们公司的日志收集系统 ELK 经常会出现查询不了最新的日志的情况，后面去查发现
ES 的节点经常也是 yellow 或者 red 的情况。

有时候会收到开发的投诉。架构图解如下:

其中 ElasticSearch 是三台服务器构成的集群，其中：

```
ElasticSearch 做倒排索引，
Logstash 跑在每个服务器上，各种日志通过 Logstash 搜集，Grok，Geoip 等插件进行处理然后统
一送到 ElasticSearch 的集群。
Kibana 做图形化的展示。
```
这种 elk 架构比较简单，也存在一些问题：

1 、Logstash 依赖 Java 虚拟机占用系统的内存和 CPU 都比较大，

2 、Logstash 在数据量较大的时候容易导致其他业务应用程序崩溃，影响业务正常使用


3 、随着时间的积累，es 空间不能满足现状

4 、Kibana 没有安全管控机制，没有权限审核，安全性较差。

5 、ElasticSearch 主节点也是数据节点，导致有时候查询较慢

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### ELK 改进之引入 Filebeat

ElasticSearch 的版本，我们还是选择原来的 6.2. x 的版本，然后重新搭建了一套 ELK 的日志系统。

ElasticSearch 6. x 的版本如果要做用于鉴权的话，必须依赖 X-Pack，但是 X-pack 是付费的产品，所以，
引入 x-pack，虽然能实现 Index 级别的权限管控，确保数据安全，但是涉及到费用的问题。

于是，ElasticSearch 的版本采用 ElasticSearch 7. x 的版本，用户鉴权采用其免费的 basic 认证实现（因
为 7. x 的新版本在性能上优化，查询和写入速度会更快）

架构图解如下:

整个架构的具体的改进方法如下:

1 、客户端选用更轻量化的 Filebeat，Filebeat 采用 Golang 语言进行编写的，优点是暂用系统资源小，
收集效率高。

2 、Filebeat 数据收集之后统一送到多个 Logstatsh 进行统一的过滤，然后将过滤后的数据写入
ElasticSearch 集群。

3 、将原有的 3 个 es 节点增加至 6 个节点，其中 3 个 ES 节点是 master 节点，其余的节点是数据节点，如果
磁盘不够用可以横向扩展数据节点。

6 、ElasticSearch 集群的硬盘采用 SSD 的硬盘

7 、ElasticSearch 做冷热数据分离

8 、 60 天之前的索引数据进行关闭，有需要用的时候手工打开

9 、ElasticSearch 的版本采用 ElasticSearch 7. x 的版本，用户鉴权采用其免费的 basic 认证实现（因为
7. x 的新版本在性能上优化，查询和写入速度会更快）

到此，我们的日志系统算暂时是正常并且能满足日志查日志的需求了，也很少出现卡顿的现象了，并且
服务器的资源使用率直接下降了一半。


###### ELK 的应用场景

```
异常分析
```
通过将应用的日志内容通过 Logstash 输入到 Elasticsearch 中来实现对程序异常的分析排查

```
业务分析
```
将消息的通讯结果通过 Logstash 输入到 Elasticsearch 中来实现对业务效果的整理

```
系统分析
```
将处理内容的延迟作为数据输入到 Elasticsearch 中来实现对应用性能的调优

但是，ELK 不适宜与超大规模 (PB 级别以上) 日志场景

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### ELK 的不足

**es 的资源占用**

一般使用 ES 时，必须要事先评估好节点配置和集群规模，可以从以下几个方面进行评估：

```
存储容量：要考虑索引副本数量、数据膨胀、ES 内部任务额外占用的磁盘空间（比如 segment
merge ) 以及操作系统占用的磁盘空间等因素，如果再需要预留 50% 的空闲磁盘空间，那么集群总
的存储容量大约为源数据量的 4 倍；
计算资源：主要考虑写入， 2 核 8 GB 的节点可以支持 5000 qps 的写入，随着节点数量和节点规格
的提升，写入能力基本呈线性增长；
索引和分片数量评估：一般一个 shard 的数据量在 30-50 GB 为宜，可以以此确定索引的分片数量
以及确定按天还是按月建索引。需要控制单节点总的分片数量，1 GB 堆内存支持 20-30 个分片为
宜。另外需要控制集群整体的分片数量，集群总体的分片数量一般不要超过 3 w 。
```
```
算下来 3 W * 50 G = 1500 T = 1.5 P
```
```
那么，elk 如何支持一天 100 PB，一个月上千 PB 规模的日志量呢？
```

从吞吐量上来说，虽然 mq 进行扩展，能支撑 100 w 级别 qps 的吞吐量

但是，后端的 logstash 吞吐峰值 15000 qps ，es 的单节点写入是 5000 qps 左右，

30 K * 100 Wqps 的日志吞吐量，如果不希望发生太大的日志延迟，消息积压，

**需要 100+个 logstash 节点， 300+个 ES 节点**

**这个需要庞大的资源成本，庞大的运维成本**

如果又要兼顾吞吐量，又要降低硬件成本和运维成本，必须要

```
缩短日志传输和处理链路，
并采用更高性能，更大压缩比例的存储组件，如 clickhouse，
```
架构如下：

clickhouse 的数据压缩比例，请参考另外一篇博客：

clickhouse 超底层原理 + 高可用实操 （史上最全）


最终，压缩后的数据，只剩下原始数据的 20%-30% ，单数据库这块，减少了 50% 的硬盘容量，

使用 elk 方案，数据有多个副本，包括 MQ（主副本 2 份），数据库（ 1 份），现在减少到数据库（ 1
份），这里至少减少 50% ，

```
这种高并发、大数据量场景下的日志方案，请参见 23 章视频：《100 Wqps 超高并发日志平台》
实操
```
所以，接下来，正式给大家介绍《ELK 日志平台（elasticsearch +logstash+kibana）原理和实操》

咱们先得把 ELK 的原理搞清楚，

**知己才能知彼，才能知道怎么去优化和改进**

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
#### Elasticsearch 概述

Elasticsearch 是一个分布式的开源搜索和分析引擎，在 _Apache Lucene_ 的基础上开发而成。

Lucene 是开源的搜索引擎工具包，Elasticsearch 充分利用 Lucene，并对其进行了扩展，使存储、索
引、搜索都变得更快、更容易，而最重要的是，正如名字中的“ elastic ”所示，一切都是灵活、有弹性
的。而且，应用代码也不是必须用 Java 书写才可以和 Elasticsearc 兼容，完全可以通过 JSON 格式的 HTTP
请求来进行索引、搜索和管理 Elasticsearch 集群。

如果你已经听说过 Lucene ，那么可能你也听说了 Solr，

Solr 也是开源的基于 Lucene 的分布式搜索引擎，跟 Elasticsearch 有很多相似之处。


但是 Solr 诞生于 2004 年，而 Elasticsearch 诞生于 2010 ，Elasticsearch 凭借后发优势和更活跃的社区、
更完备的生态系统，迅速反超 Solr，成为搜索市场的第二代霸主。

Elasticsearch 具有以下优势：

```
Elasticsearch 很快。由于 Elasticsearch 是在 Lucene 基础上构建而成的，所以在全文本搜索方
面表现十分出色。Elasticsearch 同时还是一个近实时的搜索平台，这意味着从文档索引操作到文档
变为可搜索状态之间的延时很短，一般只有一秒。因此，Elasticsearch 非常适用于对时间有严苛要
求的用例，例如安全分析和基础设施监测。
Elasticsearch 具有分布式的本质特征。 Elasticsearch 中存储的文档分布在不同的容器中，这些
容器称为分片，可以进行复制以提供数据冗余副本，以防发生硬件故障。Elasticsearch 的分布式特
性使得它可以扩展至数百台（甚至数千台）服务器，并处理 PB 量级的数据。
Elasticsearch 包含一系列广泛的功能。除了速度、可扩展性和弹性等优势以外，Elasticsearch
还有大量强大的内置功能（例如数据汇总和索引生命周期管理），可以方便用户更加高效地存储和
搜索数据。
Elastic Stack 简化了数据采集、可视化和报告过程。人们通常将 Elastic Stack 称为 ELK Stack （代
指 Elasticsearch 、 Logstash 和 Kibana ），目前 Elastic Stack 包括一系列丰富的轻量型数据采集代
理，这些代理统称为 Beats ，可用来向 Elasticsearch 发送数据。通过与 Beats 和 Logstash 进行集
成，用户能够在向 Elasticsearch 中索引数据之前轻松地处理数据。同时，Kibana 不仅可针对
Elasticsearch 数据提供实时可视化，同时还提供 UI 以便用户快速访问应用程序性能监测 (APM)、
日志和基础设施指标等数据。
```
#### logstash 概述

```
简单来说 logstash 就是一根具备实时数据传输能力的管道，负责将数据信息从管道的输入端传输到
管道的输出端；与此同时这根管道还可以让你根据自己的需求在中间加上滤网，Logstash 提供里
很多功能强大的滤网以满足你的各种应用场景。
```
```
1 logstash 常用于日志系统中做日志采集设备，最常用于 ELK 中作为日志收集器使用
```

###### logstash 作用：

###### logstash 的架构：

**Input (输入）：**

**采集各种样式，大小和相关来源数据，从各个服务器中收集数据。**

```
集中、转换和存储你的数据，是一个开源的服务器端数据处理管道，可以同时从多个数据源获取数据，并
对其进行转换，然后将其发送到你最喜欢的“存储
```
```
1
```
```
2
```
```
logstash 的基本流程架构：input | filter | output 如需对数据进行额外处理，filter 可
省略。
```
```
1
```
```
2
```
```
数据往往以各种各样的形式，或分散或集中地存在于很多系统中。
Logstash 支持各种输入选择，可以在同一时间从众多常用来源捕捉事件。
能够以连续的流式传输方式，轻松地从您的日志、指标、Web 应用、数据存储以及各种 AWS 服务采集数
据。
```
```
1
2
3
```
```
4
```

inpust：必须，负责产生事件（Inputs generate events），

常用：File、syslog、redis、beats（如：Filebeats）

**Filter (过滤器）**

**Output (输出）：**

将我们过滤出的数据保存到那些数据库和相关存储中。

```
用于在将 event 通过 output 发出之前，对其实现某些处理功能。
```
```
filters：可选，负责数据处理与转换（filters modify them），
```
```
常用：grok、mutate、drop、clone、geoip
```
```
grok：用于分析结构化文本数据。
```
```
1 2 3 4 5 6 7
```

outputs：必须，负责数据输出（outputs ship them elsewhere），

常用：elasticsearch、file、graphite、statsd

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### Logstash 的角色与不足

早期的 ELK 架构中使用 Logstash 收集、解析日志，

**但是：Logstash 对内存、cpu、io 等资源消耗比较高。**

相比 Logstash，Beats 所占系统的 CPU 和内存几乎可以忽略不计。

所以，在收集这块，一般使用 filebeat 代替 Logstash

#### filebeat 介绍

当你要面对成百上千、甚至成千上万的服务器、虚拟机和容器生成的日志时，Filebeat 将为你提供一种
轻量型方法，用于转发和汇总日志与文件，让简单的事情不再繁杂。

关于 Filebeat，记住两点：


```
轻量级日志采集器
输送至 Elasticsearch 或 Logstash，在 Kibana 中实现可视化
```
###### filebeat 和 beats 的关系

filebeat 是 Beats 中的一员。

Beats 在是一个轻量级日志采集器，其实 Beats 家族有 6 个成员，目前 Beats 包含六种工具：

```
Packetbeat：网络数据（收集网络流量数据）
Metricbeat：指标（收集系统、进程和文件系统级别的 CPU 和内存使用情况等数据）
Filebeat：日志文件（收集文件数据）
Winlogbeat：windows 事件日志（收集 Windows 事件日志数据）
Auditbeat：审计数据（收集审计日志）
Heartbeat：运行时间监控（收集系统运行时的数据）
```
###### Filebeat 是如何工作的

Filebeat 由两个主要组件组成： **inputs** 和 **harvesters** （直译：收割机，采集器）。

这些组件一起工作以跟踪文件，并将事件数据发送到你指定的输出。

Filebeat 的工作方式如下：

```
启动 Filebeat 时，它将启动一个或多个输入，这些输入将在为日志数据指定的位置中查找。
```
对于 Filebeat 所找到的每个日志，Filebeat 都会启动收割机。

每个收割机都读取一个日志以获取新内容，并将新日志数据发送到 libbeat，libbeat 会汇总事件并将汇总
的数据发送到您为 Filebeat 配置的输出。


```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
Filebeat 是一个轻量级日志传输 Agent，可以将指定日志转发到 Logstash、Elasticsearch、Kafka、
Redis 等中。

Filebeat 占用资源少，而且安装配置也比较简单，支持目前各类主流 OS 及 Docker 平台。

Filebeat 是用于转发和集中日志数据的轻量级传送程序。

作为服务器上的代理安装，Filebeat 监视您指定的日志文件或位置，收集日志事件，并将它们转发到
Elasticsearch 或 Logstash 进行索引。

**harvester 是什么**

一个 harvester 负责读取一个单个文件的内容。

harvester 逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。

每个文件启动一个 harvester。

harvester 负责打开和关闭这个文件，这就意味着在 harvester 运行时文件描述符保持打开状态。

在 harvester 正在读取文件内容的时候，文件被删除或者重命名了，那么 Filebeat 会续读这个文件。

这就有一个问题了，就是只要负责这个文件的 harvester 没用关闭，那么磁盘空间就不会释放。

默认情况下，Filebeat 保存文件打开直到 close_inactive 到达。


**input 是什么**

一个 input 负责管理 harvesters，并找到所有要读取的源。

如果 input 类型是 log，则 input 查找驱动器上与已定义的 glob 路径匹配的所有文件，并为每个文件启动一
个 harvester。

每个 input 都在自己的 Go 例程中运行。

下面的例子配置 Filebeat 从所有匹配指定的 glob 模式的文件中读取行：

**Filebeat 如何保持文件状态**

Filebeat 保存每个文件的状态，并经常刷新状态到磁盘上的注册文件（ **registry** ）。

状态用于记住 harvester 读取的最后一个偏移量，并确保所有日志行被发送（到输出）。

如果输出，比如 Elasticsearch 或者 Logstash 等，无法访问，那么 Filebeat 会跟踪已经发送的最后一行，
并只要输出再次变得可用时继续读取文件。

当 Filebeat 运行时，会将每个文件的状态新保存在内存中。

当 Filebeat 重新启动时，将使用注册文件中的数据重新构建状态，Filebeat 将在最后一个已知位置继续每
个 harvester。

对于每个输入，Filebeat 保存它找到的每个文件的状态。

因为文件可以重命名或移动，所以文件名和路径不足以标识文件。对于每个文件，Filebeat 存储惟一标
识符，以检测文件是否以前读取过。

如果你的情况涉及每天创建大量的新文件，你可能会发现注册表文件变得太大了。

（画外音：Filebeat 保存每个文件的状态，并将状态保存到 registry_file 中的磁盘。当重新启动 Filebeat
时，文件状态用于在以前的位置继续读取文件。如果每天生成大量新文件，注册表文件可能会变得太
大。为了减小注册表文件的大小，有两个配置选项可用：clean_remove 和 clean_inactive。对于你不再
访问且被忽略的旧文件，建议您使用 clean_inactive。如果想从磁盘上删除旧文件，那么使用
clean_remove 选项。）

**Filebeat 如何确保至少投递一次（at-least-once）？**

Filebeat 保证事件将被投递到配置的输出中至少一次，并且不会丢失数据。

Filebeat 能够实现这种行为，因为它将每个事件的投递状态存储在注册表文件中。

在定义的输出被阻塞且没有确认所有事件的情况下，Filebeat 将继续尝试发送事件，直到输出确认收到
事件为止。

如果 Filebeat 在发送事件的过程中关闭了，则在关闭之前它不会等待输出确认所有事件。当 Filebeat 重新
启动时，发送到输出（但在 Filebeat 关闭前未确认）的任何事件将再次发送。

这确保每个事件至少被发送一次，但是你最终可能会将重复的事件发送到输出。你可以通过设置
shutdown_timeout 选项，将 Filebeat 配置为在关闭之前等待特定的时间。

```
filebeat. inputs:
```
- type: log
paths:
- /var/log/*. log
- /var/path 2/*. log

```
1
2
3
4
5
```

```
描述
```
```
filebeat 用于启动 filebeat 的二进制文件
```
```
data 持久化数据文件的位置
```
```
logs Filebeat 创建的日志的位置
```
```
modules. d 简化 filebeat 配置的模板文件夹，如 nginx/kafka 等日志收集模板
```
```
filebeat. yml filebeat 配置文件
```
###### Filebeat 下载页面

https://www.elastic.co/cn/downloads/past-releases#filebeat

###### Filebeat 文件夹结构

**Filebeat 启动命令**

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### 配置 inputs

为了手动配置 Filebeat（代替用模块），你可以在 filebeat. yml 中的 filebeat. inputs 区域下指定一个
inputs 列表。

列表时一个 YMAL 数组，并且你可以指定多个 inputs，相同 input 类型也可以指定多个。例如：

**Log input**

从日志文件读取行

为了配置这种 input，需要指定一个 paths 列表，列表中的每一项必须能够定位并抓取到日志行。例如：

```
1 ./filebeat -e -c filebeat 配置文件
```
```
filebeat. inputs:
```
- type: log
paths:
- /var/log/system. log
- /var/log/wifi. log
- type: log
paths:
- "/var/log/apache 2/*"
fields:
apache: true
fields_under_root: true

```
1 2 3 4 5 6 7 8 9
```
```
10
11
```

你还可以应用设置其它额外的配置项（比如，fields, include_lines, exclude_lines, multiline 等等）来从
这些文件中读取行

你设置的这些配置对所有这种类型的 input 在获取日志行的时候都生效。

为了对不同的文件应用不同的配置，你需要定义多个 input 区域：

###### 配置项

**paths**

例如：/var/log/ _/_ .log 将会抓取/var/log 子目录目录下所有. log 文件。

它不会从/var/log 本身目录下的日志文件。如果你应用 recursive_glob 设置的话，它将递归地抓取所有子
目录下的所有. log 文件。

允许将扩展为递归 glob 模式。

启用这个特性后，每个路径中最右边的 **被扩展为固定数量的 glob 模式。**

**encoding**

读取的文件的编码

下面是一些 W 3 C 推荐的简单的编码：

```
plain, latin 1, utf-8, utf-16 be-bom, utf-16 be, utf-16 le, big 5, gb 18030, gbk, hz-gb-2312
```
```
filebeat. inputs:
```
- type: log
paths:
- /var/log/messages
- /var/log/*. log

```
1
2
3
4
5
```
```
filebeat. inputs:
```
- type: log # 从 system. log 和 wifi. log 中读取日志行
paths:
- /var/log/system. log
- /var/log/wifi. log
- type: log # 从 apache 2 目录下的每一个文件中读取日志行，并且在输出的时候会加上额外的
字段 apache
paths:
- "/var/log/apache 2/*"
fields:
apache: true
fields_under_root: true

```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
1 recursive_glob. enabled
```
```
例如：/foo/**扩展到/foo， /foo/*， /foo/**，等等。
```
```
如果启用，它将单个**扩展为 8 级深度*模式。
这个特性默认是启用的，设置 recursive_glob. enabled 为 false 可以禁用它。
```
```
1
2
3
4
```

```
euc-kr, euc-jp, iso-2022-jp, shift-jis, 等等
```
plain 编码是特殊的，因为它不校验或者转换任何输入。

**exclude_lines**

一组正则表达式，用于匹配你想要排除的行。Filebeat 会删除（PS：我觉得用“丢弃”更合适）这组正则表
达式匹配的行。默认情况下，没有行被删除。空行被忽略。

如果指定了 multiline，那么在用 exclude_lines 过滤之前会将每个多行消息合并成一个单行。（PS：也就
是说，多行合并成单行后再支持排除行的过滤）

下面的例子配置 Filebeat 删除以 DBG 开头的行：

**include_lines**

一组正则表达式，用于匹配你想要包含的行。Filebeat 只会导出那些匹配这组正则表达式的行。默认情
况下，所有的行都会被导出。空行被忽略。

如果指定了 multipline 设置，每个多行消息先被合并成单行以后再执行 include_lines 过滤。

下面是一个例子，配置 Filebeat 导出以 ERR 或者 WARN 开头的行：

（画外音：如果 include_lines 和 exclude_lines 都被定义了，那么 Filebeat 先执行 include_lines 后执行
exclude_lines，而与这两个选项被定义的顺序没有关系。include_lines 总是在 exclude_lines 选项前面
执行，即使在配置文件中 exclude_lines 出现在 include_lines 的前面。）

下面的例子导出那些除了以 DGB 开头的所有包含 sometext 的行：

**harvester_buffer_size**

当抓取一个文件时每个 harvester 使用的 buffer 的字节数。默认是 16384 。

**max_bytes**

**单个日志消息允许的最大字节数。超过 max_bytes 的字节将被丢弃且不会被发送。对于多行日志消息来
说这个设置是很有用的，因为它们往往很大。默认是 10 MB（ 10485760 ）。**

**json**

这些选项使得 Filebeat 将日志作为 JSON 消息来解析。例如：

```
filebeat. inputs:
```
- type: log
...
exclude_lines: ['^DBG']

```
1
2
3
4
```
```
filebeat. inputs:
```
- type: log
...
include_lines: ['^ERR', '^WARN']

```
1
2
3
4
```
```
filebeat. inputs:
```
- type: log
...
include_lines: ['sometext']
exclude_lines: ['^DBG']

```
1
2
3
4
5
```

为了启用 JSON 解析模式，你必须至少指定下列设置项中的一个：

keys_under_root

　　默认情况下，解码后的 JSON 被放置在一个以"json"为 key 的输出文档中。如果你启用这个设置，那
么这个 key 在文档中被复制为顶级。默认是 false。

overwrite_keys

　　如果 keys_under_root 被启用，那么在 key 冲突的情况下，解码后的 JSON 对象将覆盖 Filebeat 正常的
字段

add_error_key

　　如果启用，则当 JSON 反编排出现错误的时候 Filebeat 添加 "error. message" 和 "error. type: json"两
个 key，或者当没有使用 message_key 的时候。

message_key

　　一个可选的配置，用于在应用行过滤和多行设置的时候指定一个 JSON key。指定的这个 key 必须在
JSON 对象中是顶级的，而且其关联的值必须是一个字符串，否则没有过滤或者多行聚集发送。

ignore_decoding_error

　　一个可选的配置，用于指定是否 JSON 解码错误应该被记录到日志中。如果设为 true，错误将被记
录。默认是 false。

**multiline**

用于控制 Filebeat 如何扩多行处理日志消息

**exclude_files**

一组正则表达式，用于匹配你想要忽略的文件。默认没有文件被排除。

下面是一个例子，忽略. gz 的文件

**ignore_older**

如果启用，那么 Filebeat 会忽略在指定的时间跨度之前被修改的文件。如果你想要保留日志文件一个较
长的时间，那么配置 ignore_older 是很有用的。例如，如果你想要开始 Filebeat，但是你只想发送最近一
周最新的文件，这个情况下你可以配置这个选项。

你可以用时间字符串，比如 2 h（ 2 小时），5 m（ 5 分钟）。默认是 0 ，意思是禁用这个设置。

你必须设置 **ignore_older** 比 **close_inactive** 更大。

**close_***

```
json. keys_under_root: true
json. add_error_key: true
json. message_key: log
```
```
1
2
3
```
```
filebeat. inputs:
```
- type: log
...
exclude_files: ['\. gz$']

```
1
2
3
4
```

close_*配置项用于在一个确定的条件或者时间点之后关闭 harvester。关闭 harvester 意味着关闭文件处
理器。如果在 harvester 关闭以后文件被更新，那么在 scan_frequency 结束后改文件将再次被拾起。然
而，当 harvester 关闭的时候如果文件被删除或者被移动，那么 Filebeat 将不会被再次拾起，并且这个
harvester 还没有读取的数据将会丢失。

**close_inactive**

当启用此选项时，如果文件在指定的持续时间内未被获取，则 Filebeat 将关闭文件句柄。当 harvester 读
取最后一行日志时，指定周期的计数器就开始工作了。它不基于文件的修改时间。如果关闭的文件再次
更改，则会启动一个新的 harvester，并且在 scan_frequency 结束后，将获得最新的更改。

推荐给 close_inactive 设置一个比你的日志文件更新的频率更大一点儿的值。例如，如果你的日志文件每
隔几秒就会更新，你可以设置 close_inactive 为 1 m。如果日志文件的更新速率不固定，那么可以用多个
配置。

将 close_inactive 设置为更低的值意味着文件句柄可以更早关闭。然而，这样做的副作用是，如果
harvester 关闭了，新的日志行不会实时发送。

关闭文件的时间戳不依赖于文件的修改时间。代替的，Filebeat 用一个内部时间戳来反映最后一次读取
文件的时间。例如，如果 close_inactive 被设置为 5 分钟，那么在 harvester 读取文件的最后一行以后，这
个 5 分钟的倒计时就开始了。

你可以用时间字符串，比如 2 h（ 2 小时），5 m（ 5 分钟）。默认是 5 m。

**close_renamed**

当启用此选项时，Filebeat 会在重命名文件时关闭文件处理器。默认情况下，harvester 保持打开状态并
继续读取文件，因为文件处理器不依赖于文件名。如果启用了 close_rename 选项，并且重命名或者移动
的文件不再匹配文件模式的话，那么文件将不会再次被选中。Filebeat 将无法完成文件的读取。

**close_removed**

当启用此选项时，Filebeat 会在删除文件时关闭 harvester。通常，一个文件只有在它在由 close_inactive
指定的期间内不活跃的情况下才会被删除。但是，如果一个文件被提前删除，并且你不启用
close_removed，则 Filebeat 将保持文件打开，以确保 harvester 已经完成。如果由于文件过早地从磁盘
中删除而导致文件不能完全读取，请禁用此选项。

**close_timeout**

当启用此选项是，Filebeat 会给每个 harvester 一个预定义的生命时间。无论读到文件的什么位置，只要
close_timeout 周期到了以后就会停止读取。当你想要在文件上只花费预定义的时间时，这个选项对旧的
日志文件很有用。尽管在 close_timeout 时间以后文件就关闭了，但如果文件仍然在更新，则 Filebeat 将
根据已定义的 scan_frequency 再次启动一个新的 harvester。这个 harvester 的 close_timeout 将再次启
动，为超时倒计时。

**scan_frequency**

Filebeat 多久检查一次指定路径下的新文件（PS：检查的频率）。例如，如果你指定的路径是 /var/log/*
，那么会以指定的 scan_frequency 频率去扫描目录下的文件（PS：周期性扫描）。指定 1 秒钟扫描一次
目录，这还不是很频繁。不建议设置为小于 1 秒。

如果你需要近实时的发送日志行的话，不要设置 scan_frequency 为一个很低的值，而应该调整
close_inactive 以至于文件处理器保持打开状态，并不断地轮询你的文件。

默认是 10 秒。

**scan. sort**

如果你指定了一个非空的值，那么你可以决定用 scan. order 的升序或者降序。可能的值是 modtime 和
filename。为了按文件修改时间排序，用 modtime，否则用 filename。默认此选项是禁用的。


**scan. order**

可能的值是 asc 或者 desc。默认是 asc。

更多配置请查看 https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-opti
ons. html

这里再重点说一下 ignore_older , close_inactive , scan_frequency 这三个配置项

```
ignore_older： 它是设置一个时间范围（跨度），不在这个跨度范围之内的文件更新都不管
scan_frequency： 它设置的是扫描文件的频率，看看文件是否更新
close_inactive：它设置的是文件如果多久没更新的话就关闭文件句柄，它是有一个倒计时，如果
在倒计时期间，文件没有任何变化，则当倒计时结束的时候关闭文件句柄。不建议设置为小于 1
秒。
```
如果文件句柄关了以后，文件又被更新，那么在下一个扫描周期结束的时候变化发现这个改变，于是会
再次打开这个文件读取日志行，前面我们也提到过，每个文件上一次读到什么位置（偏移量）都记录在
registry 文件中。

**管理多行消息**

Filebeat 获取的文件可能包含跨多行文本的消息。例如，多行消息在包含 Java 堆栈跟踪的文件中很常见。
为了正确处理这些多行事件，你需要在 filebeat. yml 中配置 multiline 以指定哪一行是单个事件的一部分。

你可以在 filebeat. yml 的 filebeat. inputs 区域指定怎样处理跨多行的消息。例如：

上面的例子中，Filebeat 将所有不以 [ 开始的行与之前的行进行合并。

**multiline. pattern**

指定用于匹配多行的正则表达式

**multiline. negate**

定义模式是否被否定。默认 false。

**multiline. match**

指定 Filebeat 如何把多行合并成一个事件。可选的值是 **after** 或者 **before** 。

这种行为还收到 negate 的影响：

```
multiline. pattern: '^\['
multiline. negate: true
multiline. match: after
```
```
1
2
3
```

**multiline. flush_pattern**

指定一个正则表达式，多行将从内存刷新到磁盘。

**multiline. max_lines**

可以合并成一个事件的最大行数。如果一个多行消息包含的行数超过 max_lines，则超过的行被丢弃。默
认是 500 。

###### 配置 Logstash output

上面是配置 Filebeat 输出到 Logstash，那么 Logstash 本身也有配置，例如：

```
output. logstash:
hosts: ["127.0.0.1:5044"]
```
```
1
2
```

**负载均衡**

为了启用负载均衡，当你配置输出的时候你需要指定 **loadbalance: true**

#### 一键安装 es+logstash+ kibana

```
实操过程，请参见 23 章视频：《100 Wqps 超高并发日志平台》实操
```
###### 对应的镜像版本

```
elasticsearch: 7.14.0
kibana: 7.14.0
logstash: 7.14.0
filebeat: 7.14.0
```
###### docker 编码文件

```
input {
beats {
port => 5044
}
}
```
```
output {
elasticsearch {
hosts => ["http://localhost:9200"]
index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY. MM. dd}"
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
output. logstash:
hosts: ["localhost: 5044", "localhost: 5045"]
loadbalance: true
```
```
1
2
3
```
```
version: "3.5"
services:
elasticsearch:
image: andylsr/elasticsearch-with-ik-icu: 7.14.0
container_name: elasticsearch
hostname: elasticsearch
restart: always
ports:
```
- 9200:9200
volumes:
- ./elasticsearch 7/logs:/usr/share/elasticsearch/logs
- ./elasticsearch 7/data:/usr/share/elasticsearch/data

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```

- ./elasticsearch 7/config/single-
node. yml:/usr/share/elasticsearch/config/elasticsearch. yml
-
./elasticsearch 7/config/jvm. options:/usr/share/elasticsearch/config/jvm. opti
ons
-
./elasticsearch 7/config/log 4 j 2. properties:/usr/share/elasticsearch/config/lo
g 4 j 2. properties
environment:
- "ES_JAVA_OPTS=-Xms 512 m -Xmx 512 m"
- "TZ=Asia/Shanghai"
- "TAKE_FILE_OWNERSHIP=true" #volumes 挂载权限如果不想要挂载 es 文件改配
置可以删除
ulimits:
memlock:
soft: -1
hard: -1
networks:
base-env-network:
aliases:
- elasticsearch
kibana:
image: docker. elastic. co/kibana/kibana: 7.14.0
container_name: kibana
volumes:
-
./elasticsearch 7/config/kibana. yml:/usr/share/kibana/config/kibana. yml
ports:
- 15601:5601
ulimits:
nproc: 65535
memlock: -1
depends_on:
- elasticsearch
networks:
base-env-network:
aliases:
- kibana
logstash:
image:  logstash: 7.14.0
container_name: logstash
hostname: logstash
restart: always
ports:
- 19600:9600
- 15044:5044
volumes:
-
./logstash/logstash. conf:/usr/share/logstash/pipeline/logstash. conf:rw
- ./logstash/logstash. yml:/usr/share/logstash/config/logstash. yml
- ./logstash/data:/home/logstash/data
networks:
base-env-network:
aliases:
- logstash
# docker network create base-env-network
networks:
base-env-network:

13

14

15

16
17
18
19

20
21
22
23
24
25
26
27
28
29
30
31
32

33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53

54
55
56
57
58
59
60
61
62


###### 访问 kibana

[http://cdh1:15601](http://cdh1:15601)

SkyWalking

[http://cdh2:13800/](http://cdh2:13800/)

kibana

```
以上的实操过程，请参见 23 章视频：《100 Wqps 超高并发日志平台》实操
```
#### 读取 filebeat-输出到 es 集群

在分布式系统中，一台主机可能有多个应用，应用将日志输出到主机的指定目录，这时由 logstash 来搬
运日志并解析日志，然后输出到 elasticsearch 上。

由于于 logstash 是 java 应用，解析日志是非的消耗 cpu 和内存，logstash 安装在应用部署的机器上显得非
常的笨重。

```
external:
name: "base-env-network"
```
```
63
64
65
66
```

最常见的做法是用 filebeat 部署在应用的机器上，logstash 单独部署，然后由 filebeat 将日志输出给
logstash 解析，解析完由 logstash 再传给 elasticsearch。

在上面的配置中，输入数据源为 filebeat，输出源为 elasticsearch。

修改 logstash 的安装目录的 config 目录下的 logstash. conf 文件，配置如下：

```
input {
beats {
port => "5044"
}
}
```
```
filter {
```
```
if "message-dispatcher" in [tags]{
grok {
match => ["message", "%{TIMESTAMP_ISO 8601: time}\s* \s*%
{NOTSPACE: thread-id}\s* \s*%{LOGLEVEL: level}\s* \s*%{JAVACLASS: class}\s* \-
\s*%{JAVALOGMESSAGE: logmessage}\s*"]
}
```
```
}
```
```
if "ExampleApplication" in [tags]{
grok {
match => ["message", "%{TIMESTAMP_ISO 8601: time}\s* \s*%
{NOTSPACE: thread-id}\s* \s*%{LOGLEVEL: level}\s* \s*%{JAVACLASS: class}\s* \-
\s*%{JAVALOGMESSAGE: logmessage}\s*"]
}
```
```
}
mutate {
remove_field => "log"
remove_field => "beat"
remove_field => "meta"
remove_field => "prospector"
remove_field => "[host][os]"
}
}
```
```
output {
stdout { codec => rubydebug }
if "message-dispatcher" in [tags]{
elasticsearch {
hosts => [ "elasticsearch: 9200" ]
index => "message-dispatcher-%{+yyyy. MM. dd}"
}
}
if "ExampleApplication" in [tags]{
elasticsearch {
hosts => [ "elasticsearch: 9200" ]
index => "ExampleApplication-%{+yyyy. MM. dd}"
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
13
14
15
16
17
18
19
```
```
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
```

更多的输入和输出源的配置见官网

https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html

#### 在 kibana 显示的效果

在 kibana 组件上查看，可以看到创建了一个 filebeat 开头的数据索引，如下图:

在日志搜索界面，可以看到 service-hi 应用输出的日志，如图所示：

```
}
}
```
```
46
47
48
```

#### 使用 filebeat 发送日志

###### 制作 filebeat 镜像

官方文档

https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html

下载 filebeat，下载命令如下：

https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0-linux-x86_64.tar.gz

###### 制作基础的 unbantu 镜像

why unbantu？ not alpine? not centos？

```
wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.2.0-
linux-x 86_64. tar. gz
```
```
tar -zxvf filebeat-7.2.0-linux-x 86_64. tar. gz
mv filebeat-7.2.0-linux-x 86_64 /usr/share/
cd /usr/share/filebeat-7.2.0-linux-x 86_64/
```
```
1
```
```
2
3
4
5
```

Alpine 只有仅仅 5 MB 大小，并且拥有很友好的包管理机制。

Docker 官方推荐使用 Alpine 替代 Ubuntu 做为容器的基础镜像。

曾经尝试使用 alpine: 3.7 作为底层镜像, 按照 zookeeper，但是一直启动不来，换成了 centos 的镜像，排
查过程反复实验，耗时很久。

网上小伙伴构建 filebeat 镜像，基于 alpine: 3.7, 构建后的镜像运行时报“standard_init_linux. go:190:
exec user process caused "no such file or directory"”，故最后还是选择 ubuntu。

这里选择 ubuntu 的原因, 是其作为底层打包出来的镜像比 centos 要小很多。

```
# 基础镜像生成的镜像作为基础镜像
FROM ubuntu: 18.04
```
```
# 指定维护者的信息
MAINTAINER 尼恩@疯狂创客圈
```
```
# RUN apt-get update && apt-get -y install openjdk-8-jdk
```
```
#install wget, sudo, python, vim, ping and ssh command
```
```
RUN sed -i s@/archive. ubuntu. com/@/mirrors. aliyun. com/@g
/etc/apt/sources. list && apt-get clean && \
apt-get update && apt-get -y install wget && apt-get -y install sudo &&
\
apt-get -y install iputils-ping && \
apt-get -y install net-tools && \
apt install -y tzdata && \
rm -rf /etc/localtime && ln -s /usr/share/zoneinfo/Asia/Shanghai
/etc/localtime && dpkg-reconfigure -f noninteractive tzdata && \
apt-get clean
```
```
# echo "Asia/Shanghai" > /etc/timezone && dpkg-reconfigure -f
noninteractive tzdata && \
```
```
# RUN dpkg-reconfigure -f noninteractive tzdata
```
```
# RUN apt-get clean
```
```
#apt-get -y install python && \
# apt-get -y install vim && \
# apt-get -y install openssh-server && \
# apt-get -y install python-pip && \
```
```
# 复制并解压
ADD jdk-8 u 121-linux-x 64. tar. gz /usr/local/
```
```
ENV work_path /usr/local
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
13
```
```
14
15
16
17
```
```
18
19
20
```
```
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
```

**dockfile add 命令：**

ADD 指令的功能是将主机构建环境（上下文）目录中的文件和目录、以及一个 URL 标记的文件拷贝到镜
像中。

其格式是： ADD 源路径目标路径

注意事项：

1 、如果源路径是个文件，且目标路径是以 / 结尾，则 docker 会把目标路径当作一个目录，会把源文件
拷贝到该目录下。

如果目标路径不存在，则会自动创建目标路径。

2 、如果源路径是个文件，且目标路径是不是以 / 结尾，则 docker 会把目标路径当作一个文件。

如果目标路径不存在，会以目标路径为名创建一个文件，内容同源文件；

如果目标文件是个存在的文件，会用源文件覆盖它，当然只是内容覆盖，文件名还是目标文件名。

如果目标文件实际是个存在的目录，则会源文件拷贝到该目录下。注意，这种情况下，最好显示的以 /

结尾，以避免混淆。

3 、如果源路径是个目录，且目标路径不存在，则 docker 会自动以目标路径创建一个目录，把源路径目
录下的文件拷贝进来。

如果目标路径是个已经存在的目录，则 docker 会把源路径目录下的文件拷贝到该目录下。

**4 、如果源文件是个归档文件（压缩文件，比如 .tar 文件），则 docker 会自动帮解压。**

###### 推送镜像到 dockerhub

这个镜像解决了 jdk 问题，时区问题

推送到了 dockerhub，大家可以直接作为基础镜像使用

```
WORKDIR $work_path
```
```
# java
ENV JAVA_HOME /usr/local/jdk 1.8.0_121
ENV JRE_HOME /usr/local/jdk 1.8.0_121/jre
ENV CLASSPATH .:$JAVA_HOME/lib/dt. jar:$JAVA_HOME/lib/tools. jar:$JRE_HOME/lib
ENV PATH ${PATH}:${JAVA_HOME}/bin
```
```
40
41
42
43
44
45
46
47
```
```
docker login
```
```
docker tag 8 d 0 abdffe 76 f nien/ubuntu: 18.04
```
```
docker push nien/ubuntu: 18.04
```
```
1
2
3
4
5
```

###### 制作 filebeat 镜像

官方文档

https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html

下载 filebeat，下载命令如下：

https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0-linux-x86_64.tar.gz

**dockerfile**

#构建镜像

构建之后，进入容器，可以看到 /usr/local 目录下的 filebeat-7.14.0-linux-x 86_64

```
wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.2.0-
linux-x 86_64. tar. gz
```
```
tar -zxvf filebeat-7.2.0-linux-x 86_64. tar. gz
mv filebeat-7.2.0-linux-x 86_64 /usr/share/
cd /usr/share/filebeat-7.2.0-linux-x 86_64/
```
```
1
```
```
2
3
4
5
```
```
# 基础镜像生成的镜像作为基础镜像
FROM nien/ubuntu: 18.04
```
```
# 指定维护者的信息
MAINTAINER 尼恩@疯狂创客圈
```
```
# 复制并解压
ADD filebeat-7.14.0-linux-x 86_64. tar. gz /usr/local/
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
1 docker build -t filebeat: 7.14.0.
```
```
[ root@cdh2 filebeat]# docker run -it filebeat: 7.14.0 /bin/bash
root@7ba04f21f26e :/usr/local# ll
total 48
drwxr-xr-x 1 root root 4096 Apr 2 09 : 26 ./
drwxr-xr-x 1 root root 4096 Mar 16 03 : 27 ../
drwxr-xr-x 2 root root 4096 Mar 16 03 : 27 bin/
drwxr-xr-x 2 root root 4096 Mar 16 03 : 27 etc/
drwxr-xr-x 5 root root 4096 Apr 2 09 : 26 filebeat-7.14.0-linux-x 86_64/
drwxr-xr-x 2 root root 4096 Mar 16 03 : 27 games/
drwxr-xr-x 2 root root 4096 Mar 16 03 : 27 include/
drwxr-xr-x 8 uucp 143 4096 Dec 13  2016 jdk 1.8.0_121/
drwxr-xr-x 2 root root 4096 Mar 16 03 : 27 lib/
lrwxrwxrwx 1 root root 9 Mar 16 03 : 27 man -> share/man/
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```

###### 推送镜像到 dockerhub

这个镜像解决了 jdk 问题，时区问题

推送到了 dockerhub，大家可以直接作为基础镜像使用

如果要收集日志，就可以用这个基础镜像加点配置就 ok 啦

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
#### example-application 微服务的 filebeat 配置：

```
实操过程，请参见 23 章视频：《100 Wqps 超高并发日志平台》实操
```
```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### filebeat. yml 的参考配置：

```
drwxr-xr-x 2 root root 4096 Mar 16 03 : 27 sbin/
drwxr-xr-x 1 root root 4096 Apr 2 00 : 44 share/
drwxr-xr-x 2 root root 4096 Mar 16 03 : 27 src/
```
```
14
15
16
17
```
```
[ root@cdh2 filebeat]# docker tag fb 44037 ab 5 f 9 nien/filebeat: 7.14.0
```
```
[ root@cdh2 filebeat]# docker push nien/filebeat: 7.14.0
The push refers to repository [docker. io/nien/filebeat]
069 c 957 c 7 a 4 e: Pushing [=======> ]
 19 .99 MB/140 MB
b 17 e 3 cbc 28 a 1: Mounted from nien/ubuntu
5695 cc 8 dd 56 c: Mounted from nien/ubuntu
9 d 6787 a 516 e 7: Mounted from nien/ubuntu
```
```
1 2 3 4 5 6 7 8
```
```
# ============================== Filebeat inputs
===============================
filebeat. config. inputs:
enable: true
path: /work/filebeat/input. yml
reload. enabled: true
reload. period: 2 s
```
```
# ============================== Filebeat modules
==============================
```
```
1 2 3 4 5 6 7 8
```

输出到 logstsh 的地址为 logstash，这里用的是容器的名称， logstash 和这个微服务，需要在同一个网
络。

如果不是，可以使用虚拟机的名称，然后把 5044 ，映射到 15044

#### input. yml 配置：

主要配置的是日志的搜集目录为/work/logs/output. log，这个目录是应用 message-dispatcher 输出日志
的文件。

由于其他的微服务也是固定在这个文件，

所以这个路径，基本可以固定。

```
filebeat. config. modules:
# Glob pattern for configuration loading
path: ${path. config}/modules. d/*. yml
```
```
# Set to true to enable config reloading
reload. enabled: true
```
```
# Period on which files under path should be checked for changes
#reload . period: 10 s
```
```
#----------------------------- Logstash output -----------------------------
---
output. logstash:
# The Logstash hosts
hosts: ["cdh1:15044"]
```
```
# Optional SSL. By default is off.
# List of root certificates for HTTPS server verifications
#ssl . certificate_authorities: ["/etc/pki/root/ca. pem"]
```
```
# Certificate for SSL client authentication
#ssl . certificate: "/etc/pki/client/cert. pem"
```
```
# Client Certificate Key
#ssl . key: "/etc/pki/client/cert. key"
```
```
9
10
11
12
13
14
15
16
17
18
19
20
```
```
21
22
23
24
25
26
27
28
29
30
31
32
33
```
```
#filebeat . input:
```
- type: log

```
# Change to true to enable this input configuration.
enabled: true
```
```
# Paths that should be crawled and fetched. Glob based paths.
paths:
```
- /work/logs/info/*. log

```
1 2 3 4 5 6 7 8 9
```

启动 filebeat，执行一下命令：

#### 修改 dockerfile

- /work/logs/error/*. log

```
#
# - /work/logs/output. log
```
```
multiline:
pattern: '^\s*(\d{4}|\d{2})\-(\d{2}|[a-zA-Z]{3})\-(\d{2}|\d{4})' # 指定
匹配的表达式（匹配以 2017-11-15 08:04:23: 889 时间格式开头的字符串）
negate: true  # 是否匹配到
match: after  # 合并到上一行的末尾, 为了 error
日志
max_lines: 1000 # 最大的行数
timeout: 30 s  # 如果在规定的时候没有新的日志事
件就不等待后面的日志
```
```
tags: ["example-application"] #用于logstash过滤
```
```
#fields :
#source : ExampleApplication
#tags : ["GUID"]
#- /var/log/*. log
#- c:\programdata\elasticsearch\logs\*
#include_l ines: ['^ERROR']
```
```
10
11
12
13
14
15
16
17
18
```
```
19
20
```
```
21
22
```
```
23
24
25
26
27
28
29
30
31
32
33
```
```
nohup /user/local/filebeat-7.14.0-linux-x 86_64/filebeat  -c
/work/filebeat/filebeat. yaml >> /work/filebeat/out. log 2 >&1 &
```
```
1
```
```
2
```
```
FROM nien/filebeat: 7.14.0
```
```
# 指定维护者的信息
MAINTAINER 尼恩@疯狂创客圈
```
```
ADD dispatcher-provider-1.0-SNAPSHOT. jar /app/message-dispatcher. jar
ADD deploy-sit. sh /app/run. sh
RUN chmod +x /app/run. sh
```
```
# WORKDIR /app/
```
```
ENTRYPOINT /bin/bash -c "/app/run. sh start"
# ENTRYPOINT /bin/bash
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
```

#### 一键发布

使用 shell 脚本一键发布，这里的脚本，请参见视频

具体的演示，请参见视频

#### 启动之后

[http://cdh2:7703/message-dispatcher-provider/swagger-ui.html](http://cdh2:7703/message-dispatcher-provider/swagger-ui.html)

#### message-dispatcher 微服务的日志

在 SpringBoot 应用 message-dispatcher 微服务的日志，输出日志如下：

```
spatcher | ----------------------------------------------------------
message-dispatcher | UAA 推送中台 push-provider is running! Access
URLs:
message-dispatcher | Local:
http://127.0.0.1:7703/message-dispatcher-provider/
message-dispatcher | swagger-ui:
http://127.0.0.1:7703/message-dispatcher-provider/swagger-ui.html
message-dispatcher | actuator:
http://127.0.0.1:7703/message-dispatcher-provider/actuator/info
message-dispatcher | ---------------------------------------------
-------------
message-di
```
```
1 2 3 4 5 6 7
```
```
[ root@cdh2 filebeat]# cd /home/docker-compose/sit-ware/message-
dispatcher/work/logs/
[ root@cdh2 logs]# cat output. log
2022 -04-02 09 :03:30.103 [background-preinit] DEBUG
o.h.v.m.ResourceBundleMessageInterpolator: 89 - Loaded expression factory via
original TCCL
2022 -04-02 09 :03:59.633 [main] INFO
o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker: 330 -
Bean
'org. springframework. cloud. autoconfigure. ConfigurationPropertiesRebinderAuto
Configuration' of type
[org. springframework. cloud. autoconfigure. ConfigurationPropertiesRebinderAuto
Configuration$$EnhancerBySpringCGLIB$$e 81692 de] is not eligible for getting
processed by all BeanPostProcessors (for example: not eligible for auto-
proxying)
2022 -04-02 09 :04:05.331 [main] INFO
c.a.n.client. config. impl. LocalConfigInfoProcessor: 195 -
LOCAL_SNAPSHOT_PATH:/root/nacos/config
2022 -04-02 09 :04:06.034 [main] INFO
com. alibaba. nacos. client. config. impl. Limiter: 53 - limitTime: 5.0
2022 -04-02 09 :04:06.899 [main] INFO
com. alibaba. nacos. client. config. utils. JVMUtil: 47 - isMultiInstance:false
```
```
1 2 3 4 5 6 7
```

```
2022 -04-02 09 :04:07.068 [main] WARN
c.a.cloud. nacos. client. NacosPropertySourceBuilder: 87 - Ignore the empty
nacos configuration and get it based on dataId[message-dispatcher-provider]
& group[DEFAULT_GROUP]
2022 -04-02 09 :04:07.100 [main] WARN
c.a.cloud. nacos. client. NacosPropertySourceBuilder: 87 - Ignore the empty
nacos configuration and get it based on dataId[message-dispatcher-
provider. yml] & group[DEFAULT_GROUP]
2022 -04-02 09 :04:07.191 [main] INFO
o.s.c.b.c.PropertySourceBootstrapConfiguration: 101 - Located property
source: CompositePropertySource {name='NACOS', propertySources=
[NacosPropertySource {name='message-dispatcher-provider-
sit. yml, DEFAULT_GROUP'}, NacosPropertySource {name='message-dispatcher-
provider. yml, DEFAULT_GROUP'}, NacosPropertySource {name='message-dispatcher-
provider, DEFAULT_GROUP'}, NacosPropertySource {name='sharding-db-
dev. yml, DEFAULT_GROUP'}]}
2022 -04-02 09 :04:07.304 [main] INFO
c.c.s.message. start. MessageDispatchApplication: 652 - The following profiles
are active: sit
2022 -04-02 09 :04:28.417 [main] INFO
o.s.d.r.config. RepositoryConfigurationDelegate: 247 - Multiple Spring Data
modules found, entering strict repository configuration mode!
2022 -04-02 09 :04:28.418 [main] INFO
o.s.d.r.config. RepositoryConfigurationDelegate: 127 - Bootstrapping Spring
Data JPA repositories in DEFAULT mode.
2022 -04-02 09 :04:34.251 [main] INFO
o.s.d.r.config. RepositoryConfigurationDelegate: 185 - Finished Spring Data
repository scanning in 5673 ms. Found 3 JPA repository interfaces.
2022 -04-02 09 :04:37.630 [main] WARN
o.springframework. boot. actuate. endpoint. EndpointId: 131 - Endpoint ID 'nacos-
config' contains invalid characters, please migrate to a valid format.
2022 -04-02 09 :07:17.969 [main] ERROR
org. springframework. boot. SpringApplication: 823 - Application run failed
org. springframework. beans. factory. BeanCreationException: Error creating bean
with name 'messageController': Injection of resource dependencies failed;
nested exception is org. springframework. beans. factory. BeanCreationException:
Error creating bean with name 'messagePushServiceImpl': Injection of
resource dependencies failed; nested exception is
org. springframework. beans. factory. BeanCreationException: Error creating bean
with name 'rocketmqMessageService' defined in URL [jar:file:/app/message-
dispatcher. jar!/BOOT-
INF/classes!/com/crazymaker/springcloud/message/service/impl/RocketmqMessage
Service. class]: Initialization of bean failed; nested exception is
java. lang. IllegalStateException:
org. apache. rocketmq. remoting. exception. RemotingTimeoutException: wait
response on the channel <dh2/192.168.56.122:9876> timeout, 3000 (ms)
at
org. springframework. context. annotation. CommonAnnotationBeanPostProcessor. pos
tProcessProperties (CommonAnnotationBeanPostProcessor. java:325)
at
org. springframework. beans. factory. support. AbstractAutowireCapableBeanFactory
.populateBean (AbstractAutowireCapableBeanFactory. java:1404)
at
org. springframework. beans. factory. support. AbstractAutowireCapableBeanFactory
.doCreateBean (AbstractAutowireCapableBeanFactory. java:592)
at
org. springframework. beans. factory. support. AbstractAutowireCapableBeanFactory
.createBean (AbstractAutowireCapableBeanFactory. java:515)
```
```
8
```
```
9
```
10

11

12

13

14

15

16

17

18

19

20

21


然后在部署了 filebeat 的机器上部署该应用，应用的输出文件为/var/log/service-hi. log，应用启动命令如
下：

应用启动成功后日志输出如下：

这时的日志数据的传输路径如下图：

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### 查看日志索引

```
22
```
```
1 nohup java -jar elk-test-0.0.1-SNAPSHOT. jar > /var/log/service-hi. log 2 >&1
&
```
```
1
```
```
1 2019 -07-02 17 :13:13.530 INFO 31579 --- [pool-1-thread-1]
com. example. elktest. ElkTestApplication : seed:562779
2 2019 -07-02 17 :13:13.630 INFO 31579 --- [pool-1-thread-1]
com. example. elktest. ElkTestApplication : seed:963836
3 2019 -07-02 17 :13:13.730 INFO 31579 --- [pool-1-thread-1]
com. example. elktest. ElkTestApplication : seed:825694
4 2019 -07-02 17 :13:13.830 INFO 31579 --- [pool-1-thread-1]
com. example. elktest. ElkTestApplication : seed:33228
5 2019 -07-02 17 :13:13.930 INFO 31579 --- [pool-1-thread-1]
com. example. elktest. ElkTestApplication : seed:685589
```
```
1
```
```
2
```
```
3
```
```
4
```
```
5
```

效果

可以看到在 kibana 中多了两个索引

需要配置

创建一个

```
docker run --name filebeat -d \
-v /home/qw/elk/filebeat/filebeat. yml:/usr/share/filebeat/filebeat. yml \
-v /home/qw/elk/testlog/:/home/ \
elastic/filebeat: 7.2.0
```
```
1
2
3
4
```

选择

最终展示

到这里简单收集日志就完成了, 需要更多复杂业务配置, 需要大家根据需求自己配置详细信息.

#### logstash 详解

Logstash 是一款强大的数据处理工具，它可以实现数据传输，格式处理，格式化输出，


logstash 还有强大的插件功能，常用于日志处理.
logstash 我们只让它进行日志处理，处理完之后将其输出到 elasticsearch。

官方文档

https://www.elastic.co/guide/en/logstash/7.17/index.html

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### stash 第一个事件

Logstash 管道有两个必需元素，输入和输出，以及一个可选元素 filter。

输入插件使用来自源的数据，过滤器插件在您指定时修改数据，输出插件将数据写入目标。
如下图

根据官方文档 Logstash 对数据的处理主要流程是

```
1. 首先数据传入 logstash，在其内部对数据进行过滤和处理
2. logstash 将处理过的数据传递给 Elasticsearch
3. Elasticsearch 对数据进行存储、创建索引等内容
4. kibana 对数据提供可视化的支持
```
**Logstash 的核心流程的三个环节**

```
Logstash 核心分三个环节：
```
```
数据输入
数据处理
数据输出
```
其数据输入、处理、输出主要在配置中间中下面部分进行配置


###### logstash 数值类型

```
数组
```
match =>["datetime", "UNIX", "ISO 8601"]

```
布尔
```
必须是一个 true 或 false

ssl_enable => true

```
字节
```
一个字段是字节字符串字段表示有效字节的单元。它是一种方便的方式在特定尺寸的插件选项。

支持 SI (k M G T P E Z Y) 和 Binary (TiKimigipiziyiei) 单位。

二进制单元在基座单元和 Si-1024 在基底 1000 。

这个字段是大小写敏感的。如果未指定单位, 则整数表示的字符串的字节数。

```
编解码器
```
codec => "json"

```
哈希
```
哈希是一个键值对的集合中指定的格式，多个键值对的条目以空格分隔而不是逗号。

match => { "field 1" => "value 1" "field 2" =>"value 2" ... }

```
数字
```
数字必须有效的数字值 (浮点或整数)。

port => 33

```
密码
```
密码是一个字符串的单个值，则不对其进行记录或打印。

my_password => "password"

```
uri
```
my_uri =>"http://foo:bar@example.net"

```
路径
```
```
input {}
filter {}
output {}
```
```
1
2
3
```
```
my_bytes => "1113" # 1113 bytes
```
```
my_bytes => "10 MiB" # 10485760 bytes
```
```
my_bytes => "100 kib" # 102400 bytes
```
```
my_bytes => "180 mb"# 180000000 bytes
```
```
1 2 3 4 5 6 7
```

```
Text Result
```
```
\r carriage return (ASCII 13)
```
```
\n new line (ASCII 10)
```
```
\t tab (ASCII 9)
```
```
\ backslash (ASCII 92)
```
```
" double quote (ASCII 34)
```
```
' single quote (ASCII 39)
```
一个路径是一个字符串，表示系统运行的有效路径。

my_path =>"/tmp/logstash"

```
转义序列
```
默认地，转义字符没有被启用。如果你希望使用转义字符串序列，您需要在你的 logstash. yml 中设置
config. support_escapes: true

###### logstash 条件判断

有时您只想在特定条件下过滤或输出事件。为此，您可以使用条件。

Logstash 中的条件查看和行为与编程语言中的条件相同。条件语句支持 if，else if 以及 else 报表和可以被
嵌套。

**条件语法**

```
if EXPRESSION{ ... } else if EXPRESSION { ... } else { ... }
```
###### logstash 比较运算符

　　等于: ==, !=, <, >, <=, >=
　　正则: =~, !~ (checks a pattern on the right against a string value on the left)
　　包含关系: in, not in

　　支持的布尔运算符：and, or, nand, xor

　　支持的一元运算符:!


```
作用符号
```
```
等于 ==
```
```
不等于 !=
```
```
小于 <
```
```
大于 >
```
```
小于等于 <=
```
```
大于等于 >=
```
```
匹配正则 =~
```
```
不匹配正则 !~
```
```
包含 in
```
```
不包含 not in
```
```
与 and
```
```
或 or
```
```
非与 nand
```
```
非或 xor
```
```
复合表达式 ()
```
```
取反符合 !()
```
```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### 数据输入环节

```
input 配置定义了数据的来源。其主要支持下面方式
```
事件源可以是从 stdin 屏幕输入读取，可以从 file 指定的文件，也可以从 es，filebeat，kafka，redis 等读
取

**stdin**

监控控制台输入。

要测试 Logstash 安装成功，运行最基本的 Logstash 管道。执行以下的命令

-e 标志使您可以直接从命令行指定配置。

```
1 bin/logstash -e 'input { stdin { } } output { stdout {} }'
```

通过在命令行指定配置，可以快速测试配置，而无需在迭代之间编辑文件。

示例中的管道从标准输入 stdin 获取输入，并以结构化格式将输入移动到标准输出 stdout。

启动 Logstash 后，等到看到“Pipeline main started”，然后在命令提示符下输入 hello world，显示的如
下：

###### file

监控文件内容

```
path 可以用/var/log/ .log,/var/log/**/ .log，
type 通用选项. 用于激活过滤器
start_position 选择 logstash 开始读取文件的位置，begining 或者 end。
```
还有一些常用的例如：discover_interval，exclude，sincedb_path, sincedb_write_interval 等可以参考
官网

###### syslogs

从 syslogs 读取数据

```
hello world
{
"host" => "VM_0_13_centos",
"message" => "hello world",
"@version" => "1",
"@timestamp" => 2019 -07-02 T06:26:28.684 Z
}
```
```
1 2 3 4 5 6 7
```
```
file{
path => ['/var/log/nginx/access. log']  #要输入的文件路径
type => 'nginx_access_log'
start_position => "beginning"
}
```
```
1 2 3 4 5 6
```
```
syslog{
port =>"514"
type => "syslog"
}
```
```
# port 指定监听端口 (同时建立 TCP/UDP 的 514 端口的监听)
```
```
#从syslogs读取需要实现配置rsyslog ：
# cat /etc/rsyslog. conf 加入一行
*.* @172.17.128.200:514 #指定日志输入到这个端口 ，然后 logstash 监听这个端口，如果有
新日志输入则读取
# service rsyslog restart #重启日志服务
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
```

###### beats

从 Elastic beats 接收数据

###### kafka

从 kafka topic 中读取数据

```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
#### 数据处理环节

filter plugin 过滤器插件, 主要是对数据进行处理。

```
beats {
port => 5044 #要监听的端口
}
# 还有 host 等选项
```
```
# 从 beat 读取需要先配置 beat 端，从 beat 输出到 logstash。
# vim /etc/filebeat/filebeat. yml
..........
output. logstash:
hosts: ["localhost: 5044"]
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
kafka{
bootstrap_servers=> "kafka01:9092, kafka02:9092, kafka03:9092"
topics => ["access_log"]
group_id => "logstash-file"
codec => "json"
}
kafka{
bootstrap_servers=> "kafka01:9092, kafka02:9092, kafka03:9092"
topics => ["weixin_log","user_log"]
codec => "json"
}
```
```
# bootstrap_servers 用于建立群集初始连接的 Kafka 实例的 URL 列表。
# topics 要订阅的主题列表，kafka topics
# group_id 消费者所属组的标识符，默认为 logstash。kafka 中一个主题的消息将通过相同的方式
分发到 Logstash 的 group_id
# codec 通用选项，用于输入数据的编解码器。
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```
```
17
```

###### grok 解析文本并构造

Grok 是一个十分强大的 Logstash Filter 插件，它可以通过正则解析任意文本，将非结构化日志数据格
式转换为结构化的、方便查询的结构。

它是目前 Logstash 中解析非结构化日志数据最好的方式。

Grok 的语法规则是：
这里的 “语法” 指的是匹配模式，例如，使用 NUMBER 模式可以匹配出数字，IP 模式则会匹配出
127.0.0.1 这样的 IP 地址。比如按以下格式输入内容：

那么，

- %{IP: clientip} 匹配模式将获得的结果为：clientip: 172.16.213.132
- %{HTTPDATE: timestamp} 匹配模式将获得的结果为：timestamp: 16/Jun/2020:16:24: 19 +0800
- %{QS: referrer} 匹配模式将获得的结果为：referrer: “GET / HTTP/1.1”
到这里为止，我们已经获取了上面输入中前三个部分的内容，分别是 clientip、timestamp 和 referrer
三个字段。

如果要获取剩余部分的信息，方法类似。

**要在线调试 Grok，可以点击在线调试，可点击这里进行在线调试，非常方便。**

下面是一个组合匹配模式，它可以获取上面输入的所有内容：

正则匹配是非常严格的匹配，在这个组合匹配模式中，使用了转义字符 \，这是因为输入的内容中有空
格和中括号。

通过上面这个组合匹配模式，我们将输入的内容分成了 5 个部分，即 5 个字段。

将输入内容分割为不同的数据字段，这对于日后解析和查询日志数据非常有用，这正是我们使用 grok 的
目的。

Logstash 默认提供了近 200 个匹配模式（其实就是定义好的正则表达式）让我们来使用，可以在
Logstash 安装目录下找到。

例如，我这里的路径为：

此目录下有定义好的各种匹配模式，基本匹配定义在 grok-patterns 文件中。

从这些定义好的匹配模式中，可以查到上面使用的四个匹配模式对应的定义规则。

除此之外，还有很多默认定义好的匹配模式文件，比如 httpd、java、linux-syslog、redis、
mongodb、nagios 等，这些已经定义好的匹配模式，可以直接在 Grok 过滤器中进行引用。

当然也可以定义自己需要的匹配模式。

在了解完 Grok 的匹配规则之后，下面通过一个配置实例深入介绍下 Logstash 是如何将非结构化日志数
据转换成结构化数据的。

首先看下面的一个事件配置文件：

```
1 172 .16.213.132 [16/Jun/2020:16:24: 19 + 0800 ] "GET / HTTP/1.1" 403 5039
```
```
%{IP: clientip}\ \[%{HTTPDATE: timestamp}\]\ %{QS: referrer}\ %
{NUMBER: response}\ %{NUMBER: bytes}
```
```
1
```
```
/usr/local/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core-
4.1.2/patterns
```
```
1
```

在这个配置文件中，输入配置成了 stdin，在 filter 中添加了 grok 过滤插件，并通过 match 来执行正则
表达式解析，

grok 中括号中的正则表达式就是上面提到的组合匹配模式，然后通过 rubydebug 编码格式输出信息。

这样的组合有助于调试和分析输出结果。

通过此配置启动 Logstash 进程后，我们仍然输入之前给出的那段内容：

然后，查看 rubydebug 格式的日志输出，内容如下：

从这个输出可知，通过 Grok 定义好的 5 个字段都获取到了内容，并正常输出了。

###### date 日期解析

解析字段中的日期，然后转存到@timestamp

```
input{
stdin{}
}
filter{
grok{
match => ["message", "%{IP: clientip}\ \[%{HTTPDATE: timestamp}\]\ %
{QS: referrer}\ % {NUMBER: response}\ %{NUMBER: bytes}"]
}
}
output{
stdout{
codec => "rubydebug"
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
1 172 .16.213.132 [16/Jun/2020:16:24: 19 + 0800 ] "GET / HTTP/1.1" 403 5039
```
```
{
"timestamp" => "16/Jun/2020:16:24: 19 +0800",
"response" => "403",
"bytes" => "5039",
"@version" => "1",
"clientip" => "172.16.213.132",
"host" => "nnmaster. cloud",
"referrer" => "\"GET / HTTP/1.1\"",
"message" => "172.16.213.132 [16/Jun/2020:16:24: 19 +0800] \"GET /
HTTP/1.1\" 403 5039",
"@timestamp" => 2020 -06-16 T07:46:53.120 Z
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```

###### mutate 字段转换

mutate 字段转换, 对字段做处理重命名、删除、替换和修改字段。

Mutate 过滤器的配置选项

```
[2018-07-04 17 :43:35,503]
```
```
grok{
match => {"message"=>"%{DATA: raw_datetime}"}
}
date{
match => ["raw_datetime","YYYY-MM-dd HH:mm: ss, SSS"]
remove_field =>["raw_datetime"]
}
#将raw_datetime存到 @timestamp 然后删除 raw_datetime
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
#24/Jul/2018 :18:15: 05 +0800
date {
match => ["timestamp","dd/MMM/YYYY:HH:mm: ss Z]
}
```
```
1
2
3
4
```

```
选项类型
是否
必须
```
```
简述
```
```
convert hash No
```
```
转化命令，是对字段类型做转化，例如: String 转为
integer
```
```
copy hash No 将一个已经存在的字段复制给另一个字段。
```
```
gsub array No
```
```
通过正则表达式匹配字段的值，然后替换为指定的字符
串。
```
```
join hash No 使用分隔符连接数组。
```
```
lowercase array No 将 string 类型的字段值转化为小写的形式。
```
```
merge hash No
合并两个数组或者 Hash 类型的字段。string 类型的字段会
自动的合并为一个数组。
```
```
coerce hash No 为存在但是不为空的字段设置默认值
```
```
rename hash No 字段重命名
```
```
replace hash No 将一个字段的值替换为一个新的值。
```
```
split hash No 将一个字段按照指定符号切割为数组。
```
```
strip array No 去除字段中的空格。
```
```
update hash No 更新字段为一个新值。
```
```
uppercase array No 将字符串字段转化为大写形式。
```
```
capitalize array No 将字符串字段转化为首字母大写的形式。
```
```
tag_on_failure string No 错误发生时的配置
```
#### covert 类型转换

**covert** ：类型转换。类型包括：integer，float，integer_eu，float_eu，string 和 boolean

```
字段类型为 hash
没有默认值
```
将字段转化为不同的类型，例如：string 转 integer。

如果被转化的字段类型是数组，数组的所有成员都将被转化。如果对象是 hash 就不会进行转化。

**实例：**


###### split

**split** ：使用分隔符把字符串分割成数组

eg：

aaa, bbb

192,128,1,100

###### merge

**merge** ：合并字段。数组和字符串，字符串和字符串

eg：

```
filter {
mutate {
convert => {
"fieldname" => "integer"
"booleanfield" => "boolean"
}
}
}
```
```
1 2 3 4 5 6 7 8
```
```
mutate{
split => {"message"=>","}
}
```
```
1
2
3
```
```
{
"@timestamp" => 2018 - 06 - 26 T 02: 40 : 19.678 Z,
"@version" => "1",
"host" => "localhost",
"message" => [
[ 0 ] "aaa",
[ 1 ] "bbb"
]}
```
```
1 2 3 4 5 6 7 8 {
```
```
"host" => "localhost",
"message" => [
[ 0 ] "192",
[ 1 ] "128",
[ 2 ] "1",
[ 3 ] "100"
],
"@timestamp" => 2018 - 06 - 26 T 02: 45 : 17.877 Z,
"@version" => "1"
}
```
```
mutate{
split => {"message"=>","}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```

输入：abc

输入：abc,. 123

###### rename

**rename** ：对字段重命名

123

```
filter{
mutate{
add_field => {"field 1"=>"value 1"}
}
mutate{
split => {"message"=>"."} #把message字段按照 . 分割
}
mutate{
merge => {"message"=>"field 1"} #将filed1字段加入到message字段
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
{
"message" => [
[ 0 ] "abc,"
[ 1 ] "value 1"
],
"@timestamp" => 2018 - 06 - 26 T 03: 38 : 57.114 Z,
"field 1" => "value 1",
"@version" => "1",
"host" => "localhost"
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
{
"message" => [
[ 0 ] "abc,",
[ 1 ] "123",
[ 2 ] "value 1"
],
"@timestamp" => 2018 - 06 - 26 T 03: 38 : 57.114 Z,
"field 1" => "value 1",
"@version" => "1",
"host" => "localhost"
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
```
```
filter{
mutate{
rename => {"message"=>"info"}
}
}
```
```
1
2
3
4
5
```

###### remove_field：移除字段

###### join

**join** ：用分隔符连接数组，如果不是数组则不做处理

**gsub** ：用正则或者字符串替换字段值。仅对字符串有效

```
{
"@timestamp" => 2018 - 06 - 26 T 02: 56 : 00.189 Z,
"info" => "123",
"@version" => "1",
"host" => "localhost"
}
```
```
1 2 3 4 5 6 7
```
```
mutate {
remove_field => ["message","datetime"]
}
```
```
1
2
3
```
```
mutate{
split => {"message"=>": "}
}
mutate{
join => {"message"=>","}
}
```
```
1 2 3 4 5 6 7
```
```
abc: 123
{
"@timestamp" => 2018 - 06 - 26 T 03: 55 : 41.426 Z,
"message" => "abc, 123",
"host" => "localhost",
"@version" => "1"
}
aa:cc
{
"@timestamp" => 2018 - 06 - 26 T 03: 55 : 47.501 Z,
"message" => "aa, cc",
"host" => "localhost",
"@version" => "1"
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
```

**update** ：更新字段。如果字段不存在，则不做处理

**replace** ：更新字段。如果字段不存在，则创建

```
mutate{
gsub => ["message","/","_"] #用_替换/
}
```
```
------>
a/b/c/
{
"@version" => "1",
"message" => "a_b_c_",
"host" => "localhost",
"@timestamp" => 2018 - 06 - 26 T 06: 20 : 10.811 Z
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
mutate{
add_field => {"field 1"=>"value 1"}
}
mutate{
update => {"field 1"=>"v 1"}
update => {"field 2"=>"v 2"}  #field2不存在 不做处理
}
---------------->
{
"@timestamp" => 2018 - 06 - 26 T 06: 26 : 28.870 Z,
"field 1" => "v 1",
"host" => "localhost",
"@version" => "1",
"message" => "a"
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```
```
mutate{
add_field => {"field 1"=>"value 1"}
}
mutate{
replace => {"field 1"=>"v 1"}
replace => {"field 2"=>"v 2"}
}
---------------------->
{
"message" => "1",
"host" => "localhost",
"@timestamp" => 2018 - 06 - 26 T 06: 28 : 09.915 Z,
"field 2" => "v 2",  #field2不存在 ，则新建
"@version" => "1",
"field 1" => "v 1"
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
```

###### geoip

根据来自 Maxmind GeoLite 2 数据库的数据添加有关 IP 地址的地理位置的信息

###### ruby

ruby 插件可以执行任意 Ruby 代码

```
16 }
17
18
```
```
geoip {
source => "clientip"
database =>"/tmp/GeoLiteCity. dat"
}
```
```
1
2
3
4
```
```
filter{
urldecode{
field => "message"
}
ruby {
init => "@kname = ['url_path','url_arg']"
code => "
new_event =
LogStash:: Event.new (Hash[@kname.zip (event.get ('message'). split ('?'))])
event.append (new_event)"
}
if [url_arg]{
kv{
source => "url_arg"
field_split => "&"
target => "url_args"
remove_field => ["url_arg","message"]
}
}
}
# ruby 插件
# 以？为分隔符，将 request 字段分成 url_path 和 url_arg
-------------------->
http://www.test.com?test
{
"url_arg" => "test",
"host" => "localhost",
"url_path" => "www.test.com",
"message" => "www.test.com?test",
"@version" => "1",
"@timestamp" =>  2018 - 06 - 26 T 07: 31 : 04.887 Z
}
http://www.test.com?title=elk&content=学习elk
{
"url_args" => {
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
```

###### urldecode

用于解码被编码的字段, 可以解决 URL 中中文乱码的问题

###### kv

通过指定分隔符将字符串分割成 key/value

```
"title" => "elk",
"content" => "学习 elk"
},
"host" => "localhost",
"url_path" => "www.test.com",
"@version" => "1",
"@timestamp" =>  2018 - 06 - 26 T 07: 33 : 54.507 Z
}
```
```
35
36
37
38
39
40
41
42
43
```
```
urldecode{
field => "message"
}
```
```
# field : 指定 urldecode 过滤器要转码的字段, 默认值是"message"
# charset (缺省): 指定过滤器使用的编码. 默认 UTF- 8
```
```
1 2 3 4 5 6 7
```
```
kv{
prefix => "url_" #给分割后的key加前缀
target => "url_ags" #将分割后的key-value放入指定字段
source => "message" #要分割的字段
field_split => "&" #指定分隔符
remove_field => "message"
}
-------------------------->
a= 1 &b= 2 &c= 3
{
"host" => "localhost",
"url_ags" => {
"url_c" => "3",
"url_a" => "1",
"url_b" => "2"
},
"@version" => "1",
"@timestamp" => 2018 - 06 - 26 T 07: 07 : 24.557 Z
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
```

###### useragent

添加有关用户代理 (如系列, 操作系统, 版本和设备) 的信息

#### 数据输出

```
output 配置定义了数据输出目标
```
###### stdout

将数据输出到屏幕上

```
if [agent] != "-" {
useragent {
source => "agent"
target => "ua"
remove_field => "agent"
}
}
# if 语句，只有在 agent 字段不为空时才会使用该插件
#source 为必填设置, 目标字段
#target 将 useragent 信息配置到 ua 字段中。如果不指定将存储在根目录中
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
input{
file{
path=>"/home/order. log"
discover_interval => 10
start_position => "beginning"
}
}
output{
stdout { codec => rubydebug }
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```

###### file

将数据写入文件

读取指定文件-输出到文件

ps: 需要注意的是这里的输出文件必须要求 w 的权限看看是否报错

如果报错需要进入容器赋权

###### kafka

数据发送到 kafka

###### elasticseach

数据存储到 elasticseach 中

读取指定文件-输出到 es

```
input{
file{
path=>"/home/order. log"
discover_interval => 10
start_position => "beginning"
}
}
output{
file{
path=>"/home/aaa. log"
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```
```
kafka{
bootstrap_servers => "localhost: 9092"
topic_id => "test_topic" #必需的设置 。生成消息的主题
}
```
```
1
2
3
4
```
```
input{
file{
path=>"/home/order. log"
discover_interval => 10
start_position => "beginning"
}
}
output{
elasticsearch{
hosts=>["172.30.66.86:9200"]
index => "test-%{+YYYY. MM. dd}"
}
}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```

#### Kibana 查看应用日志

```
实操过程，请参见 23 章视频：《100 Wqps 超高并发日志平台》实操
```
```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
###### 1 查看应用日志

###### 2 如何搜索日志


###### 3 如何查看指定时间的应用日志

```
->右上角选择时间
```
###### 4 如何定位错误日志

```
Search 框输入 error -> Refresh
(有自己的语法规则, 要搜索一下)
```
###### 5 如何展开显示日志


```
连续点开两个箭头
```
```
关于本文的代码，和技术问题，请来尼恩发起的 Java 高并发疯狂创客圈社群交流，
```
#### es 的安全认证

通常搭建的 elk 默认是不需要身份认证, 这样就会把数据暴露在外网, 因此会显得非常危险。

下面我们介绍如何为 es 加入身份认证
es 身份认证参考链接

```
切记, 这里修改 es 配置文件和启动 es 的二进制文件的时候一定要用 es 系统用户不要用 ubuntu 或
root 用户操作。不然会报错。
```
配置了安全认证后 logstash + filebeat +es +kibfana 都需要在配置文件中加入访问的账号密码来认
证。

logstash 配置文件

kibfana 配置文件

配置 Kibana 以使用内置 kibana 用户和您创建的密码

```
elasticsearch {
hosts => ["ip: 9200"]
user => elastic --加入 es 用户
password => xxxx --加入 es 密码
index => "test-%{+YYYY-MM-dd}"
timeout => 300
}
```
```
1 2 3 4 5 6 7
```
```
server. port: 5601
server. host: "0.0.0.0"
elasticsearch. hosts: ["http://localhost:9200"]
kibana. index: ". kibana"
i 18 n. locale: "zh-CN" --配置 kibana 显示中文
elasticsearch. username: "kibana" --加入 kibana 账户
elasticsearch. password: "123456" --加入 kibana 账户的密码
```
```
1 2 3 4 5 6 7
```

#### 配置 elk 的 ElastAlert 预警插件

```
我们都知道 elk 架构是收集与分析集群的日志方便开发排错
```
```
但是预警功能是缺一不可的，如果开发人员不能及时查看线上错误日式, 这个时候就需要我们的预
警插件来实现实时推送告警。
ElastAlert : 是 python 开发一个插件因此需要配合 python 运行环境和 python-pip 包管理工具, 以及
相关依赖包
```
1. 安装相关依赖包

2. 安装 python 运行环境

配置

到此 python 环境配置完成

3. 安装 elastalert

下载源码

```
yum -y install openssl openssl-devel gcc gcc-c++ --centos 系统安装方式
--ubuntu 安装方式
sudo apt-get install openssl --openssl 依赖包
sudo apt-get install libssl-dev  --openssl-devel 依赖包
sudo apt-get install build-essential --gcc 依赖包注意: gcc 和 g++版本必须一致
sudo apt-get install g++ 7 .4  --g++ 依赖包
g++ --version --查看版本
gcc --version
wget https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tgz --下载二进制
python 源码
```
```
1 2 3 4 5 6 7 8 9
```
```
tar xf Python-3.6.9. tgz
cd Python-3.6.9./configure --prefix=/usr/local/python --with-openssl
make && make install  --编译源码
```
```
1
2
3
```
```
mv /usr/bin/python /usr/bin/python_old //把 ubuntu 自带的 python 2.7 环境移出到另外
一个文件夹
ln -s /usr/local/python/bin/python 3 /usr/bin/python //建立 python 软链接
ln -s /usr/local/python/bin/pip 3 /usr/bin/pip //建立 pip 软链接
pip install --upgrade pip //此处没有安装 pip 的需要去安装 pip
sudo apt install python 3-pip //安装 pip 3.0 版本对应了 python 3 .6.9 版本
//此处我没有动 ubuntu 自带的 python 2.7 版本的因此我们使用新的 python 使用 3.6.9 时, 按以下方
式使用:
python 3.6 --version
python 2.7 --version
pip 3 --version
//使用 python 和 pip 命令时都改为 python 3.6 与 pip 3
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
git clone https://github.com/Yelp/elastalert.git //下载源码
cd elastalert
pip 3 install "elasticsearch<8,>7"
//因为我们的 es 是 7.4.0，所以这里选用的版本是这个
pip 3 install -r requirements. txt 用 pip 安装依赖
```
```
1
2
3
4
5
```

安装成功时候 /usr/local/python/bin/目录下会有四个文件

**4. 配置 ElastAlert**
配置 config. yaml 文件 (创建)

```
rules_folder：ElastAlert 从中加载规则配置文件的位置。它将尝试加载文件夹中的每个. yaml 文
件。
```
```
没有任何有效规则，ElastAlert 将无法启动。
```
```
ls /usr/local/python/bin/elastalert* 或者这个目录下
ls /usr/local/bin/elastalert*
```
```
1
2
```
```
ln -s /usr/local/python/bin/elastalert* /usr/bin //建立软链接把这四个命令链接到
bin 目录下
```
```
1
```
```
cp config. yaml. example config. yaml
sudo vi config. yaml
```
```
1
2
```

```
run_every： ElastAlert 多久查询一次 Elasticsearch 的时间。
```
```
buffer_time：查询窗口的大小，从运行每个查询的时间开始向后延伸。对于其中
use_count_query 或 use_terms_query 设置为 true 的规则，将忽略此值。
```
```
es_host：是 Elasticsearch 群集的地址，ElastAlert 将在其中存储有关其状态，查询运行，警报和错
误的数据。
es_port：es 对应的端口。
```
```
es_username： 可选的; 用于连接的 basic-auth 用户名 es_host。
```
```
es_password： 可选的; 用于连接的 basic-auth 密码 es_host。
```
```
es_send_get_body_as： 可选的; 方法查询 Elasticsearch - GET，POST 或 source。
```
```
默认是 GET writeback_index：ElastAlert 将在其中存储数据的索引的名称。我们稍后将创建此索
引。
```
```
alert_time_limit： 失败警报的重试窗口。
```
创建 elastalert-create-index 索引告警索引

**5. 配置 Rule 告警规则配置**
所有的告警规则，通过在 example_rules 目下创建配置文件进行定义，这里简单创建一个来作为演示

```
$ elastalert-create-index
New index name (Default elastalert_status)
Name of existing index to copy (Default None)
New index elastalert_status created
Done!
```
```
1
2
3
4
5
```
```
name: Nginx_err //规则名称
use_strftine_index: true
index: 10 .0.0.153-system_cro-2020.11.18 //监听查询 es 的索引
type: any //告警规则类型有很多种这种是只要匹配到就触发告警
aggregation:
seconds: 1 //告警频率
filter:
```
- query:
query_string:
query: "status: 500 or status: 404" //触发报警的匹配条件这里可以用 kibana
的语法去匹配
num_events: 1 //事件触发次数的贬值
timeframe:
minutes: 1 //一分钟内超过 num_envents 触发的次数就触发告警
alert:
- "email" //告警类型此处是 email 例如钉钉企业微信
email_format: html //email 正文格式
alert_subject: "正式环境 Error 告警" //告警正文标题
alert_text_type: alert_text_only //正文类型

```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
11
12
13
14
15
16
17
18
```

```
alert_text: "<br><br><h3>告警详情</h3><table><tr><td style='padding: 5 px; text-
align: right; font-weight: bold; border-radius: 5 px; background-color:
#eef ;'>@timestamp:</td><td style='padding: 5 px; border-radius: 5 px; background-
color: #eef ;'>{}</td></tr><tr><td style='padding: 5 px; text-align: right; font-
weight: bold; border-radius: 5 px; background-color: #eef ;'>@version:</td><td
style='padding: 5 px; border-radius: 5 px; background-color: #eef ;'>{}</td></tr>
<tr><td style='padding: 5 px; text-align: right; font-weight: bold; border-
radius: 5 px; background-color: #eef ;'>_id:</td><td style='padding: 5 px; border-
radius: 5 px; background-color: #eef ;'>{}</td></tr><tr><td
style='padding: 5 px; text-align: right; font-weight: bold; border-radius:
5 px; background-color: #eef ;'>_index:</td><td style='padding: 5 px; border-
radius: 5 px; background-color: #eef ;'>{}</td></tr><tr><td
style='padding: 5 px; text-align: right; font-weight: bold; border-radius:
5 px; background-color: #eef ;'>ip:</td><td style='padding: 5 px; border-radius:
5 px; background-color: #eef ;'>{}</td></tr><tr><td style='padding: 5 px; text-
align: right; font-weight: bold; border-radius: 5 px; background-color:
#eef ;'>request:</td><td style='padding: 5 px; border-radius: 5 px; background-
color: #eef ;'>{}</td></tr><tr><td style='padding: 5 px; text-align:
right; font-weight: bold; border-radius: 5 px; background-color: #eef ;'>status:
</td><td style='padding:5px;border-radius: 5px;background-color: #eef;'>{}
</td></tr><tr><td style='padding: 5 px; text-align: right; font-weight:
bold; border-radius: 5 px; background-color: #eef ;'>method:</td><td
style='padding: 5 px; border-radius: 5 px; background-color: #eef ;'>{}</td></tr>
<tr><td style='padding: 5 px; text-align: right; font-weight: bold; border-
radius: 5 px; background-color: #eef ;'>bytes:</td><td
style='padding: 5 px; border-radius: 5 px; background-color: #eef ;'>{}</td></tr>
<tr><td style='padding: 5 px; text-align: right; font-weight: bold; border-
radius: 5 px; background-color: #eef ;'>source:</td><td
style='padding: 5 px; border-radius: 5 px; background-color: #eef ;'>{}</td></tr>
<tr><td style='padding: 5 px; text-align: right; font-weight: bold; border-
radius: 5 px; background-color: #eef ;'>client_ip:</td><td
style='padding: 5 px; border-radius: 5 px; background-color: #eef ;'>{}</td></tr>
<tr><td style='padding: 5 px; text-align: right; font-weight: bold; border-
radius: 5 px; background-color: #eef ;'>httpversion:</td><td
style='padding: 5 px; border-radius: 5 px; background-color: #eef ;'>{}</td></tr>
</table>" //正文内容
alert_text_args:
```
- "@timestamp" //使用的是 python 的 format 格式动态填充数据
- "@version" //这些是属性值按顺序对饮正文内容里面的 {}
- _id
- _index
- host. name
- request
- status
- method
- bytes
- message
- remote_ip
- httpversion
email:
- " xxx@xx.com " //收件人多个请依次往下填写
- " xxxx@qq.com "
- " xxxx@xx.com "
smtp_host: smtp. mxhichina. com //邮件服务器
smtp_port: 25 //邮件端口
smtp_auth_file: /home/ubuntu/elk/alert/elastalert/smtp_auth_file. yaml //此处
新建了一个文件是发件人的认证文件存放发件人账户和密码或授权码
from_addr: haoyacong@gimmake.com //发件人

19

20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39

40


运行 ElastAlert

## Prometheus+Grafana 检测预警

#### 什么是性能可观测

可观测性包括 Metrics、Traces、Logs 3 个维度。可观测能力帮助我们在复杂的分布式系统中快速排
查、定位问题，是分布式系统中必不可少的运维工具。

在性能压测领域中，可观测能力更为重要，除了有助于定位性能问题，其中 Metrics 性能指标更直接决定
了压测是否通过，对系统上线有决定性左右，具体如下：

**- Metrics，监控指标**

系统性能指标，包括请求成功率、系统吞吐量、响应时长

资源性能指标，衡量系统软硬件资源使用情况，配合系统性能指标，观察系统资源水位

```
41 email_reply_to: haoyacong@gimmake.com //收件人标头
```
```
cd ElastAlert //ElastAlert 的安装目录
python 3.6 -m elastalert. elastalert --verbose --config config. yaml --rule
./example_rules/nginx_404. yaml //指定告警规则文件
nohup python 3.6 -m elastalert. elastalert --verbose --config config. yaml --
rule ./example_rules/nginx_404. yaml & //在后台运行
//如果运行多个告警规则执行多个上面的命令如果执行 example_rules 下的全部规则文件使用以下
命令:
nohup python 3.6 -m elastalert. elastalert --verbose --config config. yaml &
```
```
1
2
```
```
3
```
```
4
```
```
5
```

**- Logs，日志**

施压引擎日志，观察施压引擎是否健康，压测脚本执行是否有报错

采样日志，采样记录 API 的请求和响应详情，辅助排查压测过程中的一些出错请求的参数是否正常，并
通过响应详情，查看完整的错误信息

**- Traces，分布式链路追踪** 用于性能问题诊断阶段，通过追踪请求在系统中的调用链路，

定位报错 API 的报错系统和报错堆栈，快速定位性能问题点

本篇阐述如何使用 Prometheus 实现性能压测 Metrics 的可观测性。

###### 系统监控的核心指标

**系统性能指标**

压测监控最重要的 3 个指标：请求成功率、服务吞吐量 (TPS)、请求响应时长 (RT)，这 3 个指标任意一个
出现拐点，都可以认为系统已达到性能瓶颈。

这里特别说明下响应时长，对于这个指标，用平均值来判断很有误导性，因为一个系统的响应时长并不
是平均分布的，往往会出现长尾现象，表现为一部分用户请求的响应时间特别长，但整体平均响应时间
符合预期，这样其实是影响了一部分用户的体验，不应该判断为测试通过。因此对于响应时长，常用
99 、 95 、 90 分位值来判断系统响应时长是否达标。

另外，如果需要观察请求响应时长的分布细节，可以补充请求建联时长 (Connect Time)、等待响应时长
(Idle Time) 等指标。

**资源性能指标**

压测过程中，对系统硬件、中间件、数据库资源的监控也很重要，包括但不限于：

- CPU 使用率
- 内存使用率
- 磁盘吞吐量
- 网络吞吐量
- 数据库连接数
- 缓存命中率
... ...

**什么是 Load 1、Load 5 负载指标？**

在 Linux 机器上，有多个命令都可以查看机器的负载信息。其中包括 uptime、top、w 等。

**uptime 命令**

uptime 命令能够打印系统总共运行了多长时间和系统的平均负载。

uptime 命令可以显示的信息显示依次为：现在时间、系统已经运行了多长时间、目前有多少登陆用户、
系统在过去的 1 分钟、 5 分钟和 15 分钟内的平均负载。


这行信息的后半部分，显示”load average”，它的意思是”系统的平均负荷”，

里面有三个数字，我们可以从中判断系统负荷是大还是小。1.74 1.87 1.97 这三个数字的意思分别是 1 分
钟、 5 分钟、 15 分钟内系统的平均负荷。 **我们一般表示为 load 1、load 5、load 15。**

一般情况下，大家最好根据自己机器的实际情况，建立一个指标的基线（如近一个月的平均值），只要
日常的 load 在基线上下范围内不太大都可以接收，如果差距太多可能就要人为介入检查了。

大部分情况下的建议的值:

```
当系统负荷持续大于 0.75，你必须开始调查了，问题出在哪里，防止情况恶化。
```
```
当系统负荷持续大于 1.0，你必须动手寻找解决办法，把这个值降下来。
```
```
当系统负荷达到 5.0，就表明你的系统有很严重的问题，长时间没有响应，或者接近死机了。你不
应该让系统达到这个值。
```
以上指标都是基于单 CPU 的，但是现在很多电脑都是多核的。所以，对一般的系统来说，是根据 cpu 数

量去判断系统是否已经过载（Over Load）的。如果我们认为 0.75 算是单核机器负载的安全线的话，那
么四核机器的负载最好保持在 3 (4*0.75 = 3) 以下。

还有一点需要提一下，在 Load Avg 的指标中，有三个值， 1 分钟系统负荷、 5 分钟系统负荷， 15 分钟系统
负荷。我们在排查问题的时候也是可以参考这三个值的。

一般情况下， 1 分钟系统负荷表示最近的暂时现象。 15 分钟系统负荷表示是持续现象，并非暂时问题。
如果 load 15 较高，而 load 1 较低，可以认为情况有所好转。反之，情况可能在恶化。

#### 什么是 prometheus

phometheus: 当前一套非常流行的开源监控和报警系统。

###### prometheus 的运行原理

通过 HTTP 协议周期性抓取被监控组件的状态。输出被监控组件信息的 HTTP 接口称为 exporter。

常用组件大部分都有 exporter 可以直接使用，比如 haproxy, nginx，Mysql, Linux 系统信息（包括磁盘、
内存、CPU、网络等待）。

###### prometheus 主要特点

```
一个多维数据模型（时间序列由 metrics 指标名字和设置 key/value 键/值的 labels 构成）。
非常高效的存储，平均一个采样数据占~3.5 字节左右， 320 万的时间序列，每 30 秒采样，保持 60
天，消耗磁盘大概 228 G。
一种灵活的查询语言（PromQL）。
无依赖存储，支持 local 和 remote 不同模型。
采用 http 协议，使用 pull 模式，拉取数据。
监控目标，可以采用服务器发现或静态配置的方式。
多种模式的图像和仪表板支持，图形化友好。
```
```
[ root@cdh1 ~]# uptime
17:35:43 up 0 min, 2 users, load average: 4.10, 0.92, 0.30
```
```
[ root@cdh1 ~]# uptime
17:35:43 up 23:41, 3 users, load averages: 1.74 1.87 1.97
```
```
1 2 3 4 5 6
```

```
通过中间网关支持推送时间。
```
#### 什么是 Grafana

虽然 Prometheus 提供的 Web UI 也可以很好的查看不同指标的视图，但是这个功能非常简单，只适合
用来调试。要实现一个强大的监控系统，还需要一个能定制展示不同指标的面板，能支持不同类型的展
现方式（曲线图、饼状图、热点图、TopN 等），这就是仪表盘（Dashboard）功能。

因此 Prometheus 开发了一套仪表盘系统 PromDash，不过很快这套系统就被废弃了，官方开始推荐使
用 Grafana 来对 Prometheus 的指标数据进行可视化，这不仅是因为 Grafana 的功能非常强大，而且
它和 Prometheus 可以完美的无缝融合。

Grafana 是一个用于可视化大型测量数据的开源系统，它的功能非常强大，界面也非常漂亮，

使用它可以创建自定义的控制面板，你可以在面板中配置要显示的数据和显示方式，

它支持很多不同的数据源，比如：Graphite、InfluxDB、OpenTSDB、Elasticsearch、Prometheus
等，而且它也支持众多的插件。

下面我们就体验下使用 Grafana 来展示 Prometheus 的指标数据。首先我们来安装 Grafana，我们使用
最简单的 Docker 安装方式：

运行上面的 docker 命令，Grafana 就安装好了！你也可以采用其他的安装方式，参考官方的安装文
档。安装完成之后，我们访问 [http://localhost:3000/](http://localhost:3000/) 进入 Grafana 的登陆页面，输入默认的用户
名和密码（admin/admin）即可。

**第一条战线：Prometheus 如何监控机器？**

采用标准的 PGOne 技术组件 Prometheus Server + Grafana + node_exporter 完成对机器的性能监控。

**第二条战线：Prometheus 如何监控 flink？**

采用技术组件 client lib（flink-metrics-prometheus_x.jar） + PushGateway + Prometheus Server +
Grafana 完成对 flink 的监控。

#### Prometheus 的体系结构

```
1 $ docker run -d -p 3000:3000 grafana/grafana
```

下图说明了 Prometheus 的体系结构及其某些生态系统组件：

Prometheus 直接或通过中间推送网关从已检测的作业中删除指标，以处理短暂的作业。它在本地存储
所有报废的样本，并对这些数据运行规则，以汇总和记录现有数据中的新时间序列，或生成警报。
Grafana 或其他 API 使用者可以用来可视化收集的数据。

#### Prometheus+Grafana 分层架构

从分层的角度来说，Prometheus+Grafana 分层架构图如下：

###### Promcthcus 体系涉及的组件


Promcthcus 负责时序型指标数据的采集及存储，但数据的分析、聚合及直观展示以及告警等功能并非由
Prometheus Server 所负责。

Prometheus 生态系统包含多个组件，其中许多是可选的：

```
Prometheus server - 收集和存储时间序列数据
Client Library: 客户端库，为需要监控的服务生成相应的
metrics 并暴露给 - Prometheus server。当 Prometheus server 来 pull 时，直接返回实时状态的
metrics。
pushgateway - 对于短暂运行的任务，负责接收和缓存时间序列数据，同时也是一个数据源
exporter - 各种专用 exporter，面向硬件、存储、数据库、HTTP 服务等
alertmanager - 处理报警
webUI 等，其他各种支持的工具，本身的界面值适合用来语句查询，数据可视化，需要第三方组
件，比如 Grafana。
```
###### 如何收集度量值


度量指标由监控系统执行的过程通常可以分为两种方法：推和拉。

Prometheus 基于 HTTP call，从配置文件中指定的网络端点 (endpoint）上周期性获取指标数据。

Prometheus 支持通过三种类型的途径从目标上“抓取 (Serape)”指标数据：

```
Exporters：被监控的目标不支持 pro 的数据格式，通过 exporters 抽取指标数据，进行格式化处理
成 pro 兼容的数据格式，再响应给 pro server。
Instrumentation：应用系统内建了 pro 兼容的指标数据格式，pro server 可以直接采集。
Push gateway：pro 采用 pull 模式，可能由于不在一个子网或者防火墙原因，导致 Prometheus
无法直接拉取各个 target 数据。在监控业务数据的时候，需要将不同数据汇总, 由 Prometheus 统
一收集。暂存在 pushgateway，等待 Prometheus server 拉取。
```

#### 指标类型

Promcthcus 使用 4 种方法来描述监视的指标。

###### 计数器

Counter:

计数器，用于保存单调递增型的数据，例如站点访问次数等；不能为负值，也不支持减少，但可以重置
回 0 ；

比如网络总访问数。

###### 仪表盘

Gauge: 仪表盘，用于存储有着起伏特征的指标数据，例如内存空闲大小等。

Gauge 是 Counter 的超集; 但存在指标数据丢失的叮能性时，Counter 能让用户确切了解指标随时间的变
化状态，而 Gauge 则可能随时问流逝而精准度越来越低。

比如 CPU 的使用率：

###### 直方图

在大多数情况下人们都倾向于使用某些量化指标的平均值，例如 CPU 的平均使用率、页面的平均响应时
间。这种方式的问题很明显，以系统 API 调用的平均响应时间为例: 如果大多数 API 请求都维持在 100 ms 的
响应时间范围内，而个别请求的响应时间需要 5 s，那么就会导致某些 WEB 页面的响应时间落到中位数的
情况，而这种现象被称为长尾问题。

Histogram 常常用于观察，一个 Histrogram 包含下列值的合并:

```
Buckets: 桶是观察的计数器。它应该有个最大值的边界和最小值的边界。它的格式为 basename>
_bucket
观察结果的和，这是所有观案的和。针对它的格式是《basename>_sum
观察结果统计，这是在本次观察的和。它的格式为《basename>_count
```
###### Summary

Summary 与 Histogram 类型类似，用于表示段时间内的数据采集结果 (通常是请求持或时间或响应大
小），但它直接存储了分位数通过客户端计算，然后展示出来，而不是通过区间来计算。

#### 指标摘要及聚合


###### 指标摘要

通常米说，单个指标对我们价值很小，往往需要联合并可视化多个指标，这其中需要一些数学变换，例
如，我们可能会统计函数应用于指标或指标组，一些可能应用常见函数包括:

```
计数: 计算特定时间间降内的观索点数，
求和: 将特定时间间隔内所有观察点的值累计相加。
平均值，提供特定时间间隔内所有伯的平均值，
中间数: 数值的几何中点，正好 50%的数值位于它前面，而另外 50%则位于它后面，
百分位数: 度量占总数特定百分比的观察点的值。
标准差: 显示指标分布中与平均值的标准差，这可以测量出数据集的差异程度。标准差为 0 表示数据
都等于平均值较高的标准兰意味着数据分布的范围很广
变化率: 显示时间序列中数据之间的变化程度。
```
###### 指标聚合

除了上述的指标描要外，你可能经常希望能看到来自当个源的指标的聚合图，例如所有应用程序服务器
的磁盘空间使用情况。指标聚合最典型的样式就是在一张图上显示多个指标，这有助于你识别环境的发
展趋势。

例如，负载均衡器中的问歇性故障可能导致多个服务器的 Web 流量下降，这通常比通过查看每个单独的
指标更容易发现。

#### 一键安装 prometheus

**bridge 网络管理**

不指定网络驱动时默认创建的 bridge 网络

查看网络内部信息

列所有列表的网络

移除指定的网络

```
1 docker network create monitor-network
```
```
1 docker network inspect monitor-network
```
```
1 docker network  ls
```
```
1 docker network rm default_network
```

**创建库**

注意，数据库提前建好 grafana 库：

###### docker 编排文件

```
/usr/bin/mysql -uroot -p 123456 --connect-expired-password <<EOF
```
```
create database grafana default character set utf 8 mb 4 collate
utf 8 mb 4_unicode_ci;
```
```
grant all privileges on grafana.* to root@'%' identified by '123456' WITH
GRANT OPTION;
```
```
flush privileges;
```
```
EOF
```
```
1 2 3 4 5 6 7 8 9
```
```
version: "3.5"
services:
prometheus:
image: prom/prometheus: v 2.30.0
container_name: prometheus
volumes:
```
- /tmp:/tmp
- ./prometheus. yml:/etc/prometheus/prometheus. yml
command: "--config. file=/etc/prometheus/prometheus. yml --
storage. tsdb. path=/prometheus --storage. tsdb. wal-compression --
storage. tsdb. retention. time=7 d"
hostname: prometheus
ports:
- "9090:9090"
depends_on:
- exporter
networks:
base-env-network:
aliases:
- prometheus

```
prometheus-nacos-sd:
image: nien/prometheus-nacos-sd: 1.0.0
container_name: prometheus-nacos-sd
hostname: prometheus-nacos-sd
volumes:
```
- /tmp:/tmp
command: "--nacos. address=192.168.56.121:8848 --nacos. namespace=public -
-output. file=/tmp/nacos_sd_public. json --refresh. interval=600"
networks:
base-env-network:
aliases:
- prometheus-nacos-sd

```
exporter:
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
```
```
27
28
29
30
31
32
```

```
image: prom/node-exporter: v 1.2.2
container_name: exporter
hostname: exporter
ports:
```
- "9100:9100"
networks:
base-env-network:
aliases:
- exporter

```
# 用于 UI 展示
# https://grafana.com/docs/grafana/latest/installation/docker
grafana:
image: grafana/grafana: 8.1.5
container_name: grafana
hostname: grafana
restart: unless-stopped
ports:
```
- "3000:3000"
# volumes:
# - "./grafana/grafana-storage:/var/lib/grafana"
# - "./grafana/public:/usr/share/grafana/public" # 这里面可处理汉化包可参
考 https://github.com/WangHL0927/grafana-chinese
# - "./grafana/conf:/usr/share/grafana/conf"
# - "./grafana/log:/var/log/grafana"
# - "/etc/localtime:/etc/localtime"
environment:
GF_EXPLORE_ENABLED: "true"
GF_SECURITY_ADMIN_PASSWORD: "admin"
GF_INSTALL_PLUGINS: "grafana-clock-panel, grafana-simple-json-
datasource, alexanderzobnin-zabbix-app"
# 持久化到 mysql 数据库
GF_DATABASE_URL: "mysql://root: 123456@192.168.56.121 : 3306/grafana" #
TODO 修改
depends_on:
- prometheus
networks:
base-env-network:
aliases:
- grafana

```
# mysql 数据库 => 用于 grafana 持久化数据
# mysql:
# image: nien/mysql: 5.7
# container_name: prometheus_mysql
# restart: unless-stopped
# volumes:
# - "./mysql 5.7/my. cnf:/etc/mysql/my. cnf"
# - "./mysql 5.7/data:/var/lib/mysql"
# - "./mysql 5.7/log/mysql/error. log:/var/log/mysql/error. log"
# environment:
# TZ: Asia/Shanghai
# LANG: en_US. UTF-8
# MYSQL_ROOT_PASSWORD: root # 设置 root 用户密码
# MYSQL_DATABASE: grafana # 初始化数据库 grafana
# ports:
# - "3306:3306"
```
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54

55
56
57
58
59
60
61

62
63

64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87


###### 一键安装 prometheus 的脚本

###### 进入 prometheus

[http://promethus所在的服务器地址:9090](http://promethus所在的服务器地址:9090)

[http://cdh1:9090/](http://cdh1:9090/)

```
# docker network create base-env-network
networks:
base-env-network:
external:
name: "base-env-network"
```
```
88
89
90
91
92
93
```
```
rm -rf /home/docker-compose/prometheus
cp -rf /vagrant/sit-ware/prometheus /home/docker-compose/
```
```
cd /home/docker-compose/
```
```
chmod 777 -R prometheus
```
```
cd prometheus
```
```
docker-compose --compatibility up  -d
```
```
docker-compose logs -f
```
```
docker-compose down
```
```
mysql/5.7.31
```
```
docker save mysql: 5.7.31  -o /vagrant/3 G-middleware/mysql. 5.7.31. tar
```
```
docker save registry. cn-hangzhou. aliyuncs. com/zhengqing/canal-server: v 1.1.5
-o /vagrant/3 G-middleware/canal-server. v 1.1.5. tar
```
```
docker save registry. cn-hangzhou. aliyuncs. com/zhengqing/canal-admin: v 1.1.5
-o /vagrant/3 G-middleware/canal-admin. v 1.1.5. tar
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
```
```
25
26
```

点击 targets

###### 进入 grafana

grafana 默认密码 admin admin

[http://cdh1:3000/explore?left=%5B%22now-1h%22,%22now%22,%22--%20Grafana%20--%22,%7](http://cdh1:3000/explore?left=%5B%22now-1h%22,%22now%22,%22--%20Grafana%20--%22,%7)
B%7 D%5 D&orgId=1


啥也没有

#### Prometheus+Grafana 监控 SpringBoot 项目 JVM 信息

###### SpringBoot 项目配置 JVM 采集

（ 1 ）maven 依赖

（ 2 ）application. properties

（ 3 ）SpringBoot 添加 PrometheusMeterRegistry 注册器

```
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
<groupId>io. micrometer</groupId>
<artifactId>micrometer-registry-prometheus</artifactId>
</dependency>
```
```
1 2 3 4 5 6 7 8
```
```
spring. application. name=springboot_jvm
server. port= 6001
management. endpoints. web. exposure. include=*
management. metrics. tags. application=${spring. application. name}
```
```
1
2
3
4
```
```
@Configuration
public class CustomPrometheusConfig
{
@Bean
@ConditionalOnMissingBean
public PrometheusMeterRegistry prometheusMeterRegistry (PrometheusConfig
prometheusConfig,
```
```
1 2 3 4 5 6
```

（ 4 ）启动 springboot 项目，访问可以看到一些统计指标

[http://localhost:18081/consumer/actuator/prometheus](http://localhost:18081/consumer/actuator/prometheus)

[http://192.168.56.1:28088/seata-seckill/actuator/prometheus](http://192.168.56.1:28088/seata-seckill/actuator/prometheus)

###### Prometheus 配置

参考末尾的基于 nacos 的服务发现

访问地址：

[http://cdh1:9090/targets#pool-spring-boot](http://cdh1:9090/targets#pool-spring-boot)

```
CollectorRegistry
collectorRegistry, Clock clock) {
return new PrometheusMeterRegistry (prometheusConfig,
collectorRegistry, clock);
}
```
```
@Bean
@ConditionalOnMissingBean
public CollectorRegistry collectorRegistry () {
return new CollectorRegistry (true);
}
}
```
```
7
```
```
8
```
```
9
10
11
12
13
14
15
16
```

#### 配置 grafana 监控 Linux 系统

```
使用 node_explorer 可以暴露 Linux 系统的指标信息，
```
```
然后 Prometheus 就可以通过定时扫描的方式获取并存储指标信息了。
```
###### 使用 Exporter 收集指标

目前为止，我们看到的都还只是一些没有实际用途的指标，如果我们要在我们的生产环境真正使用
Prometheus，往往需要关注各种各样的指标，譬如服务器的 CPU 负载、内存占用量、IO 开销、入网和
出网流量等等。

正如上面所说，Prometheus 是使用 Pull 的方式来获取指标数据的，要让 Prometheus 从目标处获得数
据，首先必须在目标上安装指标收集的程序，并暴露出 HTTP 接口供 Prometheus 查询，这个指标收集
程序被称为 **Exporter** ，不同的指标需要不同的 Exporter 来收集，目前已经有大量的 Exporter 可供使
用，几乎囊括了我们常用的各种系统和软件，官网列出了一份常用 Exporter 的清单，各个 Exporter 都
遵循一份端口约定，避免端口冲突，即从 9100 开始依次递增，这里是完整的 Exporter 端口列表。

另外值得注意的是，有些软件和系统无需安装 Exporter，这是因为他们本身就提供了暴露 Prometheus
格式的指标数据的功能，比如 Kubernetes、Grafana、Etcd、Ceph 等。

这一节就让我们来收集一些有用的数据。


###### inux 直接安装 node_exporter

首先我们来收集服务器的指标，这需要安装 node_exporter，这个 exporter 用于收集 *NIX 内核的系
统，

如果你的服务器是 Windows，可以使用 WMI exporter。

这次我们直接把安装到 Linux 服务器上，将下载的安装包解压到指定目录，并修改文件夹名称：

进入解压目录，使用如下命令运行，服务将运行在 9100 端口上；

```
使用 curl 命令访问获取指标信息接口，获取到信息表示运行成功；
```
###### 使用 Docker 容器安装 node_exporter

如果使用 Docker 容器安装，监控的会是 Docker 容器的指标信息

```
cd /mydata
tar -zxvf node_exporter-1.1.2. linux-amd 64. tar. gz
mv node_exporter-1.1.2. linux-amd 64 node_exporter
```
```
1
2
3
4
```
```
cd node_exporter
./node_exporter >log. file 2 >&1 &
```
```
1
2
```
```
curl http://localhost:9100/metrics
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes
being served.# TYPE promhttp_metric_handler_requests_in_flight
gaugepromhttp_metric_handler_requests_in_flight 1# HELP
promhttp_metric_handler_requests_total Total number of scrapes by HTTP status
code.# TYPE promhttp_metric_handler_requests_total
counterpromhttp_metric_handler_requests_total{code="200"}
2175 promhttp_metric_handler_requests_total{code="500"}
0 promhttp_metric_handler_requests_total{code="503"} 0
```
```
1
2
```
```
exporter:
image: prom/node-exporter: v 1.2.2
container_name: exporter
hostname: exporter
ports:
```
- "9100:9100"
networks:
base-env-network:
aliases:
- exporter

```
1 2 3 4 5 6 7 8 9
```
```
10
```

**注意事项**

一般情况下，node_exporter 都是直接运行在要收集指标的服务器上的，官方不推荐用 Docker 来运行
node_exporter。如果逼不得已一定要运行在 Docker 里，要特别注意，这是因为 Docker 的文件系统和
网络都有自己的 namespace，收集的数据并不是宿主机真实的指标。可以使用一些变通的方法，比如运
行 Docker 时加上下面这样的参数：

关于 node_exporter 的更多信息，可以参考 node_exporter 的文档和 Prometheus 的官方指南
Monitoring Linux host metrics with the Node Exporter，

另外，Julius Volz 的这篇文章 How To Install Prometheus using Docker on Ubuntu 14.04 也是很好的
入门材料。

###### 创建一个任务定时扫描暴露的指标信息

接下来修改 Prometheus 的配置文件，创建一个任务定时扫描暴露的指标信息；

[http://exporter:9100/metrics](http://exporter:9100/metrics)

```
docker run -d \
--net="host" \
--pid="host" \
-v "/:/host: ro, rslave" \
quay. io/prometheus/node-exporter \
--path. rootfs /host
```
```
1 2 3 4 5 6
```
```
# 采集 node exporter 监控数据，即 linux
```
- job_name: exporter
static_configs:
- targets: ['exporter: 9100']
labels:
instance: exporter

```
1 2 3 4 5 6
```

###### 创建仪表盘 grafna

重启 Prometheus 容器，可以通过加号->Dashboard 来创建仪表盘；

https://grafana.com/grafana/dashboards/


###### 导入 Dashboard

选择导入 Dashboard 并输入 ID，最后点击 Load 即可；

导入之后


#### 选择数据源为 Prometheus

选择数据源为 Prometheus，最后点击 Import；

导入成功后就可以在 Grafana 中看到实时监控信息了，是不是够炫酷！


#### 配置 grafana 监控 SpringBoot 应用

###### 主要步骤

```
通过 Prometheus 提供的 Java client 包，在 spring boot 工程中生成我们关心的业务指标，
将 spring boot 工程打成 docker 镜像
部署 docker 容器
修改 Prometheus 对应的 file_sd_configs 文件，将部署的服务追加进去
通过 Grafana 观察业务指标
```
本文主要阐述的是对容器中业务指标的监控，对容器的监控以及环境的搭建参照 **一键部署脚本** 。

```
具体步骤，请参见视频
```
###### 找 jvm 的 dashboard


###### JVM Quarkus 面板

14370


导入后的效果如下：

#### Prometheus 数据模型

Prometheus 会将所有采集到的样本数据以时间序列 (time-series) 的方式保存在内存数据库中，并且定时
保存到硬盘上。

从根本上，说 Prometheus 存储的所有数据都是 **时间序列** 。

所以说： **Prometheus 中的数据，都是时间序列值。**

###### time-series 时间序列值

一条具有时间戳的数据流，只属于单个度量指标和该度量指标下的多个标签维度。

一条 Prometheus 数据，就是一条数据流，由一个指标名称（metric）和 N 个标签（label，N >= 0）
组成的，

例如:

```
http_requests_total 表示系统收到请求总数的时间序列，
```
```
同一指标中的每个 key/value (称为 label) 都称作一个维度，如
（http_requests_total{code=200}）代表请求成功的时间序列.
```
又例如:

这条数据的指标名称为 promhttp_metric_handler_requests_total，并且包含三个标签 code、
instance 和 job，这条记录的值为 106 。

在 springboot 应用中，可以通过 Prometheus 插件，看到大量的时间序列值，如下图所示

```
promhttp_metric_handler_requests_total{code="200", instance="192.168.0.107:909
0", job="prometheus"} 106
```
```
1
```

上面说过，Prometheus 是一个时序数据库，相同指标、相同标签的数据，构成一条时间序列。

外部应用通过 PromQL 来对时间序列进行相应的查找，

在查询时还可以利用 Prometheus 提供的功能丰富的函数对时间序列中的值进行计算。

每条 time-series 通过指标名称 (metrics name) 和一组标签集 (labelset) 命名。

换句话说：

```
每个序列通过指标名称 (metrics name) 以及一组标签集 (labelset) 命名唯一确定，
```
总之，time-series 是按照时间戳和值的序列顺序存放的，我们称之为向量 (vector)。

###### Sample 样本值

指标名称 (metrics name) 相同，label 相同的记录称为一个样本值，样本形成了实际的时间序列数据。

每个采样值包括：

```
一个 64 位的浮点值
一个精确到毫秒级的时间戳
```
Prometheus 一个时间序列，存的就是各个样本值，在不同时间点的数据

除了存储时间序列数据外，Prometheus 还可以生成临时派生的时间序列作为查询的结果。


###### metrics name 指标名称

metric 度量指标名称：

```
指定监控目标系统的测量特征
```
```
如：http_requests_total - 接收 http 请求的总计数。
```
metric 度量指标可能包含 ASCII 字母、数字、下划线和冒号，

```
注意：指标名称中的冒号，是保留用于用户定义的录制规则。
```
```
它们不应被 exporter 或直接仪表使用。
```
###### label 标签

标签开启了 Prometheus 的多维数据模型：

```
对于相同的度量名称，通过不同标签列表的结合, 会形成特定的度量维度实例。
```
例如：

如（http_requests_total{code=200}）代表请求成功的时间序列.

如（ /api/tracks{method=POST}） 所有包含度量名称为 /api/tracks 的 http 请求，打上

```
method=POST 的标签，则形成了具体的 http 请求。
```
这个查询语言在这些度量和标签列表的基础上进行过滤和聚合。

改变任何度量上的任何标签值，则会形成新的时间序列图。

标签 labels 值包含任意的 Unicode 码。

###### Notation (符号)

Notation (符号) 表示一个度量指标和一组键值对标签，需要使用以下符号：

```
[metric name]{[label name]=[label value], ...}
```
例如，度量指标名称是 api_http_requests_total，标签为 method="POST",

```
handler="/messages" 的示例如下所示：
```
```
api_http_requests_total{method="POST", handler="/messages"}
```
```
1 指标名称必须配正则表达式 [a-zA-Z_:][a-zA-Z 0-9_:]* 。
```
```
标签 label 名称的命名规则：
```
```
标签 label 名称可以包含 ASCII 字母、数字和下划线。
```
```
它们必须匹配正则表达式 [a-zA-Z_][a-zA-Z 0-9_]*。
带有 _ 下划线的标签名称被保留内部使用。
```
```
1 2 3 4 5 6
```

这些命名和 OpenTSDB 使用方法是一样的。

###### TSDB 时序数据库

什么是 TSDB？

```
TSDB (Time Series Database) 时序数据库，我们可以简单的理解为一个优化后用来处理时间序列
数据的软件，并且数据中的数组是由时间进行索引的。
```
```
OpenTSDB is a distributed, scalable Time Series Database (TSDB) written on top of HBase；
```
```
基于 Hbase 的分布式的，可伸缩的时间序列数据库。主要用途，就是做监控系统；譬如收集大规模
集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。
```
时间序列数据库的特点

1 、大部分时间都是写入操作

2 、写入操作几乎时顺序添加的，大多数示函到达后都以时间排序。

3 、写操作很少写入很久之前的数据，也很少更新数据。

4 、删除操作一般为区块删除，选定开始的历史时间并指定后续的区块。

5 、基本数据大，一般超过内存大小。

6 、读操作是十分典型的升序或者降序的顺序读

7 、高并发的读操作十分常见

指标名称要表明观察的目的（不是强制的）如（http_requests_total 表示系统 **收到请求总数** 的时间

序列），

同一指标中的每个 key/value (称为 label) 都称作一个维度，如（http_requests_total{code=200}）代

表请求成功的时间序列.

外部应用通过 PromQL 来对时间序列进行相应的查找，

在查询时还可以利用 Prometheus 提供的功能丰富的函数对时间序列中的值进行计算。

#### 度量指标类型

Prometheus 提供的客户端 jar 包中, 把指标类型分为如下四种

```
Counter
Gauge
Histogram
```
```
1 大多数情况在数据被采集到数秒或者数分钟后就会被写入数据库
```
```
1 很少单独删除某个时间或者分开的随机时间的数据。
```
```
1 一般选取的只是某一小部分且规律，缓存几乎不起任何作用。
```

```
Summary
```
**注意：**

这个分类只针对客户端使用者有效，在 Prometheus server 端是不进行区分的。

对于 Prometheus server 端而言，客户端返回的，都是时间序列对应的一个 Sample.

如 http_requests_total{code=200} 290，

表示 Prometheus server 拉取指标的这个时间点，请求成功的总数是 290 次.

一个 Sample，是一个纯文本数据，

**另外：**

即便不用 Prometheus 提供的客户端，只要返回的数据满足这种格式，Prometheus server 就能正常存
储.

```
注意：Prometheus 服务还没有充分利用这些类型，将来可能会发生改变。
```
###### Counter (计数器) 类型

counter 是表示单个单调递增计数器的 **累积度量值** ，其值只能在重启时增加或重置为零。

例如，您可以使用计数器来表示所服务的请求数，已完成的任务或错误。

**注意：**

不要使用计数器来暴露可能减少的值。

**另外：**

不要使用计数器来处理当前正在运行的进程数，而是使用 gauge。

**使用场景**

Counter 对应的指标值只能是一个单独的数值，并且除了能在服务启动时重置外，只能对指标值做累加
操作，不能做减法操作，

可以用来统计请求次数、任务执行次数、关键业务对象操作次数等。

###### Gauge (计量器、测量器)

gauge 是一个度量指标，它表示一个既可以递增, 又可以递减的值。

测量器主要测量类似于温度、当前内存使用量等，也

可以统计当前服务运行随时增加或者减少的线程数量

**使用场景**

Gauge 对应的指标值只能是一个单独的数值，

与 Counter 不同的是，可以对 Gauge 代表的指标值做加减操作，一般用来表示温度、正在执行的 job 等指
标


###### Histogram (柱状图、直方图)

Histogram 柱状图，不再是简单对指标的 sample 值进行加减等操作，对于每一个 sample 值执行下面的
三个操作：

```
根据 Histogram 定义时指定的 bucket 区间，将 sample 分到各个 bucket 中，每个 bucket 中存放的是
落入这个区间的个数
对每个采样点值累计和 (sum)
对采样点的次数累计和 (count)
```
**例子**

例如我们通过 Prometheus 提供的客户端通过

```
Histogram.build (). name ("job_execute_time"). help ("job 执行时间时间分布
```
（分）"). buckets (1,5,10) .register ();

定义了一个 histogram，用来统计 job 执行时间的分布。

对应的 buckets 是（ 1 ， 5 ， 10 ），代表四个区间

```
<=1 分钟
<=5 分钟
<=10 分钟
<无穷大
```
Histogram 会生成如下 6 个维度的指标值

当我们有一个 job 执行时间为 5.6 分钟，则对应的各个维度的值变成

```
无穷大的肯定是和 job_execute_time_count 一致的
```
可以看到 Histogram 类型的指标不会保留各个 sample 的具体数值，每个 bucket 中也只是记录样本数的
counter。

**原理**

```
job_execute_time_bucket{le="1.0",}
job_execute_time_bucket{le="5.0",}
job_execute_time_bucket{le="10.0",}
job_execute_time_bucket{le="+Inf",}
job_execute_time_count
job_execute_time_sum
```
```
1 2 3 4 5 6
```
```
job_execute_time_bucket{le="1.0",} 0 .0
job_execute_time_bucket{le="5.0",} 0 .0
job_execute_time_bucket{le="10.0",} 1 .0
job_execute_time_bucket{le="+Inf",} 1 .0
job_execute_time_count 1 .0
job_execute_time_sum 5 .6
```
```
1 2 3 4 5 6
```

直方图对观察结果进行采样（通常是请求持续时间或响应大小等），并将其计入可配置存储桶中。

它还提供所有观察值的总和。

基本度量标准名称为<basename>的直方图在 scrape 区间显示多个时间序列：

```
暴露的观察桶的累积计数器：<basename>_bucket{le="<upper inclusive bound>"}
所有观测值的总和：<basename>_sum
已观察到的事件数：<basename>_count，和<basename>_bucket{le="+Inf"}相同
```
使用 histogram_quantile 函数, 计算直方图或者是直方图聚合计算的分位数阈值。

一个直方图计算 Apdex 值也是合适的, 当在 buckets 上操作时，记住直方图是累计的。

详见直方图和总结

###### Summary

Summary 采样点分位图统计, 类似于 histgram，但是采用分位数来将 sample 分到不同的 bucket 中，

具体的区别查看 HISTOGRAMS AND SUMMARIES,

虽然它还提供观察的总数和所有观测值的总和，但它在滑动时间窗口上计算可配置的分位数。

基本度量标准名称<basename>的 summary 在 scrape 期间公开了多个时间序列：

```
流φ-quantiles (0 ≤ φ ≤ 1), 显示为<basename>{quantiles="[φ]"}
<basename>_sum，是指所有观察值的总和
<basename>_count, 是指已观察到的事件计数值
```
有关φ-分位数，Summary 用法和 histogram 图差异的详细说明，详见 histogram 和 summaries

有关 summaries 的客户端使用文档：

```
Go
Java
Python
Ruby
```
###### Summary 和 Histogram 的区分

**Summary** 和 **Histogram** 的一个区分要点是 **summary** 在客户端侧计算分位数，然后直接暴露它们，

然而 **histogram** 在客户端侧暴露桶的技术，然后在服务端侧使用 **histogram_quantitle ()** 函数来计算
分位数。


```
Histogram Summary
```
```
所需的配置合理定义直方图的区间
```
```
确定好分位数和滑动窗口。其
他的分位数和滑动窗口后面是
不能计算的
```
```
客户端性能监控容易，只需要累加 Counter 即可
```
```
监控比较昂贵，因为要计算分
位数
```
```
服务端性能
```
```
服务器必须计算分位数。如果临时计算的
时间过长（例如在大型仪表板中），则可
以使用[记录规则]
```
```
服务端消耗低
```
```
时间序列的个数
（除了_sum 和
_count）
```
```
每个桶一个时间序列每个分位数一个时间序列
```
```
分位数误差相关 bucket 的宽度定义决定了误差分位数是预先定义好的
```
```
时间滑动窗口的
指定分位数
```
```
使用普罗表达式的实时查询由 client 预定义好
```
```
聚合使用普罗表达式的实时查询通常来说不可聚合
```
#### 学习 PromQL

通过上面的步骤安装好 Prometheus 之后，我们现在可以开始体验 Prometheus 了。

Prometheus 提供了可视化的 Web UI 方便我们操作，直接访问 [http://localhost:9090/](http://localhost:9090/) 即可，它

默认会跳转到 Graph 页面

###### 数据模型

要学习 PromQL，首先我们需要了解下 Prometheus 的数据模型，

一条 Prometheus 数据由一个指标名称（metric）和 N 个标签（label，N >= 0）组成的，比如下面这
个例子：

这条数据的指标名称为 promhttp_metric_handler_requests_total，并且包含三个标签 code、

```
instance 和 job，这条记录的值为 106 。
```
上面说过，Prometheus 是一个时序数据库，相同指标相同标签的数据构成一条时间序列。

如果以传统数据库的概念来理解时序数据库，可以把指标名当作表名，标签是字段，timestamp 是主
键，还有一个 float 64 类型的字段表示值（Prometheus 里面所有值都是按 float 64 存储）。

这种数据模型和 OpenTSDB 的数据模型是比较类似的，详细的信息可以参考官网文档 Data model。另
外，关于指标和标签的命名，官网有一些指导性的建议，可以参考 Metric and label naming 。

虽然 Prometheus 里存储的数据都是 float 64 的一个数值，但如果我们按类型来分，可以把
Prometheus 的数据分成四大类：

```
promhttp_metric_handler_requests_total{code="200", instance="192.168.0.107:909
0", job="prometheus"} 106
```
```
1
```

```
Counter
Gauge
Histogram
Summary
```
Counter 用于计数，例如：请求次数、任务完成数、错误发生次数，这个值会一直增加，不会减少。
Gauge 就是一般的数值，可大可小，例如：温度变化、内存使用变化。Histogram 是直方图，或称为柱
状图，常用于跟踪事件发生的规模，例如：请求耗时、响应大小。它特别之处是可以对记录的内容进行
分组，提供 count 和 sum 的功能。Summary 和 Histogram 十分相似，也用于跟踪事件发生的规模，
不同之处是，它提供了一个 quantiles 的功能，可以按百分比划分跟踪的结果。例如：quantile 取值
0.95，表示取采样值里面的 95% 数据。更多信息可以参考官网文档 Metric types，Summary 和
Histogram 的概念比较容易混淆，属于比较高阶的指标类型，可以参考 Histograms and summaries 这
里的说明。

这四种类型的数据只在指标的提供方作区分，也就是上面说的 Exporter，如果你需要编写自己的
Exporter 或者在现有系统中暴露供 Prometheus 抓取的指标，你可以使用 Prometheus client
libraries，这个时候你就需要考虑不同指标的数据类型了。如果你不用自己实现，而是直接使用一些现
成的 Exporter，然后在 Prometheus 里查查相关的指标数据，那么可以不用太关注这块，不过理解
Prometheus 的数据类型，对写出正确合理的 PromQL 也是有帮助的。

###### PromQL 入门

我们从一些例子开始学习 PromQL，最简单的 PromQL 就是直接输入指标名称，比如：

这条语句会查出 Prometheus 抓取的所有 target 当前运行情况，譬如下面这样：

也可以指定某个 label 来查询：

这种写法被称为 **Instant vector selectors** ，这里不仅可以使用 = 号，还可以使用 !=、=~、!~，比
如下面这样：

```
=~ 是根据正则表达式来匹配，必须符合 RE 2 的语法。
```
和 Instant vector selectors 相应的，还有一种选择器，叫做 **Range vector selectors** ，它可以查出一
段时间内的所有数据：

这条语句查出 5 分钟内所有抓取的 HTTP 请求数，注意它返回的数据类型是 Range vector，没办法在
Graph 上显示成曲线图，一般情况下，会用在 Counter 类型的指标上，并和 rate () 或 irate () 函数

一起使用（注意 rate 和 irate 的区别）。

```
1 # 表示 Prometheus 能否抓取 target 的指标，用于 target 的健康检查``up
```
```
up{instance="192.168.0.107:9090", job="prometheus"}
 1 ``up{instance="192.168.0.108:9090", job="prometheus"}
 1 ``up{instance="192.168.0.107:9100", job="server"}
 1 ``up{instance="192.168.0.108:9104", job="mysql"} 0
```
```
1
```
```
1 up{job="prometheus"}
```
```
1 up{job!="prometheus"}``up{job=~"server|mysql"}``up{job=~"192\. 168\. 0\. 107.+"}
```
```
1 http_requests_total[5 m]
```

此外，PromQL 还支持 count、sum、min、max、topk 等聚合操作，还支持 rate、abs、
ceil、floor 等一堆的内置函数，更多的例子，还是上官网学习吧。如果感兴趣，我们还可以把

PromQL 和 SQL 做一个对比，会发现 PromQL 语法更简洁，查询性能也更高。

###### HTTP API

我们不仅仅可以在 Prometheus 的 Graph 页面查询 PromQL，Prometheus 还提供了一种 HTTP API 的
方式，可以更灵活的将 PromQL 整合到其他系统中使用，譬如下面要介绍的 Grafana，就是通过
Prometheus 的 HTTP API 来查询指标数据的。实际上，我们在 Prometheus 的 Graph 页面查询也是使
用了 HTTP API。

我们看下 Prometheus 的 HTTP API 官方文档，它提供了下面这些接口：

```
GET /api/v 1/query
GET /api/v 1/query_range
GET /api/v 1/series
GET /api/v 1/label/<label_name>/values
GET /api/v 1/targets
GET /api/v 1/rules
GET /api/v 1/alerts
GET /api/v 1/targets/metadata
GET /api/v 1/alertmanagers
GET /api/v 1/status/config
GET /api/v 1/status/flags
```
从 Prometheus v 2.1 开始，又新增了几个用于管理 TSDB 的接口：

```
POST /api/v 1/admin/tsdb/snapshot
POST /api/v 1/admin/tsdb/delete_series
POST /api/v 1/admin/tsdb/clean_tombstones
```
#### 告警和通知

至此，我们能收集大量的指标数据，也能通过强大而美观的面板展示出来。不过作为一个监控系统，最
重要的功能，还是应该能及时发现系统问题，并及时通知给系统负责人，这就是 Alerting（告警）。

Prometheus 的告警功能被分成两部分：一个是告警规则的配置和检测，并将告警发送给
Alertmanager，另一个是 Alertmanager，它负责管理这些告警，去除重复数据，分组，并路由到对应
的接收方式，发出报警。

常见的接收方式有：Email、PagerDuty、HipChat、Slack、OpsGenie、WebHook 等。

###### 配置告警规则

我们在上面介绍 Prometheus 的配置文件时了解到，它的默认配置文件 prometheus. yml 有四大块：
global、alerting、rule_files、scrape_config，其中 rule_files 块就是告警规则的配置项，alerting 块用
于配置 Alertmanager，这个我们下一节再看。现在，先让我们在 rule_files 块中添加一个告警规则文
件：

```
# 计算的是每秒的平均值，适用于变化很慢的 counter``# per-second average rate of
increase, for slow-moving counters``rate (http_requests_total[5 m])` `# 计算的是
每秒瞬时增加速率，适用于变化很快的 counter``# per-second instant rate of increase,
for volatile and fast-moving counters``irate (http_requests_total[5 m])
```
```
1
```

然后参考官方文档，创建一个告警规则文件 alert. rules：

这个规则文件里，包含了两条告警规则：InstanceDown 和 APIHighRequestLatency。顾名思义，
InstanceDown 表示当实例宕机时（up === 0）触发告警，APIHighRequestLatency 表示有一半的 API
请求延迟大于 1 s 时（api_http_request_latencies_second{quantile="0.5"} > 1）触发告警。配置好
后，需要重启下 Prometheus server，然后访问 [http://localhost:9090/rules](http://localhost:9090/rules) 可以看到刚刚配置

的规则

###### 使用 Alertmanager 发送告警通知

虽然 Prometheus 的 /alerts 页面可以看到所有的告警，但是还差最后一步：触发告警时自动发送通

知。这是由 Alertmanager 来完成的，我们首先下载并安装 Alertmanager，和其他 Prometheus 的组
件一样，Alertmanager 也是开箱即用的：

Alertmanager 启动后默认可以通过 [http://localhost:9093/](http://localhost:9093/) 来访问，但是现在还看不到告警，因为
我们还没有把 Alertmanager 配置到 Prometheus 中，我们回到 Prometheus 的配置文件
prometheus. yml，添加下面几行：

```
rule_files:
```
- "alert. rules"

```
1
2
```
```
groups:
```
- name: example
rules:

```
# Alert for any instance that is unreachable for >5 minutes.
```
- alert: InstanceDown
expr: up == 0
for: 5 m
labels:
severity: page
annotations:
summary: "Instance {{ $labels.instance }} down"
description: " {{ $labels.instance }} of job {{ $labels.job }} has been
down for more than 5 minutes."

```
# Alert for any instance that has a median request latency >1 s.
```
- alert: APIHighRequestLatency
expr: api_http_request_latencies_second{quantile="0.5"} > 1
for: 10 m
annotations:
summary: "High request latency on {{ $labels.instance }} "
description: " {{ $labels.instance }} has a median request latency
above 1 s (current value: {{ $value }} s)"

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```
```
14
15
16
17
18
19
20
21
```
```
$ wget
https://github.com/prometheus/alertmanager/releases/download/v0.15.2/alertman
ager-0.15.2. linux-amd 64. tar. gz
$ tar xvfz alertmanager-0.15.2. linux-amd 64. tar. gz
$ cd alertmanager-0.15.2. linux-amd 64
$ ./alertmanager
```
```
1
```
```
2
3
4
```

这个配置告诉 Prometheus，当发生告警时，将告警信息发送到 Alertmanager，Alertmanager 的地址
为 [http://192.168.0.107:9093。也可以使用命名行的方式指定](http://192.168.0.107:9093。也可以使用命名行的方式指定) Alertmanager：

这个时候再访问 Alertmanager，可以看到 Alertmanager 已经接收到告警了

下面的问题就是如何让 Alertmanager 将告警信息发送给我们了，我们打开默认的配置文件
alertmanager. ym：

参考官方的配置手册了解各个配置项的功能，其中 global 块表示一些全局配置；route 块表示通知路
由，可以根据不同的标签将告警通知发送给不同的 receiver，这里没有配置 routes 项，表示所有的告警
都发送给下面定义的 web. hook 这个 receiver；如果要配置多个路由，可以参考这个例子：

```
alerting:
alertmanagers:
```
- scheme: http
static_configs:
- targets:
- "192.168.0.107:9093"

```
1 2 3 4 5 6
```
```
1 $ ./prometheus -alertmanager. url=http://192.168.0.107:9093
```
```
global:
resolve_timeout: 5 m
```
```
route:
group_by: ['alertname']
group_wait: 10 s
group_interval: 10 s
repeat_interval: 1 h
receiver: 'web. hook'
receivers:
```
- name: 'web. hook'
webhook_configs:
- url: 'http://127.0.0.1:5001/'
inhibit_rules:
- source_match:
severity: 'critical'
target_match:
severity: 'warning'
equal: ['alertname', 'dev', 'instance']

```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
```
```
routes:
```
- receiver: 'database-pager'
group_wait: 10 s
match_re:
service: mysql|cassandra
- receiver: 'frontend-pager'
group_by: [product, environment]
match:
team: frontend routes:``- receiver: \'database-pager\'`` ``group_wait:
10 s`` ``match_re:`` ``service: mysql|cassandra` `- receiver: \'frontend-
pager\'`` ``group_by: [product, environment]`` ``match:`` ``team: frontend

```
1 2 3 4 5 6 7 8 9
```
```
10
```

紧接着，receivers 块表示告警通知的接收方式，每个 receiver 包含一个 name 和一个 xxx_configs，不
同的配置代表了不同的接收方式，Alertmanager 内置了下面这些接收方式：

```
email_config
hipchat_config
pagerduty_config
pushover_config
slack_config
opsgenie_config
victorops_config
wechat_configs
webhook_config
```
虽然接收方式很丰富，但是在国内，其中大多数接收方式都很少使用。最常用到的，莫属 email_config
和 webhook_config，另外 wechat_configs 可以支持使用微信来告警，也是相当符合国情的了。

其实告警的通知方式是很难做到面面俱到的，因为消息软件各种各样，每个国家还可能不同，不可能完
全覆盖到，所以 Alertmanager 已经决定不再添加新的 receiver 了，而是推荐使用 webhook 来集成自
定义的接收方式。可以参考这些集成的例子，譬如将钉钉接入 Prometheus AlertManager
WebHook。

#### 服务发现

由于 Prometheus 是通过 Pull 的方式主动获取监控数据，所以需要手工指定监控节点的列表，当监控的
节点增多之后，每次增加节点都需要更改配置文件，非常麻烦，这个时候就需要通过服务发现（service
discovery，SD）机制去解决。Prometheus 支持多种服务发现机制，可以自动获取要收集的 targets，
可以参考这里，包含的服务发现机制包括：azure、consul、dns、ec 2、openstack、file、gce、
kubernetes、marathon、triton、zookeeper（nerve、serverset），配置方法可以参考手册的
Configuration 页面。可以说 SD 机制是非常丰富的，但目前由于开发资源有限，已经不再开发新的 SD
机制，只对基于文件的 SD 机制进行维护。

关于服务发现网上有很多教程，譬如 Prometheus 官方博客中这篇文章 Advanced Service Discovery in
Prometheus 0.14.0 对此有一个比较系统的介绍，全面的讲解了 relabeling 配置，以及如何使用 DNS-
SRV、Consul 和文件来做服务发现。另外，官网还提供了一个基于文件的服务发现的入门例子，Julius
Volz 写的 Prometheus workshop 入门教程中也使用了 DNS-SRV 来当服务发现。

###### 为什么需要服务发现

Prometheus Server 的数据抓取工作基于 Pull 模型，因而，它必须要事先知道各 target 的位置，然后
才能从相应的 Exporter 或 Instrumentation 中抓取数据。
对于小型的系统环境，使用 static_configs 指定各 target 即可解决问题，但是对于较大的集群不适用，
尤其不适用于使用容器和基于云的实例的动态集群，因为这些实例会经常出现变化、创建、或销毁的情
况。Prometheus 为此专门设计了一组服务发现机制，以便于能够基于服务注册中心自动发现、检测、
分类可被监控的各 target ，以及更新发生了变动的 target。
Prometheus 可以集成到多种不同的开源服务发现工具上，以便动态发现需要监控的目标。
Prometheus 可以很好的集成到 Kubernetes 平台上，通过其 API Server 动态发现各类被监控的 Pod、
Service、Endpoint、Ingress 及 Node 对象，还支持基于文件实现的动态发现。


###### prometheus 目前支持的服务发现类型

prometheus 目前支持的服务发现类型主要有如下几种：
1 、基于文件的服务发现
2 、基于 consul 的服务发现
3 、基于 k 8 s API 的服务发现
4 、基于 eureka 的服务发现
5 、基于 nacos 的服务发现
6 、基于 DNS 的服务发现

```
# List of Azure service discovery configurations.
azure_sd_configs:
[ - <azure_sd_config> ... ]
```
```
# List of Consul service discovery configurations.
consul_sd_configs:
[ - <consul_sd_config> ... ]
```
```
# List of DNS service discovery configurations.
dns_sd_configs:
[ - <dns_sd_config> ... ]
```
```
# List of EC 2 service discovery configurations.
ec 2_sd_configs:
[ - <ec2_sd_config> ... ]
```
```
# List of OpenStack service discovery configurations.
openstack_sd_configs:
[ - <openstack_sd_config> ... ]
```
```
# List of file service discovery configurations.
file_sd_configs:
[ - <file_sd_config> ... ]
```
```
# List of GCE service discovery configurations.
gce_sd_configs:
[ - <gce_sd_config> ... ]
```
```
# List of Kubernetes service discovery configurations.
kubernetes_sd_configs:
[ - <kubernetes_sd_config> ... ]
```
```
# List of Marathon service discovery configurations.
marathon_sd_configs:
[ - <marathon_sd_config> ... ]
```
```
# List of AirBnB's Nerve service discovery configurations.
nerve_sd_configs:
[ - <nerve_sd_config> ... ]
```
```
# List of Zookeeper Serverset service discovery configurations.
serverset_sd_configs:
[ - <serverset_sd_config> ... ]
```
```
# List of Triton service discovery configurations.
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
```

#### 基于文件的服务发现方式

###### file_sd_configs

通过这种方式，Prometheus 会自动的周期性读取文件中的内容。

当文件中定义的内容发生变化时，不需要对 Prometheus 进行任何的重启操作。

#### 基于 consul 的服务发现

```
triton_sd_configs:
[ - <triton_sd_config> ... ]
```
```
46
47
```
```
[ root@node01 prometheus]# vi prometheus. yml
```
- job_name: 'node-exporter'
file_sd_configs:
# 基于文件的服务发现方式
- files:
# 指定 json 或则 yml 文件位置
- /opt/prometheus/node-exporter. json

```
1 2 3 4 5 6 7 8
```
```
1 [ root@node01 prometheus]# vi node-exporter. json
```
```
[
{
"targets": [
"172.16.20.00:9100"
],
"labels": {
"group": "aa",
"app": "web",
"hostname": "node 01"
}
},
{
"targets": [
"172.16.20.00:9100"
],
"labels": {
"group": "bb",
"app": "devops",
"hostname": "node 02"
}
}
]
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
18
19
20
21
22
```

###### 什么是基于 consul 的服务发现

我们大家都知道，Consul 是一个服务配置、发现的中间件，常用来作为服务注册中心使用。

在基于文件的服务发现中提到，target 的更新需要修改 target. json，本质上没有解决运维操作的步骤。

而基于 Consul 的服务发现，则是通过节点主动注册信息到 Consul，prometheus 通过和 Consul 集成，从
Consul 中以 http 协议获取 target 的信息。

Consul 是 HashiCorp 公司推出的开源工具，产品基于 GO 语言开发，主要面向分布式、服务化的系统提
供服务注册、服务发现和配置管理的功能。

**consul 产品具有以下特点：**

```
1. 服务发现
```
Consul 的客户端可以注册一个服务，例如 api 或 mysql，其他客户端可以使用 Consul 来发现给定服务
的提供者。

```
2. 健康检查
```
Consul 可以根据给定的信息，对服务的状态进行检查，并获取服务的健康状态。

```
3. Key/Value 存储
```
通过 HTTP API 的方式实现 Key/Value 存储，可用于动态配置、功能标记、协商等多种场景。

```
4. 多数据中心支持
```
支持多数据中心的分布式架构。

###### Prometheus 配置

在 Prometheus 配置 Job，这里使用 Consul 的服务发现方式，并配置好 Consul 接口地址，用于发现
Consul 中的 node_exporter 节点。

**注释** ：services 用于过滤 Consul 服务，如果为空，则会获取全部服务信息。

#### 基于 eureka 的服务发现

如果 Spring Cloud 使用的是 Eureka 作为注册中心，使用到 Prometheus 做服务监控，可以基于 eureka

的服务发现

Eureka 服务发现协议允许使用 Eureka Rest API 检索出 Prometheus 需要监控的 targets，Prometheus

会定时周期性的从 Eureka 调用 Eureka Rest API，并将每个应用实例创建出一个 target。

- job_name: 'consul-prom'
consul_sd_configs:
- server: '<cousul_ip>: 8500'
services: ['node_exporter']

```
1
2
3
4
```

###### eureka 客户端暴露出 prometheus 端口

修改 application. yml 文件

主要是上方 metadata-map 中的配置。

###### prometheus 配置文件

修改 prometheus. yml 文件

```
spring:
application:
name: order-provider-10004
server:
port: 10004
```
```
management:
endpoints:
web:
exposure:
include: 'prometheus, metrics, info, health' # 暴露出 prometheus 端口
metrics:
tags:
application: ${spring. application. name} # 增加每个指标的全局的 tag，及给每个
指标一个 application 的 tag, 值是 spring. application. name 的值
server:
port: 10005
```
```
# 注册到 eureka 上
eureka:
client:
service-url:
defaultZone: http://localhost:10003/eureka/ #连接到服务注册中心的地址 ，如
果服务注册中心开启了权限需要设置 http://username:password@ip:port/eureka/格式
instance:
prefer-ip-address: true
metadata-map:
# 集成到 prometheus, 以下的这些数据都会加入到 prometheus 的重新标记之前的标签中，比
如 sys. module 会变成
"prometheus. scrape": "true"
"prometheus. path": "/actuator/prometheus"
"prometheus. port": "${management. server. port}"
"sys. module": "order"
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
```
```
15
16
17
18
19
20
21
22
```
```
23
24
25
26
```
```
27
28
29
30
```
- job_name: 'eureka'
eureka_sd_configs:
# 指定 eureka 的服务发现地址
- server: 'http://localhost:10003/eureka'
relabel_configs:
# 重写 metrics 的路径
- source_labels:
["__meta_eureka_app_instance_metadata_prometheus_path"]
action: replace
target_label: __metrics_path__
regex: (.+)
# 增加一个自定义 label sys_model 它的值从配置 eureka 中获取

```
1 2 3 4 5 6 7 8 9
```
```
10
11
```

1 、重写 metrics 的 path。
2 、重写管理端口。
3 、增加一个自定义的标签。

实现效果查看下方的效果图。

最终实现的抓取端口的路径 [http://10.1.129.254:10005/actuator/prometheus](http://10.1.129.254:10005/actuator/prometheus)

#### 基于 nacos 的服务发现

实现方案

1 、使用某种软件生成配置文件，然后 prometheus 读取

2 、使用某种提供注册服务的中间件，然后 prometheus 访问

第一种，需要 docker 启动一个容器，定时生成文件。

第二种，需要维护一套中间件，可行。

这里介绍第一种方案。

###### docker 编排文件

600 的单位是秒
需要给 tmp 文件夹下的 json 文件授予其他人读写权限。

- source_labels: ["__meta_eureka_app_instance_metadata_sys_module"]
action: replace
target_label: sys_module
regex: (.+)
# 重写管理端口
- source_labels: [__address__,
__meta_eureka_app_instance_metadata_prometheus_port]
action: replace
regex: ([^:]+)(?::\d+)?;(\d+)
replacement: $1:$2
target_label: __address__

```
12
13
14
15
16
17
```
```
18
19
20
21
```
```
prometheus-nacos-sd:
image: nien/prometheus-nacos-sd: 1.0.0
container_name: prometheus-nacos-sd
volumes:
```
- /tmp:/tmp
command: "--nacos. address=192.168.56.121:8848 --nacos. namespace=public -
-output. file=/tmp/nacos_sd_public. json --refresh. interval=600"
networks:
base-env-network:
aliases:
- prometheus-nacos-sd

```
1 2 3 4 5 6 7 8 9
```
```
10
```

源码来自于 prometheus-sd-nacos: 使用 nacos 的服务发现功能，自动获取实例配置监控 (gitee. com)

###### 生产的配置文件

```
[ root@cdh1 tmp]# ll
total 8
drwxr-xr-x 2 root root 51 Oct 7 10 : 17 hsperfdata_root
drwx------ 12 root root 4096 Sep 28 09 : 32 _MEICQuN 8 i
-rw-------  1 65534 65534 1671 Oct 7 12 : 31 nacos_sd_public. json
drwxr-xr-x 4 65534 65534 28 Oct 7 12 : 26 prometheus-nacos-sd
drwxr-xr-x 3 root root 17 Sep 27 17 : 23 tomcat. 1302545122141233788.7777
drwxr-xr-x 3 root root 17 Oct 6 17 : 35 tomcat. 1617686018198450491.7777
drwxr-xr-x 3 root root 17 Sep 27 15 : 10 tomcat. 3221381097907404877.7777
drwxr-xr-x 3 root root 17 Sep 27 17 : 24 tomcat. 4512476252670875333.7788
drwxr-xr-x 3 root root 17 Oct 7 10 : 17 tomcat. 5137307452570916842.7788
drwxr-xr-x 3 root root 17 Oct 7 10 : 17 tomcat. 5327890546776942829.7777
drwxr-xr-x 3 root root 17 Oct 6 17 : 36 tomcat. 617412571170835352.7788
drwxr-xr-x 3 root root 17 Sep 27 15 : 10 tomcat. 7205873862886476115.7788
drwxr-xr-x 3 root root 17 Sep 28 08 : 02 tomcat. 7571582908938589164.7777
drwxr-xr-x 3 root root 17 Sep 28 08 : 02 tomcat. 8805560195831088805.7788
drwxr-xr-x 2 root root 6 Oct 6 17 : 35 tomcat-
docbase. 2689477778091859199.8848
drwxr-xr-x 2 root root 6 Oct 6 17 : 36 tomcat-
docbase. 2805418669057586567.7788
drwxr-xr-x 2 root root 6 Oct 7 10 : 17 tomcat-
docbase. 2883817303932250892.7788
drwxr-xr-x 2 root root 6 Sep 28 08 : 02 tomcat-
docbase. 3930283248758999137.7777
drwxr-xr-x 2 root root 6 Sep 28 08 : 02 tomcat-
docbase. 4279845123359990434.7788
drwxr-xr-x 2 root root 6 Sep 27 17 : 23 tomcat-
docbase. 4438109191665757249.7777
drwxr-xr-x 2 root root 6 Sep 27 17 : 24 tomcat-
docbase. 5440070348330283179.7788
drwxr-xr-x 2 root root 6 Sep 28 08 : 02 tomcat-
docbase. 7378412156113599520.8848
drwxr-xr-x 2 root root 6 Oct 7 10 : 17 tomcat-
docbase. 7568598900300088529.8848
drwxr-xr-x 2 root root 6 Oct 7 10 : 17 tomcat-
docbase. 8710493257860599713.7777
drwxr-xr-x 2 root root 6 Sep 27 17 : 23 tomcat-
docbase. 8750551899014460916.8848
drwxr-xr-x 2 root root 6 Oct 6 17 : 35 tomcat-
docbase. 9006194788768165054.7777
[ root@cdh1 tmp]# cat nacos_sd_public. json
[
{
"targets": [
"192.168.56.1:9999"
],
"labels": {
"__meta_nacos_group": "DEFAULT_GROUP",
"__meta_nacos_namespace": "public",
"__meta_preserved_register_source": "SPRING_CLOUD",
"__metrics_path__": "/actuator/prometheus",
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
14
15
16
17
```
```
18
```
```
19
```
```
20
```
```
21
```
```
22
```
```
23
```
```
24
```
```
25
```
```
26
```
```
27
```
```
28
```
```
29
30
31
32
33
34
35
36
37
38
39
```

###### 修改 prometheus 配置文件

增加目录映射：/tmp/:/tmp/
增加外部文件配置：

```
"job": "springcloud-gateway"
}
},
{
"targets": [
"192.168.56.1:18081"
],
"labels": {
"__meta_management_context_path": "/consumer/actuator",
"__meta_management_endpoints_web_base_path": "/actuator",
"__meta_nacos_group": "DEFAULT_GROUP",
"__meta_nacos_namespace": "public",
"__meta_preserved_register_source": "SPRING_CLOUD",
"__meta_servlet_context_path": "/consumer",
"__meta_user_name": "admin",
"__meta_user_password": "admin",
"__metrics_path__": "/actuator/prometheus",
"job": "service-consumer-demo"
}
},
{
"targets": [
"192.168.56.1:28088"
],
"labels": {
"__meta_management_context_path": "/seata-seckill/actuator",
"__meta_management_endpoints_web_base_path": "/actuator",
"__meta_nacos_group": "DEFAULT_GROUP",
"__meta_nacos_namespace": "public",
"__meta_preserved_register_source": "SPRING_CLOUD",
"__meta_servlet_context_path": "/seata-seckill",
"__meta_user_name": "admin",
"__meta_user_password": "admin",
"__metrics_path__": "/actuator/prometheus",
"job": "seata-seckill"
}
}
][ root@cdh1 tmp]#
```
```
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
```
- job_name: 'public_nacos-discorvery'
file_sd_configs:
- files: ['/tmp/nacos_sd_public. json']
refresh_interval: 3 m
relabel_configs:
- source_labels: ["job"]
regex: "DEFAULT_GROUP@@trade-chat-netty"
action: drop

```
1 2 3 4 5 6 7 8
```

###### 修改 springboot 项目配置文件

springboot 引入 prometheus 相关 jar 包

增加一个配置

暴露 prometheus 的接口；暴露 metrics. tags，和 spring. application. name 一致。

四、遇到的问题

1 、生成的 json 文件访问时，权限不足方法：用 root 用户授权 777 给 json 文件

2 、json 文件生成过快，prometheus 读取配置时，发现机器下线，但是没有预警方法：设置
prometheus 读取配置时间为 5 分钟，json 文件生成时间为 1 小时。

3 、prometheus 经常内存占用高，搞挂机器方法：
目前在 prometheus 创建容器的时候设置存储时间和文件压缩--storage. tsdb. wal-compression --
storage. tsdb. retention. time=7 d

## 全方位 Springcloud 性能调优

Springcloud 原始的配置，性能是很低的，大家可以使用 Jmeter 测试一下，QPS 不会到 50 。要做到高并
发，需要做不少的配置优化，主要的配置优化有以下几点：

```
Feign 配置优化
hystrix 配置优化
```
```
<dependency>
<groupId>io. micrometer</groupId>
<artifactId>micrometer-registry-prometheus</artifactId>
</dependency>
```
```
1
2
3
4
```
```
spring. cloud. nacos. discovery. metadata. context_path=${server. servlet. context-
path}
```
```
1
```
```
server:
port: 8081
spring:
application:
name: my-prometheus
management:
endpoints:
web:
exposure:
include: 'prometheus'
metrics:
tags:
application: ${spring. application. name}
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
13
```

```
ribbon 优化
Servlet 容器优化
Zuul 配置优化
```
```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从下面的链接获取：语雀
或者码云
```
#### Servlet 容器优化

默认情况下，Spring Boot 使用 Tomcat 来作为内嵌的 Servlet 容器，可以将 Web 服务器切换到
Undertow 来提高应用性能，Undertow 是红帽公司开发的一款基于 NIO 的高性能 Web 嵌入式
Zuul 使用的内置容器默认是 Tomcat，可以将其换成 undertow，可以显著减少线程的数量，替换方式即
在 pom 中添加以下内容：
第一步，移除 Tomcat 依赖

第二步，增加 Untertow 依赖

第三步, Undertow 的属性配置

server. undertow. io-threads: 设置 IO 线程数, 它主要执行非阻塞的任务, 它们会负责多个连接, 默认设置每
个 CPU 核心一个线程, 不要设置过大，如果过大，启动项目会报错：打开文件数过多
server. undertow. worker-threads: 阻塞任务线程池, 当执行类似 servlet 请求阻塞 IO 操作, undertow 会从
这个线程池中取得线程, 它的值设置取决于系统线程执行任务的阻塞系数，默认值是 IO 线程数*8
server. undertow. buffer-size: 以下的配置会影响 buffer, 这些 buffer 会用于服务器连接的 IO 操作, 有点类
似 netty 的池化内存管理, 每块 buffer 的空间大小, 越小的空间被利用越充分，不要设置太大，以免影响其
他应用，合适即可
server. undertow. buffers-per-region: 每个区分配的 buffer 数量 , 所以 pool 的大小是 buffer-size *
buffers-per-region
server. undertow. direct-buffers: 是否分配的直接内存 (NIO 直接分配的堆外内存)

```
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
<exclusions>
<exclusion>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-tomcat</artifactId>
</exclusion>
</exclusions>
</dependency>
```
```
1 2 3 4 5 6 7 8 9
```
```
10
```
```
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-undertow</artifactId>
</dependency>
```
```
1
2
3
4
```
```
server:
undertow:
io-threads: 16
worker-threads: 256
buffer-size: 1024
buffers-per-region: 1024
direct-buffers: true
```
```
1 2 3 4 5 6 7
```

#### Zuul 配置优化

我们知道 Hystrix 有隔离策略：THREAD 以及 SEMAPHORE ，默认是 SEMAPHORE 。
Zuul 默认是使用信号量隔离，并且信号量的大小是 100 ，请求的并发线程超过 100 就会报错，可以调大该
信号量的最大值来提高性能，配置如下：

表示，当 Zuul 的隔离策略为 SEMAPHORE 时，设置指定服务的最大信号量为 5000 。对于特定的微服务，
可以通过下面的方式，设置最大信号量

设置默认最大信号量：

zuul:
semaphore:
max-semaphores: 5000 # 默认值
设置指定服务的最大信号量：

为了方便 ThreadLocal 的使用，也可以改为使用线程隔离的策略，这种场景下，就需要调大 hystrix 线程
池线程大小，该线程池默认 10 个线程，调整的配置示例如下：

hystrix. threadpool. default. allowMaximumSizeToDivergeFromCoreSize：是否让 maximumSize 生
效，false 的话则只有 coreSize 会生效
hystrix. threadpool. default. maxQueueSize：线程池的队列大小，-1 代表使用 SynchronousQueue 队列
hystrix. threadpool. default. maximumSize：最大线程数量
hystrix. threadpool. default. allowMaximumSizeToDivergeFromCoreSize：是否让 maximumSize 生
效，false 的话则只有 coreSize 会生效
hystrix. threadpool. default. maxQueueSize：线程池的队列大小，-1 代表使用 SynchronousQueue 队列
zuul. ribbon-isolation-strategy: 设置线程隔离，thread 线程隔离，SEMAPHORE 表示信号量隔离

默认配置都可以去 HystrixThreadPoolProperties 和 ZuulProperties 这两个 java 文件中查找

#### Feign 配置优化

feign 默认不启用 hystrix，需要手动指定 feign. hystrix. enabled=true 开启熔断
feign 启用压缩也是一种有效的性能优化方式，具体的配置如下

```
zuul:
semaphore:
max-semaphores: 5000
```
```
1
2
3
```
```
zuul:
eureka:
<commandKey>:
semaphore:
max-semaphores: 5000
```
```
1
2
3
4
5
```
```
zuul:
ribbonIsolationStrategy: THREAD
hystrix:
threadpool:
default:
coreSize: 100
maximumSize: 400
allowMaximumSizeToDivergeFromCoreSize: true
maxQueueSize: -1
```
```
1 2 3 4 5 6 7 8 9
```

feign HTTP 请求方式选择
feign 默认使用的是基于 JDK 提供的 URLConnection 调用 HTTP 接口，不具备连接池, 所以资源开销上有点
影响，经测试 JDK 的 URLConnection 比 Apache HttpClient 快很多倍。Apache HttpClient 和 okhttp 都支
持配置连接池功能, 也可以使用 okhttp 请求方式。
当使用 HttpClient 时，可如下设置：

当使用 OKHttp 时，可如下设置：

max-connections 设置整个连接池最大连接数（该值默认为 200 ），根据自己的场景决定
max-connections-per-route 设置路由的默认最大连接（该值默认为 50 ），限制数量实际使用

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从下面的链接获取：语雀
或者码云
```
#### hystrix 配置优化

首先需要设置参数 hystrix. threadpool. default. coreSize 来指定熔断隔离的线程数，这个数需要调优，经
测试线程数我们设置为和提供方的容器线程差不多，吞吐量高许多。

其次，启用 Hystrix 后，很多服务当第一次访问的时候都会失败是因为初始化负载均衡一系列操作已经超
出了超时时间了，因为默认的超时时间为 1 S，需要修改超时时间参数，方可解决这个问题。
参考的 hystrix 配置如下：

```
feign:
compression:
request:
enabled: true
mime-types: text/xml, application/xml, application/json
response:
enabled: true
```
```
1 2 3 4 5 6 7
```
```
feign:
httpclient:
enabled: true
max-connections:1000
max-connections-per-route: 200
```
```
1
2
3
4
5
```
```
feign:
okhttp:
enabled: true
httpclient:
max-connections: 1000
max-connections-per-route: 200
```
```
1 2 3 4 5 6
```
```
hystrix:
threadpool:
default:
coreSize: 500
command:
default:
circuitBreaker:
requestVolumeThreshold: 1000
```
```
1 2 3 4 5 6 7 8
```

hystrix. command. default: 全局的作用域，作用的所有的 hystrix 的客户端, 如果需要对某个微服务，可以
写 serviceId
hystrix. command. default. fallback. enabled 是否开启回退方法
hystrix. command. default. execution. isolation. thread. timeoutInMilliseconds 请求处理的超时时间，
缺省为 1000, 表示默认的超时时间为 1 S

hystrix. threadpool. default. coreSize 核心线程池数量
hystrix. command. default. fallback. isolation. semaphore. maxConcurrentRequests 回退最大线程数
hystrix. command. default. circuitBreaker. requestVolumeThreshold 熔断器失败的个数，进入熔断器
的请求达到 1000 时服务降级（之后的请求直接进入熔断器）

#### ribbon 优化

Ribbon 进行客户端负载均衡的 Client 并不是在服务启动的时候就初始化好的，而是在调用的时候才会去
创建相应的 Client，所以第一次调用的耗时不仅仅包含发送 HTTP 请求的时间，还包含了创建
RibbonClient 的时间，这样一来如果创建时间速度较慢，同时设置的超时时间又比较短的话，很容易就
会出现上面所描述的显现。

因此我们可以通过设置:

参数说明:

ribbon. eager-load. enabled : 开启 Ribbon 的饥饿加载模式

ribbon. eager-load. clients: 指定需要饥饿加载的服务名，如果不指定服务名称，饥饿加载模式无效

Zuul 的饥饿加载，没有设计专门的参数来配置，而是直接采用了读取路由配置来进行饥饿加载。所以，
如果我们使用默认路由，而没有通过配置的方式指定具体路由规则，那么 zuul. ribbon. eager-
load. enabled=true 的配置就没有什么作用了。

如果需要真正启用 Zuul 的饥饿加载，需要通过 zuul. ignored-services=*来忽略所有的默认路由，让
所有路由配置均维护在配置文件中，以达到网关启动的时候就加载好各个路由的负载均衡对象。

关于 Zuul 的默认路由，这里详细介绍一下。假设你的注册服务中心有三个已经注册的服务名称 service-
a，service-b，service-c。但是在 zuul 配置文件中，只映射了 service-a，service-b，如下：

```
fallback:
enabled: true
execution:
isolation:
thread:
timeoutInMilliseconds: 100000
```
```
9
10
11
12
13
14
```
```
ribbon:
eager-load:
enabled：true
clients：service-1, service-2, service-n
```
```
1
2
3
4
```

这里，虽然没有配置 service-c 的映射，但是，由于 zuul 有默认的映射机制，还是可以通过http://ip:port/
service-c/的 Url，访问到你的 service-c 服务，如果不想向外界暴露默认的服务映射，可以加上
zuul. ignored-services:*

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从下面的链接获取：语雀
或者码云
```
## 高质量实操：SpringCloud 高并发实战案例

#### 1 、超高并发 10 Wqps 秒杀实操

为何要以秒杀做为高并发实战案例？

秒杀案例在生活中几乎随处可见：比如商品抢购，比如春运抢票，还是就是随处可见的红包也是类似
的。

另外，在跳槽很频繁的 IT 行业，大家都会有面试的准备要求。在面试中, 秒杀业务或者秒杀中所用到的分
布式锁、分布式 ID、数据一致性、高并发限流等问题，一般都是成为重点题目和热门题目，为面试官和
应聘者津津乐道.

尼恩说明：

高并发秒杀的案例，请参见本书的高阶版本，《Java 高并发核心编程卷 3 加强版》

#### 2 、超高并发 100 Wqps 车联网实操

很多小伙伴，在生产项目上主要是 crud，涉及不到核心技术和组件，所以导致大家在涨薪的时候，缺失
硬核的项目、核心经验做支撑，最终就业失败、涨薪受挫，直接影响后续的发展。

很多的培训机构，在介绍核心组件实操的时候，没有找到真实的业务场景，不是生产级的项目，导致练
习不到核心技术。

《超高并发 100 Wqps 车联网实操》是真刀真枪的核心组件实操：

（ 1 ）200 W+qps 行驶数据网关服务设计、开发、实操、测试

```
zuul:
ribbon:
eager-load:
enabled: true
ignored-services: ‘*’
routes:
a:
path: /a/**
serviceId: service-a
b:
path: /b/**
serviceId: service-b
```
```
1 2 3 4 5 6 7 8 9
```
```
10
11
12
```

（ 2 ） 5 W+qps 安全认证服务设计、开发、实操、测试

（ 3 ） 10 W+qps 消息服务设计、开发、实操、测试

（ 4 ） 2 W+qps 用户中心设计、开发、实操、测试

（ 5 ） 2 W+qps 异常监控服务设计、开发、实操、测试

（ 6 ） 5 W+qps 车辆服务设计、开发、实操、测试

（ 7 ） 亿级数据分库分表设计、开发、实操、测试

（ 8 ） devops 流水线部署、验证， K 8 S 自动化部署、验证

在学习、实操《100 W 级 Qps 车联网项目实操》之后，能具备 3 年以上核心 Java 的开发、设计经验。

#### 3 、N 多其他的超高并发实操项目

尼恩和《技术自由圈》其他的 P 7、P 8 同学，会一起给大家奉献更多的超高质量实操项目

并且帮助大家写入简历，保证简历脱胎换骨，金光闪闪。

## 技术自由的实现路径：

#### 尼恩 N 篇硬核架构文章，帮你实现架构自由：

《吃透 8 图 1 模板，人人可以做架构》

《10 Wqps 评论中台，如何架构？B 站是这么做的！！！》

《阿里二面：千万级、亿级数据，如何性能优化？教科书级答案来了》

《峰值 21 WQps、亿级 DAU，小游戏《羊了个羊》是怎么架构的？》

《 100 亿级订单怎么调度，来一个大厂的极品方案》

《 2 个大厂 100 亿级超大流量红包架构方案》

_......._ 更多架构文章，正在添加中

#### 实现你的响应式自由：

《响应式圣经：10 W 字，实现 Spring 响应式编程自由》

这是老版本《Flux、Mono、Reactor 实战（史上最全）》

#### 实现你的 spring cloud 自由：

《Spring cloud Alibaba 学习圣经》 PDF


《分库分表 Sharding-JDBC 底层原理、核心实战（史上最全）》

《一文搞定：SpringBoot、SLF 4 j、Log 4 j、Logback、Netty 之间混乱关系（史上最全）》

**实现你的 linux 自由：**

《Linux 命令大全：2 W 多字，一次实现 Linux 自由》

#### 实现你的网络自由：

《TCP 协议详解 (史上最全)》

《网络三张表：ARP 表, MAC 表, 路由表，实现你的网络自由！！》

#### 实现你的分布式锁自由：

《Redis 分布式锁（图解 - 秒懂 - 史上最全）》

《Zookeeper 分布式锁 - 图解 - 秒懂》

#### 实现你的王者组件自由：

《队列之王：Disruptor 原理、架构、源码一文穿透》

《缓存之王：Caffeine 源码、架构、原理（史上最全，10 W 字超级长文）》

《缓存之王：Caffeine 的使用（史上最全）》

《Java Agent 探针、字节码增强 ByteBuddy（史上最全）》

#### 实现你的面试题自由：

4000 页《尼恩 Java 面试宝典》 40 个专题

## 参考文献

```
1. 疯狂创客圈 JAVA 高并发总目录
https://www.cnblogs.com/crazymakercircle/p/9904544.html
ThreadLocal（史上最全）
https://www.cnblogs.com/crazymakercircle/p/14491965.html
2. 3000 页《尼恩 Java 面试宝典》的 35 个面试专题 ：
https://www.cnblogs.com/crazymakercircle/p/13917138.html
3. 价值 10 W 的架构师知识图谱
https://www.processon.com/view/link/60fb9421637689719d246739
```
4 、架构师哲学
https://www.processon.com/view/link/616f801963768961e9d9aec8

5 、尼恩 3 高架构知识宇宙
https://www.processon.com/view/link/635097d2e0b34d40be778ab4

《Java 高并发核心编程卷 3 加强版》

《响应式圣经：10 W 字，实现 Spring 响应式编程自由》

全链路异步，让你的 SpringCloud 性能优化 10 倍+


https://blog.csdn.net/crazymakercircle/article/details/125057545

https://yq.aliyun.com/articles/726499?spm=a2c4e.11155472.0.0.25bf72fc0yL85h

https://nacos.io/zh-cn/docs/what-is-nacos.html

https://blog.csdn.net/qq_38826019/article/details/109433231

https://blog.csdn.net/wpc2018/article/details/122634049

https://www.jianshu.com/p/7d80b94068b3

https://blog.csdn.net/yhj_911/article/details/119540000

[http://bjqianye.cn/detail/6845.html](http://bjqianye.cn/detail/6845.html)

https://blog.csdn.net/hao134838/article/details/110824092

https://blog.csdn.net/hao134838/article/details/110824092

https://blog.csdn.net/weixin_34096182/article/details/91436704

https://blog.csdn.net/fly910905/article/details/121682625

https://gaocher.github.io/2020/01/05/mono-create/

https://cloud.spring.io/spring-cloud-gateway/reference/html

https://cloud.spring.io/spring-cloud-gateway/reference/html/#configuring-predicates-and-filters-f
or-discoveryclient-routes

https://www.cnblogs.com/satire/p/15092894.html

https://gitee.com/bison-fork/loki/blob/v2.2.1/production/docker-compose.yaml

SkyWalking 官网 [http://skywalking.apache.org/zh/](http://skywalking.apache.org/zh/)
SkyWalking 的 docker github 地址 https://github.com/apache/sky...
elasticsearch https://www.elastic.co/guide/...
skywalking 中文文档 https://skyapm.github.io/docu...
agent config https://github.com/apache/sky...

skywalking 和其它 agent 一起使用的处理

https://zhuanlan.zhihu.com/p/163809795

https://www.cnblogs.com/you-men/p/14900249.html

https://cloud.tencent.com/developer/article/1684909

https://www.cnblogs.com/javaadu/p/11742605.html

https://www.jianshu.com/p/2fa99bd1997e

https://blog.csdn.net/weixin_42073629/article/details/106775584

https://www.cnblogs.com/kebibuluan/p/14466285.html

https://blog.csdn.net/weixin_42073629/article/details/106775584

https://blog.csdn.net/Jerry_wo/article/details/107937902


https://www.cnblogs.com/wzxmt/p/11031110.html

https://blog.csdn.net/zhangshng/article/details/104558016

https://blog.csdn.net/yurun_house/article/details/109025588

https://blog.csdn.net/weixin_40228200/article/details/123930498

https://blog.csdn.net/lanxing_huangyao/article/details/119795303

https://www.codenong.com/pzlong372468585/

https://www.cnblogs.com/gaofeng-henu/p/12594820.html

https://www.jianshu.com/p/2e905ab659fb

https://www.jianshu.com/p/ccffd6b9e3d1

https://www.jianshu.com/p/e6ec6a8a6c38

https://www.freesion.com/article/83831098412/

https://blog.csdn.net/zhipengfang/article/details/122646692

https://www.likecs.com/show-379874.html

https://help.aliyun.com/document_detail/29342.html

https://blog.csdn.net/fanliunian/article/details/112259986

https://www.modb.pro/db/325486

https://nacos.io/zh-cn/docs/monitor-guide.html

Nacos 监控手册

https://github.com/vovolie/lua-nginx-prometheus

https://blog.csdn.net/weixin_44268481/article/details/122355075

https://blog.csdn.net/fu_huo_1993/article/details/114876026

https://blog.csdn.net/u012888704/article/details/126232665

https://www.my607.com/jianzhanjingyan/2020-10-31/92668.html

https://www.jianshu.com/p/acbc04040519

https://blog.csdn.net/qq_43437874/article/details/120348217

https://blog.csdn.net/nandao158/article/details/120306729

https://blog.csdn.net/b644ROfP20z37485O35M/article/details/120124219


```
技术自由圈^
```
### 免费领取 11 个技术圣经 PDF


```
技术自由圈^
```
### 硬核推荐：尼恩 Java 硬核架构班

详情：https://www.cnblogs.com/crazymakercircle/p/9904544.html


技术自由圈^


```
技术自由圈^
```
##### 架构班（社群 VIP）的起源：^

最初的视频，主要是给读者加餐。很多的读者，需要一些高质量的实操、理论视频，所以，我就围绕书，和底层，做了几个

实操、理论视频，然后效果还不错，后面就做成迭代模式了。

##### 架构班（社群 VIP）的功能：^

提供高质量实操项目整刀真枪的架构指导、快速提升大家的:
 开发水平

 设计水平
 架构水平

弥补业务中 CRUD 开发短板，帮助大家尽早脱离具备 3 高能力，掌握：

 高性能
 高并发

 高可用
作为一个高质量的架构师成长、人脉社群，把所有的卷王聚焦起来，一起卷：

 卷高并发实操

 卷底层原理
 卷架构理论、架构哲学

 最终成为顶级架构师，实现人生理想，走向人生巅峰

##### 架构班（社群 VIP）的目的：^

 高质量的实操，大大提升简历的含金量，吸引力，增强面试的召唤率

 为大家提供九阳真经、葵花宝典，快速提升水平
 进大厂、拿高薪

 一路陪伴，提供助学视频和指导，辅导大家成为架构师

 自学为主，和其他卷王一起，卷高并发实操，卷底层原理、卷大厂面试题，争取狠卷 3 月成高手，狠卷 3 年成为顶级
架构师


```
技术自由圈^
```
##### N 个超高并发实操项目：简历压轴、个顶个精彩


```
技术自由圈^
```
【样章】第 17 章：横扫全网 Rocketmq 视频第 2 部曲: 工业级 rocketmq 高可用（HA）底层原
理和实操

工业级 rocketmq 高可用底层原理，包含：消息消费、同步消息、异步消息、单向消息等不同消息的底层原理和源码实现；
消息队列非常底层的主从复制、高可用、同步刷盘、异步刷盘等底层原理。

工业级 rocketmq 高可用底层原理和搭建实操，包含：高可用集群的搭建。

解决以下难题：
1 、技术难题：RocketMQ 如何最大限度的保证消息不丢失的呢？RocketMQ 消息如何做到高可靠投递？

2 、技术难题：基于消息的分布式事务，核心原理不理解
3 、选型难题： kafka or rocketmq ，该娶谁？

下图链接：https://www.processon.com/view/6178e8ae0e3e7416bde9da19


```
技术自由圈^
```
##### 成功案例：^2 年翻^3 倍，^35 岁卷王成功转型为架构师^

详情：http://topcoder.cloud/forum.php?mod=forumdisplay&fid=43&page=1


技术自由圈^


技术自由圈^


技术自由圈^


```
技术自由圈^
```
### 简历优化后的成功涨薪案例（VIP 含免费简历优化）


技术自由圈^


技术自由圈^


技术自由圈^


技术自由圈^


技术自由圈^


技术自由圈^


```
技术自由圈^
```
### 修改简历找尼恩（资深简历优化专家）

 如果面试表达不好，尼恩会提供简历优化指导

 如果项目没有亮点，尼恩会提供项目亮点指导

 如果面试表达不好，尼恩会提供面试表达指导

作为 40 岁老架构师，尼恩长期承担技术面试官的角色：

 从业以来，“阅历”无数，对简历有着点石成金、改头换面、脱胎换骨的指导能力。

 尼恩指导过刚刚就业的小白，也指导过 P 8 级的老专家，都指导他们上岸。

如何联系尼恩。尼恩微信，请参考下面的地址：

语雀：https://www.yuque.com/crazymakercircle/gkkw8s/khigna

码云：https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md


