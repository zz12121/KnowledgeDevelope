```
疯狂创客圈^
```
# 牛逼的职业发展之路

##### 40 岁老架构尼恩用一张图揭秘: Java 工程师的高端职业发展路径，走向食物链顶端的之路

##### 链接：https://www.processon.com/view/link/618a2b62e0b34d73f7eb3cd


```
疯狂创客圈
```
# 史上最全：价值 10 W 的架构师知识图谱

##### 此图梳理于尼恩的多个 3 高生产项目：多个亿级人民币的大型 SAAS 平台和智慧城市项目

##### 链接：https://www.processon.com/view/link/60fb9421637689719d


###### 疯狂创客圈

# 牛逼的架构师哲学

##### 40 岁老架构师尼恩对自己的 20 年的开发、架构经验总结

##### 链接：https://www.processon.com/view/link/616f801963768961e9d9aec


```
疯狂创客圈
```
# 牛逼的 3 高架构知识宇宙

##### 尼恩 3 高架构知识宇宙，帮助大家穿透 3 高架构，走向技术自由，远离中年危机

##### 链接：https://www.processon.com/view/link/635097d2e0b34d40be778ab


###### 疯狂创客圈

# 尼恩 Java 高并发三部曲（卷 1 加强版）

##### 老版本：《Java 高并发核心编程卷 1 ：NIO、Netty、Redis、ZooKeeper》（已经过时，不建

##### 议购买）

#### 新版本：《Java 高并发核心编程卷 1 加强版 ：NIO、Netty、Redis、ZooKeeper》

######  由浅入深地剖析了高并发 IO 的底层原理。

 图文并茂的介绍了 TCP、HTTP、WebSocket 协议的核心原理。

 细致深入地揭秘了 Reactor 高性能模式。

 全面介绍了 Netty 框架，并完成单体 IM、分布式 IM 的实战设计。

 详尽地介绍了 ZooKeeper、Redis 的使用，以帮助提升高并发、可扩展能力

##### 详情：https://www.cnblogs.com/crazymakercircle/p/16868827.html


```
疯狂创客圈
```
# 尼恩 Java 高并发三部曲（卷 2 加强版）

##### 老版本：《Java 高并发核心编程卷 2 ：多线程、锁、JMM、JUC、高并发设计模式》

##### （已经过时，不建议购买）

#### 新版本：《Java 高并发核心编程卷 2 加强版 ：多线程、锁、JMM、JUC、高并发设计模式》

 由浅入深地剖析了 Java 多线程、线程池的底层原理。

 总结了 IO 密集型、CPU 密集型线程池的线程数预估算法。

 图文并茂的介绍了 Java 内置锁、JUC 显式锁的核心原理。

 细致深入地揭秘了 JMM 内存模型。

 全面介绍了 JUC 框架的设计模式与核心原理，并完成其高核心组件的实战介绍。

 详尽地介绍了高并发设计模式的使用，以帮助提升高并发、可扩展能力

##### 详情参阅：https://www.cnblogs.com/crazymakercircle/p/16868827.html


###### 疯狂创客圈

# 尼恩 Java 高并发三部曲（卷 3 加强版）

##### 老版本：《SpringCloud Nginx 高并发核心编程》（已经过时，不建议购买）

#### 新版本：《Java 高并发核心编程卷 3 加强版 ：亿级用户 Web 应用架构与实战》

 在当今的面试场景中， 3 高知识是大家面试必备的核心知识，本书基于亿级用户 3 高 Web 应用

```
的架构分析理论，为大家对 3 高架构系统做一个系统化和清晰化的介绍。
```
 从 Java 静态代理、动态代理模式入手，抽丝剥茧地解读了 Spring Cloud 全家桶中 RPC 核心原

```
理和执行过程，这是高级Java工程师面试必备的基础知识。
```
 从 Reactor 反应器模式入手，抽丝剥茧地解读了 Nginx 核心思想和各配置项的底层知识和原理，

```
这是高级Java工程师、架构师面试必备的基础知识。
```
 从观察者模式入手，抽丝剥茧地解读了 RxJava、Hystrix 的核心思想和使用方法，这也是高级

```
Java工程师、架构师面试必备的基础知识。
```
##### 详情：https://www.cnblogs.com/crazymakercircle/p/16868827.html


```
疯狂创客圈
```
# 尼恩 Java 面试宝典

##### 35 个专题（卷王专供+ 史上最全 + 2023 面试必备）

##### 详情：https://www.cnblogs.com/crazymakercircle/p/13917138.html


# 本书封面


# 本书前言

###### 在平时的开发工作中，大部分小伙伴并不能涉及高并发、高可用、高性能（简称“ 3 高”）的场景，

###### 而这些“ 3 高”理论和实操知识又是大家面试必备的核心知识，笔者希望通过本书，基于亿级用户“ 3

高”Web 应用的架构分析理论，为大家对“ 3 高”架构系统做一个系统化和清晰化的介绍。
从实操的维度来说，SpringCloud+Nginx 系统架构毫无疑问是当今“ 3 高”Web 应用的主流技术
之一。分布式 SpringCloud 微服务框架和高性能的 Nginx 反向代理 Web 服务的优秀组合，满足了各大
产品和项目的可扩展、高可用、高性能架构的需求。
本书从基础设计模式、基础原理出发，理论与实战相结合，对“ 3 高”Web 应用的基础理论与
知识体系，以及 SpringCloud+Nginx 高并发编程的核心原理做了非常系统和详尽的介绍。本书旨在
帮助初、中、高级开发工程师弥补在“ 3 高”Web 应用知识体系、SpringCloud 微服务、Nginx 反向代
理核心知识方面的短板，为广大开发人员顺利成长为优秀的 Java 高级工程师、系统架构师提供帮助。

## 本书内容

###### 本书内容分为 11 章，分别说明如下：

第 1 章介绍亿级用户 Web 应用的架构所涉及的分层架构、扩展架构、缓存架构、数据层架构、
异步架构、高可用架构等基础原理，介绍亿级用户量场景下的流量预估方法和实操技巧。
第 2 章介绍接入层 Nginx 和服务层 SpringCloud 高并发核心编程的学习准备，包括知识背景、开
发和自验证环境的准备。
第 3 章介绍服务层 SpringCloud 入门实战，包括注册中心、配置中心、微服务提供者的入门开发
和配置。
第 4 章介绍服务层 SpringCloudRPC 远程调用的核心原理，从设计模式的代理模式开始，抽丝剥
茧、层层递进地揭秘 SpringCloudFeign 的底层 RPC 远程调用的核心原理。
第 5 章介绍 RxJava 响应式编程框架。在 SpringCloud 框架中涉及 Ribbon 和 Hystrix 两个重要的组件，
它们都用到了 RxJava 响应式编程框架。作为非常重要的编程基础知识，本书特意设立本章对 RxJava
的原理和使用进行详细介绍。
第 6 章介绍服务层 HystrixRPC 保护的原理，从 RxJava 响应式编程框架的应用开始，溯本求源、
循序渐进地揭秘 SpringCloudHystrix 的底层 RPC 保护的核心原理。
第 7 章介绍微服务网关与用户身份识别。微服务网关是微服务架构中不可或缺的部分，它统一
解决 Provider 路由、负载均衡、权限控制等问题。
第 8 章详解接入层 Nginx/OpenResty，从高性能传输模式 Reactor 模型入手，寻踪觅源、由浅入深
地揭秘 Nginx 反向代理 Web 服务器的核心知识，包括 Reactor 模型、Nginx 的模块化设计、Nginx 的请
求处理流程等。
第 9 章介绍接入层 NginxLua 编程。在高并发场景下，NginxLua 编程是解决性能问题的利器，
本章介绍 NginxLua 编程的基础知识。


###### 第 10 章介绍限流原理与实战。高并发系统用三把利器——缓存、降级和限流来保护系统，本章

###### 介绍计数器、令牌桶、漏桶这三大限流策略的原理和实现。

第 11 章介绍 SpringCloud+Nginx 秒杀实战，通过这个综合性的实战案例说明缓存、降级和限流
的应用。

### 读者对象

```
1 ）对SpringCloud+Nginx系统架构感兴趣的大专院校师生。
2 ）需要学习SpringCloud+Nginx分布式高并发技术和高并发架构的初、中、高级Java工程师。
3 ）生产场景中需要用到SpringCloud+Nginx组合或者其中某个框架的架构师或工程师。
```
### 资源下载

```
本书的源代码可以通过扫描二维码进行下载，若下载有问题，请发送电子邮件至booksaga@ 126 .com。
```
```
秒杀前端 秒杀后台
```
### 致谢

###### 首先感谢卞诚君老师在笔者写书过程中给予的指导和帮助。没有他的提议，我不会想到将自己

###### 的“疯狂创客圈”社群中高并发方面的博客整理成图书出版。感谢“疯狂创客圈”社群中的小伙伴

###### 们，虽然大家在群里抛出的很多技术难题笔者不一定能给出更佳的解决方案，但正是因为一路同行，

###### 一直坦诚、纯粹地进行技术交流，才能相互启发技术灵感，进而扩充小伙伴们的技术视野，最终提

###### 升编程水平。欢迎大家在“疯狂创客圈”社群提出问题，也欢迎大家多多交流。

###### 写书不仅仅是一种技术活，更是一种工匠活，为了保证书中的知识是全面的、系统化的，笔者

###### 需要不断地思考和总结，不断地检查与修正。为了保证书中的每一行代码是正确的，笔者需要反复

###### 地编写 LLT 用例进行验证。尽管如此，还是不能保证书中没有瑕疵，不妥之处希望读者批评指正。

###### 完成一本优质的书需要投入大量的业余时间，这也意味着牺牲了本该陪伴家人的时间，在这里

###### 特别感谢我的家人给予的理解、支持和帮助。

###### 尼恩

###### 2022 年 9 月 26 日


# 自序

身边常常有小伙伴问我怎样提高 Java 技术水平。下面给出两个简单的例子：
小伙伴 A（ 6 年经验）说：尼恩，使用 Java 编程时，我在思路和速度上都赶不上小伙伴 B（ 5 年
经验），尤其是在解决复杂问题的时候，我该怎么办？
小伙伴 C（ 12 年经验）说：尼恩，我司刚刚引进了一位高薪的 Java 核心架构师，他的薪酬挺令
人心动的，如何才能提高我的 Java 技术水平，成为核心架构师呢？

遇到这类问题，我一概回答：“多读书、多画图、多实操。就目前看来，这是一条快捷、经
济、有效地提高 Java 水平的途径。”
为什么这么说呢？首先，以我本人为例，身为核心架构师，我在技术能力方面早已得到团队认
可，在团队内长期居于 Bug 排除榜前列，专门负责解决复杂、困难的技术问题。实际上，方法很简
单，就是多阅读专业图书，我家里的技术书都可以用汗牛充栋来形容了。其次，给大家简单地分析
一下具体原因。目前学习技术的途径大致有三种：（ 1 ）阅读博文；（ 2 ）观看视频；（ 3 ）阅读图
书。通过途径（ 1 ）（阅读博文）获得的知识往往过于碎片化，难成体系。这种途径更适用于了解
技术趋势、解决临时的技术问题。通过途径（ 2 ）（观看视频）获取知识需要耗费大量的时间，而
且很多视频是填鸭式的知识灌输。所以，途径（ 2 ）更适用于初学者，或者用于掌握某个完整的知
识体系。对于有经验、能动性高的 Java 工程师来说，途径（ 2 ）的不足之处在于效率太低、时间成
本高。通过途径（ 3 ）（阅读图书）获取知识有一个显著的优势：图书能以很小的体积承载巨量知
识，而且所承载的是系统化、层次化的知识。
上述三种途径各有优劣，鉴于 Java 高并发所涉及的核心技术比较多，包括“ 3 高”架构理论体
系、SpringCloud、Netty、Nginx、JUC、JMM、Kafka、ElasticSearch 等，我将结合博文、视频、
图书三种形式，为大家提供一个立体的、全方位的 Java 高并发核心编程知识仓库。在“疯狂创客圈”
（我发起的 Java 高并发交流社群）中，将规划中的图书整合成一个高并发核心编程的图书系列，大
致清单如下：

1 ）《Java 高并发核心编程卷 1 （加强版）：NIO、Netty、Redis、ZooKeeper》：从操作系统
底层 IO 模式和原理、Reactor 高并发 IO 模式入手，介绍 Java 分布式、高并发通信原理，并指导大家进
行高并发 IM 实战。
卷 1 详细介绍 Reactor 模式、Netty、ZooKeeper、Redis、TCP、HTTP、WebSocket、NIO 等 Java
高性能通信的核心原理和编程知识，并指导大家编写一个高并发的分布式 IM 实战程序——CrazyIM。

2 ）《Java 高并发核心编程卷 2 （加强版）：多线程、锁、JMM、JUC、高并发设计模式》：
聚焦 Java 高并发基础知识，内容包括多线程、线程池、JMM 内存模型、JUC 并发包、AQS 同步器、
高并发容器类、高并发设计模式等。
卷 2 为大家建立高并发、高性能 Java 应用的底层知识体系，是本系列图书中最为基础、最为核
心的一卷书。


3 ）《Java 高并发核心编程卷 3 （加强版）：亿级用户 Web 应用架构与实战》：从亿级用户的
Web 应用架构入手，介绍“ 3 高”架构所涉及的理论知识体系和核心实操知识，涵盖 SpringCloud、
Nginx 的核心原理和编程知识，并指导大家编写一个高并发的秒杀实战程序。
卷 3 通过高并发架构的介绍和实操指导，引导大家建立架构师知识框架体系，并且指导大家做
一些架构师必备的实操。

本书是《SpringCloud、Nginx 高并发核心编程》的加强版。自《SpringCloud、Nginx 高并发核
心编程》出版后的两年以来，在和广大读者小伙伴的答疑、交流过程中，以及在对 Java 顶级高并发
组件的研究过程中，笔者进行了大量的修订、完善、充实，增加了大量的内容，形成了此书。此书
加强的要点内容如下：

1 ）加强了“ 3 高”理论和实操知识。在当今的面试场景中，“ 3 高”知识是大家面试必备的核
心知识，本章基于亿级用户“ 3 高”Web 应用的架构分析理论，为大家对“ 3 高”架构系统做一个系
统化和清晰化的介绍。
2 ）加强了计数器、漏桶、令牌桶三大限流算法的实操代码，并且对计数器限流算法的临界问
题，进行细致深入的说明。

Java 高并发核心编程系列图书的初衷是为大家奉上一系列有关 Java 高并发方面的“原理级”“思
想级”的图书，帮助大家轻松、切实、快捷地获取 Java 高并发核心知识，从而扎稳自己的知识底盘，
提升自己的开发内功。

由于本书篇幅着实有限，“ 3 高”知识体系又非常庞大，所以，笔者还编写了大量博客文章作
为本书的配套知识和补充知识，具体的博客地址请加“疯狂创客圈”社群获取。

###### 尼恩

###### 2022 年 9 月 26 日


# 第 1 章亿级用户 Web 应用的架构与实操

在本书的上一卷——《Java 高并发核心编程卷 2 （加强版）：多线程、锁、JMM、JUC、高并
发设计模式》的开头部分，笔者提到：尼恩作为核心架构师、技术主管，会经常性地组织技术面试，
而在此期间，尼恩遇到过很多有意思的候选人，遇到过很多有意思的小故事。尼恩介绍完小故事之
后，给大家做了总结：在项目的开发过程中，大部分开发小伙伴都在做业务类 CRUD（增查改删）
开发工作，而那些 CRUD 业务对吞吐量、可用行、性能的要求都不高，所以导致大家对高并发的实
操不足、理论了解不够。
正因为在平时的业务开发中，大部分小伙伴并不能涉及高并发、高可用、高性能（简称“ 3 高”）
的场景，所以笔者希望通过本书，基于亿级用户 Web 应用的架构分析理论，对“ 3 高”架构系统做
一个彻底介绍。

1. (^1) 高并发基本原理
什么是高并发？高并发（HighConcurrency）是互联网分布式系统架构设计中必须考虑的因素
之一，它通常是指通过设计保证系统能够同时并行处理多个请求。
高并发相关的一些常用指标有：
 响应时间（ResponseTime，RT）
 吞吐量（Throughput）
 错误率

#### 1. 1. 1 响应时间

什么是响应时间呢？响应时间是指 Web 应用对用户的请求做出响应的时间。简单来说，响应
时间是用户对 Web 应用的性能的主观感受。响应时间既包括了整个 Web 应用系统处理用户请求的时
间，也包括了请求数据、传输结果数据的时间。一个 Web 应用的响应时间通常会包括应用所有功能
的平均时间，以及所有功能的最大响应时间。
响应时间和用户的接受程度有关：不同的应用，用户所能接受的响应时间范围不一样。比如，
对于一个游戏软件来说，响应时间小于 100 毫秒时用户体验会非常好，没有什么卡顿，响应时间在 1
秒左右属于勉强可以接受，如果响应时间达到 3 秒就完全难以接受了。
但是，对于一些耗时的计算应用来说，响应时间可能是几十分钟、几个小时甚至更长时间，
用户也都是可以接受的。比如笔者曾经工作过的一个亿级数据的搜索中台系统，一次索引的全量刷
新时间需要 5 天以上。
那么，响应时间如何度量呢？一般来说，一份 Web 应用的性能聚合报告大致会包含 Average、
Median、 90 %Line、 95 %Line、 99 %Line、Min、Max 六个 RT 时间指标，具体如下：

```
 Average（平均值）：平均响应时间（单位：毫秒），默认是单个请求的平均响应时间。
```

 Median（中位数）： 50 %的用户响应时间小于这个值。
 90 %Line（ 90 %百分位）： 90 %的用户响应时间小于这个值。
 95 %Line（ 95 %百分位）： 95 %的用户响应时间小于这个值。
 99 %Line（ 99 %百分位）： 99 %的用户响应时间小于这个值。
 Min（最小值）：用户响应时间最小值。
 Max（最大值）：用户响应时间最大值。
这些指标的值越小效果越好，表示接口响应越快。在实际工作中，大家一般会关注 90 %Line
指标，表示 90 %的响应时间小于某个值。如果 90 %的响应时间小于 1 秒，代表使用这个接口的 90 %
的用户都能在 1 秒内得到响应。
在实际的工作中，有些项目也比较关注 Average（平均响应时间）指标。但是由于受到极端值
的影响，总体来说，Average 指标的参考意义并没有 90 %Line 这个指标的参考意义大。
当然，不同的项目对于响应时间都有具体的要求。如果项目中没有明确的响应时间的要求，
则可以使用一些行业的通用要求。对于用户响应时间，IT 行业有一个通用的评价原则，就是所谓的
响应时间的“ 2 - 5 - 8 原则”，大致如下：

 当用户能够在 2 秒以内得到响应时，会感觉系统的响应很快。
 当用户在 2 ~ 5 秒得到响应时，会感觉系统的响应速度还可以。
 当用户在 5 ~ 8 秒得到响应时，会感觉系统的响应速度很慢，但是尚可以接受。
 当用户在超过 8 秒后仍然无法得到响应时，会感觉系统糟透了，从而会选择终止请求离开
应用，或者发起第二次请求。
如果一个普通应用没有特定的性能要求，那么按照响应时间的“ 2 - 5 - 8 原则”，该应用的 90 %Line
指标值在 2 秒以内就是合格的了。

#### 1. 1. 2 吞吐量

###### 什么是吞吐量呢？吞吐量是指系统在单位时间内处理请求的数量。简单地从数据量的维度来

###### 说，吞吐量与响应时间成严格的反比关系，吞吐量越大，响应时间越小。

###### 对于并发系统，通常使用吞吐量作为性能指标，并不使用响应时间作为性能指标。两个具有

###### 不同用户数和用户使用模式的系统，如果它们的最大吞吐量基本一致，则可以判断两个系统的处理

###### 能力基本一致。

###### 那么，吞吐量如何度量呢？一般来说，吞吐量的度量指标大致如下：

######  QPS（每秒查询数）

###### QPS 是对一个特定的查询服务器在 1 秒内所处理流量多少的衡量标准。对一个特定的查询服务器

###### （例如读写分离架构中的读服务器），QPS 指的是在规定时间内所处理的查询流量的规模大小，对应

的英文意思为 fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。

 TPS（每秒事务数）
TPS 的全称为 TransactionsPerSecond，意思是每秒事务数。一个事务是指一个客户端向服务器
（例如读写分离架构中的写服务器）发送写入请求，然后服务器做出反应的过程。


###### 每个事务包括了如下 3 个过程：

###### 1 ）用户请求服务器。

###### 2 ）服务器自己的内部处理（包含应用服务器、数据库服务器等）。

###### 3 ）服务器返回给用户。

###### 如果每秒能够完成 N 个事务，TPS 的值就是 N。

###### 从字面意思的角度来说，TPS 仅针对修改类型的请求，QPS 仅针对查询类型的请求。

###### 但是在实际使用的过程中，TPS 不仅包括修改类型的请求，还会包含查询类型的请求。

###### QPS（每秒查询数）与 TPS（每秒事务数）有什么关系呢？

###### 如果一次请求只会调用一个后端的接口或者访问后端的一个资源，那么 QPS=TPS。具体的例

###### 子是：如果是对一个查询接口做压力测试（简称压测），且这个接口内部不会再去请求其他接口，

###### 那么 TPS=QPS。尽管从字面意思的角度来说，TPS 仅针对修改类型的请求，但是在实际使用过程

###### 中，单一查询类型的请求也可以计入 TPS 范围。

###### 如果一次请求会调用后端的多个接口或者访问后端的多个资源，那么 QPS≠TPS。可以把一次

###### 请求中所访问的接口（或者资源）理解为子请求，那么在一次整体的请求过程中，每对一个资源发

###### 送一次子请求，就可以计入 QPS 之中。此时，QPS>TPS。

当前的应用基本上都是前后端分离的，所以性能也分为前端性能和后端性能。对于 Java 程序员
来说，大部分时候所关注的是后端性能，即服务端性能。大家通常需要对服务端接口做压测。
在做后端压测时，如果是对一个接口压测（单接口场景），且这个接口内部不会再去请求其
他接口，此时，QPS=TPS；如果是对多个接口压测（多接口场景），需要用到 Jmeter 压测工具的事
务控制器组件，最终的结果才是整个场景的 TPS，此时，QPS≠TPS。在 Jmeter 聚合报告中，吞吐量
通常由 TPS 来表示，具体如图 1 - 1 所示。
在 Jmeter 聚合报告中，吞吐量是指 Throughput 项（即 TPS），实际上，这里的 TPS 已经远远超过
了写入操作的范围，而是表示服务器每秒处理请求数或任务数。该 TPS 值越大，表示服务器处理能
力越强。
QPS、TPS 是从秒级别时间维度去度量吞吐量的。除了这组指标之外，还有一组从日级别时间
维度去度量吞吐量的指标，具体如下：

```
图 1 - 1 Jmeter聚合报告中的性能指标
 日PV（PageView）值
日PV值也简称为PV值。PV值即页面访问量，该指标反映的是网站的页面日浏览量，网页每刷
```

###### 新一次，都会计算一次。日 PV 值与来访者的数量成正比，但 PV 值并不是页面的来访者数量，而是

###### 网站被访问的页面数量。网站每日访问量 IP 越多，PV 越多，访问量越大。

 日 UV（UniqueVisitor）值
日 UV（UniqueVisitor）值也简称为 UV 值。UV 访问数指独立访客访问数。什么是独立访客呢？
简单来说，一个独立的访问终端（计算机、平板电脑等）可以理解为一个访客，UV 可以理解成访
问某网站的终端的数量。判断来访计算机的身份，可以通过来访计算机的 Cookies 身份标识而不是
IP 去实现，也就是说，更换了 IP 但不清除 Cookies，再访问相同网站，该网站的统计中的 UV 数是不
变的。一个 UV 可以产生多个 PV，所以 PV 个数≥UV 个数。
以下是一个使用 PV/UV 进行较大时间范围维度的吞吐量统计的案例：
日均 UV/PV 访问量约为 60000 / 240000
表示：日均访问网站次数为 240000 次，日均独立访客访问数为 60000 个，也就是说这 60000 个
独立访客一共访问网站 240000 次。

#### 1. 1. 3 错误率

###### 什么是错误率呢？这个指标比较好理解，就是响应结果的错误比例，具体的公式为：

###### 错误率=错误的请求的数量/请求的总数

在 Jmeter 聚合报告中，错误率是指“Error%()”一栏的值。在所有的度量指标中，错误率越低
越好，错误率为 0 ，表示没有错误请求。
一般来说，错误率要在万分之一以下。当然考虑到不同业务的区别，这个标准可能会有变化，
具体以项目的要求为准。

### 1. 2 提升系统的并发能力

如何提升系统的并发能力呢？这个问题在高级 Java 面试中十分常见。下面是一道来自疯狂创客
圈社群的面试真题。


###### 社群小伙伴：尼恩老师，我遇到这样一道面试题：一个系统中，如果每个请求都是一个线程，

###### 服务器承受不了大的流量，那么该怎么优化？

尼恩答曰：纵向扩展（ScaleUp）和横向扩展（ScaleOut）。所谓纵向扩展，就是提升单进程
的处理能力，核心就是提升单线程的处理能力。比如 500 线程，如果一次请求纵向优化到 100 毫秒，
一个线程 1 秒可以处理 10 个请求，那么 500 线程可以达到 5000 TPS。所谓横向扩展，就是从一个进
程提供服务扩展到由多个服务进程提供服务。从物理资源角度来说，一个服务器可以开 2 个进程，
或者 N 个进程（如 10 个）。当然，横向扩展得考虑资源是否足够。假设资源足够，一个服务器开 10
个进程，就是 5 万 TPS。利用 K 8 S 等虚拟化管理平台进行横向扩展十分方便。对于纵向扩展来说，异
步是基础，还有各种性能优化，具体如何做性能优化，可以看看 Netty 源码、RocketMQ 源码，真可
谓“无所不用其极”，非常复杂。和 Netty 那种高性能优化措施相比，其实横向扩展简单多了。

在互联网分布式架构设计领域，如果想提高系统的并发能力，使得应用能扛住更多的并发请
求，主要有以下两种手段：

```
 纵向扩展
 横向扩展
```
#### 1. 2. 1 纵向扩展

###### 什么是纵向扩展呢？所谓纵向扩展，就是提升单进程的处理能力。由于进程内部可以通过多

###### 线程并行处理，所以纵向扩展的核心就是提升单线程的处理能力。纵向扩展通常也叫作垂直扩展。

###### 纵向扩展可以从以下两个维度进行展开：

######  提升单节点硬件性能

###### 通过增加内存、CPU 核数、存储性能（如将机械硬盘升级为固态硬盘）等提升硬件性能的方

###### 式来提升并发能力。

######  提升进程的软件性能

###### （ 1 ）使用各级缓存，把计算结果缓存起来，减少耗时操作的次数

通过分析高性能应用的标杆——Netty 的源码，大家可以看到，Netty 大量使用了缓存，包括对
象池、Map 缓存、内存池等。

（ 2 ）使用无锁模式和无锁数据结构来提升性能，减少处理时间
Netty 的源码中使用的 Mpsc 队列、ThreadLocal 都是无锁数据结构，这些结构的使用大大提升了
性能。
无论是 Java 内置锁、JUC 显示锁，还是 volatile 底层为保障内存可见性使用的 CPU 总线锁，都有
很大的性能消耗。在高并发场景下，锁的抢占与释放的性能消耗是非常高的，笔者进行过 JUC 有锁
线程池和 Netty 无锁线程池的对比测试，有锁线程池大概要消耗 30 %左右的性能。
Netty 的源码中使用了无锁模式进行反应器任务调度，保障了同一个 Channel 的多个处理任务在
直接进行数据同步操作时，不需要加锁。

```
（ 3 ）使用零复制来减少IO次数
进行IO操作时，使用直接内存而不是堆内存来进行读写；进行文件发送时，通过sendfile底层
```

###### 调用直接传输文件从而避免让 CPU 去执行数据复制的操作。使用直接内存时注意要和内存池配合使

###### 用，因为通过操作系统的系统调用进行内存分配和回收的成本实在太高。

###### （ 4 ）使用异步操作来增加吞吐量

###### 异步操作相比于同步操作的优势在于：同步等待的时间被异步的方式拿来执行其他的处理，

###### 因此，相同线程数量下，异步支持的并发和吞吐量都会高一些。

###### （ 5 ）使用对象的池化机制，减少 GC 次数

对于一些频繁使用的 Java 对象，可以使用对象池进行缓存，避免 JVM 频繁地进行 GC 操作，这
样可以减少 GC 的次数和 STW 卡顿的次数。

总之，提升进程的软件性能是一件非常有技术含量的事情。在这个领域，Netty 源码可以作为
标杆去学习和研究。当然，RocketMQ 源码同样应用了大量的性能优化技术与优化模式，同样可以
作为学习和研究的样板。
但是，不管是提升单节点硬件性能，还是提升进程的软件性能，都有一个致命的不足：单节
点的性能总是有极限的。如何解决单节点的性能极限呢？解决方案是横向扩展。

#### 1. 2. 2 横向扩展

###### 什么是横向扩展？简单来说，横向扩展就是通过增加服务器数量来线性扩充系统性能。横向

###### 扩展通常也叫作水平扩展。

###### 如何进行横向扩展呢？横向扩展的策略总体来说大致如下：

######  做好分层架构

###### 做好分层架构是横向扩展的前提。首先，高并发系统往往业务复杂，通过分层处理可以简化

###### 复杂问题，更容易做到横向扩展。其次，做好分层架构之后，可以更加方便地进行读写访问链路解

###### 耦和冷热链路解耦，更容易做到靶向式的有效横向扩展。

######  分层进行水平扩展

###### 横向扩展的一条简单的原则是：无状态水平扩容，有状态做分片路由。接入网关、业务服务

###### 层通常能设计成无状态的，可以很方便地进行水平扩容；而数据库层、缓存层往往是有状态的，因

###### 此需要设计分区键做好存储分片。

######  访问链路解耦

###### 比如可以将读写访问链路解耦，通过读写分离的方案提升读性能。当然，这种策略需要数据

###### 库层、缓存层主从同步机制进行配合。

#### 1. 2. 3 高并发架构中的分层策略

###### 常见的高并发系统在高并发架构上可以分为以下几层：

```
1 ）客户端层：用户使用的交互工具，包含浏览器Browser、手机APP等。
2 ）接入层：系统入口，反向代理。
3 ）服务层：实现核心应用逻辑，接收用户请求，处理完后返回HTML或者JSON。
```

###### 4 ）缓存层：加速数据的访问。

###### 5 ）数据库层：数据存储。

```
6 ）中间件：含分布式协调组件如ZooKeeper、消息队列如RocketMQ、搜索引擎如ElasticSearch等。
接下来按照层次依次介绍系统的高并发架构。
```
1. (^3) 接入层横向扩展高并发架构

#### 1. 3. 1 硬负载均衡

###### 什么是硬负载均衡？顾名思义，硬负载均衡是在服务器节点之间安装的专门的硬件设备，负

###### 责完成负载均衡的横向扩展工作，如图 1 - 2 所示。

图 1 - 2 接入层使用硬负载均衡的架构图
F 5 是负载均衡产品的一个品牌，其地位类似于 iPhone 手机在手机品牌中的位置。除了 F 5 以外，
Radware、Array、A 10 、Cisco、深信服、华夏创新都有负载均衡硬件产品，只是 F 5 在这类产品中影
响最大，所以经常说 F 5 负载均衡。
如果接入层选择 F 5 硬件完成负载均衡，前端进入的用户流量就会被“打”到 F 5 硬件，再通过
F 5 转发到上游的 LVS/Nginx。
硬负载均衡的性能很高，通常可以达到 100 万 TPS，但 F 5 硬件价格昂贵，相对来说，是一种性
价比很低的横向扩展策略。


###### 硬负载均衡的优点如下：

######  功能强大：全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡。

######  性能强大：硬负载均衡可以支持 100 万 TPS 以上的并发，相比较而言，软负载均衡只支持 10

###### 万 TPS 级并发量，硬负载均衡的性能是软负载均衡的十倍甚至数十倍。

######  稳定性高：商用硬负载均衡经过严格测试，经过大规模使用，稳定性高。

```
 支持安全防护：硬件均衡设备除具备负载均衡功能外，还具备防火墙、防DDoS攻击等安
全功能。
硬负载均衡的缺点是：
 价格昂贵。
 扩展能力差。硬件设备可以根据业务进行配置，但无法进行扩展和定制。
```
#### 1. 3. 2 软负载均衡

###### 什么是软负载均衡？顾名思义，软负载均衡通过负载均衡软件来实现负载均衡功能。常见的

负载均衡软件有 LVS 和 Nginx。虽然一般情况下，LVS 是属于第 4 层的负载均衡软件，但是 LVS 本质
上可以工作在 2 ~ 4 层。同理，虽然一般情况下，Nginx 是属于第 7 层的负载均衡软件，但是 Nginx 本
质上可以工作在 4 ~ 7 层。这里所谓的层来自 OSI 模型结构，具体如下：

```
 第 1 层：物理层。硬负载均衡（如F 5 ）实际上属于这一层。
 第 2 层：数据链路层。LVS的DR模型属于这一层，在此模型中可以通过修改MAC地址实现
高性能的负载均衡。
 第 3 层：网络层，如IP，ICMP。
 第 4 层：传输层，如TCP、UDP。LVS的NAT模型属于这一层。经过合理的配置，Nginx也
可以进行TCP的负载均衡。
 第 5 层：会话层，如DNS、SMTP（简单邮件传输协议）。
 第 6 层：表示层，如Telnet、SNMP（简单网管协议）
 第 7 层：应用层，如HTTP、NFS、FTP、TFTP等。Nginx主要负责这一层的负载均衡。
LVS是Linux内核的 2 ~ 4 层负载均衡软件，大致作用如下：
 LVS主要用于多服务器的负载均衡。
 实现高性能、高可用的服务器集群技术。它可以把许多低性能的服务器组合在一起形成一
个超级服务器。
 配置非常简单，且有多种负载均衡的方法。
 稳定可靠，即使集群的服务器中某台服务器无法正常工作，也不影响整体效果。
 LVS可扩展性非常好。
Nginx是 4 ~ 7 层的负载均衡软件，大致作用如下：
 反向代理：将多台服务器代理成一台服务器。
 负载均衡：将多个请求均匀分配到多台服务器上，减轻每台服务器的压力，提高服务的吞
吐量。
```

######  动静分离：可以用作静态文件的缓存服务器，以提高访问速度。

 脚本编程：可以通过 Lua 执行高性能 Lua 脚本，从而完成超高并发场景下的业务预处理、用
户的安全校验等操作。
Nginx 和 LVS 的区别就在于协议和灵活性，Nginx 不但可以支持 TCP 的负载均衡，而且还可以支
持 HTTP、E-mail、Websocket 等应用层协议；而 LVS 是第 4 层的负载均衡，和应用层协议无关，反
过来说，LVS 几乎支持所有应用层协议（第 7 层的协议），例如，聊天、数据库等，如图 1 - 3 所示。

图 1 - 3 接入层使用软负载均衡的架构图
软件和硬件的最主要区别就在于性能：硬负载均衡性能远远高于软负载均衡性能。具体来说，
Nginx 的性能是万级，一般来说，一个 Nginx 大概能到 5 万 TPS；LVS 的性能是 10 万级，据说可达到
80 万 TPS；而 F 5 设备的性能是百万级，从 200 万 TPS 到 800 万 TPS 都有。
软负载均衡的优点如下：
 简单：无论是部署还是维护，软负载均衡都比较简单。
 便宜：只需买台服务器，装上负载均衡软件即可。
 灵活：可以根据业务进行负载均衡算法的选择；也可以根据业务进行负载均衡算法的扩展。
例如，可以通过 Nginx 的插件来实现业务的负载均衡算法定制化。
软负载均衡的缺点如下：
 性能一般：一个 Nginx 大约能支撑 5 万 TPS 并发量。
 安全防护弱：在安全防护领域，软负载均衡没有硬负载均衡那么强大，软负载均衡一般不
具备防火墙和防 DDoS 攻击等安全功能。
当然，软负载均衡的最大优势是比硬负载均衡便宜。


#### 1. 3. 3 LVS 和 Nginx 的配合使用

一般来说，为了更好地发挥 LVS 和 Nginx 各自的优势，在生产项目中会将二者进行配合使用，
如图 1 - 4 所示。

```
图 1 - 4 接入层LVS和Nginx配合使用的架构图
接入层LVS和Nginx配合使用的策略如下：
 Nginx工作在网络的应用层，主要做反向代理；LVS工作在网络层，主要做负载均衡。
 一般来说，LVS处于接收请求的最前端、最前线，而Nginx可作为LVS节点机器使用。这主
要是因为Nginx的载度和稳定度不及LVS。
 Nginx可以结合Lua脚本完成一些超高并发的简单操作，比如秒杀令牌的发放、用户的黑白
名单校验等。
 Nginx对网络的依赖较小，LVS就比较依赖网络环境。
```
#### 1. 3. 4 DNS 负载均衡

###### 前面所介绍的 LVS 是对同一个地理区域，甚至是同一个机房的同一个集群内的物理主机之间

的负载均衡，在术语上属于 SLB（ServerLoadBalancing）类型。还有一种更大地理范围的负载均
衡技术，是在不同地理区域的物理集群之间进行负载均衡，它在术语上叫作 GSLB（GlobalServer
LoadBalance，全局负载均衡）。SLB 一般局限于一定的区域范围内，其目标是在特定的区域范围


###### 内寻找一台最适合的节点提供服务。GSLB 主要的目的是在整个网络范围内将用户的请求定向到最

###### 近的节点（或者区域）。因此，就近性判断是全局负载均衡的主要功能。这里的负载均衡不只是简

###### 单的流量均匀分配，而是根据策略的不同实现不同场景的应用交付。

###### 在实际的应用中，GSLB 技术一般和 DNS 一起使用，所以这里把 GSLB 负载均衡技术归纳到 DNS

###### 负载均衡的范围内。

什么是 DNS 呢？DNS 是 DomainNameSystem 的缩写，也就是域名解析系统，它的作用非常简
单，就是根据域名查出对应的 IP 地址。可以把 DNS 想象成一本巨大的电话本。举个例子，如果要
访问域名 www. 163 .com，首先要通过 DNS 查出这个域名对应的 IP 地址，比如 112. 48. 162. 8 ，然后才
能通过这个 IP 地址进行服务的访问。DNS 的作用就是把域名解析到一个具体的 IP 地址。
DNS 的技术非常复杂，复杂的原因不在于 DNS 有很复杂的原理，而是在于 DNS 的配置和使用
涉及大量的技术细节。由于篇幅的原因，这里不对 DNS 做细节的阐述，仅仅聚焦于 DNS 的负载均衡
技术。DNS 的负载均衡技术分为两类：A 记录解析负载均衡和智能 DNS 负载均衡。

 A 记录解析负载均衡
ISP（InternetServiceProvider）一般会有自己的 DNS 服务器，在这些 DNS 服务器上，一个域名
可以绑定多条 A 记录（IP 记录）。在进行域名解析的时候，ISP 的 DNS 服务器可以在多个 A 记录之间
进行负载均衡。A 记录解析负载均衡的流程如图 1 - 5 所示。

```
图 1 - 5 A记录解析负载均衡的流程
由图 1 - 5 可以看出，在DNS服务器中可以为一个域名配置多个A记录，如：
http://www.crazymaker.comINA 10. 1. 1. 1
http://www.crazymaker.comINA 10. 1. 1. 2
http://www.crazymaker.comINA 10. 1. 1. 3
```

###### 因此，每次域名解析请求都会根据对应的负载均衡算法计算出一个不同的 IP 地址并返回，这

###### 样在 A 记录中配置多个服务器就可以构成一个集群，并可以实现负载均衡。

图 1 - 5 中，用户请求www.crazymaker.com，DNS 根据 A 记录和负载均衡算法计算得到一个 IP 地
址 10. 1. 1. 3 ，并返回给浏览器，浏览器根据该 IP 地址访问真实的物理服务器 10. 1. 1. 3 。所有这些操作
对用户来说都是透明的，用户只需知道www.crazymaker.com这个域名。
DNS 的 A 记录解析负载均衡有如下优点：
1 ）将负载均衡的工作交给 DNS，省去了网站管理和维护负载均衡服务器的麻烦。
2 ）技术实现比较灵活方便、简单易行、成本低，可用于大多数 TCP/IP 应用。
3 ）对于部署在服务器上的应用来说不需要进行任何的代码修改即可实现不同机器上的应用
访问。
4 ）除了简单的 A 记录轮询，一些 DNS 软件还支持基于地理位置的域名解析，即会将域名解析
成距离用户地理位置最近的一个服务器地址，这样就可以加速用户访问，改善性能。

有优点必有缺点，DNS 的 A 记录解析负载均衡也存在如下缺点：
1 ）DNS 是多级解析的，每一级 DNS 都有缓存机制，都可能缓存 A 记录。这种缓存会带来新的
问题，就是当某台服务器下线之后，即使运维人员去 ISP 的 DNS 服务器修改了 A 记录，修改后新的
负载均衡算法或策略生效也需要较长的时间，在这段时间，DNS 仍然会将域名解析到已下线的服务器
上，最终导致用户访问失败。
2 ）减少 DNS 的缓存刷新时间之后，可能会造成额外的网络问题。为了使本地 DNS 服务器和其
他 DNS 服务器及时交互，保证 DNS 数据及时更新，一般都要将 DNS 的刷新时间设置得比较小，但太
小将会使 DNS 流量大增造成额外的网络问题。
3 ）A 记录负载均衡的策略有限，不能够按服务器的处理能力来分配负载。DNS 负载均衡采用
的是简单的轮询算法，不能区分服务器之间的差异，不能反映服务器当前的运行状态，所以其负载
均衡效果并不是太好。

 智能 DNS 负载均衡
ISP 一般会有自己的 DNS 服务器，在这些 DNS 服务器上，一个域名可以绑定多条 A 记录（IP 记
录），也可以通过 CNAME 别名将这个域名绑定到另一个智能的域名服务器。这种专用的、智能的
域名服务器具有动态在多个目标服务器上直接进行更加智能化的负载均衡的能力。
在 DNS 配置的时候，A 记录、CNAME 记录是常见的配置类型，二者的区别如下：
 A 记录：全称为 Address 记录，又称 IP 指向记录。用户可以在此设置子域名并指到目标主机
地址上，从而实现域名到 IP 地址的转换。
 CNAME 记录：又称别名记录，相当于给域名起个别名。和 A 记录一样，是一种指向关系，
只是指向了另一个域名服务器。
在 DNS 服务器配置目标域名（如www.crazymaker.com）时，通过 CNAME 别名将目标域名设置
为具有智能 DNS。智能 DNS 具有更加智能的负载均衡能力，它会先根据一些静态或动态策略进行目
标服务器的智能计算，这些策略大致如下：


######  服务器的“健康状况”。

######  地理区域距离。

######  会话保持。

######  响应时间。

######  IP 地址权重。

######  会话能力阈值。

######  往返时间（TTL）。

######  其他信息，包括服务器当前可用会话数、最少选择次数、轮询等。

###### 智能 DNS 的负载均衡策略非常复杂，最终的目标是返回给用户最合适的 IP（列表）。智能 DNS

###### 的负载均衡流程如图 1 - 6 所示。

图 1 - 6 智能 DNS 的负载均衡流程
由图 1 - 6 可以看出，在 DNS 服务器中可以为一个域配置一个 CNAME 别名记录，如：
[http://www.crazymaker.com](http://www.crazymaker.com) NS ns 1 .crazymaker. com
在上面的配置中，ns 1 .crazymaker. com 是智能 DSN 域名服务器的地址，客户端对域名解析的请
求最终都会提交给 ns 1 .crazymaker. com 去执行智能解析，实现更加智能化的负载均衡。
图 1 - 6 中，广州的用户请求www.crazymaker.com，智能 DNS 负载均衡算法计算得到一个广州机
房的 IP 地址 10. 1. 1. 3 并返回给客户端，客户端根据该 IP 地址访问真实的物理服务器 10. 1. 1. 3 。北京的
用户请求www.crazymaker.com，智能 DNS 负载均衡算法计算得到一个北京机房的 IP 地址 10. 1. 1. 1 并


###### 返回给客户端，客户端根据该 IP 地址访问真实的物理服务器 10. 1. 1. 1 。所有这些智能化的解析操作

对用户来说都是透明的，用户只需知道www.crazymaker.com这个域名就可以访问就近的服务器。

1. (^4) 动静分离与接入层的缓存架构

###### 只要需要做性能优化、提升并发访问能力，就可以考虑缓存架构。这在尼恩的 3 高架构理论中

###### 已经被归纳为一条思想，或者说抽取为一种模式。从物理层的 CPU 内部到操作系统的设备管理器内

部，到中间件 Netty、RocketMQ 的内部实现，到高并发 Web 应用架构，无一例外，都充斥着缓存架
构的模式和思想。
在大部分应用中，静态资源（如 HTML 文件、CSS 文件、JavaScript 文件等）往往变化较少，但
是动态资源（如后端的 Rest 接口）却往往变化较大，如果每次访问时静态资源和动态资源全部都重
新加载，则既浪费带宽，又影响性能。
能不能通过缓存架构在最为接近用户的地方，或者说在最接近使用的地方，把静态资源缓存
起来，从而提升静态资源的访问速度呢？当然是可以的。
静态资源的缓存策略包括了接入层网关缓存和 CDN 缓存。

#### 1. 4. 1 接入层网关缓存

###### 使用静态资源的缓存策略有一个前提，就是静态资源和动态资源分离，或者说解耦。为什么

###### 静态资源和动态资源需要分离呢？只有将静态资源和动态资源分开处理之后，才能比较方便地通过

接入层网关 Nginx 对静态资源进行缓存。
接入层网关缓存对静态资源的缓存策略可以分为两种：
 将静态资源的 URL 请求结果暂存到 Nginx 本地缓存，后面访问直接从本地缓存返回。
 将静态资源的 URL 请求结果暂存到单独的缓存服务（如 Varnish 或者 Squid），后面访问直
接从缓存服务返回。
Varnish 或者 Squid 是专业的缓存服务，类似于动态资源的缓存服务 Redis 或者 MemCache，可以
理解为分布式缓冲；而 Nginx 本地缓存由第三方模块完成，可以理解为进程内缓冲，类似 Java 中
的 Map。
在旧的应用架构中前后端是没有分离的，静态资源存放在后端的 Web 应用的资源目录中，部
署的时候，静态资源和动态资源一起部署。但是，目前大部分应用已经演进为前后端分离的架构，
基本上都是 SpringCloud+Vue 架构，Vue 工程为前端工程。在新的架构中，前端工程的代码一般独
立部署，大部分情况都通过 Nginx 部署。所以说，在前后端分离的架构中已经直接将主要的静态资
源部署在 Nginx 上，如图 1 - 7 所示。
另外，还可以通过配置 expires、cache-control、if-modified-since 来对浏览器端的缓存行为进行
控制，使得浏览器端在一段时间内对于静态资源不会重复请求。当然，这一层可以称为浏览器端的
缓存。


```
图 1 - 7 接入层缓存使用的架构图
```
#### 1. 4. 2 CDN 加速

###### 前面提到：缓存架构，是尽可能在最接近用户的地方，或者说在最接近使用的地方，把静态

资源缓存起来，从而提升静态资源的访问速度。那么，有没有比接入层网关 Nginx 更加接近用户的
地方呢？有，就是 CDN 的就近节点。所以对于静态资源而言，除了通过接入层网关 Nginx 进行缓存
加速之外，还可以缓存到更加靠近用户的地方，即 CDN 就近节点（或者说就近的 PoP 点）。
什么是 CDN 呢？CDN 的全称是 ContentDeliveryNetwork，即内容分发网络。CDN 是构建在现
有网络基础之上的智能虚拟网络，依靠部署在各地的边缘服务器通过中心平台的负载均衡、内容分
发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。
CDN 的关键技术主要有内容存储和分发技术。在 CDN 技术中，有一个核心的概念叫作 PoP 点。
什么是 PoP 点呢？在计算机网络中，PoP（Point-of-Presence）表示入网点。PoP 点位于企业网
络的边缘外侧，是访问企业网络内部的进入点，外界提供的服务通过 PoP 点进入，这些服务包括互
联网接入、广域连接以及电话服务（PSTN）。PoP 点提供通往外部服务和站点的链路，可以直接
连接到一家或多家互联网服务提供商（ISP），企业内部用户便可以通过这些链路来访问互联网。
CDN 厂商的覆盖范围往往更广，每个运营商在每个地区都有自己的 PoP 点，所以总有更加靠近
用户的相同运营商的就近地点的 CDN 节点就近获取静态数据，避免了跨运营商和跨地域的访问。
在使用了 CDN 之后，用户访问资源的时候会通过 CDN 专用的域名进行访问，在进行 DNS 解析
的时候，会将域名的解析权交给 CDN 厂商的 DNS 服务器，而 CDN 厂商的 DNS 服务器可以通过 CDN
厂商的 GSLB 找到最接近客户的 POP 点，将数据返回给用户。在使用了 CDN 之后用户访问资源的请
求执行流程如图 1 - 8 所示。

```
Web Web
```

图 1 - 8 在使用 CDN 之后用户访问资源的请求执行流程
当在 CDN 中没有找到缓存数据的时候，则需要到真正的服务器中去拿，这个称为回源。仅非
常少数的流量需要回源，大大减少了服务器的压力。
在 CDN 网络中，这些分布在各个地方的各个数据中心的 PoP 节点，在术语上也叫作“边缘节点”
（edgenode）。由于边缘节点数目比较多，但是每个 PoP 节点存储规模比较小，不可能缓存下来所
有的东西，因而可能无法命中，这样就会在边缘节点之上建立区域节点，区域节点规模就要更大，
缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。
如果还不命中，就只好回源网站访问了。所以，CDN 的内容分发系统的架构是一种多层级架构，
具体如图 1 - 9 所示。

```
图 1 - 9 CDN的内容分发系统的多层级架构
```

###### 从图 1 - 9 中可以看到，CDN 系统的缓存是一层一层的，查找缓存的时候一路向上层查找。这类似

于 JVM 里边的双亲委派 Class 查找机制，在查找缓存内容时，如果最近的北京局找不到，就到它的上
一级华北局找，如果华北局找不到，再到中心节点找，如果还找不到，就回源到最初的业务系统。
当然，对于并发量或者突发流量不高的场景，CDN 的静态资源可以在访问的时候加载，类似
于 Java 单例模式的懒加载。对于并发量或者突发流量很高的场景，CDN 的静态资源可以预加载，就
是通过 CDN 的特定接口，将内容主动推送到 CDN。

#### 1. 4. 3 接入层的缓存架构的原则

###### （ 1 ）缩短用户的请求距离，使用 CDN 进行静态资源加速

###### CDN 就是将静态的资源分发到位于多个地理位置机房中的服务器上，因此它能很好地解决数

###### 据就近访问的问题，也就加快了静态资源的访问速度。

###### （ 2 ）前端工程，建议迁移到最外层网关

对于有多层 Nginx 反向代理的复杂应用，可以把前端工程部署在最外层的 Nginx 反向代理，如
图 1 - 10 所示。

图 1 - 10 前端工程部署在最外层的 Nginx 代理
（ 3 ）缓存静态资源
将静态资源的 URL 请求结果暂存到 Nginx 本地缓存，或者单独的缓存层如 Varnish 或者 Squid。
（ 4 ）动态数据静态化
在动静分离之后，静态页面可以很好地缓存，那么，动态的数据能不能被缓存呢？答案是可
以的。首先，动态的数据来自于后端服务，在缓存中没有数据的时候还是会向后端服务进行请求，
然后进行动态数据静态化，然后，对静态化之后的数据进行缓存。在接入层还可以通过 Redis 或者
Memcached 对动态资源进行缓存。
动态数据静态化之后的一个问题是：如果后端的动态数据变化了，如何让缓存中的数据保持
一致性呢？这就需要引入缓存一致性策略。比如，可以让接入层缓存定时轮询后端的应用，当有数
据修改的时候，先进行新版本的动态数据的静态化，然后更新到缓存。轮询策略的缺点是更新的速
度有些慢，对于大促场景下的并发访问高的页面可以进行如此的处理。
更为简单的数据一致性策略是：一旦有数据变更就主动刷新动态数据缓存，保障数据的及时
更新。这种策略适用于数据强一致的场景。
加入了缓存架构之后，接入层数据访问的流程大致如图 1 - 11 所示。


```
图 1 - 11 加入缓存架构之后，接入层数据访问的流程
```
### 1. 5 服务层的横向扩展高并发架构

###### 接入层的核心职责：接收用户请求，通过多种策略（如缓存策略）提升系统的读写吞吐量，

###### 通过多种策略（如限流策略）防止系统雪崩，通过多种策略（如黑名单）提升系统的访问安全性。

###### 服务层的核心职责：实现核心业务逻辑，接收用户请求参数，完成业务处理之后最终返回 HTML

###### 或者 JSON 给用户。

在 SpringCloud 分布式应用中，接入层通过 LVS 或者 Nginx 可以将请求路由到服务层多个 Spring
CloudGateway 网关服务，请求顺利从接入层进入服务层。服务层的横向扩展高并发架构可以分为：

```
 微服务网关的高并发横向扩展。
 微服务Provider（提供者）的高并发横向扩展。
 微服务Provider的自动伸缩。
```
#### 1. 5. 1 微服务网关的高并发横向扩展

当微服务层网关 Zuul/SpringCloudGateway 成为瓶颈时，可以进行横向扩展：增加服务节点数
量，新增 Zuul/SpringCloudGateway 服务的部署。在接入层 Nginx 配置中配置新的 Zuul/SpringCloud


Gateway 的 IP 和端口就能扩展服务网关的性能，做到理论上的无限高并发。
微服务网关的高并发横向扩展如图 1 - 12 所示。

```
图 1 - 12 微服务网关的高并发横向扩展
```
#### 1. 5. 2 微服务 Provider 高并发横向扩展

微服务统一的入口是微服务网关 Zuul 或者 SpringCloudGateway，那么微服务网关和接入层网
关 Nginx 最大的不同是什么呢？答案是微服务网关能自动发现后端微服务 Provider（提供者），而接
入层网关仅有反向代理、负载均衡的作用，并不能进行后端微服务 Provider 的自动发现。当微服务
Provider 有所变化时，微服务网关能够对目标 Provider 进行动态发现、动态的负载均衡，而接入层网
关却做不到这点。
微服务 Provider 的动态发现、动态的负载均衡，不仅是微服务网关的基础能力，也是各个 RPC
客户端的能力。各个微服务 Provider 之间需要进行 RPC 远程调用时，作为 RPC 客户端，对目标 Provider
也需要进行动态发现、动态的负载均衡。而各微服务 Provider 可以动态地加入集群（自动上线），
或者可以动态地离开集群（自动下线）。
常见的 RPC 框架是 Dubbo 和 SpringCloud，服务的注册中心可以是 ZooKeeper、Consul、Etcd、
Eureka、Nacos 等。
微服务 Provider 高并发横向扩展策略就是基于注册中心的服务注册发现、RPC 客户端的动态发
现、动态负载均衡机制完成的。而 RPC 客户端、微服务网关对目标微服务 Provider 的动态发现、动
态的负载均衡，需要基于注册中心（如 Eureka、Zookeeper、Nacos 等）去实现。
注册中心具备微服务 Provider 的管理能力、健康检查能力，能够完成微服务 Provider 的自动注


###### 册与发现，并且将这些信息作为集群的重要元数据信息。RPC 客户端、微服务网关会从注册中心（如

Eureka、Zookeeper、Nacos 等）去定期更新元数据信息，并且缓存在本地，然后基于这些元数据信
息实现微服务 Provider 的动态发现、动态的负载均衡。
在生产环境上，一旦发现 Java 服务（微服务 Provider）集群的吞吐能力不足，具体来说就是集
群吞吐量不足以支撑线上的用户请求规模，就需要进行微服务 Provider 的高并发横向扩展，开启新
的 Java 服务。
微服务 Provider 的高并发横向扩展架构大致如图 1 - 13 所示。

```
图 1 - 13 微服务Provider的高并发横向扩展架构
```
#### 1. 5. 3 微服务 Provider 的自动伸缩

微服务 Provider 的自动伸缩也叫作自动扩容、自动缩容。自动伸缩是和手动伸缩相对而言的，
就是通过运维工具实现资源监控，然后根据资源的紧张程度自动开启新的 Java 服务（微服务
Provider），或者自动关闭一些空闲的 Java 服务。
传统的微服务 Provider 扩容策略是手动的，由运维人员手动进行。一旦发现现有的微服务
Provider 能力不够，运维人员就会开启新的 Java 服务并且动态地加入集群；一旦发现现有的微服务
Provider 能力有富余，运维人员就会关闭部分 Java 服务，这些 Provider 会动态地离开集群（自动下线）。
手动伸缩的问题是无法面对突发流量。一旦出现突发流量，等到运维人员收到监控系统的资
源预警信息后再去进行微服务 Provider 扩容，中间会有较大的时间延迟，在这个时间延迟内，系统
可能已经发生雪崩了。所以，对于会出现突发流量的系统需要用到自动扩容、自动缩容的策略。


常见的微服务 Provider 的自动伸缩策略有以下两种：
1 ）通过 KubernetesHPA 组件实现自动伸缩。
2 ）通过微服务 Provider 自动伸缩伺服组件实现自动伸缩。
自动伸缩策略之 1 ：通过 KubernetesHPA 组件实现自动伸缩。
为了方便物理资源的细粒度管理和调度，现在大多数应用都是基于 Kubernetes（简称 K 8 S）容
器化平台部署的。在 K 8 S 平台中，Pod 是最小部署单元，负责运行实际的应用程序。一般来说，在
进行微服务部署的时候，一个 Java 进程（微服务实例）作为一个 Pod（容器）进行部署。K 8 S 容器
化平台有一项功能叫作 KubernetesHPA（HorizontalPodAutoscaling），通过 HPA 可以进行 Pod 水平
自动伸缩。
通过 KubernetesHPA 功能，只需简单的配置，微服务 Provider 集群就可以利用监控指标（CPU
利用率等）自动地扩容或者缩容，或者说自动地增加或者减少服务 Pod 数量。KubernetesHPA 的自
动伸缩架构具体如图 1 - 14 所示。

图 1 - 14 KubernetesHPA 的自动伸缩架构
在 K 8 S 平台上，Deployment（用来管理发布的控制器）或 RC（ReplicationController）的主要
功能之一是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户
指定的副本数量。Deployment 配置文件为 Pod 提供声明式更新，通过 Deployment 配置文件配置 Pod
的目标状态，DeploymentController 就会将 Pod 的实际状态改变为配置的目标状态。Deployment 是
一种资源，是属于控制器（Controller）类型的资源。
Deployment 控制器并不直接管理 Pod，而是通过管理 ReplicaSet（简称 RS，是 RC 的增强版）来
间接管理 Pod，即 DeploymentController 管理 ReplicaSet，ReplicaSet 管理 Pod。
下面的例子是 K 8 S 官方的一个 Deployment 配置文件，文件名称为 hap-deploy-demo. yaml，使用
这个配置文件可以创建一个 ReplicaSet，这个 ReplicaSet 会创建 1 个 Nginx 应用的 Pod。
apiVersion: apps/v 1
kind:Deployment
metadata:


```
name:hpa-nginx-deploy
spec:
selector:
matchLabels:
run:hpa-nginx-deploy
replicas: 1
template:
metadata:
labels:
run:hpa-nginx-deploy
spec:
containers:
```
- name:nginx
    image: hub. test. com/library/mynginx: v 1 #镜像地址
    ports:
    - containerPort: 80
    resources:
       limits: #最大限制
          cpu: 500 m #CPU最大是 500 微核
       requests: #最低保证
          cpu: 200 m #CPU最小是 200 微核
使用 kubectlcreate 命令创建 Deployment Controller（-ntest 指定命名空间）：
$ kubectlcreate-fhpa-deploy-demo. yaml-ntest
deployment. apps/hpa-nginx-deploycreated
使用 kubectlgetdeploy 命令查询 Deployment Controller：
$ kubectlgetdeploy-ntest
NAME READY UP-TO-DATE AVAILABLE AGE
hpa-nginx-deploy 1 / 1 1 1 37 s
使用 kubectlgetpod 命令查询 Pod 状态信息：
$ kubectlgetpod-ntest
NAME READY STATUS RESTARTS AGE
hpa-nginx-deploy- 85 ff 79 dd 56 - jcl 67 1 / 1 Running 0 6 m 41 s
HorizontalPodAutoscaler（HPA）在 K 8 S 平台中被设计为 Controller，可以简单地使用 kubectl
autoscale 命令来创建。
$kubectlautoscaledeploymenthpa-nginx-deploy--cpu-percent= 20 - -min= 1 - -max= 10 - n
test
horizontalpodautoscaler. autoscaling/hpa-nginx-deployautoscaled
此命令创建了一个关联资源 hpa-nginx-deploy 的 HPA，最小的 Pod 副本数为 1 ，最大为 10 。HPA
会根据设定的 CPU 使用率（ 20 %）动态地增加或者减少 Pod 数量。kubectlautoscale 命令的参数如下：

```
 --cpu-percent= 20 HPA会通过Pod伸缩保持平均CPU利用率在 20 %以内。
 --min= 1 HPA允许Pod的伸缩范围，最小的Pod副本数为 1 。
 --max= 10 HPA允许Pod的伸缩范围，最大的Pod副本数为 10 。
可以通过kubectlgethpa命令查看HPA当前状态：
$ kubectlgethpa-ntest
NAME REFERENCE TARGETS MINPODS MAXPODSREPLICAS AGE
hpa-nginx-deploy Deployment/hpa-nginx-deploy 0 %/ 20 % 1 10 1 37 m
HPA由一个控制循环实现，循环周期由kube-controller-manager中的--horizontal-pod-autoscaler-
```

sync-period 参数指定（默认是 15 秒）。在每个周期内，kube-controller-manager 会查询 HorizontalPodAutoscaler
中定义的指标度量值，并且与创建时设定的值和指标度量值作对比，从而实现自动伸缩的功能。
在 HPA 的第一个版本中，需要通过 Heapster 组件提供 CPU 和内存指标。在 HPAV 2 版本中，需要
安装 MetrcisServer，MetricsServer 可以通过标准的 KubernetesAPI 把监控数据暴露出来。
HPAController 的自动伸缩原理：通过调整 Pod 副本数量使得 CPU 使用率（或者其他的配置指标）
尽量向期望值靠近，而且不是完全相等。另外，自动扩展的决策可能需要一段时间才会生效，例如
当 Pod 所需要的 CPU 负荷过大从而开始扩容，但是在创建一个新 Pod 的过程中，系统的 CPU 使用量可
能同样会有一个攀升的过程。所以，在每一次做出决策后的一段时间内将不再进行扩展决策。对于
扩容而言，这个时间段为 3 分钟，缩容为 5 分钟。
HPAController 每次扩容或者缩容的 Pod 数量的计算方法为：Ceil（当前采集到的使用率/用户自
定义的使用率×Pod 数量）；每次最大扩容 Pod 数量不会超过当前副本数量的 2 倍。

自动伸缩策略之 2 ：通过微服务 Provider 自动伸缩伺服组件实现自动伸缩。
使用 K 8 S 平台的 HPAController 方案的问题是必须基于 K 8 S 平台使用。在实际的生产项目中，
很多项目并不是基于 K 8 S 平台进行资源管理和应用部署的，而是通过自动化 Shell 脚本的方式，甚至
手工方式进行资源管理和应用部署的。在非 K 8 S 部署的场景中，HPAController 自动伸缩的方案就
没有办法使用了，只能通过自定义的微服务 Provider 伺服组件实现自动伸缩。
自定义的微服务 Provider 伺服组件在架构上一般分为两个部分：
 AutoScaler 伺服进程。
 AutoScaler 监控中心。
通过微服务 Provider 自动伸缩伺服组件实现自动伸缩的系统架构，大致如图 1 - 15 所示。

图 1 - 15 通过微服务 Provider 自动伸缩伺服组件实现自动伸缩的系统架构
AutoScaler 伺服进程的职责：作为伺服进程运行在服务器上；通过 RPC 机制和 AutoScaler 监控
中心进行通讯，接收扩容和缩容命令；根据命令进行微服务 Provider 进程的扩容和缩容管理。


AutoScaler 监控中心的职责：收集各种系统资源、服务器资源、微服务 Provider 实例的参数指
标，然后根据配置的指标阈值计算实际需要的微服务 Provider 实例数量，产生扩容和缩容的命令；
通过 RPC 机制把扩容和缩容的命令发送到相应的 AutoScaler 伺服进程，由 AutoScaler 伺服进程执行扩
容和缩容的命令，完成扩容和缩容操作。

1. (^6) 缓存层的高并发架构
狭义的缓存层指的是动态数据的分布式缓存中间件 Redis 或者 Memcached。
缓存层的高并发架构指的是在服务层和数据层直接加上一层分布式缓存中间件，实现数据的
高并发写入/读取，从而提升性能、缓解数据库压力。缓存的高并发架构如图 1 - 16 所示。
图 1 - 16 缓存的高并发架构
如何使用缓存呢？有三种经典的缓存模式：
 旁路缓存（Cache-AsidePattern）模式。
 读/写穿透（Read/WriteThrough）模式。
 异步回写（WriteBehind）模式。
2. 6. (^1) 旁路缓存模式
旁路缓存模式又称为旁路路由策略，在这种模式中，读取缓存、读取数据库和更新缓存的操
作都在应用程序中完成。此模式是业务系统最常用的缓存策略。
旁路缓存模式分为读缓存和写缓存。旁路缓存模式在读的时候，先读缓存，缓存命中（cachehit）
的话则直接返回数据；如果缓存没有命中（cachemiss），就去读数据库，从数据库中取出数据放
入缓存，同时将读到的数据返回给数据查询方。
旁路缓存模式的读操作流程具体如下：


###### 应用程序接收用户的数据查询请求。

###### 应用程序优先从缓存中查找数据。

```
如果数据存在（缓存命中，即cachehit），则从缓存中查询出来，返回查询到的数据。
如果数据不存在（缓存未命中，即cachemiss），则从数据库中查询数据并存入缓存
中，返回查询到的数据。
旁路缓存模式的读操作流程具体如图 1 - 17 所示。
```
图 1 - 17 旁路缓存模式的读操作流程
旁路缓存模式的写操作流程具体如下：
接收用户的数据写入请求。
先写入数据库。
再写入缓存。
旁路缓存模式的写操作流程具体如图 1 - 18 所示。
数据什么时候从数据库（如 MySQL 集群）加载到缓存（如 Redis 集群）呢？有以下两种加载模
式可被选择：懒汉模式、饿汉模式。懒汉模式、饿汉模式可以分别理解为及时加载模式和延迟加载
模式。

 所谓懒汉模式，就是在使用时临时加载缓存。具体来说，当需要使用数据时，从数据库中
把它查询出来，然后写入缓存。第一次查询之后，后续的请求都能从缓存中查询到数据。
 所谓饿汉模式，就是提前预加载缓存。具体来说，在项目启动时，预加载数据到缓存，当
需要使用数据时，能直接从缓存获取数据，而不需要从数据库获取。
饿汉模式可以提前预加载数据到缓存，能极大地提升请求处理的性能、提升系统的吞吐量。
此模式适合缓存那些不经常变更的数据（例如商品类目数据），或者那些访问非常频繁的极热数据


###### （例如秒杀商品数据）。

```
图 1 - 18 旁路缓存模式的写操作流程
```
```
懒汉模式、饿汉模式这组名词来自于Java的单例模式，关于Java的单例模式的详
细介绍，请参考《Java高并发核心编程卷 2 （加强版）：多线程、锁、JMM、JUC、高并发
设计模式》。
```
#### 1. 6. 2 读/写穿透模式

###### 在读写穿透模式中，应用程序把缓存作为主要数据存储。应用程序跟数据库缓存的交互都是

通过抽象缓存层完成的。而读写穿透模式（Read/WriteThrough）实际只是在缓存旁路模式
（Cache-Aside）之上进行了一层封装，封装出一层独立的缓存程序 CacheProvider，数据的读写由
CacheProvider 负责完成，它会让程序代码变得更简洁，同时也减少了数据库的负载。

```
为什么说读写穿透模式减少了数据库的负载呢？因为在这个模式中，所有的应
用实例不用直接和数据库直连，所有的应用实例都访问CacheProvider，只有CacheProvider
去直接访问缓存组件和数据库组件。
```
```
读穿透模式的读操作流程具体如下：
应用程序接收用户的数据查询请求。
应用程序通过CacheProvider查询数据。
CacheProvider自己决定从哪查询数据。如果数据存在Redis缓存（cachehit）中，
则从缓存中查询出来，返回查询到数据。
如果数据不存在Redis中（cachemiss），则CacheProvider从数据库中查询数据并
存入缓存中，返回查询到的数据。
```

###### 读穿透模式的读操作流程具体如图 1 - 19 所示。

```
图 1 - 19 Read-Through（读穿透）模式的读操作流程
```
读穿透模式的写操作流程具体如下：

```
应用程序接收用户的数据写入请求。
应用程序通过CacheProvider写入数据。
CacheProvider先写入缓存。
CacheProvider再写入数据库。
```
写穿透模式的写操作流程具体如图 1 - 20 所示。


图 1 - 20 Write-Through（写穿透）模式的写操作流程
写穿透模式下所有的写操作都经过缓存，每次 CacheProvider 向 Cache 中写数据，写完缓存
（Cache）之后，再把数据持久化到对应的数据库中去。CacheProvider 的两次写操作，都在一个事
务中完成，因此，只有两次都写成功了才是最终写成功了。CacheProvider 保证了数据一致性，但
是会带来了一些写延迟。
读/写穿透和旁路缓存模式很相似，不同点在于：在读/写穿透模式中，应用程序不需要管理从
哪去读数据（是从缓存还是数据库读取数据），它会直接从 CacheProvider 中读数据，由 CacheProvider
决定从哪查询数据，或者去哪写入数据。经过简单比较会发现：读/写穿透模式让程序代码变得更
简洁。
读/写穿透模式与旁路缓存模式另外一个显著的不同是：在写流程中，写穿透模式是直接写入
缓存，而不是删除缓存中的数据。默认情况下，旁路缓存模式都会选择删除缓存中的数据，并且是
双删。
读/写穿透模式的适用场景：写操作较多，且数据一致性要求高的场景。注意：此模式的性能
比较低。为什么呢？为了避免前面提到并发双写导致的一致性问题，需要给更新数据库和更新缓存
的操作进行同步，比如使用分布式锁，这样就会极大地降低性能。读/写穿透模式的潜在使用案例
是银行系统。

#### 1. 6. 3 异步回写模式

异步回写模式和读/写穿透模式很相似，两者都是由 CacheProvider 服务来负责缓存和数据库的读写。
与读/写穿透模式的不同：在写入时，异步回写模式只更新缓存，不同步更新数据库，而改为
异步批量的方式来写入数据库。读/写穿透模式是同步更新缓存和数据库。
异步回写模式在写入时使用了异步、批量写入模式，这是一种超高性能的写入模式，尼恩在
做 Netty 源码研究和分析时发现 Netty 在进行 IO 写入的时候，也是用的这种超高并发写入模式。实际
上，由于异步、批量写入模式的性能非常高，因此使用得非常广泛。比如消息队列中消息的异步写
入磁盘、MySQL 的 InnoDBBufferPool 机制都用到了这种策略。
很明显，异步、批量写入模式对数据一致性带来了更大的挑战。比如在缓存数据还没异步更
新到数据库的时候，如果缓存服务崩溃，就会导致那些没有来得及持久化的数据丢失。
总体来说，异步回写模式就是先写缓存，再由 CacheProvider 定时在数据库负载较低的时候写
入数据库。可以看出，此方案的缓存与数据库为弱一致性，且有丢数据的风险，需做好缓存的高可
用，对于一致性要求高的系统应慎用此方案。
所以，异步回写模式性能高，但是数据可靠性低，非常适合一些数据变化频繁、又对数据一
致性要求没那么高的场景，比如浏览量、点赞量。

#### 1. 6. 4 三大缓存使用模式的比较

###### 旁路缓存、读/写穿透、异步回写三种模式的比较：

######  旁路缓存更新模式实现起来比较简单，但是应用程序需要维护两套数据存储: 一套是缓存

```
（Cache）、一套是数据库（DB）。
 在读/写穿透模式中，应用程序和CacheProvider配合使用，简化了应用程序的开发，但是需要
```

设计和实现一套专用的 CacheProvider 组件，CacheProvider 实现起来其实是比较复杂的。


######  异步回写模式和读/写穿透模式类似，区别是异步回写模式的数据持久化操作是异步的，但

###### 是读/写穿透模式的数据持久化操作是同步的。

######  异步回写优点是速度快，因为此模式直接操作缓存，而不是直接操作数据库，然后将多次

###### 操作合并之后批量持久化到数据库。缺点是数据可能会丢失，例如系统断电时等。

#### 1. 6. 5 旁路缓存模式如何保证双写的数据一致性

###### 旁路缓存模式是日常开发中使用最多的缓存层高并发访问模式，所以面试官也喜欢围绕这种

模式进行提问。一个非常高频的问题是：旁路缓存模式（Cache-Aside）在写入的时候，为什么是
删除缓存而不是更新缓存呢？很多大厂也喜欢问这个领域的问题，下面就是一道来自于社群的美团
面试题。

美团面试题
旁路缓存模式如何保证数据库和缓存双写的数据一致性？
要完美地回答这个问题，需要把旁路缓存模式下的数据库和缓存双写的策略做一个系统化的
梳理，大概分为如下五大策略：

 策略一：先更新数据库，再更新缓存。
 策略二：先删除缓存，再更新数据库。
 策略三：先更新数据库，再删除缓存。
 策略四：延迟双删。
 策略五：先更新数据库，再基于队列删除缓存。
如果能在面试的时候把每一种策略的角色功能、适用场景、执行流程、优势弱点、改进策略
进行系统化、体系化的陈述，则无论哪个大厂都一定会十分认可候选人的能力。

1 .策略一：先更新数据库，再更新缓存
在实际的业务场景中，一种常见的并发场景是：微服务 Provider 实例 A、B 同时进行同一个数据
的更新操作。按照先更新数据库，再更新缓存的策略，微服务 Provider 实例 A、B 可能会出现下面的
执行次序：

微服务 A 去执行更新数据库（updateDB）操作。
微服务 B 去执行更新数据库（updateDB）操作。
微服务 B 去执行更新缓存（updateCache）操作。
微服务 A 去执行更新缓存（updateCache）操作。
上面的执行流程是典型的并发写入场景，具体如图 1 - 21 所示。
从图 1 - 21 中可以看出，ProviderA 进行数据的写入，ProviderB 也进行数据的写入。最终的结果
是数据库中的数据是 ProviderB 的数据，缓存中的数据是 ProviderA 的数据，出现数据库和缓存数据
不一致问题。
具体的原因是：ProviderB 更新到缓存中的数据被 ProviderA 更新到缓存中的数据覆盖了。数据
库的更新次序是先 A 后 B，理论上缓存中的数据更新也应该是先 A 后 B，即最终在缓存中的数据应该


是 ProviderB 的数据而不是 ProviderA 的数据。所以，在上述流程执行完毕后，缓存中的 ProviderA
的数据为脏数据。

图 1 - 21 先更新数据库，再更新缓存的并发执行案例
之所以出现这个问题，是因为以上流程中步骤 3 与步骤 4 执行的均为操作缓存，都是高并发的
操作，很难保证先后次序，所以缓存出现脏数据的概率很大。

2 .为何不更新缓存而是删除缓存
一个非常高频的问题是：旁路缓存模式在写入的时候，为什么是删除缓存而不是更新缓存呢？
回到上一节的例子，在图 1 - 21 中的并发写入的场景中，ProviderA 进行数据的写入，ProviderB
也进行数据的写入。
在这个例子中，写入数据库的次序如下：
 ProviderA 先发起一个写操作，更新了数据库。
 ProviderB 再发起一个写操作，更新了数据库。
现在，由于分布式系统无法保证并发操作的有序性，因此写入缓存的次序可能如下：
 ProviderB 先发起一个缓存写操作，更新了缓存。
 ProviderA 再发起一个缓存写操作，更新了缓存。
这时，缓存保存的是 ProviderA 的数据（旧数据），数据库保存的是 ProviderB 的数据（新数据），
于是出现了数据库和缓存中数据不一致的情况，缓存中出现了脏数据。
如果使用删除操作取代更新操作，则缓存不会出现上面的脏数据问题。具体如图 1 - 22 所示。
除了出现脏数据之外，更新缓存相对于删除缓存还有两点劣势：
1 ）如果写入缓存的值是经过复杂计算才得到的，更新缓存频率高的话，就会大大降低性能。
2 ）及时更新缓存属于饿汉模式，适用于数据读取高频的场景。在写多读少的情况下，数据很
多时候还没被读取到就又被更新了，这也浪费了缓存的空间，降低了性能。


图^1 -^22 删除缓存避免脏数据
3 .策略二：先删除缓存，再更新数据库
在实际的业务场景中，一种常见的并发场景是：微服务 Provider 实例 A 进行数据的写操作，而
微服务 Provider 实例 B 同时进行同一个数据的读操作。按照先删除缓存，再更新数据库的策略，微
服务 Provider 实例 A、B 可能会出现下面的执行次序：

```
微服务A去执行删除缓存（deleteCache）中数据的操作。
微服务B去执行从数据库加载（loadfromDB）数据的操作。
微服务B去执行更新缓存（updateCache）的操作。
微服务A去执行更新数据库（updateDB）的操作。
上面的执行流程是典型的并发读写场景，具体如图 1 - 23 所示。
```
```
图^1 -^23 先删缓存，再更新数据库的并发执行案例
```

从图 1 - 23 中可以看出 ProviderA 进行数据的写入，ProviderB 进行数据的查询。最终，数据库中
的数据是 ProviderA 的更新数据，缓存中的数据是 ProviderB 从数据库加载的数据，而这个数据已经
过时了，出现数据库和缓存中数据不一致的问题。
具体的原因是：ProviderB 查询缓存的时候，缓存中的数据被删除，ProviderB 只能去数据库中
查找，然后将数据更新到缓存，而 ProviderA 在 ProviderB 查询完之后更新了数据库，导致了数据库
和缓存的不一致。
出现这种数据库和缓存中数据不一致问题的根本原因：写操作是先删缓存（操作 1 ）再写数据
库（操作 2 ），如果在此期间发生并发读，读取的动作很容易发生在操作 1 和操作 2 的中间，从而读
取到过时的数据，最终导致缓存和数据库不一致。更为严重的是，读操作把过期数据刷入缓存后会
导致后面比较长时间的不一致。这个时间会一直持续到缓存过期，比如 4 个小时（以项目中的配置
时间为准）。
缓存和数据库不一致将导致一个严重的后果：因为后续的读操作都会使用缓存中的数据，所
以后面的读操作都会使用过时数据。

4 .策略三：先更新数据库，再删除缓存
先更新数据库，再删除缓存，基本上可以解决并发读写场景中缓存和数据库中数据不一致的
问题。但是，在一些特殊的场景中，还是会存在数据不一致的问题。
一种非常特殊的并发场景是：微服务 Provider 实例 A 进行数据的写操作，先写数据库（操作 1 ），
再删缓存（操作 2 ），如果由于某种原因微服务 Provider 实例 A 出现了卡顿，没有及时操作缓存，或
者说没有及时执行删除缓存的操作，简单地说就是操作 2 发生了滞后。此时，微服务 Provider 实例 B
进行一个数据的读操作，读取的次序仍然是先读缓存，再读数据库，则很容易导致数据库和缓存中
数据的不一致性。
按照先更新数据库，再删除缓存的策略，微服务 Provider 实例 A、B 可能会出现下面的执行次序：
微服务 A 去执行更新数据库（updateDB）的操作。
微服务 B 去执行从缓存中加载（loadfromCache）数据的操作。
微服务 A 去执行删除缓存（deleteCache）中数据的操作，但是发生了延迟。
上面的执行流程具体如图 1 - 24 所示。
在图 1 - 24 中的并发读写的场景中，ProviderA 进行数据的写入，ProviderB 进行数据的查询。
微服务 Provider 实例 A 先写数据库（操作 1 ），再删缓存（操作 2 ），如果 Provider 实例 A 由于卡顿
或者网络延迟等异常的问题导致操作 2 严重滞后，在操作 2 执行完成之前，数据库和缓存中的数据是
不一致的。在此期间，其他的数据读操作都会读取缓存中的过期数据，出现数据库和缓存中数据不
一致问题。
出现数据库和缓存中数据不一致问题的根本原因是：先写入数据库（操作 1 ）再从缓存中删除
数据（操作 2 ），如果在此期间发生并发读，读操作很容易发生在操作 1 和操作 2 的中间，从而导致并
发读操作从缓存读取到过时的数据，最终导致缓存和数据库中数据不一致。但是等到写操作和删除
缓存的操作执行完成之后，缓存和数据库中的数据会恢复一致性。
无论如何，策略三（先写数据库再删缓存）比策略二（先删缓存再写数据库）发生数据不一
致的时间短。相比较而言，推荐使用策略三而不是策略二。


图 1 - 24 先更新数据库，再删除缓存的并发执行案例
那么，策略三有什么问题呢？
1 ）写入数据库（操作 1 ）和删缓存（操作 2 ）之间，存在短时间的数据不一致。
2 ）如果删缓存失败，则会存在较长时间的数据不一致，这个时间会一直持续到缓存过期。
如何解决策略三中缓存删除失败所导致的数据库和缓存较长时间的数据不一致呢？可以使用
策略四：延迟双删。

5 .策略四：延迟双删
什么是延迟双删呢？延迟双删是基于策略二进行的改进，就是先删缓存，后写数据库，最后
延迟一定时间再删缓存。
在实际的业务场景中，一种常见的并发场景是：微服务 Provider 实例 A 进行数据的写操作，而
微服务 Provider 实例 B 同时进行同一个数据的读操作。按照先删缓存，后写数据库，最后延迟一定
时间再删缓存的策略，微服务 Provider 实例 A、B 可能会出现下面的执行次序：

微服务 A 去执行删除缓存（deleteCache）中数据的操作。
微服务 B 去执行从数据库中加载（loadfromDB）数据的操作。
微服务 B 去执行更新缓存（updateCache）的操作。
微服务 A 去执行更新数据库（updateDB）的操作。
微服务 A 去执行延迟删除缓存（delaydeleteCache）中数据的操作。
上面的执行流程具体如图 1 - 25 所示。
在图 1 - 25 中的并发读写的场景中，ProviderA 进行数据的写入，ProviderB 进行数据的查询。最
终微服务 Provider 实例 A 先删缓存（操作 1 ），再写数据库（操作 2 ），最后再次延迟删除缓存（操作 3 ）。
在操作 2 之前，如果发生并发读，从数据库读取到过时数据，可能出现数据库和缓存数据不一致的问题。
出现数据库和缓存中数据不一致问题的根本原因是：写操作是先删缓存（操作 1 ）再写数据库
（操作 2 ），如果在此期间发生并发读，读操作容易发生在（操作 1 ）和（操作 2 ）的中间，从数据
库读到过时数据，最终导致缓存和数据库不一致。但是，这一轮的数据不一致的持续时间不会太长。


###### 为什么呢？因为写操作还有一个兜底的动作，即再次延迟删除缓存（操作 3 ），从而保证数据一致。

###### 所以延迟双删也会存在数据不一致的问题，不过持续时间比较短而已。

图 1 - 25 先删缓存，后写数据库，再次延迟删缓存的并发执行案例
那么，策略四有什么问题呢？
1 ）如果写操作比较频繁，可能会对 Redis 造成一定的压力。
2 ）在极端情况下，第二次延迟删缓存失败，操作的效果退化到策略二：数据库和缓存存在较
长时间的数据不一致，这段时间会一直持续到缓存过期，比如 4 个小时（以项目中的配置时间为准）。

如何解决策略四的以上两个问题呢？可以使用策略五：先更新数据库，再基于队列删除缓存。
6 .策略五：先更新数据库，再基于队列删除缓存
来到策略五：先更新数据库，再基于队列删除缓存。那么，如何基于任务队列删缓存呢？实
质上，策略五是基于策略三进行的改进。首先回顾一下策略三的问题？

1 ）写数据库（操作 1 ）和删缓存（操作 2 ）之间存在短时间的数据不一致。
2 ）如果删缓存失败，则会存在较长时间的数据不一致，这个时间会一直持续到缓存过期。
策略五主要的操作次序和策略三保持一致，依然是先写数据库后删缓存。不同的是，策略五
引入队列，把删缓存的操作加入队列，后台会有一个异步线程或者进程去异步消费（即执行）队列
中的删除任务，去执行删缓存的操作。
基于队列删除缓存，可以细分为：
 基于内存队列删除缓存。
 基于消息队列删除缓存。
 基于 binlog+消息队列删除缓存。


######  第一种细分方案：基于内存队列删除缓存

###### 此方案把删缓存的操作加入任务队列，后台会有一个异步线程去异步执行任务队列中的删除

###### 任务，去执行删缓存的操作，如果缓存删除失败，可以重试多次，确保删除成功。

在实际的业务场景中，一种常见的并发场景是：微服务 Provider 实例 A 进行数据的写操作，而
微服务 Provider 实例 B 同时执行同一个数据的读操作。Provider 实例 A 先写数据库，然后将删缓存操
作加入任务队列；Provider 实例 B 则是先读缓存，没有数据再读数据库。微服务 Provider 实例 A、B
可能会出现下面的执行次序：

```
微服务A去执行更新数据库（updateDB）的操作。
微服务A将删除缓存（deleteCache）操作加入任务队列。
微服务B去执行从缓存中加载（loadfromCache）数据的操作。
消费线程从任务队列提取删除缓存（deleteCache）操作，执行删除缓存的操作，直
到删除成功。
上面的执行流程具体如图 1 - 26 所示。
```
图 1 - 26 先更新数据库，后基于内存队列删除缓存的并发执行案例
在图 1 - 26 中的并发读写的场景中，ProviderA 进行数据的写入，ProviderB 进行数据的查询。最
终微服务 Provider 实例 A 先写数据库（操作 1 ），再将删除缓存的操作加入任务队列（操作 2 ）。在
删除缓存的操作真正执行完成之前，其他的数据读操作都会读取缓存中的过期数据，出现数据库和
缓存中数据不一致的问题。但是这种不一致是短暂的。任务队列的消费线程会异步执行删除缓存的
任务，并且会不断重试确保成功，删除缓存之后，数据库和缓存中数据不一致问题就会得到解决。


###### 保存删除缓存任务的队列建议使用阻塞队列。任务队列的消费线程可参考

```
Rocketmq源码中的ServiceThread异步服务线程，其设计思想和执行性能都非常优越。后面尼
恩会通过本书配套的教学视频演示基于队列删除缓存的实操。
```
策略五也会出现数据库和缓存不一致的问题，尤其是当写操作非常频繁时，队列的任务比较
多，消费可能会比较慢，导致数据库和缓存不一致的时间会延长。在这种情况下，可以根据任务队
列的拥塞程度开启多个线程，提升并发执行的效率。
与策略四相比，策略五的优势是：
1 ）在写操作比较频繁的场景，策略四有两次删缓存操作，可能会对 Redis 造成一定的压力；策
略五只有一次删缓存操作，Redis 的压力小一半。
2 ）策略四如果删缓存失败，没有引入重试策略；策略五会多次重试，确保删缓存成功，如果
重试多次仍然不成功，可以执行运维预警。
3 ）策略四将写数据库和删缓存这两个操作耦合在了一起，没有很好地做到单一职责；策略五
将写数据库和删缓存这两个操作解耦，模块职责更加单一。

那么，策略五有什么问题呢？
1 ）如果写操作非常频繁，队列的任务比较多，消费可能会比较慢，需要引入多线程机制，加
快消费速度。
2 ）程序复杂度成倍上升，引入了消费线程、任务队列，并且还需要不断进行性能优化。
3 ）内存队列是 JVM 进程的内部队列，如果 JVM 崩溃，内存队列没有来得及处理的缓存记录删
除任务会丢失，导致这些数据的缓存记录和数据库记录会长时间不一致。

 第二种细分方案：基于消息队列删除缓存
在第一种细分方案中，将删除缓存的任务保存在内存队列；这不是高可靠的。
为了保证高可靠地删除缓存记录，这里引入高可用的独立组件——RocketMQ 消息队列。需要
注意的是，这里引入的 RocketMQ 消息队列是高可用的类型消息队列，不是单节点的类型消息队列，
从而保障消息记录的高可用，保障缓存的删除操作只要没有被成功执行，就不会丢失。
引入高可用 RocketMQ 消息队列之后，执行双写操作的 ProviderA 的操作流程有小幅度的调整。
ProviderA 需要将删除缓存的操作序列化成 RocketMQ 消息，然后写入高可用 RocketMQ 消息队列中
间件，再由专门的消费者（CacheDeleteConsumer）进行消息的消费，根据消息内容执行删除缓存
记录的工作。
数据库和 Redis 双写的场景下，ProviderA 先更新数据库，然后基于消息队列删除缓存的并发执
行案例的执行流程如图 1 - 27 所示。
引入高可用的独立组件 RocketMQ 消息队列之后，ProviderA 的写入逻辑变得很简单，删缓存的
时候，只需要发送消息到 RocketMQ 即可，大大简化了 ProviderA 程序的写入逻辑。只是为了保证消
息的高可靠传递，这里 ProviderA 在发送消息的时候需要使用同步发送模式，而不能使用异步发送
的模式。
在消息投递的环节，由 RocketMQ 高可用组件的 ACK 机制保证消息的高可靠投递。如果消息第
一次消费失败，RocketMQ 会重复多次进行投递，确保消息被正常消费，如果一直不能被成功消费，


###### 在重复投递一定的次数之后（默认 16 次），消息会进入死信队列。系统的监控程序会对死信队列进

###### 行监控，一旦发现死信消息，监控程序会进行运维警告，由运维人员解决最终的缓存删除问题。除

非 Redis 集群崩溃，一般都不会出现这种极端情况。

图 1 - 27 先更新数据库，然后基于消息队列删除缓存的并发执行案例
和基于内存队列删除缓存相比，基于消息队列删除缓存的优势是：增加了缓存删除的可靠性，
避免了因 JVM 崩溃所导致的内存队列中的记录丢失的问题。
那么，Provider 在执行数据库和缓存双写时，能不能进一步减少双写的负担，将发送删除缓存
消息的操作从双写逻辑中剥离，交给其他的组件去完成呢？答案是可以的。具体来说，就是使用基
于 binlog+消息队列删除缓存的方案。

 第三种细分方案：基于 binlog+消息队列删除缓存
以 MySQL 为例，可以使用阿里的 Canal 中间件采集在数据写入 MySQL 时生成的 binlog 日志，然
后将日志发送到 RocketMQ 队列。在消费端，可以编写一个专门的消费者（CacheDeleteConsumer）
缓存 binlog 日志订阅，筛选出其中的更新类型日志，解析之后执行对应缓存的删除操作，并且通过
RocketMQ 队列的 ACK 机制确认处理这条更新日志，保证缓存删除能够最终成功执行。
数据库和 Redis 双写的场景下，ProviderA 先更数据库，然后基于 binlog+消息队列删除缓存的并
发执行案例的执行流程如图 1 - 28 所示。
基于 binlog+消息队列删除缓存的方案的优势是：微服务 Provider 在执行数据库和缓存双写时，
只需要执行写入数据库的操作就可以了，大大简化了微服务 Provider 的业务逻辑。缓存的删除工作
已经完全被 Canal、RocketMQ、专门的消费者（CacheDeleteConsumer）三者相互结合接管了。
这么多的旁路缓存模式（Cache-Aside）保证了双写的数据一致性方案，该如何选型呢？只有
更合适，没有最合适，大家可以根据项目和团队的情况选择最合适的。具体的方案选型，大家可以
在高并发社群——疯狂创客圈的微信群里边交流。


图 1 - 28 先更新数据库，然后基于 binlog+消息队列删缓存的并发执行案例
7 .从 CAP 视角分析数据库与缓存的数据一致性
CAP 理论作为分布式系统的基础理论，描述的是一个分布式系统在一致性（Consistency）、可
用性（Availability）、分区容错性（Partitiontolerance）这三个特性中，最多满足其中的两个特性：
要么满足 CA，要么满足 CP，要么满足 AP，无法同时满足 CAP。也就是说 AP 和 CP 是一组“天敌”，
要满足 AP 高性能，只能舍弃 CP。
在数据库和缓存的分布式架构中，加入分布式缓存是为了获得高性能、高吞吐，即为了获得分布
式系统的 AP 特性。所以，如果需要数据库和缓存数据保持强一致（强 CP 特性），就不适合使用缓存。
所以，从 CAP 的理论出发，使用缓存提升性能就是会有数据更新的延迟，就会产生数据的不
一致。使用分布式缓存，可以通过一些方案优化保证弱一致性，通过不断地方案迭代减少不一致性
的时间长度。这需要在设计缓存时结合业务仔细思考是否适合使用缓存，结合业务仔细思考缓存过
期时间。
缓存一定要设置过期时间，这个时间太短或者太长都不好。如果过期时间太短，请求可能会
比较多地落到数据库上，这也意味着失去了缓存的优势。如果过期时间太长，缓存中的脏数据会使
系统长时间处于一个延迟的状态，而且系统中长时间没有人访问的数据会一直存在内存中不过期，
浪费内存。
为什么数据库和缓存没有办法强一致呢？主要因为是写数据库和删缓存是两个独立的操作，
这两个操作并没有保证原子性。如果一定要强 CP，就需要非常复杂的低性能方案保证写数据库和
删缓存两个操作的原子性，比如引入分布式锁，并且需要引入 CP 类型的 ZooKeeper 分布式锁，或者
引入 CP 类型的 RedisRedLock，而不是引入 AP 类型的普通 Redis 分布式锁。所以，如果一定要强 CP，
就需要非常复杂的低性能方案，这有点得不偿失。


#### 1. 6. 6 本地缓存架构

###### 广义的缓存层覆盖整个系统架构的不同系统层级进行数据缓存，以提升访问效率。它覆盖的

###### 范围非常大，主要有：

######  客户端缓存。

######  服务端本地缓存。

######  文件系统缓存。

 分布式缓存（如 Redis）。
 CPU 缓存。
首先看看什么是客户端缓存。客户端浏览器一般针对图片、CSS 脚本、HTML 文件等静态内容
进行缓存。客户端浏览器能够根据服务器端返回的响应头缓存设置，将响应内容缓存到浏览器。如
果客户端本地缓存没有过期，则浏览器可以直接从本地读取数据，从而减少浏览器端和服务器端之
间来回传输的数据量，节省带宽。
其次看看什么是 CPU 缓存和文件系统缓存。这两部分内容都非常复杂，在此不做展开，而且
由于篇幅限制，也没有办法展开。关于 CPU 的缓存架构，请参考尼恩编著的《Java 高并发核心编程
卷 2 （加强版）：多线程、锁、JMM、JUC、高并发设计模式》里边对 CPU 的多级缓存架构，以及
对 CPU 多级缓存的数据一致性，做了非常深入的介绍。关于文件系统缓存的架构和原理，这部分内
容更复杂，后续将在疯狂创客圈博客中通过视频方式进行介绍。
本小节所讨论的缓存，主要是服务端的进程内的本地缓存，主要有两类：
 Java 应用本地缓存。
 Nginx 接入层本地缓存。
1 .Java 应用本地缓存
Java 应用本地缓存简单一点的可以是 Map，复杂一点的可以使是 Guava、Caffeine 这样的第三方
组件。Java 应用本地缓存类似于寄生虫，占用的是 JVM 进程的内存空间。
GuavaCache 是 Google 开源的一款本地缓存工具库，它的设计灵感来源于 ConcurrentHashMap，
使用多个 Segments 方式的细粒度锁，在保证线程安全的同时，支持高并发场景需求，同时支持多种
类型的缓存清理策略，包括基于容量的清理、基于时间的清理、基于引用的清理等。
Caffeine 是 Spring 5 默认支持的缓存，Spring 抛弃 Guava 转向了 Caffeine，可见 Spring 对它的看重。
Caffeine 因使用 WindowTinyLfu 回收策略而提供了一个近乎最佳的命中率。
Caffeine 的底层数据存储采用 ConcurrentHashMap。因为 Caffeine 面向 JDK 8 ，而在 JDK 8 中
ConcurrentHashMap 增加了红黑树，所以在 Hash 冲突严重时 Caffeine 也能有良好的读性能。
如果要在 Java 应用中使用本地缓存，建议使用 Caffeine 组件。
2 .Nginx 接入层本地缓存
Nginx 有三类本地缓存：
 proxy_cache（代理缓存）。
 shared_dict（共享字典）。


 lua-resty-lrucache 缓存。
（ 1 ）Nginx 的 proxy_cache（代理缓存）
第一类 Nginx 缓存是 proxy_cache（代理缓存），此类缓存是 Nginx 的标准缓存机制。Nginx 作为
反向代理，在请求转发的过程中可将中间数据在本地进行缓存，这样若在未来一段时间内请求相同
的数据，Nginx 可以直接返回本地的副本（缓存），而不是再次向后端服务发起请求，因此可以大
大降低后端服务器的压力。
#定义缓存路径 、名称、空间大小等
proxy_cache_path/data/nginx/cachekeys_zone=my_cache_name: 10 m;
server{
listen 8000 ;
server_name localhost;
#添加缓存的http状态头
add_headerX-Cache-Status$upstream_cache_status;
location/{
#使用缓存名称
proxy_cachemy_cache_name;
#定义缓存key
proxy_cache_key $host$ uri $is_args$ args;
proxy_cache_valid 20030410 m;
proxy_cache_bypass $arg_nocache$ http_nocahe;
proxy_passhttp://localhost: 8080 ;
}
}

```
proxy_cache用于缓存请求的中间结果，涉及两个指令：proxy_cache_path、
proxy_cache。
```
proxy_cache_path 指令用于定义缓存。其第一个强制参数为用于缓存内容的本地文件系统路径；
第二个强制参数为 keys_zone，用于定义共享内存区（sharedmemoryzone）的名称和大小，该共享
内存用于保存缓存项目的元数据。上面例子中共享内存区域的名称为 my_cache_name，大小是 10 MB。
注意，proxy_cache_path 定义的这块内存区域不是用来放置缓存内容的，而是用来放置缓存内
容的元数据，比如缓存内容的 url、参数、大小、磁盘上的文件等。缓存内容放在哪里呢？实际上
是存储在 proxy_cache_path 配置的磁盘目录下。
location 区块中的 proxy_cache 指令用来指定共享内存区的名称，即 proxy_cache_path 指令中的
keys_zone 参数中的名称，此处为 my_cache_name。
proxy_cache 缓存的特点：缓存的内容存储在磁盘，缓存的元数据信息存储在内存。所以，
proxy_cache 缓存的读写并不是纯内存的操作，而是存在着大量的磁盘 IO 操作，这也符合请求的中
间结果数据规模很大的特点。一般来说，请求的中间结果数据量是很大的，可能是几十个 GB，甚
至几百个 GB。

（ 2 ）Nginx 的 shared_dict（共享字典）
和 proxy_cache 的不同之一：Nginx 的第二类本地缓存 shared_dict（共享字典）缓存、第三类本
地缓存 lrucache 缓存不是 Nginx 的标准缓存，而是 Nginx 的 Lua 扩展缓存。
和 proxy_cache 的不同之二：shared_dict、lrucache 是纯内存的操作，受限于内存大小，这两类
本地缓存的尺寸都比较小。


shared_dict 的数据结构类似于 Java 中的 Map，以 Key-Value（键－值对）的形式使用。shared_dict
只能缓存字符串对象，缓存的数据有且只有一份，每一个 NginxWorker 都可以进行访问，所以常
用于 NginxWorker 之间的数据通信。
使用 shared_dict 的时候，第一步是在 nginx. conf 里面添加 shared_dict 配置，通过 lua_shared_dict
命令实现，具体如下：
lua_shared_dict my_cache 128 m; --mycahe 是 lua_shared_dict 的缓存名字
第二步是在代码里面进行键－值对（Key-ValuePair）的设置和获取，与 Map 的使用非常类似，
从 shared_dict 获取 Value 的示例代码如下：
functionget_from_cache (key)
localmy_cache=ngx. shared. my_cache--拿到共享字典
localvalue=my_cache: get (key)--获取值
returnvalue
End
shared_dict 是内存缓存，同一个服务器上的不同 NginxWorker 之间是共享缓存的，操作的是同
一个 Key-Value，所以必须保证操作的原子性。可以使用 resty_lock 保障 shared_dict 设置的原子性，
参考代码如下：
localresty_lock=require"resty. lock"
localcache=ngx. shared. my_cache

- -创建锁
locallock, err=resty_lock: new ("my_locks")
- -抢占锁
localelapsed, err=lock: lock (key)
ifnotelapsedthen
    returnfail ("failedtoacquirethelock: ", err)
end
- -省略 key 的新 value 的获取过程
- -使用新 value，设置缓存
localok, err=cache: set (key, val, 1 )
ifnotokthen
    returnfail ("failedtosetcache: ", err)
end
- -释放锁
localok, err=lock: unlock ()
ifnotokthen
    returnfail ("failedtounlock: ", err)
end
（ 3 ）Nginx 的 lrucache 缓存
lrucache 是通过 Lua 脚本实现的最近经常使用的缓存。与 shared_dict 不同，lrucache 缓存可以缓
存所有 Lua 对象，而且 lrucache 缓存是进程独享的，只能在单个 NginxWorker 进程内访问，有多少个
NginxWorker，就会有多少份 lrucache 缓存数据。
lrucache 的数据结构仍然类似于 Java 中的 Map，以键－值对（Key-ValuePair）的形式使用，可
以进行键－值对的 get\set\delete 操作。使用 lrucache 的示例代码如下：
locallrucache=require"resty. lrucache"
- -创建缓存，缓存的大小，可以容纳 200 个元素
localc=lrucache.new ( 200 )
ifnotcthen
returnerror ("failedtocreatethecache: ".. (error"unknown"))
end


- -设置缓存
c: set ("foo", 32 )
- -获取缓存
ngx.say ("foo: ",c: get ("foo"))
Nginx 扩展的本地缓存可以是 Map 或者其他的 JDK 容器，也可以是 Guava、Caffeine 这样的第三
方组件。

3 .什么场景使用本地缓存
和 Redis 分布式缓存相比，本地缓存的优势是性能高很多。性能高的原因是本地缓存是纯内存
操作，避免了网络 IO，所以访问的速度最快。
什么数据使用本地缓存呢？
 修改频率低、数据量不大的数据。比如一个管理系统的组织机构类目信息。
 极热的数据，尤其是查询 QPS 极高的数据。比如秒杀热点商品缓存。
总之，本地缓存通过纯内存操作，避免了网络 IO，速度比 Redis 更快。对性能有极致要求的场
景可以充分考虑使用本地缓存。

#### 1. 6. 7 多级、细粒度缓存架构

###### 假设一个互联网平台的日均 PV 过百万，QPS 峰值过万，需要通过多级缓存来提升性能，那么

###### 就需要对缓存的细粒度、多级模式进行改造。

###### 1 .多级缓存架构

###### 什么是多级缓存呢？就是根据分布式缓存、本地缓存的特点，对缓存进行分级。在整个系统

###### 架构的不同系统层级进行数据缓存，以提升访问的高并发吞吐量。

```
从Java程序在访问缓存时的距离远近的角度对缓存进行分级，可以将缓存划分为：
 一级缓存：JVM本地缓存，如GuavaCache、Caffeine等。
 二级缓存：经典的分布式缓存，如RedisCluster集群。
 三级缓存：在接入层的本地缓存，如Nginx的shared_dict（共享字典）。
一个多级缓存架构的案例如图 1 - 29 所示。
将数据按照不同的访问热度进行划分，可以分为：
 极热数据：访问热度最高的数据。
 较热的数据：访问热度没有那么高的数据。
 普通数据：访问热度比较一般的数据。
不同热度的数据可以按照不同的层级进行存放：
 对于访问热度最高的数据，可以在接入层Nginx的shared_dict（共享字典）缓存，此为三级
缓存（规模在 1 GB以内），比如秒杀系统中的优惠券详情、秒杀商品详情信息，这些信息
访问得非常频繁。
```

图 1 - 29 一个多级缓存架构的案例
 对于访问热度没有那么高但也访问频繁的数据，可以在 JVM 进程内缓存（如 Caffeine），
这部分的数据规模也不能太大，大概在 1 GB 以内，作为一级缓存。
 对于访问热度比较一般的数据，存放到 RedisCluster 集群，作为二级缓存，这部分的数据
规模最大，可以以 10 GB 为节点单位进行横向扩展。
缓存级别的划分标准
关于缓存级别的划分，业内没有一个统一的标准。以上的一级、二级、三级是以 Java 程序在访
问缓存时的距离远近来划分的。一级、三级缓存都是纯内存操作，避免了网络请求，对性能有极致
要求，速度比二级缓存 Redis 更快。但是一级、三级缓存在规模上无法与二级缓存相比。

2 .细粒度缓存架构
在没有使用多级缓存的场景下，缓存的粒度是比粗的，通常一个应用的代码中有一个 Redis 客
户端模块，需要缓存的数据统一使用 Redis 访问客户端模块进行设置和读取。在使用多级缓存的情
况下，缓存的粒度是比较细致的，从业务的维度需要进行数据的细粒度划分，划分的时候，需要考
虑数据的多个属性：

 热度属性
 规模属性
比如在秒杀系统中，可以将缓存数据的细粒度划分为：秒杀商品数据、商品类目数据、普通
商品数据、用户数据。
不同细粒度的数据使用不同层级的缓存进行保存：
 秒杀商品数据：极热，规模在 1 GB 以内，使用三级缓存。
 商品类目数据：次热，规模在 1 GB 以内，使用一级缓存。
 普通商品数据、用户数据：普通热度，规模在 10 GB 以上，使用二级缓存。


###### 使用了缓存之后，一定不要忘记去解决缓存的数据一致性问题。在三级缓存的场景下，不同

###### 级别的缓存有不同的数据一致性的保证方案：

```
 一级缓存，可以基于Canal+RocketMq的饿汉加载模式。
 二级缓存，可以使用旁路缓存模式（Cache-Aside）延迟双删，或者Canal+RocketMQ删除
模式。
 三级缓存，可以基于Canal+RocketMQ的饿汉加载模式。
```
1. (^7) 数据层的横向扩展高并发架构

###### 在数据规模量很大的场景下，数据层数据库涉及数据横向扩展高并发架构，将原本存储在一

###### 台服务器上的数据层数据库拆分到不同服务器上，以达到扩充系统性能的目的。

###### 数据层的横向扩展高并发架构主要包括两个方面：

######  结构化数据的高并发架构方案：分库分表。

```
 异构数据、复杂查询的高并发架构方案：NoSQL海量存储（如ElasticSearch、HBASE、
ClickHouse）。
```
#### 1. 7. 1 数据库服务器的能力参考数据

###### 首先要弄清楚一个核心问题：一台数据库服务器能够承受多大的并发量、数据量？数据库服

###### 务器的能力参考数据受多方面因素影响，大致如下：

###### （ 1 ）数据库的类型

读者使用的数据库是 MySQL 还是 Oracle 或是 DB 2 、PostgreSQL 等？要知道不同数据库的性能在
不同的场景中是不一样的。
由于大家平时主要使用 MySQL 数据库，因此接下来以 MySQL 为样本进行阐述。
（ 2 ）数据库服务器是什么配置
 CPU 是几核？现代数据库应用都充分运用了多核 CPU 的并行处理能力。
 内存多大？数据库的索引数据、缓存数据都会进入内存中。
 磁盘 IO 能力？是机械硬盘还是固态硬盘？数据库文件都存储在磁盘中，所以磁盘的 IO 能力
将是影响数据库性能的最直接因素。
 网络带宽？是千兆网卡还是万兆网卡？网络的上行和下行带宽，数据库服务器可支持的最
大连接数是多少？
度量数据库并发量/吞吐量的最好办法是做数据库压力测试。在做压力测试的时候，可以一点
一点地加压力，逐步得出数据库的以下参数：

```
 QPS（QueriesPerSecond）：每秒处理的查询数（如果是数据库，就相当于读取）。
 TPS（TransactionsPerSecond）：每秒处理的事务数（如果是数据库，就相当于写入、修改）。
 IOPS：每秒磁盘进行的I/O操作次数。
```

###### 除了弄清楚线上数据库实际的吞吐量/并发量指标值之外，业内对于结构化数据库（主要针对

MySQL）有一些比较共识的参考数据（基线值）：

 单表的记录参考上限： 500 万~ 1000 万。
 单库的 TPS 上限： 1000 ~ 1500 TPS。
在进行架构设计时，如果没有数据库实际的吞吐量/并发量指标值，可以按照这些基线值进行
架构设计。

#### 1. 7. 2 结构化数据的高并发架构方案：分库分表

###### 为什么要分库分表？随着数据量的增长，数据库很容易产生性能瓶颈：IO 瓶颈、CPU 瓶颈。

###### 1 .IO 瓶颈

###### IO 瓶颈包括：磁盘读 IO 瓶颈、磁盘写 IO 瓶颈。

###### 什么是磁盘读 IO 瓶颈？由于热点数据太多，数据库缓存完全存放不下，查询时会产生大量的

###### 磁盘 IO，查询速度会比较慢，这样会产生大量的活跃连接，最终可能会导致产生无连接可用的后

###### 果。如果要解决 IO 瓶颈，可以采用一主多从、读写分离的方案，用多个从库分摊查询流量；或者采

用分库+水平分表（把一个表的数据拆成多个表来存放，比如订单表可以按 user_id 来拆分）的方案。
什么是磁盘写 IO 瓶颈？由于数据库写入频繁，会产生大量的磁盘写入 IO 操作，磁盘写入的性
能是很低的，频繁的磁盘写入同样会产生大量的活跃连接，最终同样会导致产生无连接可用的后果。
这时只能采用分库方案，用多个库来分摊写入压力。再加上水平分表的策略，分表后单表存储的数
据量会更小，插入数据时索引查找和更新的成本会更低，插入速度自然会更快。
两种 IO 瓶颈都会导致数据库的活跃连接数增加，进而达到数据库可承受的最大活跃连接数阈
值，最终导致应用服务无连接可用，造成灾难性后果。

2 .CPU 瓶颈
什么是 CPU 瓶颈？即 CPU 利用率满载，一般体现为长时间 CPU 利用率大于 99 %。导致数据库
CPU 瓶颈的问题有：

1 ）慢 SQL 问题。如果 SQL 中包含 join、groupby、orderby、非索引字段条件查询等增加 CPU
运算的操作，会对 CPU 产生明显的压力。解决的方案为：可以考虑 SQL 优化，创建适当的索引；也
可以把一些计算量大的 SQL 逻辑放到应用中处理。
2 ）单表数据量太大。由于单个表数据量过大，比如超过 1000 万，查询时遍历树的层次太深或
者扫描的行太多，SQL 效率会很低，也会非常消耗 CPU。这时可以根据业务场景水平分表。

出现 IO 瓶颈、CPU 瓶颈时，可以先从代码、SQL、索引几方面进行优化。如果这几方面已经
没有太多优化的余地，就该考虑分库分表了。
什么是分库？就是将一个数据库分为多个数据库。一个库一般最多支撑并发 2000 TPS，较为合
理是 1500 TPS。如果吞吐量需要达到 1 万 TPS，则考虑分为 8 个库，一个库支撑 1250 TPS。
什么是分表？就是把一个表的数据放到多个表中，然后查询的时候只查一个表。
什么场景下需要分库，什么场景下需要分表呢？具体请参考表 1 - 1 。


```
表 1 - 1 分库分表的使用场景
场 景 分库分表前 分库分表后
并发支撑情况 数据库单机部署，扛不住高并发 数据库从单机到多机，能承受的并发增加了多倍
磁盘使用情况 数据库单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低
SQL执行性能 单来表越数慢据量太大，SQL执行效率越 单表数据量减少，SQL执行效率明显提升
```
#### 1. 7. 3 结构化分库分表：水平拆分与垂直拆分

###### 1 .水平拆分

###### 水平拆分就是拆分数据，不拆分结构。具体来说，水平拆分把一个表的数据，拆分到多个库

###### 的多个表里，但是每个库的表结构都一样，只不过每个库表存放的数据是不同的，所有库表的数据

###### 加起来就是全部数据。

###### 水平拆分的意义就是将数据均匀地存放在更多的库里，然后用多个库来抗更高的并发；还有

###### 就是用多个库的存储容量来进行扩容。

###### 水平拆分的案例—— 1000 万记录大表水平拆分为 4 个 250 万记录的小表，具体如图 1 - 30 所示。

图 1 - 30 1000 万记录大表水平拆分
水平拆分的时候，会让每个表的数据记录（Row）的数量控制在一定范围内，保证 SQL 的性能。
因为单个表数据量越大，SQL 性能就越差。一般单表的数据记录的数量在 500 万~ 1000 万行左右。
水平拆分总的原则：SQL 越复杂，就让单表数据记录数越少。
水平拆分一般是分表和分库的结合。拆分出来的子表可以扩展到新库，具体扩展多少新库，
可以根据吞吐量规模而定。

 单库的 TPS 上限： 1000 ~ 1500 TPS。
假设预估的吞吐量为： 5000 TPS，那么需要 4 个库。
2 .垂直拆分
垂直拆分就是既拆分结构，也拆分数据。具体来说，垂直拆分就是把一个有很多字段的表从
结构上拆分成多个表或者是多个库，每个库表的结构都不一样，每个库表都包含部分字段。
垂直拆分的原则：将较少的访问频率很高的字段放到一个表里，将较多的访问频率很低的字
段放到另外一个表里。原因是什么呢？因为数据记录是有缓存的，访问频率高的字段越少，就可以


###### 在同样大的缓存里边放置更多的数据记录，这样性能就越好。所以，垂直拆分的出发点是充分利用

###### 数据库的缓存，提升性能。

###### 垂直拆分的案例——一个有 10 个列的宽表垂直拆分为 4 个窄表，具体如图 1 - 31 所示。

图 1 - 31 10 个列的宽表垂直拆分
一个生产场景的案例：在电商系统中，可以把一个大的订单宽表拆分成 3 个窄表——订单表、
订单支付表、订单商品表。

#### 1. 7. 4 亿级库表规模架构设计

###### 了解了分库分表的扩展策略之后，接下来看看海量规模结构化数据存储的架构设计，分为两

###### 个维度：

######  亿级库表规模架构设计。

######  百亿级库表规模架构设计。

###### 首先来看相对小规模的维度——亿级库表规模架构设计。

###### 1 .表的数量规划

###### 假设一个系统，其中某个表每天增长 100 万记录， 2 年内保持稳定增长，那么表的数据量预估为：

###### 两年数据记录总量是 7. 3 亿（每天 100 万× 730 天）。

###### 假设每个表的标准值为 500 万，库中表的数量平均是 146 个，若用 2 的幂次方形式表示，则比较

###### 接近 146 的是 27 ，即 128 。接下来，按照 128 个表进行折算，单个表存放 570 万（ 570 万= 7. 3 亿/ 128 ）的

###### 数据，其数据规模也是可以接受的。

###### 按照上面的算法，最终按照 128 个的数量进行表的规划，具体的规划路线如图 1 - 32 所示。

###### 2 .库的数量规划

###### 假设按照 TPS 峰值 1 万的要求进行表库的规划：

###### 相对乐观一点，假设每个库正常承载的写入并发量是 1500 TPS。那么 8 个库就可以承载 8 × 1500

###### = 12000 TPS 的写并发。


图 1 - 32 亿级数据的表库规划的具体路线
如果每秒写入不超过 1 万 TPS， 8 个库是可以胜任的。如果每秒写入超过 1 万 TPS，比如 5 万 TPS
呢？那么可以通过 RocketMQ 削峰+批写入的异步降级策略，进行高并发异步批量写入。
由于 RocketMQ 集群的写入吞吐量可以轻松到达 10 万 TPS 级别，所以通过 RocketMQ 削峰+批写
入的异步降级策略， 10 万 TPS 以内的数据写入吞吐量还是比较容易实现的。
如果写入的吞吐量超高 10 万 TPS 呢？这种超高的写入吞吐量的解决方案非常复杂，具体的解决
方案需要根据场景而定，在这里不做展开。如果对这个问题感兴趣，可以在疯狂创客圈社群交流。

#### 1. 7. 5 百亿级库表架构设计

###### 接下来看相对大规模的维度：百亿级库表规模架构设计。

###### 1 .表的数量规划

###### 假设一个系统，其中某个表每天增长 1000 万记录， 2 年内保持稳定增长，那么表的数据量预估为：

###### 两年总量是 73 亿（每天 1000 万× 730 天）

###### 假设每个表的标准值为 500 万，库中表的数量平均是 1460 个，若用 2 的幂次方形式表示，则比

###### 较接近 1460 的是 210 ，即 1024 个。

###### 反过来计算，单表的数据量为 692 万（ 692 万= 73 亿/ 1024 ）< 1000 万，也是可以接受的。

###### 按照上面的算法，最终按照 1024 个的数量进行表的规划，具体的规划路线如图 1 - 33 所示。

```
图 1 - 33 百亿级数据的表规划的具体路线
```
```
按照 1024 个规划
```

###### 2 .库的数量规划

###### 假设按照 TPS 峰值 5 W 的要求进行表库的规划：

###### 相对乐观一点，假设每个库正常承载的写入并发量是 1500 TPS。那么 32 个库就可以承载 32 ×

###### 1500 = 48000 TPS 的写并发。

吞吐量在 5 万 TPS，那么可以通过 RocketMQ 削峰+批写入的异步降级策略，进行高并发异步批
量写入。

#### 1. 7. 6 百亿级数据的异构查询

###### 比如对于订单库，当对其分库分表后，如果不是按照数据分片键而是按照分片键之外的商家

###### 维度或者按照用户维度进行查询，那么是非常困难的，性能也是非常低的。如果需要进行跨库查询

###### 或者按照分片键之外的复杂维度去查询，可以通过异构数据库来解决这个问题。采用如图 1 - 34 所示

的 ES（ElasticSearch）+HBase 的异构查询架构。

图 1 - 34 ES+HBase 的异构查询架构
ES+HBase 组合方案的特点：将索引与数据存储隔离。
ES 的优势是进行高速的分布式全文检索，所以那些参与条件检索的字段都会在 ES 中建一份索
引，例如商家、商品名称、订单日期等。HBase 的优势是支持海量存储，所有全量数据的副本都保
存一份到 HBase 中。
HBase 的一个高速的特点是根据 Rowkey 查询速度超快，业内戏称“快如闪电”。
ES+HBase 组合方案利用了 ES 的多条件检索能力非常强大的特点，也利用了 HBase 基于 Rowkey
查询速度超快的特点，可以说这个方案把 ES 和 HBase 的优点发挥得淋漓尽致。

```
由于 ES 中存储的是参与搜索的数据的副本，HBase 中存储的是全量数据的副本，
因此这种方案归根结底属于以空间换时间的方案。
```

#### 1. 7. 7 ES+HBase 组合方案的数据查询过程

```
ES+HBase 组合方案的查询过程大致如下：
 先根据搜索条件去 ES 相应的索引上查询符合条件的 Rowkey 值。
 然后用 Rowkey 值去 HBase 查询，这一步查询速度极快。
ES+HBase 组合方案的数据查询过程具体如图 1 - 35 所示。
```
图 1 - 35 ES+HBase 组合方案的数据查询过程
通过这种以空间换时间的方案，可以解决根据各种字段条件进行复杂查询、跨库查询的业务
需求。那么，ES、HBase 中的数据何时进入呢？数据应该何时进入 ES、HBase 中的解决方案非常复
杂，需要根据场景而定，在这里不做展开。如果对这个问题感兴趣，可以在疯狂创客圈社群交流。

### 1. 8 亿级用户量场景下的流量预估

###### 在介绍亿级用户量场景下请求流量预估的方法之前，先来看两种请求处理模型：

######  直筒型。

######  漏斗型。

#### 1. 8. 1 请求处理模型

###### 1 .直筒型

###### 直筒型请求处理模型指的是用户请求 1 : 1 地洞穿到数据库层，如图 1 - 36 所示。在比较简单的业

###### 务中才会采用这个模型，比如传统的低并发、低性能、低可用项目，就是直筒型请求处理模型。直

###### 筒型请求处理模型的适用场景：

######  用户规模较小。

######  请求峰值和平均值相差不大。


######  请求峰值不会超过数据层的处理能力。

###### 2 .漏斗型

###### 漏斗型请求处理模型指的是用户的请求从客户端到数据库层逐层递减，递减的程度视业务而

###### 定。例如当 10 万人去抢 1000 个物品时，数据库层的请求在个位数量级 1000 以内，这就是比较理想的

###### 模型，如图 1 - 37 所示。

图 1 - 36 直筒型请求处理模型图 1 - 37 漏斗型请求处理模型
互联网应用（如秒杀）基本都是这种请求处理模型。漏斗型请求处理模型的适用场景：
 用户规模大。
 请求峰值和平均值相差巨大。
 请求峰值远远超出最后一层（数据层）的处理能力。
漏斗型请求处理模型的核心策略：对请求进行分层过滤，从而过滤掉一些无效的请求。
3 .案例：秒杀系统的分层过滤
在秒杀系统中，请求分别经过 CDN、接入层（含 Nginx）、微服务层和数据库这几层。以秒
杀商品详情的访问为例，一次秒杀商品详情的访问请求的分层过滤流程大致如下：

1 ）大部分数据和流量在用户浏览器或者 CDN 上获取，这一层可以拦截大部分静态资源的
读取。
2 ）闯过了第一层 CDN 之后，进入接入层 Nginx。此时，请求尽量得通过 NginxCache，如果缓
存命中，可以过滤一些访问服务层的请求。
3 ）在第二层 Nginx 上也可以进行流控，还可以进行黑名单过滤，拦截一些无效的流量。
4 ）闯过了第二层 Nginx 之后，再到服务层，进入微服务网关时可以做用户的授权检验，对系
统做好保护和限流，这样数据量和请求就进一步减少了。
5 ）请求到底服务层的业务层的微服务 Provider 之后，还可以进行数据的有效性、一致性过滤，
这里又减少了一些流量。

这样就像漏斗一样，把数据量和请求量一层一层地过滤和减少了。
分层过滤的核心思想：在不同的层次尽可能地过滤掉无效请求，让“漏斗”最末端的才是有
效请求。而要达到这种效果，就必须对数据做分层的校验。


###### 分层过滤的基本原则是：

######  通过在不同的层次尽可能地过滤掉无效请求，尽早处理请求。

######  通过 CDN 过滤掉大量的图片即静态资源的请求。

######  读请求尽量命中缓存，不要穿透到数据库。

######  尽量将动态读数据请求命中在三级缓存或者二级缓存，过滤掉无效的数据读。

######  对写入操作进行削峰，争取批量写入，提高写入的吞吐量。

#### 1. 8. 2 旧系统的流量架构

###### 如果是旧系统，做流量架构的时候可以参考现有的监控数据、服务能力、流量指标。

###### 对着旧的架构版本进行偏离指标的计算，折算成冗余系数，完成流量架构的工作：

###### 1 ）做出系统在不同用户量、不同场景（高峰、平峰、低峰）下的流量（吞吐量）预估，包含

###### 未来一段时期如两年的流量预估。

###### 2 ）做出系统在不同用户量、不同场景下的各层组件的部署架构。

1. 8. (^3) 新系统的流量架构
接下来重点介绍一下新系统，也是实际工作中遇到得最多的情况。
有的读者可能说：我们公司的项目就是什么都没有，三无产品，没有业务监控，没有中间件
日志，也没有日活数据，那怎么评估预期指标。
如果是新系统，线上并没有任何的历史监控数据和日志数据，所以之前介绍的方法就不再适
用，这个时候需要使用另外一种方法来评估性能指标，那就是“二八定律”。
对新系统来说，完成流量架构的工作包括：
1 ）根据二八定律做出系统在不同用户量、不同场景下的流量（吞吐量）预估，包含未来一段
时期如两年的流量预估。
2 ）做出系统在不同用户量、不同场景下的各层组件的部署架构。

#### 1. 8. 4 二八定律

什么是二八定律？二八定律的别名很多，又名 80 / 20 定律、帕累托法则（Pareto’sprinciple）、
巴莱特定律、朱伦法则（Juran'sPrinciple）、关键少数法则（VitalFeRule）、不重要多数法则（Trivial
ManyRule）、最省力的法则、不平衡原则等，被广泛应用于社会学及企业管理学等。
二八定律是 19 世纪末 20 世纪初意大利经济学家帕累托发现的。他认为在任何一种事物中，最
重要的只占其中一小部分，约 20 %，其余 80 %尽管是多数，却是次要的。
二八定律的使用场景非常多，在大量的场景中都有应用。例如在软件测试场景：
 80 %的测试成本花在 20 %的软件模块中。
 80 %的 BUG 多发生在软件的 20 %的模块中，在回归测试的时候，这 20 %的高发地带是关注
的重点。
 80 %的错误是由 20 %的模块引起的。


###### 在经济学场景：

######  从经济学上看，世界上 80 %的财富都集中的 20 %的人手里。

###### 在心理学场景：

######  从心理学来说，人类 80 %的智慧都集中在 20 %的人身上。

###### 二八定律是一种社会准则，符合大多数社会现象的规律，同样也适用于互联网领域。比如互

###### 联网行为场景常应用到，比如二八原则：

######  一个网站有成千上万的用户，但是 80 %的用户请求是发生在 20 %的时间内，比如大家去网

###### 上购物，基本都集中在中午休息或者晚上下班后。

###### 二八定律的核心原则是关注重要部分，忽略次要部分。系统性能如果能支撑发生在 20 %时间内

###### 的高并发请求，必然也能支持非高峰期的 80 %的访问。

#### 1. 8. 5 通过二八定律进行流量预估

###### 下面具体介绍一下怎么通过二八定律计算预期指标。

###### 1 .通过用户量来预估 QPS

###### 首先预估系统的每日总请求数，这个没有固定的方法，如果没有任何历史数据可以参考，一

###### 般是通过用户量或者其他关联系统来评估。

###### 术语说明：

QPS=req/sec=请求数/秒。对于单次接口调用的请求，QPS=TPS。
第一步：通过用户量来推算 PV。
参照淘宝的经验，每一个活跃用户每天的点击次数大概为 30 ～ 50 次，大致的 PV 的估算公式
如下：

（总用户数× 20 %）×每天的大致点击次数（淘宝经验值为 30 ～ 50 次）=PV
例如：用户数是 1000 万，PV 值是多少？
答： 1000 万× 20 %× 30 = 6000 万。
第二步：通过 PV 推算 QPS。
按照二八定律， 80 %的请求发送在 20 %的时间里边，然后通过 PV 推算 QPS/TPS，大致的估算公
式如下：

```
（总 PV 数× 80 %）/（每天秒数× 20 %）=峰值时间每秒请求数（QPS）
例如：每天 6000 万 PV，多少 QPS？
答：（ 6000 万× 0. 8 ）/（ 86400 × 0. 2 ）= 4800 万/ 17280 = 2700 （QPS）。
```

###### 第三步：乘上冗余系数。

###### 评估出指标后，为了更加保险一些，最好再乘以一个冗余系数（偏离系数），提高预期指标，

###### 防止人为评估造成预期指标偏低的情况。

###### 这个冗余系数一般定为 2 ～ 5 （行业经验），上面计算出来的 QPS 指标为 2700 ，如果再乘以一个

###### 冗余系数 4 ，那么最终 QPS 指标就是 10800 。

###### 2700 （QPS）× 4 = 10800 （QPS）

###### 总结一下，第三步使用二八定律的方式为：

###### 80 %的请求/ 20 %的时间×冗余系数

###### 第四步：冗余系数的迭代。

###### 将来项目上线后，可以通过项目接口的峰值监控来对比之前评估的算法结果，调整冗余系数，

###### 最终随着不断的数据积累形成一套本项目的性能模型。

###### 2 .十万级用户量的压力预估

###### 假设这个网站预估的用户数是 10 万，那么根据二八定律，每天会来访问这个网站的用户占到

###### 20 %，也就是每天会有 2 万用户来访问。

###### 第一步：通过用户量来推算 PV。

###### 公式：（总用户数× 20 %）×每天的大致点击次数（淘宝经验值为 30 ～ 50 次）=PV

###### 例如：用户数是 10 万，PV 值是多少？

###### 答： 10 万× 20 %× 30 = 60 万。

###### 通常假设平均每个用户每次过来会有 30 次的点击，那么总共就有 60 万的点击（PV）。

###### 第二步：通过 PV 推算 QPS。

###### 公式：（总 PV 数× 80 %）/（每天秒数× 20 %）=峰值时间每秒请求数（QPS）

###### 例如： 5 小时内会有 60 万点击，多少 QPS？

###### 答：（ 60 万× 0. 8 ）/（ 5 × 3600 ）= 27 （QPS）。

###### 第三步：乘上冗余系数。

###### 27 （QPS）× 4 = 108 （QPS）

###### 3 .百万级用户量的压力预估

###### 假设这个网站预估的用户数是 100 万，那么根据二八定律，每天会来访问这个网站的用户占到

###### 20 %，也就是每天会有 20 万用户来访问。

###### 第一步：通过用户量来推算 PV。

###### 公式：（总用户数× 20 %）×每天的大致点击次数（淘宝经验值为 30 ～ 50 次）=PV

###### 问：用户数是 100 万，PV 值是多少？

###### 答： 100 万× 20 %× 30 = 600 万。

###### 通常假设平均每个用户每次过来会有 30 次的点击，那么总共就有 600 万的点击（PV）。


###### 第二步：通过 PV 推算 QPS。

###### 公式：（总 PV 数× 80 %）/（每天秒数× 20 %）=峰值时间每秒请求数（QPS）

###### 问： 5 小时内会有 600 万点击，多少 QPS？

###### 答：（ 600 万× 0. 8 ）/（ 5 × 3600 ）= 270 （QPS）。

###### 第三步：乘上冗余系数。

###### 270 （QPS）× 4 = 1080 （QPS）

###### 4 .千万级用户量的压力预估

###### 假设这个网站预估的用户数是 1000 万，那么根据二八定律，每天会来访问这个网站的用户占

###### 到 20 %，也就是每天会有 200 万用户来访问。

###### 第一步：通过用户量来推算 PV。

###### 公式：（总用户数× 20 %）×每天的大致点击次数（淘宝经验值为 30 ～ 50 次）=PV

###### 问：用户数是 1000 万，PV 值是多少？

###### 答： 1000 万× 20 %× 30 = 6000 万。

###### 通常假设平均每个用户每次过来会有 30 次的点击，那么总共就有 6000 万的点击（PV）。

###### 第二步：通过 PV 推算 QPS。

###### 公式：（总 PV 数× 80 %）/（每天秒数× 20 %）=峰值时间每秒请求数（QPS）

###### 问： 5 小时内会有 6000 万点击，多少 QPS？

###### 答：（ 6000 万× 0. 8 ）/（ 5 × 3600 ）= 2700 （QPS）。

###### 第三步：乘上冗余系数。

###### 2700 （QPS）× 4 = 10800 （QPS）

###### 5 .亿级用户量的压力预估

###### 假设这个网站预估的用户数是 10000 万，那么根据二八定律，每天会来访问这个网站的用户占

###### 到 20 %，也就是每天会有 2000 万用户来访问。

###### 第一步：通过用户量来推算 PV。

###### 公式：（总用户数× 20 %）×每天的大致点击次数（淘宝经验值为 30 ～ 50 次）=PV

###### 问：用户数是 1000 万，PV 值是多少？

###### 答： 10000 万× 20 %× 30 = 60000 万。

###### 通常假设平均每个用户每次过来会有 30 次的点击，那么总共就有 60000 万的点击（PV）。

###### 第二步：通过 PV 推算 QPS。

###### 公式：（总 PV 数× 80 %）/（每天秒数× 20 %）=峰值时间每秒请求数（QPS）

###### 问： 5 小时内会有 6000 万点击，一共多少 QPS？

###### 答：（ 60000 万× 0. 8 ）/（ 5 × 3600 ）= 27000 （QPS）。

###### 第三步：乘上冗余系数。

###### 27000 （QPS）× 4 = 108000 （QPS）


###### 6 .实际与理论的差距

###### 将来项目上线后，接口的访问量真的和计算的一模一样吗？这个肯定不会，大家一定得知道

###### 一个原则：性能测试从来都不是一项非常精确的技术。二八定律也并不是 100 %适用于所有业务场

###### 景。在没有任何历史数据参考的背景下，二八定律是一种相对靠谱的算法，最起码有一定的理论依

###### 据，比猜的值靠谱多了。

### 1. 9 高并发架构

###### 异步架构是一种常见的高并发架构，与之相对的架构模式就是同步架构。

#### 1. 9. 1 同步架构

###### 什么是同步架构？以方法调用为例，同步调用代表调用方要阻塞等待，一直到被调用方法中

###### 的逻辑执行完成，返回结果。这种方式下，如果被调用方法响应时间较长，会造成调用方长久阻塞，

###### 在高并发下会造成整体系统性能下降甚至发生雪崩。

###### 在分层架构中，一般都是上层调用下层，同步调用的流程如图 1 - 38 所示。

图 1 - 38 同步调用的流程
在同步调用的架构模式（简称同步架构）中，假设接入层的吞吐量在 10 万级 TPS，服务层的吞吐
量在 1 万级 TPS，就会出现速率严重不匹配导致接入层大量的请求被阻塞和积压，严重拖慢了性能。
怎么解决呢？可以采用异步架构。

#### 1. 9. 2 异步架构

###### 什么是异步架构？异步调用与同步调用相反，调用方不需要等待被调用方法中的逻辑执行完

###### 成，调用方提交请求后就可以返回执行其他的逻辑。在被调用方法执行完毕后，调用方通过回调、

###### 事件通知、定时查询等方式获得结果。

###### 异步架构在高并发场景中得到了大量的使用：

 分布式服务框架 Dubbo 中有异步方法调用模式。
 IO 模型中有异步 IO 模式。
 异步调用在大规模高并发系统中被大量使用，比如大家熟知的 12306 网站。
当我们在 12306 网站订票时，页面会显示系统正在排队，这个提示就代表着系统在异步处理我们的
订票请求。在 12306 系统中查询余票、下单和更改余票状态都是比较耗时的操作，可能涉及多个内
部系统的互相调用。如果是同步架构，那么 12306 网站在高峰时期会出现严重拥塞。而采用异步架


###### 构，上层处理时会把请求写入请求队列，同时快速响应用户，告诉用户正在排队处理，然后释放出

###### 资源来处理更多的请求。订票请求处理完成之后，再通知用户订票成功或者失败，如图 1 - 39 所示。

图 1 - 39 异步架构
采用异步架构后，请求移到异步处理程序中，Web 服务的压力小了，资源占用得少了，自然
就能接收更多的用户订票请求，系统承受高并发的能力也就提升了。

```
这里的队列可以是内存队列、消息队列，具体的选型需要依据场景而定。
```
```
异步架构的使用场景非常多，如果对异步架构的使用场景有疑问或者感兴趣，
可以来疯狂创客圈社群交流。
```
### 1. 10 高可用架构

###### 介绍完系统的高并发架构之后，接下来开始介绍系统的高可用架构。

#### 1. 10. 1 什么是高可用

高可用（HighAvailability，简称为 HA）是分布式系统架构设计中的核心架构维度。
高可用表示系统可以提供正常服务的时间。高可用力争保障系统全年不停机、无故障，而不
是隔三差五地出线上事故、宕机。所以 HA 架构通常是指通过设计减少系统不能提供服务的时间。
假设系统一直能够提供服务，就说系统的可用性是 100 %。如果系统每运行 100 个时间单位，会
有 1 个时间单位无法提供服务，就说系统的可用性是 99 %。很多公司的高可用目标是 4 个 9 ，也就是
99. 99 %，这就意味着系统的年停机时间为 8. 76 个小时。
百度的搜索首页是业内公认的高可用保障非常出色的系统，甚至人们会通过www.baidu.com
能不能访问来判断网络的连通性，百度高可用的服务让人留下“网络通畅，百度就能访问”“百度
打不开，应该是网络连不上”的印象，这其实是对百度 HA 最高的褒奖。


#### 1. 10. 2 高可用的度量

###### 业界高可用的通用指标是用几个 9 来评判一个系统的可用性。比如， 5 个 9 ，即 99. 999 %， 5 个 9

###### 代表该系统在所有的运行时间中 99. 999 %的时间都是可用的，代表一年业务不可用的时间是 5 分钟，

###### 这样的系统就是非常高可用。 4 个 9 ，即 99. 99 %，代表一年业务的不可用时间是 50 分钟。

#### 1. 10. 3 高并发架构中的分层策略

###### 常见的互联网分布式架构分为：

1 ）客户端层：用户使用的交互工具，包含浏览器（Browser）、手机 APP 等。
2 ）接入层：系统入口，反向代理。
3 ）服务层：实现核心应用逻辑，返回 HTML 或者 JSON。
4 ）缓存层：加速数据的访问。
5 ）数据库层：数据存储。
6 ）中间件：含分布式协调组件如 ZooKeeper、对象队列如 RocketMQ、搜索引擎如 ElasticSearch
等。

整个系统的高可用又是通过每一层的冗余+自动故障转移来综合实现的。
最为关键的要点：系统的各个环节都需要规避单点瓶颈。单点瓶颈往往是系统高可用最大的
风险和敌人，应尽量避免在系统设计的过程中形成单点瓶颈。
理论上，高可用保证的原则是“集群化”，或者叫“冗余”：只有一个单点，挂了服务会受
影响；如果有冗余备份，挂了还有其他备份能够顶上。所以，要保证系统高可用，架构设计的核心
准则是冗余。仅有冗余还不够，每次出现故障都需要人工介入去恢复势必会增加系统的不可服务实
践。所以，又通过“自动故障转移”来实现系统的高可用。典型的互联网架构通过冗余+自动故障
转移来保证系统的高可用特性。

#### 1. 10. 4 分层规避单点瓶颈+自动故障转移

分层规避单点瓶颈+自动故障转移策略是高可用的核心要点。先用常用的缓存层 Redis 举例。
如何保证 Redis 缓存高可用呢？答案就是使用集群，避免单点故障。当我们使用一个 Redis 实例作为
缓存的时候，这个 Redis 实例挂了，整个缓存服务可能就挂了。反过来，当我们使用集群后，一个 Redis
实例出现了故障，则会发生故障转移，另外一个 Redis 实例顶上。
再用接入层的 Nginx 举例。如果 Nginx 服务器出现了问题，则无法对外提供服务。如何实现 Nginx
高可用呢？我们可以采用主－备或主－主的形式安装部署 Nginx 服务，然后通过 Keepalived 组件实
现，具体的架构如图 1 - 40 的右边部分所示。


```
图 1 - 40 通过 Keepalived 组件实现高可用
```
1. 10. (^5) 其他的高可用策略
除了通过分层规避单点瓶颈+自动故障转移方案之外，还有哪些高可用策略呢？答案是有很多策
略。下面列出一些主要的高可用策略：
 注重代码质量，测试严格把关。
 做好容错机制，对超时重试和错误重试进行优雅设置。
 同步降级为异步调用。
 限流。
 熔断。
 隔离。
 (^360) °无死角的监控与预警系统。
 核心应用和服务优先使用更好的硬件。
 注意备份，必要时回滚。
 脚本的定期假死监测。
以上高可用策略如果展开介绍，内容实在太多，限于篇幅，这里不做展开，笔
者会在疯狂创客圈博客中专门介绍这些内容。当然，有兴趣的小伙伴也可以来社群交流。

### 1. 11 高性能架构

###### 高性能体现了系统的并行处理性价比，在有限的硬件投入下提高性能意味着节省成本。那么

可以通过哪些方式来提高 Java 程序的性能呢？有如下方法：


######  多线程并发处理，通过多线程将串行逻辑并行化。

######  锁选择，读多写少的场景读写锁，以空间换时间。在争用激烈场景中，通过分段锁的方式

###### 减少争用。

```
 尽量使用无锁编程，比如 ThreadLocal、JUC 并发包、Reactor 模式都属于无锁编程方案。
 JVM 优化，尽可能减少 GC 频率和耗时，这里涉及新生代和老年代的大小、GC 算法的选择等。
 减少 IO 次数，比如批量读写数据库、批量读写缓存、批量调用 RPC 的接口。
 减少 IO 时的数据包大小，包括采用轻量级的通信协议、合适的数据结构、去掉接口中的多
余字段、减少缓存 key 的大小、压缩缓存 value 等。
 各种池化技术的使用和池大小的设置，包括数据库连接池、HTTP 请求池、线程池（考虑
CPU 密集型还是 IO 密集型设置核心参数）、Redis 连接池等。
```
```
以上高性能架构策略如果展开介绍，内容实在太多，限于篇幅，这里不做展开，
笔者会在疯狂创客圈博客中专门介绍这些内容。当然，有兴趣的小伙伴也可以来社群交流。
```
3 高架构的接入层主要是 Nginx 技术，服务层主要是 SpringCloud 技术，所以下一章开始介绍
SpringCloud+Nginx 高并发核心编程的知识。


### 本章的知识扩展

##### 1 .阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna
（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2 .本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html

Caffeine（史上最全）
https://www.cnblogs.com/crazymakercircle/p/ 14385641 .html

##### 3 .相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 4 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 2 章 Spring Cloud+Nginx 高并发核心

# 编程的学习准备

SpringCloud+Nginx 相结合的分布式 Web 应用架构已经成为 IT 领域的事实架构标准。由于其高
度的可伸缩、高可用、高并发能力，使之成为各新产品、新项目技术选型时的最佳架构之一，也成
为老产品、老项目技术升级选型时的最佳架构之一。目前，无论是一线互联网公司（如阿里巴巴、
百度、美团等大型互联网公司）还是中小型互联网企业，都广泛地使用 SpringCloud+Nginx 架构。
尽管 SpringCloud+Nginx 架构已经成为主流架构，但广大 Java 工程师对 SpringCloud 微服务、
Nginx 反向代理核心知识的掌握还是不够，大多数人仅停留在配置、使用阶段。
本书的目标是帮助初、中、高级开发工程师补齐 SpringCloud 微服务、Nginx 反向代理核心知识
的短板。本书从基础设计模式、基础原理出发，理论与实战相互结合，系统且详尽地介绍 SpringCloud
+Nginx 高并发核心编程。
本书的初衷是为大家掌握 SpringCloud 和 Nginx 架构的核心知识，最终顺利成长为优秀的 Java
工程师甚至成为优秀的架构师尽一份绵薄之力。

## 2. 1 SpringCloud+Nginx 架构的主要组件

以 crazy-SpringCloud 开发脚手架为例，一个基于 SpringCloud+Nginx 的应用架构大致如图 2 - 1 所示。
Nginx 作为反向代理服务器，代理内部 Zuul 网关服务，通过 Nginx 自带的负载均衡算法进行客
户端请求的代理转发、负载均衡等功能。
Zuul 网关主要实现了微服务集群内部的请求路由、负载均衡、统一校验等功能。虽然在服务
路由和负载均衡方面，Zuul 和 Nginx 的功能比较类似，但是 Zuul 通过自身注册到 Eureka/Nacos，通过
微服务的 serviceID 实现微服务提供者之间的路由和转发。


图 2 - 1 基于 SpringCloud+Nginx 的应用架构
Eureka、Nacos 都是 SpringCloud 技术体系中的服务注册与发现中间件。Eureka 是 Netflix 开源的
一款产品，提供了完整的服务注册和发现，是 SpringCloud“全家桶”中核心的组件之一。
Nacos 是阿里巴巴推出来的一个开源项目，也是一个服务注册与发现中间件，它用于完成服务
动态注册、动态发现、服务管理，还兼备了配置管理的功能。Nacos 提供了一组简单易用的特性集，
用于实现动态服务发现、服务配置、服务元数据及流量管理。
新版本的 Eureka 已经闭源，而阿里巴巴的 Nacos 除了具备 Eureka 注册中心功能外，还具备 Spring
CloudConfig 配置中心的功能，因此大大地降低了使用和维护的成本。另外，Nacos 还具有分组隔离
功能，一套 Nacos 集群可以支撑多项目、多环境。综合上述的多个原因，在实际的开发场景中，推
荐大家使用 Nacos。但是，本书出于学习目的，注册中心和配置中心的内容还是介绍 Eureka+Config
组合，其实在原理上，Nacos 和 Eureka+Config 组合是差不多的。
除了一系列的基础设施中间件技术组件之外，微服务架构中大部分的独立业务模型都是以微
服务提供者的角色出现的。一般来说，系统可以按照各类业务模块进行细粒度的微服务拆分，例如
秒杀系统中的用户、商品等，每个业务模块拆分成一个微服务提供者 Provider 组件，作为独立应用
程序进行启动执行。
在 SpringCloud 生态中，微服务提供者 Provider 之间的远程调用是通过 Feign+Ribbon+Hystrix 组
合完成的：Feign 用于完成 RPC 远程调用的代理封装；Ribbon 用于在客户端完成各远程目标服务实
例之间的负载均衡；Hystrix 用于完成自动熔断降级等多个维度的 RPC 保护。
在 Nginx+SpringCloud 架构中还存在一系列的日志记录、链路跟踪、应用监控、JVM 性能指标、
物理资源监控中间件等。本书没有对上述的辅助中间件做专门介绍，具体有多方面的原因：一是监
控的软件太多，如果介绍太全，则篇幅不够，如果介绍太少，对读者不一定有帮助；二是有关监控
软件的使用大多是一些软件的操作步骤和说明，其原理性的内容比较少，实操性内容比较多，因此


###### 使用视频的形式会比文字形式知识传递的效果会更好，疯狂创客圈后续可能（但不一定）会推出一

###### 些微服务监控方面的教学视频供大家参考，大家可关注社群博客。

### 2. 2 SpringCloud+Nginx 核心知识的广泛欠缺

尽管 SpringCloud+Nginx 架构已经是目前的 Java 主流，但是无论对 SpringCloud 微服务的核心原理，
还是对 Nginx 的核心知识，大部分开发人员还是了解较少，更多的是处于怎么使用、配置的阶段。
前段时间，笔者在一场 Java 架构师的现场面试交流中，问了面试者几个稍微基础一点的问题，
大致如下：

问题 1 ：SpringCloud 的 RPC 调用流程是什么？
其答案仅仅涉及 Eureka、Zuul、Hystrix 熔断器等基本概念和浅层次的知识，对于 Feign 内部 RPC
调用流程以及 Feign 和 Hystrix 之间的关系根本不了解。

问题 2 ：SpringCloud 的性能有哪些优化点？
其答案仅仅是简单粗暴地加缓冲、加节点，并肯定完全正确。其答案没有涉及 Feign 连接池、
Hystrix 线程池等组件的高性能配置，也没有涉及 HTTP 长连接等更加底层的配置。

问题 3 ：请介绍一下 Hystrix 中的滑动窗口。
其答案竟然是完全不知道。
问题 4 ：有没有用过 Nginx，请说一说 Nginx 的 HTTP 请求处理流程分为哪些步骤。
其答案也仅仅是介绍了一下 Nginx 的简单配置，对于其基础结构和 HTTP 请求处理流程根本没
有接触过，更加不知道如何进行高性能的应用开发。

问题 5 ：有没有做过限流，请说一说令牌桶和漏桶的区别。
少部分的面试者，竟然没有听说过。大部分面试者，仅仅了解令牌桶和漏桶的基础概念，对
令牌桶和漏桶的区别的回答都不太准确。说明大家没有真正地了解令牌桶和漏桶。

回答以上问题的面试者有 10 多年的开发经验。毫无疑问，通过上述问题的答案可以看出，这
位有着 10 多年经验的资深 Java 工程师，对 SpringCloud 和 Nginx 核心知识十分欠缺，显然不大可能架
构出高性能的 Java 应用，当然也胜任不了 Java 架构师岗位。
而实际上，通过笔者的交流了解到，对于 SpringCloud 和 Nginx 核心知识的缺乏不是以上面试者
的个人问题，笔者周围很多工程师都存在这个问题。
总之，对 SpringCloud 和 Nginx 架构的核心知识缺乏深入的了解和掌握，目前来说是一个涉及面
较广的共性问题。本书的初衷就是为大家提供 SpringCloud 和 Nginx 架构的核心知识，为大家顺利成
长为优秀的 Java 工程师甚至架构师尽一份绵薄之力。

### 2. 3 SpringCloud 和 SpringBoot 的版本选择

```
SpringCloud 是基于 SpringBoot 构建的，它们之间的版本也有对应的配套关系。在构建项目时，
```

###### 注意版本之间的对应关系，版本若对不上则会出现问题。

SpringCloud 和 SpringBoot 的版本配套关系如表 2 - 1 所示。
表 2 - 1 SpringCloud 与 SpringBoot 的版本配套关系
SpringCloud SpringBoot
Camden 1. 4 .x
Dalston 1. 5 .x
Edgware 1. 5 .x
Finchley 2. 0 .x
Greenwich 2. 1 .x
Hoxton 2. 2 .x
SpringCloud 包含了一系列的子组件，如 SpringCloudConfig、SpringCloudNetflix、SpringCloud
OpenFeign 等，为了防止与这些子组件的版本号混淆，SpringCloud 的版本号全部使用英文单词形式
命名。具体来说 SpringCloud 的版本号使用了英国伦敦地铁站的名称来命名，并按字母 A~Z 次序发
布版本，其第一个版本叫作 Angel，第二个版本叫作 Brixton，以此类推。另外，每个大版本在解决
了一个严重的 BUG 后，SpringCloud 会发布一个 ServiceRelease 版本（小版本），简称 SRX 版本，其
中 X 是顺序的编号，比如 Finchley. SR 4 是 Finchley 大版本的第 4 个小版本。
大家做技术选型时非常喜欢选用最高版本，但是对于 Spring“全家桶”的选择来说，高版本不
一定是最佳选择。比如，目前最高的 SpringCloudHoxton 版本基于 SpringBoot 2. 2 构建，SpringBoot
2. 2 又基于 SpringFramework 5. 2 构建，也就是说，这是一次整体的、全方位的大版本的升级。大家
在项目上都会用到非常多的第三方组件，总会有一些组件没有来得及进行配套升级而不能兼容
SpringBoot 2. 2 或 SpringFramework 5. 2 ，如果贸然地进行基础框架的整体升级，就会给项目开发带
来各种各样的疑难杂症，甚至带来潜在的线上 BUG。
除此之外，SpringCloud 高版本大量推荐了不少自家新组件，但是这些新组件没有经过大规模
的使用，其功能尚待丰富和完善。以负载均衡组件为例，SpringCloudHoxton 推荐的自家组件
spring-cloud-loadbalancer 在功能上与 Ribbon 的负载均衡功能相比就弱很多。
实际上，SpringCloudFinchley 到 Greenwich 版本的升级很小，可以说微乎其微，主要是提升了
对 Java 11 的兼容性。然而，在当前的生产场景中 Java 8 才是各大项目的主流选择，另外 Java 11 （ 2019
年 4 月之后的升级补丁）已经不完全免费了。当然，和 Java 11 一样，Java 8 在 2019 年 4 月之后的补丁
版本也面临收费的问题。使用 Java 8 的理由是，自 2014 年 3 月 18 发布起至今，Java 8 已经被广泛使用
且被维护了很多年，已经非常成熟和稳定了。
综上所述，本书选用了 SpringCloudFinchley 作为学习、研究和使用的版本，并且推荐使用的
子版本为 Finchley. SR 4 。具体的 Maven 依赖坐标如下：
<dependencyManagement>
<dependencies>
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-dependencies</artifactId>
<version>Finchley. SR 4 </version>
<type>pom</type>
<scope>import</scope>
</dependency>
<dependency>


```
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-dependencies</artifactId>
<version> 2. 0. 8 .RELEASE</version>
<scope>import</scope>
<type>pom</type>
</dependency>
</dependencies>
</dependencyManagement>
```
### 2. 4 SpringCloud 微服务开发所涉及的中间件

基于 crazy-SpringCloud 脚手架（其他的脚手架也类似）的微服务开发和自验证过程中，所涉及
的基础中间件大致如下：

（ 1 ）ZooKeeper
ZooKeeper 是一个分布式的、开放源码的分布式协调应用程序，是大数据框架 Hadoop 和 HBase
的重要组件。在分布式应用中，它能够高可用地提供很多保障数据一致性的基础能力：分布式锁、
选主、分布式命名服务等。
在 crazy-SpringCloud 脚手架中，高性能分布式 ID 生成器用到了 ZooKeeper。有关其原理和使用
可参见《Java 高并发核心编程卷 1 （加强版）：NIO、Netty、Redis、ZooKeeper》一书。

（ 2 ）Redis
Redis 是一个高性能的缓存数据库。在高并发的场景下，Redis 可以对关系数据库起到很好的缓
冲作用；在提高系统的并发能力和响应速度方面，Redis 至关重要。crazy-SpringCloud 脚手架的分
布式 Session 用到了 Redis。

（ 3 ）Eureka
Eureka 是 Netflix 开发的服务注册和发现框架，它本身是一个 REST 微服务提供者，主要用于定
位运行在 AWS（Amazon 云）的中间层服务，以达到负载均衡和中间层服务故障转移的目的。Spring
Cloud 将它集成在其子项目 spring-cloud-netflix 中，以实现 SpringCloud 的服务注册和发现功能。

（ 4 ）SpringCloudConfig
SpringCloudConfig 是 SpringCloud“全家桶”中最早的配置中心，虽然在生产场景中很多的企
业已经使用 Nacos 或者 Consul 整合型的配置中心替代了独立的配置中心，但是 Config 依然适用于
SpringCloud 项目，通过简单配置即可使用。

（ 5 ）Zuul
Zuul 是 Netflix 开源网关，可以和 Eureka、Ribbon、Hystrix 等组件配合使用，SpringCloud 对 Zuul
进行了整合与增强，使用它作为微服务集群的内部网关，负责对集群内部各个 Provider 微服务提供
者进行 RPC 路由和请求过滤。

（ 6 ）Nginx/OpenResty
Nginx 是一个高性能 HTTP 和反向代理服务器，是由伊戈尔·赛索耶夫为俄罗斯访问量第二的
Rambler. ru 站点开发 Web 服务器。Nginx 源代码以类 BSD 许可证的形式发布，其第一个公开版本 0. 1. 0


在 2004 年 10 月 4 日发布，其 1. 0. 4 版本在 2011 年 6 月 1 日发布。Nginx 因高稳定性、丰富的功能集、内存
消耗少、并发能力强的特点而闻名全球，并且得到非常广泛的使用，比如百度、京东、新浪、网易、
腾讯、淘宝等都是它的用户。OpenResty 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了
大量精良的 Lua 库、第三方模块以及大多数的依赖项，用于快速搭建能够处理超高并发、扩展性极
高的动态 Web 应用、Web 服务和动态网关。
以上中间件的端口配置以及部分安装与使用的演示视频如表 2 - 2 所示。这些演示视频可在疯狂
创客圈博客中观看。

```
表 2 - 2 中间件的端口配置以及部分安装与使用的演示视频
中间件端口安装和使用的演示视频
Redis 6379 LinuxRedis 安装视频
zookeeper 2181 LinuxZooKeeper 安装视频
RabbitMQ 3306 LinuxZooKeeper 安装视频
cloud-eureka 7777 Eureka 使用视频
SpringCloudConfig 7788 SpringCloudConfig 使用视频
Zuul 7799
Nginx/OpenResty 80
```
### 2. 5 SpringCloud 微服务开发和自验证环境

在开始进入 SpringCloud 核心编程学习之前，先介绍一下开发和自验证的环境准备、中间件安
装、抓包工具的准备。

#### 2. 5. 1 开发和自验证环境的系统选项和环境变量配置

首先介绍开发和自验证的系统选型。大部分开发人员学习开发都使用过 Windows 环境，在这种
情况下，强烈建议使用虚拟机装载 CentOS 作为自验证环境。为什么要推荐 CentOS 呢？

（ 1 ）提前暴露生产环境下的问题
在生产环境下， 90 %以上的 Java 应用基本上都是使用 Linux 环境（如 CentOS）来部署的。因此，
使用 CentOS 作为自验证环境可以提前暴露在生产环境下的潜在问题，避免在开发时没有发现问题
的程序一旦部署到生产环境就出问题（笔者亲历）。

（ 2 ）学习 Shell 命令和脚本
在生产环境定位、分析、解决线上 BUG 时，需要用到基础的 Shell 命令和脚本，因此平时要多
使用、多练习。另外，Shell 命令和脚本也是 Java 程序员必知必会的面试题。使用 CentOS 作为自验证
环境能方便大家学习 Shell 命令和脚本。
当然，可以借助一些文件同步或共享工具提高开发效率。比如，可以通过 VMwareTools 共享
Windows 和 CentOS 之间的文件夹，这样在后续的 Lua 脚本的开发和调试过程中能避免来回地复制文件。
这里给大家介绍一下 crazy-SpringCloud 脚手架开发和自验证环境的准备，主要涉及两个方面：
1 ）中间件（含 Eureka、Redis、MySQL 等）相关信息的环境变量配置。


###### 2 ）主机名称的配置。

###### 对于中间件的相关信息（如 IP 地址、端口、用户账号等），很多项目都是直接以明文编码的

###### 方式放在配置文件中，这样存在安全隐患甚至会引发泄密的风险。对于这些信息，建议尽量通过操

###### 作系统环境变量进行配置，然后在配置文件中使用环境变量而不是明文编码。

例如，可以将 Eureka 的 IP 提前配置好环境变量 EUREKA_ZONE_HOST，然后在配置文件
bootstrap. yml 中按照如下方式来使用：
eureka:
client:
serviceUrl:
defaultZone:${SCAFFOLD_EUREKA_ZONE_HOSTS:http://localhost: 7777 /eureka/}
在上面的配置中，通过${SCAFFOLD_EUREKA_ZONE_HOSTS}表达式从环境变量中获取
Eureka 的 IP 地址。环境变量 SCAFFOLD_EUREKA_ZONE_HOSTS 后面跟着一个冒号和一个默认值，
表示如果环境变量值为空，就会使用默认值http://localhost: 7777 /eureka/作为配置项的值。
通过环境变量配置中间件的信息有什么好处呢？一是可以使得配置信息的切换多了一层灵活
性，如果切换 IP，那么只需修改环境变量即可；二是可以不用在配置文件中以明文编码方式存放密
码之类的敏感信息，多了一层安全保障。
crazy-SpringCloud 微服务开发脚手架用到的环境变量较多，以自验证环境 CentOS 中的配置文
件/etc/profile 为例，部分内容大致如下：
exportSCAFFOLD_DB_HOST= 192. 168. 233. 128
exportSCAFFOLD_DB_USER=root
exportSCAFFOLD_DB_PSW=root
exportSCAFFOLD_REDIS_HOST= 192. 168. 233. 128
exportSCAFFOLD_REDIS_PSW= 123456
exportSCAFFOLD_EUREKA_ZONE_HOSTS=http:// 192. 168. 233. 128 : 7777 /eureka/
exportRABBITMQ_HOST= 192. 168. 233. 128
exportSCAFFOLD_ZOOKEEPER_HOSTS= 192. 168. 233. 128 : 2181
以上环境变量中的 192. 168. 233. 128 是笔者自验证环境 CentOS 虚拟机 IP 地址，Redis、ZooKeeper、
Eureka、MySQL、Nginx 等中间件都运行在这台虚拟机上，大家在运行 crazy-SpringCloud 微服务开
发脚手架之前需要进行相应的更改。
最后介绍一下有关主机名称的配置。如果在调试过程中直接通过 IP 访问 REST 接口，那么在
Fiddler 工具抓包中查看报文就不方便。为了方便抓包，将 IP 地址都映射成了主机名称。在笔者命名
的 Windows 开发环境中，hosts 文件内配置的主机名称如下：
127. 0. 0. 1 crazydemo. com
128. 0. 0. 1 file. crazydemo. com
129. 0. 0. 1 admin. crazydemo. com
130. 0. 0. 1 xxx. crazydemo. com
131. 168. 233. 128 eureka. server
132. 168. 233. 128 zuul. server
133. 168. 233. 128 nginx. server
134. 168. 233. 128 admin. nginx. server
注意，本书后文的演示用例用到的 URL 会使用以上主机名称取代 IP 地址。


#### 2. 5. 2 使用 Fiddler 工具抓包和查看报文

###### 在微服务程序开发和验证过程中，对 HTTP 接口发起请求有多种方式：

###### 1 ）直接发起请求。

2 ）通过内部网关代理（如 Zuul）发起请求。
3 ）通过外部网关反向代理（如 Nginx）发起请求。
以 crazy-SpringCloud 脚手架中的 uaa-provider 服务的 HTTP 接口/api/user/detail/v 1 为例，通过以上
3 种方式发起请求的 HTTP 链路示意图如图 2 - 2 所示。

图 2 - 2 3 种方式请求 uaa-provider 的 HTTP 链路示意图
在生产环境下，为了满足内外网之间的转发、多服务器之间的负载均衡要求，生产环境中外
部反向代理（Nginx）往往不止一层。因此，请求的 HTTP 链路往往更加复杂。
无论是在开发环境、自验证环境、测试环境还是生产环境中，查看 HTTP 接口的访问链路和报
文内容对于定位、分析、解决问题来说都非常重要，这就需要抓包工具。抓包工具的类型比较多，
笔者目前用得较多的为 Fiddler。
比如，在调试本书 crazy-SpringCloud 脚手架中 uaa-provider 功能时，使用 Fiddler 能全面地查看到
发向服务端的 HTTP 报文的请求头和响应头，具体如图 2 - 3 所示。


图 2 - 3 使用 Fiddler 查看 HTTP 报文的请求头和响应头
在开发过程中，Fiddler 这类抓包工具的使用对于分析和定位问题非常有用。笔者经常使用
Fiddler 完成下面的工作：

1 ）查看 REST 接口的处理时间，在解决性能问题时帮助查看接口的整体时间。
2 ）查看 REST 接口请求头、响应头、响应内容，主要用于查看请求 URL、请求头、响应头是
否正确，并且在必要的时候可以将所有的请求头一次性地复制到 Postman 等请求发起工具，帮助新
请求快速地构造同样的 HTTP 头部。
3 ）请求重发，除了可以使用独立的请求工具（如 Swagger-UI/Postman 等）重发请求之外，还
可以在 Fiddler 上直接进行请求重发，重发的请求有相同的头部和参数，调试时非常方便。

### 2. 6 crazy-SpringCloud 微服务开发脚手架

###### 无论是单体应用还是分布式应用，如果从零开始开发，那么都会涉及很多基础性的、重复性

的工作，比如用户认证、session 管理等。有了开发脚手架，这块基础工作就可以省去，直接利用脚
手架提供的基础模块，然后按照脚手架的规范进行业务模块的开发即可。
笔者看了开源平台的不少开源的脚手架，发现这些脚手架很少可以直接拿来进行业务模块的
开发，要么封装得过于重量级而不好解耦，要么业务模块分包不清晰而不方便开发，所以本着简洁
和清晰的原则，笔者发起的疯狂创客圈社群推出了自己的微服务开发脚手架 crazy-SpringCloud，它
的模块和功能如下：
crazymaker-server -- 根项目
│ ├─cloud-center -- 微服务的基础设施中心
│ │ ├─cloud-eureka -- 注册中心
│ │ ├─cloud-config -- 配置中心
│ │ ├─cloud-zuul -- 网关服务
│ │ ├─cloud-zipkin -- 监控中心
│ ├─crazymaker-base -- 公共基础依赖模块


│ │ ├─base-common --普通的公共依赖，如 utils 类的公共方法
│ │ ├─base-redis --公共的 redis 操作模块
│ │ ├─base-zookeeper --公共的 zookeeper 操作模块
│ │ ├─base-session --分布式 session 模块
│ │ ├─base-auth --基于 JWT+SpringSecurity 的用户凭证与认证模块
│ │ ├─base-runtime --各 provider 的运行时公共依赖，装配了一些通用 SpringIOCBean 实例
│ ├─crazymaker-uaa --业务模块: 用户认证与授权
│ │ ├─uaa-api --用户 DTO、Constants 等
│ │ ├─uaa-client --用户服务的 Feign 远程客户端
│ │ ├─uaa-provider --用户认证与权限的实现，包含 controller 层、service 层、dao 层的代码
实现
│ ├─crazymaker-seckill --业务模块：秒杀练习
│ │ ├─seckill-api --秒杀 DTO、Constants 等
│ │ ├─seckill-client --秒杀服务的 Feign 远程调用模块
│ │ ├─seckill-provider--秒杀服务核心实现，包含 controller 层、service 层、dao 层的代码实现
│ ├─crazymaker-demo --业务模块：练习演示
│ │ ├─demo-api --演示模块的 DTO、Constants 等
│ │ ├─demo-client --演示模块的 Feign 远程调用模块
│ │ ├─demo-provider --演示模块的核心实现，包含 controller 层、service 层、dao 层的代码
实现
在业务模块如何分包的问题上，大部分企业都有自己的统一规范。crazy-SpringCloud 脚手架从职
责清晰、方便维护、能快速导航代码的角度出发，将每一个业务模块细分成以下 3 个子模块：

（ 1 ）{module}-api
该子模块定义了一些公共的 Constants 业务常量和 DTO 传输对象，该子模块既被业务模块内部
依赖，也可能被依赖该业务模块的外部模块依赖。

（ 2 ）{module}-client
该子模块定义了一些被外部模块依赖的 Feign 远程调用客户类。该子模块是专供外部的模块，
不能被内部的其他子模块依赖。

（ 3 ）{module}-provider
该子模块是整个业务模块的核心，也是一个能够独立启动、运行的微服务提供者（Application），
该模块包含涉及业务逻辑的 controller 层、service 层、dao 层的完整代码实现。

crazy-SpringCloud 微服务开发脚手架在以下两方面进行了弱化：
1 ）在部署方面对容器的介绍进行了弱化，没有使用 Docker 容器而是使用 Shell 脚本，有多方面
的原因：一是本脚手架初心是学习，使用 Shell 脚本而不是 Docker 去部署方便大家学习 Shell 命令和
脚本；二是 Java 和 Docker 其实整合得很好，学习非常容易，稍加配置就能做到一键发布，找点资料
就可以掌握；三是部署和运维是一个专门的工作，生产环境的部署甚至是整个自动化构建和部署的
工作，实际上属于运维的专项工作，由专门的运维岗位人员去完成，而部署的核心仍然是 Shell 脚
本，所以对于开发人员来说掌握 Shell 脚本才是重中之重。
2 ）对监控软件的介绍进行了弱化。本书没有对链路监控、JVM 性能指标、熔断器监控软件的
使用做专门介绍，有多方面的原因：一是监控的软件太多，如果介绍太全，篇幅又不够，介绍太少，
大家又不一定用到；二是监控软件的使用大多是一些软件的操作步骤和说明，原理性的内容比较少，
使用视频的形式会比文字形式知识传递的效果会更好。疯狂创客圈后续可能会推出一些微服务监控
方面的教学视频供大家参考，请大家关注社群博客。不论如何，只要掌握了 SpringCloud 核心原理，
那么那些监控组件的使用对大家来说基本上都是一碟小菜。


### 2. 7 以秒杀作为 SpringCloud+Nginx 的实战案例

###### 本书的综合性实战案例是一个高性能的秒杀练习案例。为何要以秒杀作为本书的综合性实战案

###### 例呢？先回顾一下在单体架构还是主流的年代，大家学习 J 2 EE 技术时的综合性实战案例一般来说都

###### 是从 0 开始编写代码，一行一行地写一个购物车应用。通过购物车应用能对 J 2 EE 有一个全方位的练习，

包括前端的 HTML 网页、JavaScript 脚本，后端的 MVC 框架、数据库、事务、多线程等各种技术。
时代在变，技术的复杂度在变，前端和后端的分工也变了。现在的 J 2 EE 开发已经进入分布式
微服务架构的时代，前端和后端框架都变得非常复杂，前端和后端工程师已经有了比较明确的分工。
后端程序员专门做 Java 开发，前端程序员专门做前台的开发。后端程序员可以不需要懂前台的技术
如 Vue、TypeScript 等，当然，很多的前端程序员也不一定需要懂后台技术。
相比单体服务时代，现在的分布式开发时代学习 Java 后端技术的难度大多了。首先面临一大堆
分布式、高性能中间件的学习，比如 Netty、ZooKeeper、RabbitMQ、SpringCloud、Redis 等都是后
端程序员必知必会的。然后像 Jmeter 这类压力测试工具和 Fiddler 这类的抓包工具，已经成为每个后
端程序员必须掌握的内容。因为在分布式环境下需要定位、发现并解决数据一致性、高可靠性等问
题，通过压力测试，本来很正常的代码也会在运行时出现很多性能相关的问题。
另外，随着移动互联网、物联网的发展，当前面临的高并发场景已经不只是局限于电商，在
其他的应用中也越来越多。所以，现在高并发开发技术由少数工程师需要掌握的高精尖技术变成了
大多数人都需掌握的基础技能。一般来说，高并发开发的三大利器为缓存、降级和限流。
缓存的目的是提升系统访问速度和增大系统能处理的容量，它是对抗高并发的银弹；降级是
当服务出问题或者服务影响到核心流程时，可以将服务暂时屏蔽掉，待高峰或者问题解决后再打开；
而有些场景并不能用缓存和降级来解决，比如稀缺资源（秒杀、抢购）、写数据（如评论、下单）
等，可以使用限流措施来对接口进行保护。
有了缓存、降级和限流这三大利器，像京东 618 、阿里双 11 这样的高并发应用场景，才能不用
担心瞬间流量导致系统雪崩，哪怕是最终只能做到有损的服务，也不会出现某些小电商平台在活动
期间服务器宕机数小时的事故。
秒杀程序的业务足够简单，涉及的技术又足够全面，可以说是分布式应用场景非常好的实战案
例。另外，现在 IT 行业人才流动性比较大，大家都会为面试做准备。在面试中，秒杀业务所覆盖的
缓存、降级、高并发限流、分布式锁、分布式 ID、数据一致性等问题都是面试的重点、热门问题。


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html

**3.** 相关的面试题，请参考 (^3000) 页《尼恩 **Java** 面试宝典》的 (^35) 个面试专题 ：
https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 3 章 Spring Cloud 入门实战

SpringCloud 全家桶是 Pivotal 团队提供的一整套微服务开源解决方案，包括服务注册与发现、
配置中心、全链路监控、服务网关、负载均衡、熔断器等组件。以上组件主要是通过对 NetflixOSS
套件中的组件进行整合完成的，该开源子项目叫作 spring-cloud-netflix，其中比较重要的组件有：

1 ）spring-cloud-netflix-Eureka：注册中心。
2 ）spring-cloud-netflix-hystrix：RPC 保护组件。
3 ）spring-cloud-netflix-ribbon：客户端负载均衡组件。
4 ）spring-cloud-netflix-zuul：内部网关组件。
SpringCloud 全家桶技术栈除了对 NetflixOSS 的开源组件做整合之外，还整合了一些选型中立
的开源组件。比如，SpringCloudZooKeeper 组件整合了 ZooKeeper，提供了另一种方式的服务发现
和配置管理。
SpringCloud 架构中的单体业务服务基于 SpringBoot 应用。SpringBoot 是由 Pivotal 团队提供的全新
框架，用来简化新 Spring 应用的初始搭建以及开发过程。SpringCloud 与 SpringBoot 是什么关系呢？

1 ）SpringCloud 利用 SpringBoot 的开发便利性巧妙地简化了分布式系统基础设施的开发。
2 ）SpringBoot 专注于快速方便地开发单体微服务提供者，而 SpringCloud 解决的是各微服务提
供者之间的协调治理关系。
3 ）SpringBoot 可以离开 SpringCloud 独立使用开发项目，但是 SpringCloud 离不开 SpringBoot，
它依赖 SpringBoot 而存在。

最终，SpringCloud 将 SpringBoot 开发的一个个单体微服务整合并管理起来，为各单体微服务
提供配置管理、服务发现、熔断器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等
基础的分布式协助能力。


### 3. 1 Eureka 服务注册与发现

###### 一套微服务架构的系统会由很多个单一职责的服务单元组成，而每个服务单元又有众多运行实

例。例如，世界最大的收费视频网站 Netflix 的系统是由 600 多个服务单元构成的，运行实例的数量
就更加庞大了。由于各服务单元颗粒度较小、数量众多，相互之间成网状依赖关系，因此需要服务
注册中心来统一管理微服务实例，维护各服务实例的健康状态。

#### 3. 1. 1 什么是服务注册与发现

从宏观角度，微服务架构下的系统角色可以简单分为服务注册中心（RegistrationCenter）、微
服务提供者（ServiceProvider）、远程客户端组件（ServiceConsumer）。
什么是服务注册呢？是指微服务提供者将自己的服务信息（如服务名、IP 地址等）告知服务注
册中心。
什么是服务发现呢？注册中心客户端组件从注册中心查询所有的微服务提供者信息，当其他的
服务下线后，服务注册中心能够告知注册中心客户端组件。
远程客户端组件与微服务提供者之间一般使用某种 RPC 通信机制来进行服务消费，常见的 RPC
通信方式为 RESTAPI，底层为 HTTP 传输协议。微服务提供者通常以 Web 服务的方式提供 RESTAPI
接口；远程客户端组件则通常以模块组件的方式完成 RESTAPI 的远程调用。
注册中心、微服务提供者、远程客户端组件之间的关系大致如图 3 - 1 所示。

图 3 - 1 注册中心、微服务提供者、远程客户端组件三者之间的关系
注册中心的主要功能如下：
1 ）服务注册表维护：这是注册中心的核心，用来记录各个微服务提供者实例的状态信息。注
册中心提供 Provider 实例清单的查询和管理 API，用于查询可用的 Provider 实例列表，管理 Provider
实例的上线和下线。
2 ）服务健康检查：注册中心使用一定机制定时检测已注册的 Provider 实例，如发现某实例长时
间无法访问，就会从服务注册表中移除该实例。


###### 微服务提供者的主要功能如下：

1 ）服务注册：服务注册是指 Provider 微服务实例在启动时将自己的信息注册到注册中心上的
过程。
2 ）心跳续约：Provider 实例会定时向服务注册中心提供“心跳”，以表明自己还处于可用的状
态。当一个 Provider 实例停止心跳一段时间后，注册中心会认为该服务实例不可用了，就会将该服
务实例从服务注册列表中剔除。如果被剔除的 Provider 实例过一段时间后继续向注册中心提供心跳，
那么服务注册中心会将该 Provider 实例重新加入服务注册表中。
3 ）健康状况查询：Provider 实例能提供健康状况查看的 API，注册中心或者其他的微服务 Provider
能够获取其健康状况。

微服务提供者的服务注册和心跳续约一般都会通过注册中心客户端组件来完成。注册中心客户
端组件还有如下功能：

1 ）服务发现：从注册中心查询可用 Provider 实例清单。
2 ）实例缓存：将从注册中心查询到的 Provider 实例清单缓存到本地，不需要在每次使用时都去
注册中心临时获取。

总体来说，注册中心、微服务提供者需要作为独立应用进行部署和运行，而注册中心客户端组
件、远程调用客户端组件则不同，它们一般会作为一个模块组件被微服务提供者使用。
SpringCloud 生态体系中存在多种注册中心框架，例如 Eureka、Nacos、Consul、ZooKeeper 等。
本书将以 Eureka 为例讲解注册中心的使用。

#### 3. 1. 2 EurekaServer 注册中心

Eureka 本身是 Netflix 开源的一款注册中心产品，并且 SpringCloud 提供了相应的集成封装，选择
它作为注册中心的讲解实例是出于以下原因：

1 ）Eureka 在业界的应用十分广泛（尤其是国外），整个框架也经受住了 Netflix 严酷生产环境
的考验。
2 ）除了 Eureka 注册中心，Netflix 的其他服务治理功能也十分强大，包括 Ribbon、Hystrix、Feign、
Zuul 等组件结合到一起组成了一套完整的服务治理框架，使得服务的调用、路由变得异常容易。

那么，Netflix 和 SpringCloud 是什么关系呢？
Netflix 是一家互联网流媒体播放商，是美国视频巨头，访问量非常大。正因如此，Netflix 把整
体的系统迁移到了微服务架构上，并且 Netflix 把它的几乎整个微服务治理生态中的组件都开源贡献
给了 Java 社区，叫作 NetflixOSS。
SpringCloud 是 Spring 背后的 Pivotal 公司（由 EMC 和 VMware 联合成立的公司）在 2015 年推出的
开源产品，主要对 Netflix 开源组件进一步封装，方便 Spring 开发人员构建微服务架构的应用。
SpringCloudEureka 是 SpringCloudNetflix 微服务套件的一部分，基于 NetflixEureka 做了二次封
装，主要负责完成微服务实例的自动注册与发现，这也是微服务架构中的核心和基础功能。
Eureka 所治理的每个微服务实例被称为 ProviderInstance（提供者实例）。每个 ProviderInstance 包含
一个 EurekaClient 组件（相当于注册中心客户端组件），它主要的工作如下：


1 ）向 EurekaServer 完成 ProviderInstance 的注册、续约和下线等操作，主要的注册信息包括服
务名、机器 IP、端口号、域名等。
2 ）向 EurekaServer 获取 ProviderInstance 清单，并且缓存在本地。
一般来说，EurekaServer 作为服务治理应用会独立地部署和运行。一个 EurekaServer 注册中心
应用在新建时，首先需要在 pom. xml 文件中添加 eureka-server 依赖库。
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
</dependency>
然后需要在启动类中添加注解@EnableEurekaServer，声明这个应用是一个 EurekaServer。启动
类的代码如下：
packagecom. crazymaker. SpringCloud. cloud. center. eureka;
importorg. springframework. boot. SpringApplication;
importorg. springframework. boot. autoconfigure. SpringBootApplication;
importorg. springframework. cloud. netflix. eureka. server. EnableEurekaServer;
//在启动类中添加注解@EnableEurekaServer
@EnableEurekaServer
@SpringBootApplication
publicclassEurekaServerApplication{
publicstaticvoidmain (String[]args){
SpringApplication.run (EurekaServerApplication. class, args);
}
}
接下来，在应用配置文件 application. yml 中对 EurekaServer 的一些参数进行配置。一份基础的配
置文件大致如下：
server:
port: 7777
spring:
application:
name: eureka-server
eureka:
client:
register-with-eureka:false
fetch-registry:false
service-url:
#服务注册中心的配置内容 ，指定服务注册中心的位置
defaultZone:${SCAFFOLD_EUREKA_ZONE_HOSTS:http://localhost: 7777 /eureka/}
instance:
hostname:${EUREKA_ZONE_HOST: localhost}
server:
enable-self-preservation:true #开启自我保护
eviction-interval-timer-in-ms: 60000 #扫描失效服务的间隔时间 （单位为毫秒，默认是 60
× 1000 毫秒，即 60 秒）
以上的配置文件中包含 3 类配置项：作为服务注册中心的配置项（eureka. server.*）、作为 Provider
提供者的配置项（eureka. instance.*）、作为注册中心客户端组件的配置项（eureka. client.*），它们
的具体含义稍后介绍。
配置完成后，通过运行启动类 EurekaServerApplication 就可以启动 EurekaServer，然后通过浏览
器访问 EurekaServer 的控制台界面（其端口为 server. port 配置项的值），大致如图 3 - 2 所示。


图 3 - 2 EurekaServer 的控制台界面
实际上一个 EurekaServer 实例身兼三个角色：注册中心、微服务提供者、注册中心客户端，主
要原因如下：

1 ）对于所有 ProviderInstance 而言，EurekaServer 的角色是注册中心。
2 ）对于 EurekaServer 集群中其他的 EurekaServer 而言，EurekaServer 的角色是注册中心客户端。
3 ）EurekaServer 对外提供 REST 接口的服务，当然也是微服务提供者。
1 .EurekaServer 作为服务注册中心角色的配置
（ 1 ）eureka. server. enable-self-preservation
此配置项用于设置是否关闭注册中心的保护机制。什么是保护机制呢？EurekaServer 会定时统
计 15 分钟之内心跳成功的 Provider 实例的比例，如果低于 85 %将会触发保护机制，处于保护状态的
EurekaServer 不剔除失效的微服务提供者。enable-self-preservation 的默认值为 true，表示开启自我保
护机制。如果 15 分钟之内心跳成功的 Provider 实例的比例高于 85 %，那么 EurekaServer 仍然会处于正
常状态。

（ 2 ）eureka. server. eviction-interval-timer-in-ms
配置 EurekaServer 清理无效节点的时间间隔，默认为 60000 毫秒（即 60 秒）。但是，如果 Eureka
Server 处于保护状态，此配置就无效。

2 .EurekaServer 作为微服务提供者的配置项
EurekaServer 自身也是一种特殊的微服务提供者，对外提供 REST 服务，所以需要配置一些作
为 Provider 实例专属的配置项，大致如下：

```
（ 1 ）eureka. instance. hostname
设置当前实例的主机名称。
```

（ 2 ）eureka. instance. appname
设置当前实例的服务名称。默认值取自 spring. application. name 配置项的值，如果该选项没有值，则
eureka. instance. appname 值为 unknown。在 Eureka 服务器上，微服务提供者的名称不区分字母大小写。

（ 3 ）eureka. instance. ip-address
设置当前实例的 IP 地址。
（ 4 ）eureka. instance. prefer-ip-address
如果配置为 true，就使用 IP 地址的形式来定义 Provider 实例的访问地址，而不使用主机名来定义
Provider 实例的地址。如果同时设置了 eureka. instance. ip-address 选项，就使用该选项所配置的 IP，否
则自动获取网卡的 IP 地址。默认情况下，此配置项的值为 false，使用主机名来定义 Provider 实例的访
问地址。

（ 5 ）eureka. instance. lease-renewal-interval-in-seconds
定义 Provider 实例到注册中心续约（心跳）的时间间隔，单位为秒，默认值为 30 秒。
（ 6 ）eureka. instance. lease-expiration-duration-in-seconds
定义 Provider 实例失效的时间，单位为秒，默认值为 90 秒。
（ 7 ）eureka. instance. status-page-url-path
定义 Provider 实例状态页面的 URL，此选项所配置的是相对路径，默认使用 HTTP 访问，如果需
要使用 HTTPS 则使用绝对路径配置。默认的相对路径为/info。

（ 8 ）eureka. instance. status-page-url
定义 Provider 实例状态页面的 URL，此选项配置的是绝对路径。
（ 9 ）eureka. instance. health-check-url-path
定义 Provider 实例健康检查页面的 URL，此选项所配置的是相对路径，默认使用 HTTP 访问，如
果需要使用 HTTPS 则使用绝对路径配置。默认的相对路径为/health。

（ 10 ）eureka. instance. health-check-url
定义 Provider 实例健康检查页面的 URL，此选项配置的是绝对路径。
3 .EurekaServer 作为 EurekaClient 注册中心客户端角色的配置
如果集群中配置了多个 EurekaServer，那么节点和节点之间是对等的，在角色上一个 Eureka
Server 还是其他 EurekaServer 实例的客户端，其注册中心客户端角色的相关配置项大致如下：

（ 1 ）eureka. client. register-with-eureka
作为 EurekaClient，eureka. client. register-with-eureka 表示是否将自己注册到其他的 EurekaServer，
默认为 true。因为当前集群只有一个 EurekaServer，所以需要设置成 false。

（ 2 ）eureka. client. fetch-registry
作为 EurekaClient，是否从 EurekaServer 获取注册信息，默认为 true。因为本例是一个单点的
EurekaServer，不需要同步其他 EurekaServer 节点数据，所以设置为 false。


（ 3 ）eureka. client. registery-fetch-interval-seconds
作为 EurekaClient，从 EurekaServer 获取注册信息的间隔时间，单位为秒，默认值为 30 秒。
（ 4 ）eureka. client. eureka-server-connect-timeout-seconds
EurekaClient 组件连接 EurekaServer 的超时时间，单位为秒，默认值为 5 秒。
（ 5 ）eureka. client. eureka-server-read-timeout-seconds
EurekaClient 组件读取 EurekaServer 信息的超时时间，单位为秒，默认值为 8 秒。
（ 6 ）eureka. client. eureka-connection-idle-timeout-seconds
EurekaClient 组件到 EurekaServer 连接空闲关闭的时间，单位为秒，默认值为 30 秒。
（ 7 ）eureka. client. filter-only-up-instances
从 EurekaServer 获取 Provider 实例清单时是否进行过滤，只保留 UP 状态的实例，默认值为 true。
（ 8 ）eureka. client. service-url. defaultZone
作为 EurekaClient，需要向远程的 EurekaServer 自我注册、发现其他的 Provider 实例。此配置项
用于设置 EurekaServer 的交互地址，在注册中心集群的情况下，多个 EurekaServer 之间可以使用半
角逗号分隔。
此配置项涉及 SpringCloud 中 Region（地域）与 Zone（可用区）两个概念，两者都借鉴 AWS
（Amazon 云）的概念。在非 AWS 环境下，Region 和 Zone（AvailabilityZone）可以理解为服务器的
位置，即 Region 可以理解为服务器所在的地域，Zone 可以理解成服务器所处的机房。一个 Region 地
域可以包含多个 Zone 机房。不同的 Region 地域的距离很远，一个 Region 地域的不同 Zone 间的距离往
往较近，也可能在同一个物理机房内。
在网络环境跨地域、跨机房的情况下，Region 与 Zone 都可以在配置文件中进行配置。配置 Region
与 Zone 的主要目的是，在网络环境复杂的情况下帮助客户端就近访问需要的 Provider 实例。负载均
衡组件 SpringCloudRibbon 的默认策略是优先访问同客户端处于同一个 Zone 中的服务端实例，只有
当同一个 Zone 中没有可用服务端实例时，才会访问其他 Zone 中的实例。
如果网络环境不复杂，比如所有服务器都在于同一个地域同一个机房，就不需要配置 Region
与 Zone。如果不配置 Region 地域选项值，那么其默认值为 us-east- 1 ；如果不配置 Zone 的 Key 值，那
么其默认的 Key 值为 defaultZone。可以通过 eureka. client. serviceUrl. defaultZone 选项设置默认 Zone 的注
册中心 EurekaServer 的访问地址。
SpringCloud 的注册中心地址是以 Zone 为单位进行配置的，一个 Zone 如果有多个注册中心，就
要使用逗号隔开。如果有多个机房，就配置多个 eureka. client. serviceUrl. ZoneName 配置项。举个例子，
假设在北京区域有两个机房，每个机房有一个注册中心 EurekaServer，那么 EurekaServer 配置文件
中有关 Zone 和注册中心的配置大致如下：
eureka:
client:
region: 'Beijing #指定Region区域为北京
availabilityZones:
Beijing: 'zone- 2 ,zone- 1 #指定北京的机房为zone- 2 ,zone- 1
serviceUrl:
zone- 1 :http://localhost: 7777 /eureka/ #zone- 1 机房的 EurekaServer
zone- 2 :http://localhost: 7778 /eureka/ #zone- 2 机房的 EurekaServer


在配置服务注册中心地址时，如果 EurekaServer 加入了安全验证，则注册中心的 URL 格式为：
[http://<username>:<password>@localhost:](http://<username>:<password>@localhost:) 8761 /eureka
其中<username>为安全校验的用户名，<password>为该用户的密码。
（ 9 ）eureka. client. serviceUrl.*
此配置项是上面第（ 8 ）项的上一级配置项，用于在多个 Zone 的场景下配置服务注册中心，其
类型为 HashMap，key 为 Zone，Value 为机房中的所有注册中心地址。如果没有多个 Zone，那么此配
置项有一个默认的可用区，Key 为 defaultZone。

#### 3. 1. 3 微服务提供者 Provider 的创建和配置

注册中心 EurekaServer 创建并启动好之后，接下来介绍如何创建一个 Provider 微服务提供者并
注册到 EurekaServer 中，并且提供一个 REST 接口给其他服务调用。
在本书的配套源码 crazy-SpringCloud 脚手架中设计 3 个 Provider：uaa-provider（用户账号与认证）、
demo-provider（演示用途）、seckill-provider（秒杀服务），它们的关系如图 3 - 3 所示。

图 3 - 3 本书的配套源码中的微服务提供者
这里，以 uaa-provider 微服务提供者为例来介绍 Provider 的创建和配置。
首先一个 Provider 微服务提供者至少需要以下两个组件包依赖：SpringBootWeb 服务组件、
EurekaClient 组件，如下所示：
<dependencies>
<!--SpringBootWeb服务组件-->
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
</dependency>
<!--EurekaClient组件-->
<dependency>
<groupId>org. springframework. cloud</groupId>


<artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
</dependencies>
SpringBootWeb 服务组件用于提供 REST 接口服务，EurekaClient 组件用于服务注册与发现。从
以上的 Maven 依赖可以看出，在 SpringCloud 技术体系中，一个 Provider 首先是一个 SpringBoot 应用，
所以在学习 SpringCloud 微服务技术之前必须具备一些基本的 SpringBoot 开发知识。
然后在 SpringBoot 应用的启动类上加上@EnableDiscoveryClient 注解，用于启用 EurekaClient 客
户端组件。启动类的代码如下：
packagecom. crazymaker. SpringCloud. user. info. start;
//省略 import
@SpringBootApplication
/*
* 启用 EurekaClient 客户端组件
*/
@EnableEurekaClient
publicclassUAACloudApplication
{
publicstaticvoidmain (String[]args)
{
SpringApplication.run (UAACloudApplication. class, args);
}
}
接下来，在 Provider 模块（或者项目）的 src/main/resources 的 bootstrap 启动属性文件中
（bootstrap. properties 或 bootstrap. yml）增加 Provider 实例相关的配置，具体如下：
spring:
application:
name: uaa-provider
server:
port: 7702
servlet:
context-path:/uaa-provider
eureka:
instance:
instance-id:${spring. cloud. client. ip-address}:${server. port}
ip-address:${spring. cloud. client. ip-address}
prefer-ip-address: true #访问路径优先使用IP地址
status-page-url-path:
/${server. servlet. context-path}${management. endpoints. web. base-path}/info
health-check-url-path:
/${server. servlet. context-path}${management. endpoints. web. base-path}/health
client:
egister-with-eureka: true #注册到eureka服务器
fetch-registry: true #是否去注册中心获取其他服务
serviceUrl:
defaultZone:http://${EUREKA_ZONE_HOST: localhost}: 7777 /eureka/
在详细介绍上面的配置项之前，先启动一下 Provider 的启动类，控制台的日志大致如下：
... com. netflix. discovery. DiscoveryClient-
DiscoveryClient_UAA-PROVIDER/ 192. 168. 233. 128 : 7702 : registeringservice...
...
... com. netflix. discovery. DiscoveryClient-DiscoveryClient_UAA-PROVIDER/ 192. 168.
233. 128 : 7702 - registrationstatus: 204


如果看到上面的日志，就表明 Provider 实例已经启动成功，可以进一步通过 EurekaServer 检查
服务是否注册成功：打开 EurekaServer 的控制台界面，可以看到 uua-provider 的一个实例已经成功注
册，具体如图 3 - 4 所示。

图 3 - 4 uua-provider 实例已经成功注册到 EurekaServer
前面讲到，SpringCloud 中一个 Provider 实例身兼两个角色：微服务提供者和注册中心客户端。
所以，在 Provider 的配置文件中包含了两类配置：Provider 实例角色的相关配置和 EurekaClient 角色
的相关配置。

1 .Provider 实例角色的相关配置
在微服务集群中，EurekaServer 自身也是一种特殊的微服务提供者，对外提供 REST 服务，所
以可以配置一些 Provider 实例专属的配置项。

（ 1 ）eureka. instance. instance-id
此项用于配置 Provider 实例 ID，如果不进行 ID 配置，默认值的格式如下：
${spring. cloud. client. hostname}:${spring. application. name}:${server. port}}
翻译过来就是“主机名: 服务名称: 服务端口”。默认情况下，在 EurekaWeb 控制台查看到的
uua-provider 实例的 instance-id 为 localhost: demo-provider: 7700 。
大多数时候需要将 IP 显示在 instance-id 中，只要把主机名替换成 IP 即可，假设用“IP: 端口”的
格式来定义，可以使用下面的配置：
eureka. instance. instance-id=${spring. cloud. client. ip-address}:${server. port}
从“IP: 端口”的格式一看就知道 uua-provider 在哪台机器上，端口是多少。我们还可以单击 Eureka
Server 控制台的服务 instance-id 进行跳转，去查看实例的详细信息。跳转链接的默认路径是主机名，
如果链接路径需要使用 IP，就要将配置项 eureka. instance. preferIpAddress 设置为 true。

（ 2 ）eureka. instance. ip-address
设置当前实例的 IP 地址。${spring. cloud. client. ip-address}是从 SpringCloud 依赖包中导入的配置
项，存放了客户端的 IP 地址。

（ 3 ）eureka. instance. prefer-ip-address
如果配置为 true，就使用 IP 地址的形式来定义 Provider 实例的地址，而不是使用主机名来定义
Provider 实例的地址。

（ 4 ）eureka. instance. status-page-url-path
定义 Provider 实例状态页面的 URL，此选项配置的是相对路径，默认使用 HTTP 访问，如果需要
使用 HTTPS，就使用绝对路径配置。默认的相对路径为/info。


（ 5 ）eureka. instance. health-check-url-path
定义 Provider 实例健康检查页面的 URL，此选项配置的是相对路径，默认使用 HTTP 访问，如果
需要使用 HTTPS，就使用绝对路径配置。默认的相对路径为/health。

2 .EurekaClient 客户端组件的相关配置
（ 1 ）eureka. client. register-with-eureka
作为 EurekaClient，eureka. client. register-with-eureka 表示是否将自己注册 EurekaServer，这里设
置为 true，表示需要将 Provider 实例注册到 EurekaServer。

（ 2 ）eureka. client. fetch-registry
作为 EurekaClient，是否从 EurekaServer 获取注册信息，这里设置为 true，表示需要从 Eureka
Server 定期获取注册了的 Provider 实例清单。

（ 3 ）eureka. client. service-url. defaultZone
作为 EurekaClient，需要向远程的 EurekaServer 自我注册、查询其他的提供者。此配置项用于
设置此客户端默认 Zone（类似于默认机房）的 EurekaServer 的交互地址，这里配置的是 3. 1. 2 节启动
的端口为 7777 的 EurekaServer：
eureka. client. service-url. defaultZone=http://${EUREKA_ZONE_HOST: localhost}:
7777 /eureka/
为了安全和方便，地址中并没有硬编码 EurekaServer 的 IP 地址，而是使用了提前在操作系统配
置好的指向 EurekaIP 地址的环境变量 EUREKA_ZONE_HOST，之所以这样配置，主要是为了后续
EurekaServer 的 IP 地址发生变化时只需要修改环境变量的值，而不需要修改配置文件。

#### 3. 1. 4 微服务提供者的续约（心跳）

微服务提供者的续约（心跳）保活是由 ProviderInstance 主动定期执行的，每隔一段时间调用
EurekaServer 提供的 REST 保活接口，发送 ProviderInstance 的状态信息给注册中心，告诉注册中心注
册者还在正常运行。
ProviderInstance 的续约默认是开启的，续约默认的间隔是 30 秒，也就是每 30 秒会向 Eureka
Server 发起续约（Renew）操作。如果要修改 ProviderInstance 的续约时间间隔，可以使用如下的配
置选项：
eureka:
instance:
lease-renewal-interval-in-seconds: 5 #心跳时间 ，即服务续约间隔时间（默认为 30 秒）
lease-expiration-duration-in-seconds: 15 #租约有效期 ，即服务续约到期时间（默认为 90 秒）
对其中的两个配置项介绍如下：
（ 1 ）eureka. instance. lease-renewal-interval-in-seconds
表示 ProviderInstance 的 EurekaClient 组件发送续约（心跳）给 EurekaServer 的时间间隔，上面
的配置表示每隔 5 秒发送一次续约心跳。

```
（ 2 ）eureka. instance. lease-expiration-duration-in-seconds
此配置项设置了租约有效期，在租约时间内如果 EurekaClient 未续约（心跳），则 EurekaServer
```

将剔除该服务。上面配置的租约有效期为 15 秒，心跳为 5 秒，也就是说，ProviderInstance 实例有 3
次心跳重试机会。
租约有效期需要合理设置，如果有效期太大，很可能在服务消费客户端访问的时候，该 Provider
Instance 已经宕机了。如果该值设置太小了，那么 ProviderInstance 很可能因为临时的网络抖动而被
EurekaServer 剔除掉。
EurekaServer 提供了多个和 ProviderInstance 相关的 Spring 上下文应用事件（ApplicationEvent）。
当 Server 启动、服务注册、服务下线、服务续约等事件发生时，EurekaServer 会发布相对应的应用
事件，以方便应用程序监听。
下面是几个常见的 EurekaServer 应用事件：
1 ）EurekaInstanceRenewedEvent：服务续约事件。
2 ）EurekaInstanceRegisteredEvent：服务注册事件。
3 ）EurekaInstanceCanceledEvent：服务下线事件。
4 ）EurekaRegistryAvailableEvent：Eureka 注册中心启动事件。
5 ）EurekaServerStartedEvent：EurekaServer 启动事件。
如果需要监听 ProviderInstance 的服务注册、服务下线、服务续约等事件，那么可以在 Eureka
Server 编写相应的事件监听程序，如下所示：
packagecom. crazymaker. SpringCloud. cloud. center. eureka;
//省略 import
@Component
@Slf 4 j
publicclassEurekaStateChangeListner{
/**
* 服务下线事件
*/
@EventListener
publicvoidlisten (EurekaInstanceCanceledEventevent){
log.info ("{}\t{}服务下线",event.getServerId (), event.getAppName ());
}
/**
*服务上线事件
*/
@EventListener
publicvoidlisten (EurekaInstanceRegisteredEventevent){
InstanceInfoinst=event.getInstanceInfo ();
log.info ("{}:{}\t{}服务上线",
inst.getIPAddr (), inst.getPort (), inst.getAppName ());
}
/**
*服务续约 (服务心跳) 事件
*/
@EventListener
publicvoidlisten (EurekaInstanceRenewedEventevent){
log.info ("{}\t{}服务续约",event.getServerId (), event.getAppName ());
}
@EventListener
publicvoidlisten (EurekaServerStartedEventevent){
log.info ("EurekaServer 启动");
}
}


加上事件监听后，一旦 EurekaServer 收到续约（心跳）事件，就会在控制台有输出，下面是节
选的部分日志：
... EurekaStateChangeListner: 192. 168. 142. 1 : 7700 DEMO-PROVIDER 服务续约
... EurekaStateChangeListner: 192. 168. 233. 128 : 7702 UAA-PROVIDER 服务续约
... EurekaStateChangeListner: 192. 168. 142. 1 : 7700 DEMO-PROVIDER 服务续约
... EurekaStateChangeListner: 192. 168. 233. 128 : 7702 UAA-PROVIDER 服务续约
... EurekaStateChangeListner: 192. 168. 142. 1 : 7700 DEMO-PROVIDER 服务续约
... EurekaStateChangeListner: 192. 168. 233. 128 : 7702 UAA-PROVIDER 服务续约
... EurekaStateChangeListner: 192. 168. 142. 1 : 7700 DEMO-PROVIDER 服务续约
... EurekaStateChangeListner: 192. 168. 233. 128 : 7702 UAA-PROVIDER 服务续约

#### 3. 1. 5 微服务提供者的健康状态

EurekaServer 并不记录 Provider 的所有健康状况信息，仅仅维护了一个 Provider 清单。Eureka
Client 组件查询的 Provider 提供者注册清单中，包含每个 Provider 的健康状况的检查地址。通过该健
康状况的地址可以查询 Provider 提供者的健康状况。
为了方便演示，这里启动两个 uaa-provider 实例并注册到 Eureka，具体如图 3 - 5 所示。

图 3 - 5 Eureka 控制台界面上的两个 uaa-provider 实例
通过 EurekaServer 的/apps/{provider-id}接口地址，可以获取某个 Provider 实例的详细信息。获取
演示案例中的 UAA-Provider 详细信息的 URL 如下：

```
http://eureka.server: 7777 /eureka/apps/UAA-PROVIDER
在浏览器输入该地址，返回的响应大致如下：
<application>
<name>UAA-PROVIDER</name>
<instance>
<instanceId> 192. 168. 142. 1 : 7702 </instanceId>
<hostName> 192. 168. 142. 1 </hostName>
<app>UAA-PROVIDER</app>
<ipAddr> 192. 168. 142. 1 </ipAddr>
<status>UP</status>
<portenabled="true"> 7702 </port>
<securePortenabled="false"> 443 </securePort>
<countryId> 1 </countryId>
...
<homePageUrl>http:// 192. 168. 142. 1 : 7702 /</homePageUrl>
<statusPageUrl>
http:// 192. 168. 142. 1 : 7702 /uaa-provider/actuator/info
</statusPageUrl>
<healthCheckUrl>
http:// 192. 168. 142. 1 : 7702 /uaa-provider/actuator/health
</healthCheckUrl>
```

```
...
</instance>
<instance>
<instanceId> 192. 168. 233. 128 : 7702 </instanceId>
<hostName> 192. 168. 233. 128 </hostName>
<app>UAA-PROVIDER</app>
<ipAddr> 192. 168. 233. 128 </ipAddr>
<status>UP</status>
<portenabled="true"> 7702 </port>
...
<homePageUrl>http:// 192. 168. 233. 128 : 7702 /</homePageUrl>
<statusPageUrl>
http:// 192. 168. 233. 128 : 7702 /uaa-provider/actuator/info
</statusPageUrl>
<healthCheckUrl>
</healthCheckUrl>
...
</instance>
</application>
```
```
请求地址时，/apps/{provider-id}中的 provider-id 名称不区分字母大小写。
```
在 EurekaServer 响应的 Provider 的详细信息中，有 3 个与 Provider 实例的健康状态有关的信息：
（ 1 ）status
status 是 Provider 实例本身发布的健康状态。status 的值为 UP 表示应用程序状态正常。除了 UP 外，
应用健康状态还有 DOWN、OUT_OF_SERVICE、UNKONWN 等其他取值，不过只有状态为“UP”
的 Provider 实例会被 EurekaClient 组件请求。

（ 2 ）healthCheckUrl
healthCheckUrl 是 Provider 实例的健康信息 URL 地址，默认为 SpringBootActuator 组件中 ID 为
health 的 Endpoint（端点），其默认 URL 地址为/actuator/health。

（ 3 ）statusPageUrl
statusPageUrl 是 Provider 实例的状态 URL 地址，默认为 SpringBootActuator 组件中 ID 为 info 的
Endpoint（端点），其默认 URL 地址为/actuator/info。
在实际场景中，Provider 的健康信息和状态 URL 地址可能都经过了定制，从 EurekaServer 查询
的每个实例的 healthCheckUrl 和 statusPageUrl 值可能与 Provider 实例用到的实际值不同，可以通过
Provider 的配置文件进行修改，具体如下：
eureka:
client:
egister-with-eureka: true #注册到eureka服务器
fetch-registry: true #要不要去注册中心获取其他服务
serviceUrl:
defaultZone:http://${EUREKA_ZONE_HOST: localhost}: 7777 /eureka/
instance:
instance-id:${spring. cloud. client. ip-address}:${server. port}
status-page-url-path:/${server. servlet. context-path}
${management. endpoints. web. base-path}/info
health-check-url-path:
/${server. servlet. context-path}${management. endpoints. web. base-path}/health
以上 eureka. instance 配置项的两个重要的子配置项说明如下：


（ 1 ）health-check-url-path
定义 Provider 的健康检查 URL，此配置项的值将修改 EurekaServer 中的本 Provider 实例的健康检
查路径 healthCheckUrl 的值。建议配置为 Provider 实例的 SpringBootActuator 组件的 health 端点的实际
URL 的相对路径（如果是 HTTPS 协议，可以使用绝对路径），示例的配置路径为：
instance:
health-check-url-path:
/${server. servlet. context-path}${management. endpoints. web. base-path}/health
server. servlet. context-path 部分指的是 Provider 的上下文路径，如果没有 Web 服务的 context 上下文
路径，就可以不配置；management. endpoints. web. base-path 部分指的是 SpringBootActuator 组件的默
认基础路径；health 部分是 SpringBootActuator 的 health 端点的 ID。Provider 实例的健康检查路径访问
结果如图 3 - 6 所示。

图 3 - 6 uua-provider 实例的健康信息
（ 2 ）status-page-url-path
定义 Provider 实例的状态信息 URL，此配置项的值将修改 EurekaServer 中的本 Provider 实例的状
态页面 statusPageUrl 的值。建议配置为 SpringBootActuator 组件的 info 端点的实际 URL 的相对路径
（如果是 HTTPS 协议，可以使用绝对路径），示例的配置路径为：
instance:
status-page-url-path:
/${server. servlet. context-path}${management. endpoints. web. base-path}/info
server. servlet. context-path 部分为 Provider 上下文路径，management. endpoints. web. base-path 部分
为 SpringBootActuator 组件的基础路径，/info 部分是 SpringBootActuator 的 info 端点的 ID。Provider
实例的状态页面访问结果如图 3 - 7 所示。

图^3 -^7 uua-provider 实例的状态信息
Provider 定制的 status-page-url-path 和 health-check-url-path 地址值将会被 EurekaClient 组件发送到
EurekaServer 注册中心，其他节点从 EurekaServer 获取 Provider 信息时将获取到新的配置值。
Provider 的健康信息和状态 URL 地址都是 SpringBootActuator 的端点路径，Actuator 是 Spring
Boot 技术生态中一个非常强大的组件，用于对应用程序进行监视和管理，通过 RESTAPI 接口请求来


监管、审计、收集应用的运行情况。使用之前需要在项目中引入 SpringBootActuator 的 Maven 依赖，
配置代码如下：
<dependency>
<groupId>org. springframework. boot</groupId>
<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
Actuator 提供的 RESTAPI 被称为 Endpoint（端点），Actuator 内置了非常多的端点，如 health、
info、beans、metrics、httptrace、shutdown 等。端点的名称可以称为端点 ID，每个 Endpoint 都可以启
用和禁用。默认情况下，Endpoint 的 URL 带有/actuator 基础路径，例如，health 端点默认映射到
/actuator/health，但是 Endpoint 的基础路径可以通过配置进行修改。除了内置的端点外，Actuator 同
时允许大家扩展自己的端点。
实际上，RESTAPI 仅仅是 ActuatorEndpoint 提供对外访问的一种形式，端点还可以从 JMX 的形
式对外暴露，只不过大部分应用选择 HTTPRESTAPI 的暴露形式。
可以对 SpringBootActuator 的配置进行一些定制，下面是一个简单的定制实例：
management:
endpoints:
#暴露EndPoint以供访问 ，有 JMX 和 Web 两种方式，exclude 的优先级高于 include
jmx:
exposure:
#exclude : '*'
include: '*'
web:
exposure:
#exclude : '*'
include:
["health","info","beans","mappings","logfile","metrics","shutdown","env"]
base-path:/actuator #配置Endpoint的基础路径
cors: #配置跨域资源共享
allowed-origins:http://crazydemo.com,http://zuul.server,http://nginx.server
allowed-methods: GET, POST
enabled-by-default: true #修改全局Endpoint默认设置

#### 3. 1. 6 Eureka 自我保护模式与失效 Provider 的快速剔除

Provider 服务实例注册到 EurekaServer 后会维护一个心跳连接，告诉 EurekaServer 自己还活着。
EurekaServer 在运行期间会统计所有 Provider 实例的心跳，如果失败比例在一段时间间隔（如 15 分钟）
之内低于阈值（如 85 %），EurekaServer 就会将当前所有的 Provider 实例的注册信息保护起来，让这
些实例不过期。当 EurekaServer 运行在保护模式时会有一个警告信息：“EMERGENCY! EUREKA
MAYBEINCORRECTLYCLAIMINGINSTANCESAREUPWHENTHEY'RENOT. RENEWALS
ARELESSERTHANTHRESHOLDANDHENCETHEINSTANCESARENOTBEINGEXPIRED
JUSTTOBESAFE.”该警告信息的界面如图 3 - 8 所示。
保护模式可能会导致一些问题。有时 Provider 服务实例由于内存溢出、网络故障等原因不能正
常运行，而处于保护模式的 EurekaServer 不一定会将其从服务列表中剔除出去，所以会导致客户端
调用失败。为了使得失效的 Provider 能被快速剔除，可以停用 EurekaServer 的保护模式，然后启用客
户端的健康状态检查。


图 3 - 8 EurekaServer 运行在保护模式
首先，在 EurekaServer 注册中心增加两个配置项，分别是关闭自我保护和设置清理失效服务的
时间间隔，具体如下：
server:
port: 7777
spring:
application:
name: eureka-server
eureka:
...
server:
enable-self-preservation: false #关闭自我保护 ，防止失效的服务也被一直访问（默认为 true）
eviction-interval-timer-in-ms: 10000 #清理失效服务的间隔时间 （单位为毫秒，默认为 60 × 1000
毫秒，即 60 秒）
其中的两个关键配置项的说明如下：
1 ）eureka. server. enable-self-preservation=false，将自我保护参数值设置为 false，以确保注册中
心 Eureka 停止自我保护，在心跳比例小于阈值（如 85 %）的情况下，也能将不可用的实例删除。
2 ）eureka. server. eviction-interval-timer-in-ms= 10000 ，设置清理失效服务的间隔时间为 10 秒，如
果 Provider 确实已经失效，则能确保被快速剔除；默认的清理失效服务的时间间隔为 60 秒，这个失
效服务的剔除周期算是比较长的。

其次，在 Provider（EurekaClient）微服务提供者开启健康状态检查，主要的配置如下：
eureka:
client:
healthcheck:
enabled: true #开启客户端健康检查
instance:
lease-renewal-interval-in-seconds: 5 #续约 (心跳) 频率
lease-expiration-duration-in-seconds: 15 #租约有效期
以上 5 个关键的 Provider（EurekaClient）端的配置项说明如下：
1 ）eureka. client. healthcheck. enabled=true，此项在 Client 注册一个 EurekaHealthCheckHandler 健康
检查处理器实例，该处理器会将磁盘空间状态（DiskSpaceHealthIndicator）、Hystrix 健康状态
（HystrixHealthIndicator）等多个维度的健康指标通过心跳发送到 EurekaServer。如果没有注册
EurekaHealthCheckHandler，Provider 实例的运行状况由默认的 HealthCheckHandler 实例确定，只要应
用程序正在运行，默认的 HealthCheckHandler 始终发送 UP 状态到 EurekaServer。


2 ）eureka. instance. lease-renewal-interval-in-seconds= 5 ，此配置项设置 Client 续约（心跳）的时间
间隔为 5 秒，默认为 30 秒。
3 ）eureka. instance. lease-expiration-duration-in-seconds= 15 ，此配置项设置租约的有效期为 15 秒，
默认为 90 秒。在租约时间内，如果 Client 未续约（心跳），那么 Eureka 服务器将剔除该服务。
默认的续约频率为 30 秒，默认的租约有效期为 90 秒，也就是说，在默认情况下，一个 Provider
实例有 3 次心跳重试机会。

```
配置项 eureka. client. healthcheck. enabled=true 应该放在 application. yml 文件中，而不
应该放在 bootstrap. yml 文件中。如果该选项配置在 bootstrap. yml 文件中，就可能导致 Provider
实例在 Eureka 上的状态为 UNKNOWN，具体如图 3 - 9 所示。
```
```
图 3 - 9 Provider 实例在 Eureka 的状态为 UNKNOWN
```
将 Provider（如 UUA-PROVIDER）的配置项 eureka. client. healthcheck. enabled=true 从 bootstrap. yml
文件移动到 application. yml 文件，然后重启其实例。之后可以看到 Provider 的实例（如 UUA-PROVIDER
的两个实例）在 Eureka 上的状态从 UNKNOWN 变成了 UP，具体如图 3 - 10 所示。

图 3 - 10 UUA-PROVIDER 的两个实例从 UNKNOWN 变成 UP
4 ）eureka. server. enable-self-preservation=true，此配置项设置 EurekaServer 是否开启自我保护模
式，默认为 true。
默认情况下，如果 EurekaServer 在一定时间内（默认为 90 秒）没有接收到某个微服务实例的心
跳，EurekaServer 将认为该实例已经故障，会注销该实例。当发生网络通信故障时，微服务与 Eureka
Server 之间无法正常通信，以上行为可能变得非常危险了——因为微服务本身其实是健康的，此时
本不应该注销这个微服务。Eureka 通过“自我保护模式”来解决这个问题——当 EurekaServer 节点
在短时间内丢失过多客户端（可能发生了网络故障）时，这个节点就会进入自我保护模式。一旦进
入该模式，EurekaServer 就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不
会注销任何微服务）。当网络故障恢复后，该 EurekaServer 节点会自动退出自我保护模式。
综上所述，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留


###### 所有微服务（健康的微服务和不健康的微服务都会保留），也不盲目注销任何健康的微服务。使用

自我保护模式可以让 Eureka 集群更加健壮、稳定。
5 ）eureka. server. eviction-interval-timer-in-ms= 60000 ，此配置项设置 EurekaServer 清理无效节点
的时间间隔，默认为 60000 毫秒（即 60 秒）。

测试环境 EurekaServer 参考配置具体如下：
server:
port: 7777
spring:
application:
name: eureka-server
eureka:
client:
register-with-eureka: false #单机版部署 ，注册中心不向其他注册中心注册自己
fetch-registry: false #单机版部署 ，注册中心不做 Provider 实例清单检索
service-url:
#浏览器打开 :http://localhost: 7777 /
#服务注册中心的配置内容 ，指定服务注册中心的位置
defaultZone:${SCAFFOLD_EUREKA_ZONE_HOSTS:http://localhost: 7777 /eureka/}
instance:
prefer-ip-address: true #访问路径可以显示IP地址
instance-id:${spring. cloud. client. ip-address}:${server. port}
ip-address:${spring. cloud. client. ip-address}
server:
enable-self-preservation:false #关闭自我保护 ，防止失效的服务也被一直访问（默认是 true）
eviction-interval-timer-in-ms: 10000 #扫描失效服务的间隔时间 （单位为毫秒，默认是 60 × 1000
毫秒，即 60 秒）
本实例为单机版的 EurekaServer 参考配置，生产环境一般会使用集群模式，甚至使用 Nacos 集
群替代 EurekaServer。无论是 EurekaServer 集群还是 Nacos 集群，具体的配置原理都比较简单，这里
不再赘述。

### 3. 2 Config 配置中心

###### 在采用分布式微服务架构的系统中，由于服务数量巨多，为了便于服务配置文件统一管理，需

###### 要分布式配置中心组件。如果各个服务的配置分散管理，那么上线之后配置的如何保持一致将会是

一个很令人头疼的问题。因此，各个服务的配置定然需要集中管理。SpringCloudConfig 配置中心
是一个比较好的解决方案。使用 SpringCloudConfig 配置中心涉及两个部分：

```
1 ）config-server：服务端配置。
2 ）config-client：客户端配置。
```
#### 3. 2. 1 config-server 服务端组件

通过 SpringCloud 构建一个 config-server 服务大致需要 3 步。首先，在 pom. xml 中引入
spring-cloud-config-server 依赖，如下所示：
<dependency>


<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-config-server</artifactId>
</dependency>
其次，在创建的 SpringBoot 程序的启动类上添加@EnableConfigServer 注解，开启 ConfigServer
服务，代码如下：
@EnableConfigServer
@SpringBootApplication
publicclassApplication{
publicstaticvoidmain (String[]args){
newSpringApplicationBuilder (Application. class). web (true). run (args);
}
}
最后，设置属性文件的位置。SpringCloudConfig 提供本地存储配置的方式。在 bootstrap 启动
属性文件中，设置属性 spring. profiles. active=native，并且设置属性文件所在的位置，如下所示：
server:
port: 7788 #配置中心端口
spring:
application:
name: config-server #服务名称
profiles:
active: native #设置读取本地配置文件
cloud:
config:
server:
native:
searchLocations:classpath: config/ #申明本地配置文件的存放位置
配置说明：
1 ）spring. profiles. active=native，表示从本地读取配置，而不是从 Git 读取配置。
2 ）search-locations=classpath: config/，表示查找文件的路径，在类路径的 config 下。
服务端的配置文件放置规则：在配置路径下，以{label}/{application}-{profile}. properties 的命令
规范放置对应的配置文件。上面实例放置了以下配置文件：
/dev/crazymaker-common. yml
/dev/crazymaker-db. yml
/dev/crazymaker-redis. yml
以上文件分别对通用（common）、数据库（db）、缓存（redis）的相关属性进行设置。作为
示例，缓存（redis）的配置如下所示：
spring:
redis:
blockWhenExhausted: true #连接耗尽时是否阻塞
database: 0 #指定redis数据库
host:${SCAFFOLD_REDIS_HOST: localhost} #redis主机IP
maxIdle: 100 #最大空闲连接数
maxTotal: 2000 #最大连接数
maxWaitMillis: 60000 #获取连接最大等待毫秒
minEvictableIdleTimeMillis: 1800000 #最小空闲时间
numTestsPerEvictionRun: 1024 #每次释放连接的最大数目
password:${SCAFFOLD_REDIS_PSW: 123456 } #密码 ，如果没有设置密码这个配置可以不设置
port: 6379 #redis端口


softMinEvictableIdleTimeMillis: 10000 #连接空闲多久后释放
testOnBorrow: false #在使用时监测有效性
testWhileIdle: true #获取连接时检查有效性
timeBetweenEvictionRunsMillis: 30000 #释放连接的扫描间隔 （毫秒）
connTimeout: 6000 #连接超时毫秒数
readTimeout: 6000 #读取超时毫秒数
Config 配置中心启动之后，可以使用以下的地址格式直接访问加载好的配置属性：
[http://${CONFIG-HOST}:${CONFIG-PORT}/{application}/{profile}[/{label}]](http://${CONFIG-HOST}:${CONFIG-PORT}/{application}/{profile}[/{label}])
例如，通过地址 http:// 192. 168. 233. 128 : 7788 /crazymaker/redis/dev 访问示例中配置的缓存 Redis 的
相关属性，具体如图 3 - 11 所示。

图 3 - 11 返回的配置信息
特别说明：SpringCloudConfig-server 支持多种配置方式，比如 Git、Native、SVN 等。虽然官
方建议使用 Git 方式进行配置，但是这里没有重点介绍 Git 方式，而是使用了本地文件的方式。有以
下三个原因：

```
1 ）对于学习或者一般的开发来说，本地文件的配置方式更简化。
2 ）生产环境建议使用 Nacos，它具备注册中心和配置中心相结合的功能，更加方便、简单。
3 ）掌握了 Native 的配置方式之后，对于 Git 的配置方法就能触类旁通。
```
#### 3. 2. 2 config-client 客户端组件

客户端 config-client 同 config-server 一样，需要新增 spring-cloud-starter-eureka 的依赖用来注册服
务，然后增加 spring-cloud-starter-config 依赖，引入配置相关的 JAR 包。
<dependencies>
...
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-config</artifactId>


```
</dependency>
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-eureka</artifactId>
</dependency>
</dependencies>
在 bootstrap. properties 中，按如下规则增加客户端配置的映射规则：
spring. cloud. config. label：dev #对应服务器端规则中的 {label}部分
spring. application. name：crazymaker #对应服务器端规则中的 {application}部分
spring. cloud. config. profile：redis #对应服务器端规则中的 {profile}部分
spring. cloud. config. uri：http://${CONFIG-HOST}: 7788 / #配置中心config-server独立的uri
地址
其效果如图 3 - 12 所示。
```
图 3 - 12 增加客户端配置的映射规则
如果是与 Eureka 的客户端配合使用，那么建议开启配置服务的自动发现机制，使用如下的配置：
spring. cloud. config. discovery. enabled:true
spring. cloud. config. discovery. service-id: config-server
配置中心的两种发现机制不能同时存在，二者选其一即可。
客户端 config 属性的相关配置，只有配置在 bootstrap. properties（或 bootstrap. yml）中，config 部
分内容才能被正确加载。原因是 config 的相关配置必须早于 application. properties，而
bootstrap. properties 的加载也是早于 application. properties 的。

3. (^3) 微服务的 RPC 远程调用

###### 微服务的调用涉及远程接口访问的 RPC 框架，包括序列化、反序列化、网络框架、连接池、收

###### 发线程、超时处理、状态机等重要的基础技术。

通常情况下，SpringCloud 全家桶生态中的 RPC 框架是通过 Feign+Hystrix+Ribbon 组合完成的。
具体来说，Feign 负责基础的 REST 调用的序列化和反序列化，Hystrix 负责熔断器、熔断和隔离，Ribbon
负责客户端负载均衡。


```
本节先介绍基础的 REST 接口远程调用，后面再重点介绍 Ribbon 和 Hystrix 的使用和原理。
```
#### 3. 3. 1 RESTfull 风格简介

REST（RepresentationalStateTransfer）是 RoyFielding 提出的一个描述互联系统架构风格的名
词。REST 定义了一组体系架构原则，可以根据这些原则设计 Web 服务。
RESTfull 风格使用不同的 HTTP 方法来进行不同的操作，并且使用 HTTP 状态码来表示不同的结
果。如 HTTP 的 GET 方法用于获取资源，HTTP 的 DELETE 方法用于删除资源。
HTTP 协议中，大致的请求方法如下：
1 ）GET：通过请求 URI 得到资源。
2 ）POST：用于添加新的资源。
3 ）PUT：用于修改某个资源，若不存在则添加。
4 ）DELETE：删除某个资源。
5 ）OPTIONS：询问可以执行哪些方法。
6 ）HEAD：类似于 GET，但是不返回 body 信息，用于检查资源是否存在，以及得到资源的元
数据。
7 ）CONNECT：用于代理进行传输，如使用 SSL。
8 ）TRACE：用于远程诊断服务器。
在 RESTfull 风格中，资源的 CRUD 操作包括创建、读取、更新和删除操作，与 HTTP 方法之间
有一个简单的对应关系：

```
1 ）若要在服务器上创建资源，则应该使用 POST 方法。
2 ）若要在服务器上检索某个资源，则应该使用 GET 方法。
3 ）若要在服务器上更新某个资源，则应该使用 PUT 方法。
4 ）若要在服务器上删除某个资源，则应该使用 DELETE 方法。
```
#### 3. 3. 2 RestTemplate 远程调用

SpringBoot 提供了一个很好用的 REST 接口远程调用组件，叫作 RestTemplate 模板组件。该组件提
供了多种便捷访问远程 REST 服务的方法，能够大大提高客户端的编写效率。比如，可以通过 getForEntity ()
方法发送一个 GET 请求，该方法的返回值是一个 ResponseEntity。ResponseEntity 是 Spring 对 HTTP 响
应的封装，包括了几个重要的元素，如响应码、contentType、contentLength、响应消息体等。
SpringBoot 自动配置了一个 RestTemplateBuilder 建造者 IOC 容器实例来供应用程序自己去创建
需要的 RestTemplate 实例。下面是一个小实例：
packagecom. crazymaker. SpringCloud. demo. controller;
...
@RestController
@RequestMapping ("/api/call/uaa/")
@Api (tags="演示 uaa-provider 远程调用")
publicclassUaaCallController
{
//注入 SpringBoot 自动配置 RestTemplateBuilder 建造者 IOC 容器实例


@Resource
privateRestTemplateBuilderrestTemplateBuilder;
@GetMapping ("/user/detail/v 1 ")
@ApiOperation (value="RestTemplate 远程调用")
publicRestOut<JSONObject>remoteCallV 1 ()
{
/**
*根据实际的地址调整：UAA 服务获取的用户信息地址
*/
Stringurl="http://crazydemo.com: 7702 /uaa-provider/api/user/detail/
v 1 ?userId= 1 ";
/**
* 使用建造者的 build () 方法，建造 restTemplate 实例
*/
RestTemplaterestTemplate=restTemplateBuilder.build ();
ResponseEntity<String>responseEntity=
restTemplate.getForEntity (url, String. class);
TypeReference<RestOut<UserDTO>>pojoType=
newTypeReference<RestOut<UserDTO>>(){};
/**
*用到了阿里 FastJson, 将远程的响应体转成 json 对象
*/
RestOut<UserDTO>result=
JsonUtil.jsonToPojo (responseEntity.getBody (), pojoType);
/**
*组装成最终的结果，然后返回到客户端
*/
JSONObjectdata=newJSONObject ();
data.put ("uaa-data", result);
returnRestOut.success (data). setRespMsg ("操作成功");
}
}
在代码中，getForEntity () 的第一个参数为要调用的 Rest 服务地址，这里调用了 UAA 微服务提供
者提供的用户详细信息接口；getForEntity () 的第二个参数为响应体的封装类型，这里是 String。
本质上，RestTemplate 实现了对 HTTP 请求的封装处理，并且形成了一套模板化的调用方法。该
组件通过这一套请求调用方法实现各种类型 Rest 资源的请求处理，比如通过 getForEntity () 实现 GET
类型的 Rest 资源请求处理。
{
"respCode": 0 ,
"respMsg": "操作成功",
"datas":{
"uaa-data":{
"respCode": 0 ,
"respMsg": "操作成功",
"datas":{
"id": null,
"userId": 1 ,
"username": "test",
"password": "$ 2 a$ 10 $AsCxXPI 8 B/JDzKK 56 ZACjuH 9 Pi 2 TuT 6 LLC 0 Nwh 8 Qt 3 a 2 eFp 04 gziy",
"nickname": "测试用户 1 ",
...
}
}
}
}


#### 3. 3. 3 Feign 远程调用

Feign 是什么？Feign 是在 RestTemplate 基础上封装的，使用注解的方式来声明一组与微服务提
供者 Rest 接口所对应的本地 JavaAPI 接口方法。Feign 将远程接口抽象成为一个声明式的 Rest 客户端，
并且负责完成 Rest 接口和服务提供方的接口绑定。
Feign 具备可插拔的注解支持，包括 Feign 注解和 JAX-RS 注解。同时，对于 Feign 自身的一些主
要组件，比如编码器和解码器等，它也以可插拔的方式提供以便在有需求时扩张和替换它们。使用
Feign 的第 1 步，在项目的 pom. xml 文件中添加 Feign 依赖：
<!--添加Feign依赖-->
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
使用 Feign 的第 2 步，在主函数的类上添加@EnableFeignClient，在客户端启动 Feign：
packagecom. crazymaker. SpringCloud. user. info. start;
...
//启动 Feign
@EnableFeignClients (basePackages=
{"com. crazymaker. SpringCloud. seckill. remote. client"},
defaultConfiguration={TokenFeignConfiguration. class}
)
publicclassUserCloudApplication{
publicstaticvoidmain (String[]args){
SpringApplication.run (UserCloudApplication. class, args);
}
}
使用 Feign 的第 3 步，编写声明式接口。这一步将远程服务抽象成为一个声明式的 Rest 客户端，
示例如下：
packagecom. crazymaker. SpringCloud. seckill. remote. client;
...
/**
*@description：远程服务的本地声明式接口
*/
@FeignClient (value="seckill-provider", path="/api/demo/")
publicinterfaceDemoClient{
/**
*测试远程调用
*@returnhello
*/
@GetMapping ("/hello/v 1 ")
Result<JSONObject>hello ();
/**
*非常简单的一个回显接口，主要用于远程调用
*@returnecho 回显消息
*/
@RequestMapping (value="/echo/{word}/v 1 ",
method=RequestMethod. GET)
Result<JSONObject>echo (
@PathVariable (value="word") Stringword);
}


在上面接口的@FeignClient 注解配置中，使用 value 指定了需要绑定的服务，使用 path 指定接口
的 URL 前缀。然后使用@GetMapping 和@RequestMapping 两个方法级别的注解分别声明了两个远程
调用接口。
使用 Feign 的第 4 步，调用声明式接口。这一步非常简单，代码如下：
packagecom. crazymaker. SpringCloud. user. info. controller;
...
@Api (value="基础学习 DEMO", tags={"基础学习 DEMO"})
@RestController
@RequestMapping ("/api/demo")
publicclassDemoController{
//注入@FeignClient 注解所配置的客户端实例
@Resource
DemoClientdemoClient;
@GetMapping ("/say/hello/v 1 ")
@ApiOperation (value="Feign 远程调用")
publicResult<JSONObject>hello (){
Result<JSONObject>result=demoClient.hello ();
JSONObjectdata=newJSONObject ();
data.put ("remote", result);
returnResult.success (data). setMsg ("操作成功");
}
}
通过以上 4 步可以看出，通过 Feign 进行 RPC 调用比直接通过 RestTemplate 要简单得多。

### 3. 4 Feign+Ribbon 实现客户端负载均衡

理论上，如果服务端同一个微服务提供者 Provider 存在多个运行实例，一般的负载均衡的方案
分为以下两种：

（ 1 ）服务端负载均衡
在消费者和微服务提供者中间使用独立的反向代理服务进行负载均衡。可以通过硬件的方式提
供反向代理服务，比如 F 5 专业设备；也可以通过软件的方式提供反向代理服务，比如 Nginx 反向代
理服务器；更多的情况是两种方式结合，并且有多个层级的反向代理。

（ 2 ）客户端负载均衡
客户端自己维护一份从注册中心获取的 Provider 列表清单，根据自己配置的 Provider 负载均衡选
择算法在客户端进行请求的分发。Ribbon 就是一个客户端的负载均衡开源组件，是 Netflix 发布的开
源项目。
Feign 组件自身不具备负载均衡能力，SpringCloudFeign 通过集成 Ribbon 组件实现了客户端的
负载均衡。Ribbon 在客户端以轮询、随机、权重等多种方式实现负载均衡。由于在微服务架构中同
一个微服务 Provider 经常被部署多个运行实例，因此客户端的负载均衡可以说是基础能力。


#### 3. 4. 1 SpringCloudRibbon 基础

SpringCloudRibbon 是 SpringCloud 集成 Ribbon 开源组件的一个模块，它不像服务注册中心
EurekaServer、配置中心 SpringCloudConfig 那样独立部署，而是作为基础设施模块，几乎存在于每
一个 SpringCloud 微服务提供者中。微服务间的 RPC 调用，以及 API 网关的代理请求的 RPC 转发调用，
实际上都需要通过 Ribbon 来实现负载均衡。有关 Ribbon 的详细资料请参考其官网，本书只介绍基本
的使用。
虽然 SpringCloud 集成了 Ribbon 组件，但是要在 Provider 微服务中开启 Ribbon 负载均衡组件，还
需要在 Maven 的 pom 文件中增加以下 SpringCloudRibbon 集成模块的依赖：
<!--导入SpringCloudRibbon-->
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-netflix-ribbon</artifactId>
</dependency>
打开该依赖模块的配置文件 spring-cloud-starter-netflix-ribbon-{version}. pom（这里的 version 版本
号为 2. 0. 0 .RELEASE），发现 SpringCloudRibbon 集成模块主要依赖如表 3 - 1 所示的 Ribbon 组件模块。

表 3 - 1 Ribbon 组件模块
组件模块名称说明
ribbon-loadbalancer 负载均衡模块，可独立使用，也可以和别的模块一起使用
ribbon Ribbon 组件的主模块，内置的负载均衡算法都在其中实现
ribbon-httpclient
基于 ApacheHttpClient 封装的 REST 客户端，该模块具备负载均衡能力，可以直接
在需要进行 REST 调用的项目中使用，实现客户端负载均衡
ribbon-core 一些比较核心且具有通用性的代码，客户端 API 的一些配置和其他 API 的定义
在 SpringCloud 的 Provider 中使用 Ribbon，只需要导入 SpringCloudRibbon 依赖，Ribbon 在 RPC
调用时就会生效。下面以 Cray-SpringCloud 微服务脚手架为例演示 Ribbon 的执行过程。整体的演示
需要启动 3 个 Provider 微服务提供者，具体如图 3 - 13 所示。

```
图 3 - 13 Feign+Ribbon 客户端负载均衡演示 Provider 实例示意图
```

###### 演示过程大致如下：

1 ）demo-provider 模块在增加 SpringCloudRibbon 依赖后，Feign+Ribbon 的客户端负载均衡将自
动生效。演示还是使用“/api/call/uaa/user/detail/v 2 ”REST 接口，这一次它将以负载均衡的方式访问
uaa-provider 的“/api/user/detail/v 1 ”REST 接口。
2 ）启动两个 uaa-provider 微服务提供者：可以在 IDEA 调试环境（localhost）启动一个，在另一
台主机（如虚拟机）启动一个。在 Eureka 上查看 uaa-provider 实例清单，确保两个 uaa-provider 提供者
实例都成功启动。
3 ）在 IDEA 调试环境启动 demo-provider 实例，在 demo-provider 的 swagger-ui 界面上发起对
uaa-provider 微服务提供者的 RPC 调用。这里为了演示客户端的负载均衡，可以在提供者 uaa-provider
的 swagger-ui 界面上多次访问“/api/call/uaa/user/detail/v 2 ”REST 接口。
4 ）给 demo-provider 重要的源码打上断点，通过断点可以查看每次 RPC 实际访问的目标提供者
uaa-provider 的实例。断点之一设置在 ribbon-loadbalancer 组件 LoadBalancerContext 类的
getServerFromLoadBalancer 方法的某行代码上（见图 3 - 14 ），该方法的功能为获取目标 Provider 实例。每次
RPC 请求调用过来时，可以查看 Ribbon 负载均衡计算出来的 Provider，它放置在类型为 Server 的 svc 变量中。

图 3 - 14 Ribbon 计算出来的 Provider 值示意图
断点之二可以设置在 ribbon-loadbalancer 组件的 AbstractLoadBalancerAwareClient 类的方法
executeWithLoadBalancer 的某行代码上（见图 3 - 15 ）中，可以看到每次 RPC 调用的最终 URL 地址保
存在 finalUri 变量中。
多次执行并观察断点处的变量值可以发现 uaa-provider 的两个实例轮番被 RPC 访问到。


图 3 - 15 Ribbon 计算出来的最终 URL 地址值示意图
本小节的演示过程请参见疯狂创客圈社群网盘小视频：“SpringCloud 实战视频：Feign+Ribbon
实现客户端负载均衡. mp 4 ”。

#### 3. 4. 2 SpringCloudRibbon 的负载均衡策略

Ribbon 负载均衡的原理是：从 EurekaClient 类的 Bean 获取 Provider 提供者服务列表清单，并且定
期通过 IPing 类的 Bean 去判断 Provider 的可用性。每次 RPC 到来时，在 Provider 服务列表中根据 IRule
策略类的 Bean 计算出每次 RPC 要访问的最终 Provider。
Ribbon 内部有一个负载均衡器接口 ILoadBalance，定义了添加 Provider、获取所有的 Provider 列
表、获取可用的 Provider 列表等基础的操作。该接口的核心实现类 DynamicServerListLoadBalancer 会
通过 EurekaClient（实现类为 DiscoveryClient）获取 Provider 清单，并且通过 IPing 实例定期（如每 10
秒）向每个 Provider 实例发送“ping”，根据 Provider 是否有响应来判断该 Provider 提供者实例是否可
用。如果该 Provider 的可用性发生了改变，或者 Provider 清单中的数量和之前的不一致，就从注册中
心更新或者重新拉取 Provider 服务实例清单。
每次 RPC 请求到来时，由 Ribbon 的 IRule 负载均衡策略接口的某个实现类就来进行负载均衡。主
要的负载均衡的策略实现类如下：

（ 1 ）随机策略（RandomRule）
RandomRule 实现类从 Provider 服务列表清单中随机选择一个 Provider 服务实例，作为 RPC 请求的
目标 Provider。

（ 2 ）线性轮询策略（RoundRobinRule）
RoundRobinRule 和 RandomRule 相似，只是每次都取下一个 Provider 服务器。假设一共有 5 台
Provider 服务节点，使用线性轮询策略，第 1 次取第 1 台，第 2 次取第 2 台，第 3 次取第 3 台，以此类推。

（ 3 ）响应时间权重策略（WeightedResponseTimeRule）
WeightedResponseTimeRule 为每一个 Provider 服务维护一个权重值，其规则简单概况为 Provider
服务响应时间越长，其权重就越小。在进行服务器选择时，权重值越小，被选择的机会越少。
WeightedResponseTimeRule 继承了 RoundRobinRule，开始时每一个 Provider 都没有权重值，每当 RPC
请求过来时，由其父类的轮询算法完成负载均衡。该策略类有一个默认的每 30 秒执行一次的权重更


新定时任务，该定时任务会根据 Provider 实例的响应时间更新 Provider 权重列表。后续有 RPC 过来时，
将根据权重值进行负载均衡。

（ 4 ）最少连接策略（BestAvailableRule）
在进行服务器选择时，该策略类遍历 Provider 清单，选出可用的且连接数最少的一个 Provider。
该策略类里面有一个 LoadBalancerStats 类型的成员变量，会存储所有 Provider 的运行状况和连接数。
在进行负载均衡计算时，如果选取的 Provider 为 null，就会调用线性轮询策略重新选取。
如果第一次 RPC 请求时 LoadBalacneStats 成员为 null，就会使用线性轮询策略来获取满足要求的
实例，后续的 RPC 在选择的时候，才能选择连接数最少的服务。每次 RPC 请求时，BestAvailableRule
都会统计 LoadBalacneStats，作为后续请求负载均衡计算的输入。

（ 5 ）重试策略（RetryRule）
该类会在一定的时限内进行 Provider 循环重试。RetryRule 会在每次选取之后对选举的 Provider
进行判断，如果为 null 或者 notalive，就会在一定的时限（如 500 毫秒）内会不停地选取和判断。

（ 6 ）可用过滤策略（AvailabilityFilteringRule）
该类扩展了线性轮询策略，会先通过默认的线性轮询策略选取一个 Provider，再去判断该
Provider 是否超时可用，当前连接数是否超过限制，如果都满足要求，就成功返回。
简单来说，AvailabilityFilteringRule 将对候选的 Provider 进行可用性过滤，会先过滤掉多次访问
故障而处于熔断器跳闸状态的 Provider 服务，还会过滤掉并发的连接数超过阈值的 Provider 服务，然
后对剩余的服务列表进行线性轮询。

（ 7 ）区域过滤策略（ZoneAvoidanceRule）
该类扩展了线性轮询策略，除了过滤超时和连接数过多的 Provider 之外，还会过滤掉不符合要
求的 Zone 区域中的所有节点。

Ribbon 实现的负载均衡策略不止以上 7 种，还可以实现自定义的策略类。本书使用的 Spring
CloudRibbon 版本中默认使用了 ZoneAvoidanceRule 负载均衡策略。可以通过 Provider 配置文件的
ribbon. NFLoadBalancerRuleClassName 配置项更改实际的负载均衡策略。 3. 4. 1 节的演示中，微服务
demo-provider 对 uaa-provider 的 RPC 调用使用 RetryRule 负载均衡策略，demo-provider 的具体配置如下：
uaa-provider:
ribbon:
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. RetryRule #重试 +线性轮询
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. BestAvailableRule #最少连接
策略
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. RandomRule #随机选择
如果要配置全局的、针对所有的 Provider 都使用的负载均衡策略，可以在配置文件中直接使用
ribbon. NFLoadBalancerRuleClassName 配置项进行配置，具体如下：
ribbon:
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. RetryRule #重试 +线性轮询
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. BestAvailableRule #最少连接
策略
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. RandomRule #随机选择


#### 3. 4. 3 SpringCloudRibbon 的常用配置

3. 4. 2 节介绍了负载均衡的配置，本小节介绍 Ribbon 的一些常用配置以及配置 Ribbon 的两种方式：
代码方式和配置文件方式。

1 .手工配置 Provider 实例清单
如果 Ribbon 没有和 Eureka 集成，Ribbon 消费者客户端就不能从 Eureka（或者其他的注册中心）
拉取到 Provider 清单。如果不需要和 Eureka 集成，可以使用如下方式手工配置 Provider 清单：
ribbon:
eureka:
enabled: false #禁用Eureka
uaa-provider:
ribbon:
listOfServers: 192. 168. 142. 1 : 7702 , 192. 168. 233. 128 : 7702 #手动配置Provider清单
这个配置是针对 uaa-provider 这个具体服务的，配置项的前缀就是 RPC 目标服务名称。配置完之
后，demo-provider 服务就可以通过目标服务名称 uaa-provider 来调用其接口。
无论在开发环境还是在测试环境，手工配置 Provider 清单的方式都用得很少，之所以在此介绍
该方式，仅仅是为了让大家更加明白 Ribbon 的工作方式。

2 .RPC 请求超时配置
Ribbon 中有两种和时间相关的设置，分别是请求连接的超时时间 ConnectTimeout 和请求处理的
超时时间 ReadTimeout。
大家都知道，HTTP 请求有 3 个阶段：建立连接阶段、数据传送阶段、断开连接阶段。
ConnectTimeout 指的是第一个阶段建立连接所能用的最长时间。第一阶段需要进行三次握手，
ConnectTimeout 时长为三次握手完成的最长时间。如果在 ConnectTimeout 设置的时间内消费端连接
不上目标 Provider 服务，连接就会超时。这个超时也许是目标 Provider 宕机所导致的，也许是网络的
延迟所导致的。
ReadTimeout 指的是连接成功之后，从服务器读取到可用数据所占用的最长时间。如果在
ReadTimeout 设置的时间内目标 Provider 没有及时返回数据，将会导致读超时，也常常被称之为请求
处理超时。Ribbon 设置 RPC 请求超时的规则如下：
ribbon:
ConnectTimeout: 30000 #连接超时时间 ，单位为毫秒
ReadTimeout: 30000 #读取超时时间 ，单位为毫秒
在实际场景中，每个目标 Provider 的性能要求也许是不一样的。可以单独为某些 Provider 目标服
务设置特定的超时时间，只要通过服务名称进行指定即可：
uaa-provider:
ribbon:
ConnectTimeout: 30000 #连接超时时间 ，单位为毫秒
ReadTimeout: 30000 #读取超时时间 ，单位为毫秒
3 .重试机制配置
在有很多 Provider 实例同时运行的集群环境中难免会有某个 Provider 节点出现故障。如果某个目标
Provider 节点已经挂掉，但其信息还是缓存在消费者的 Ribbon 实例清单中，就会导致 RPC 时请求失败。


要解决上述问题，简单的方法就是利用 Ribbon 自带的重试策略进行重试，此时只需要指定消费
者的负载策略为重试策略并且配置适当的重试参数即可。
为进行具体的演示，demo-provider 微服务的重试策略和参数配置如下：
ribbon:
MaxAutoRetries: 1 #同一台实例的最大重试次数 ，不包括首次调用，默认为 1 次
MaxAutoRetriesNextServer: 1 #重试其他实例的最大重试次数 ，不包括首次调用，默认为 0 次
OkToRetryOnAllOperations: true #是否对所有操作都重试 ，默认为 false
ServerListRefreshInterval: 2000 #从注册中心刷新Provider的时间间隔 ，默认为 2000 毫秒，即 2 秒
retryableStatusCodes: 400 , 401 , 403 , 404 , 500 , 502 , 504
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. RetryRule #负载均衡配置为重试策略
在上面的配置中，选项 retryableStatusCodes 用于配置对特定的 HTTP 响应码进行重试，常见的
HTTP 请求的状态码如下：


（ 1 ） 2 xx（成功）
这类状态码标识客户端的请求被成功接收、理解并接受。常见如 200 （OK）、 204 （NoContent）。
（ 2 ） 3 xx（重定向）
这类状态码标识请求发起端/请求代理要做出进一步的动作来完成请求，常见如 301
（MovedPermanently）、 302 （MovedTemprarily）。

（ 3 ） 4 xx（客户端错误）
这类状态码是在客户端出错时使用的，常见如 400 （BadRequest）、 401 （Unauthorized）、 403
（Forbidden）、 404 （NotFound)。

（ 4 ） 5 xx（服务器错误）
这类状态码表示服务器知道自己出错或者没有能力执行请求，常见如 500 （InternalServerError）、
502 （BadGateway）、 504 （GatewayTimeout）。

如果一个消费者依赖很多的 Provider，就可以使用上面的重试策略与参数，针对特定的目标
Provider 进行单独配置。只要在配置时通过微服务名称指定即可：
uaa-provider:
ribbon:
MaxAutoRetries: 1
MaxAutoRetriesNextServer: 1
OkToRetryOnAllOperations:true
ServerListRefreshInterval: 2000
retryableStatusCodes: 400 , 401 , 403 , 404 , 500 , 502 , 504
NFLoadBalancerRuleClassName: com. netflix. loadbalancer. RetryRule
4 .代码配置 Ribbon
配置 Ribbon 最简单的方式是使用配置文件，除此之外，还可以通过代码的方式进行配置。
一个常见的场景：实际的 RPC 往往需要传递一些特定请求头，比如说认证令牌，这时可以通过
代码配置的方式对 Ribbon 的请求模板 template 进行请求头设置，完成请求头的传递。参考代码如下：
packagecom. crazymaker. SpringCloud. standard. config;
...
/**
*通过代码配置 Ribbon
*/
@Configuration
publicclassFeignConfigurationimplementsRequestInterceptor
{
/**
*配置 RPC 时的请求头部与参数，将来自用户的令牌传递给目标 Provider
*@paramtemplate 请求模板
*/
@Override
publicvoidapply (RequestTemplatetemplate)
{
/**
*从用户请求的上下文属性获取用户令牌
*/
ServletRequestAttributesattributes=
(ServletRequestAttributes) RequestContextHolder.getRequestAttributes ();


if (null==attributes)
{
return;
}
HttpServletRequestrequest=attributes.getRequest ();
/**
*获取令牌
*/
Stringtoken=request.getHeader (SessionConstants. AUTHORIZATION_HEAD);
if (null!=token)
{
token=StringUtils.removeStart (token,"Bearer");
/**
*设置令牌
*/
template.header ("token", newString[]{token});
}
...
}
/**
*配置负载均衡策略
*/
@Bean
publicIRuleribbonRule ()
{
/**
*配置为线性轮询策略
*/
returnnewRoundRobinRule ();
}
...
}
在以上配置代码的 apply () 方法中，为 Ribbon 的 RPC 请求模板 template 增加了一个叫作 token 的请
求头，用于在 RPC 调用时进行用户令牌的传递；另外，在以上代码的 ribbonRule () 方法中，通过程序
的方式配置 Ribbon 的负载均衡策略为线性轮询。
如何使以上自定义的配置程序生效呢？如果需要对所有的 Feign 客户端生效，就可以在启动类
上进行配置，将自定义的 Ribbon 配置类赋值给@EnableFeignClients 注解的 defaultConfiguration 属性即
可，示例如下：
...
@EnableFeignClients (
basePackages="com. crazymaker. SpringCloud. user. info. remote. client",
defaultConfiguration=FeignConfiguration. class)
publicclassDemoCloudApplication
{
publicstaticvoidmain (String[]args)
{
SpringApplication.run (DemoCloudApplication. class, args);
...
}
}


如果需要对某个特定的（部分的）Feign 客户端生效，则可以在特定 Feign 客户端接口上进行配
置，将自定义的 Ribbon 配置类赋值给@FeignClient 注解的 configuration 属性即可，示例如下：
packagecom. crazymaker. SpringCloud. user. info. remote. client;
...
/**
*@description：用户信息远程调用接口
*createby 尼恩@疯狂创客圈
*/
@FeignClient (value="uaa-provider",
configuration=FeignConfiguration. class,
fallback=UserClientFailBack. class,
path="/uaa-provider/api/user")
publicinterfaceUserClient
{
...
}

### 3. 5 Feign+Hystrix 实现 RPC 调用保护

在 SpringCloud 微服务架构下，RPC 保护可以通过 Hystrix 开源组件实现，并且 SpringCloud 对
Hystrix 组件进行了集成，使用起来非常方便。
Hystrix 翻译过来是豪猪，豪猪身上长满了刺，能保护自己不受天敌的伤害，代表了一种防御机
制。Hystrix 开源框架是 Netflix 开源的一个延迟和容错的组件，主要用于在远程 Provider 服务异常时，
对消费端的 RPC 进行保护。有关 Hystrix 的详细资料，请参考其官网，本书只对它的基本原理和使用
进行介绍。
使用 Hystrix 之前，需要在 Maven 的 pom 文件中增加以下 SpringCloudHystrix 集成模块的依赖：
<!--引入SpringCloudHystrix依赖-->
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-netflix-hystrix</artifactId>
</dependency>
在 SpringCloud 架构中，Hystrix 是和 Feign 组合起来使用的，所以还需要在应用的属性配置文件
中开启 Feign 对 Hystrix 的支持：
feign:
hystrix:
enabled: true #开启Hystrix对Feign的支持
...
在启动类上添加@EnableHystrix 或者@EnableCircuitBreaker。注意，@EnableHystrix 中包含了
@EnableCircuitBreaker。作为示例，下面是 Demo-provider 启动类的部分代码：
packagecom. crazymaker. SpringCloud. demo. start;
...
/**
*在启动类上启用 Hystrix
*/
@EnableHystrix
publicclassDemoCloudApplication


{
publicstaticvoidmain (String[]args)
{
SpringApplication.run (DemoCloudApplication. class, args);
...
}
}
SpringCloudHystrix 的 RPC 保护功能包括失败回退、熔断、重试、舱壁隔离等。接下来介绍一
下 Hystrix 的失败回退、熔断两大功能。

#### 3. 5. 1 SpringCloudHystrix 失败回退

什么是失败回退呢？当目标 Provider 实例发生故障时，RPC 的失败回退会产生作用，返回一个
后备的结果。一个失败回退的演示如图 3 - 16 所示：有 A、B、C、D 四个 Provider 实例，A-Provider 和
B-Provider 对 D-Provider 发起 RPC 远程调用，但是 D-Provider 发生了故障，在 A、B 受到失败回退保护
的情况下，最终会拿到失败回退提供的后备结果（或者 Fallback 回退结果）。

图 3 - 16 RPC 远程调用失败回退示意图
如何设置 RPC 调用的回退逻辑呢？有两种方式：
1 ）定义和使用一个 Fallback 回退处理类。
2 ）定义和使用一个 FallbackFactory 回退处理工厂类。
1 .定义和使用一个 Fallback 回退处理类
定义和使用一个 Fallback 回退处理类具体的实现可以分为两步：第一步是为需要拥有回退机制
的 Feign 客户端远程调用接口编写一个 Fallback 回退处理类，并将 RPC 失败后的回退逻辑编写在回退
处理类中对应的实现方法中；第二步在 Feign 客户端接口的关键性注解@FeignClient 上配置失败处理
类，将该注解的 Fallback 属性的值配置为上一步定义的 Fallback 回退处理类。
下面介绍一个具体实例，演示如何定义和使用一个 Fallback 回退处理类。在 crazy-SpringCloud
脚手架的 uua-client 模块中，有一个用于对 uaa-provider 进行 RPC 的 Feign 客户端远程调用接口叫作
UserClient，其目的是获取用户信息。第一步为 UserClient 接口定义一个简单的 Fallback 回退处理类，
代码如下：


packagecom. crazymaker. SpringCloud. user. info. remote. fallback;
//省略 import
/**
* Feign 客户端接口的 Fallback 回退处理类
*/
@Component
publicclassUserClientFallbackimplementsUserClient
{
/**
* 获取用户信息 RPC 失败后的回退方法
*/
@Override
publicRestOut<UserDTO>detail (Longid)
{
returnRestOut.error ("failBack：userdetailrest 服务调用失败");
}
}
第二步在 UserClient 客户端接口的@FeignClient 注解中将 fallback 属性的值配置为上一步定义的
Fallback 回退处理类 UserClientFallback 类，代码如下：
packagecom. crazymaker. SpringCloud. user. info. remote. client;
//省略 import
/**
*Feign 客户端接口
*@description：获取用户信息的 RPC 接口类
*/
@FeignClient (value="uaa-provider",
configuration=FeignConfiguration. class,
fallback=UserClientFallback. class, #配置回退处理类
path="/uaa-provider/api/user")
publicinterfaceUserClient
{
@RequestMapping (value="/detail/v 1 ", method=RequestMethod. GET)
RestOut<UserDTO>detail (@RequestParam (value="userId") LonguserId);
}
回退处理类的实现已经完成，如何进行验证呢？仍然使用前面定义的 demo-provider 的 REST 接
口/api/call/uaa/user/detail/v 2 ，该接口通过 UserClient 对 uaa-provider 进行了远程调用。具体的演示方式
为：停掉所有 uaa-provider 服务，然后在 demo-provider 的 swagger-ui 界面访问其 REST 接口/api/call/uaa/
user/detail/v 2 ，该接口的内部代码会通过 UserClient 远程调用 Feign 接口对目标 uaa-provider 的 REST 接
口/api/user/detail/v 1 发起 FeignRPC 远程调用，而 uaa-provider 全部服务处于宕机状态，因此 Feign 将会
触发 Hystrix 回退，执行 Fallback 回退处理类 UserClientFallback 的回退实现方法，返回 Fallback 回退处
理的内容，输出的内容具体如图 3 - 17 所示。

2 .定义和使用一个 Fallback 回退处理工厂类
定义和使用一个 Fallback 回退处理工厂类具体的实现也可以分为两步：第一步创建一个 Fallback
回退处理工厂类，该工厂类需要实现 Hystrix 的 FallbackFactory 回退工厂接口，实现其抽象的 create 方
法，在该方法的实现代码中返回一个 Feign 客户端接口的实现类，方法中的具体实现即为回退处理
实例。可以通过匿名类的方式创建一个新的回退处理类，并在该匿名类的每个方法的实现代码中编


写好 RPC 回退逻辑。第二步在 Feign 客户端接口的关键性注解@FeignClient 上配置失败处理工厂类，
将 fallbackFactory 属性的值配置为上一步定义的 FallbackFactory 回退处理工厂类。

图 3 - 17 UserClientFallback 回退处理类生效后的示意图
下面介绍一个具体实例，演示如何定义和使用一个 FallbackFactory 回退处理工厂类。这里仍然
以 uua-client 模块中的 RPC 调用接口 UserClient 为例来演示。第一步为 UserClient 接口定义一个简单的
FallbackFactory 回退处理工厂类，代码如下：
packagecom. crazymaker. SpringCloud. user. info. remote. fallback;
//省略 import
/**
* Feign 客户端接口的回退处理工厂类
*/
@Slf 4 j
@Component
publicclassUserClientFallbackFactoryimplementsFallbackFactory<UserClient>
{
/**
*创建 UserClient 客户端的回退处理实例
*/
@Override
publicUserClientcreate (finalThrowablecause){log.error ("RPC 异常了，回退!", cause);
/**
*创建一个 UserClient 客户端接口的匿名回退实例
*/
returnnewUserClient (){
/**
*方法: 获取用户信息 RPC 失败后的回退方法
*/
@Override
publicRestOut<UserDTO>detail (LonguserId)
{
returnRestOut.error ("FallbackFactoryfallback：userdetailrest 服务调用
失败");
}
};
}
}


第二步，在 Feign 客户端接口 UserClient 的@FeignClient 注解上，将 fallbackFactory 属性的值配置
为上一步定义的 UserClientFallbackFactory 回退处理工厂类，代码如下：
packagecom. crazymaker. SpringCloud. user. info. remote. client;
//省略 import
/**
*Feign 客户端接口
*@description：获取用户信息的 RPC 接口类
*/
@FeignClient (value="uaa-provider",
configuration=FeignConfiguration. class,
fallbackFactory=UserClientFallbackFactory. class, #配置回退处理工厂类
path="/uaa-provider/api/user")
publicinterfaceUserClient
{
@RequestMapping (value="/detail/v 1 ", method=RequestMethod. GET)
RestOut<UserDTO>detail (@RequestParam (value="userId") LonguserId);
}
第二种方式回退工厂类的具体验证过程与第一种方式回退类的验证相同：停掉所有的
uaa-provider 服务，然后在 demo-provider 的 swagger-ui 界面访问其 REST 接口/api/call/uaa/user/detail/v 2 ，
此 REST 接口的内部代码会通过 UserClient 远程调用 Feign 接口对目标 uaa-provider 的 REST 接口
/api/user/detail/v 1 发起 FeignRPC 远程调用，而 uaa-provider 全部服务是宕机的，Feign 将会触发 Hystrix
回退，执行 Fallback 回退处理工厂类 UserClientFallbackFactory 的 create 方法创建一个回退处理类实例，
并执行回退处理类实例中回退处理逻辑，返回回退处理的结果。
在进行失败回退时，使用第一种方式的回退类和使用第二种方式的回退工厂类有什么区别呢？
答案是在使用第一种方式的回退类时，远程调用 RPC 过程中所引发的异常已经被回退逻辑彻底
屏蔽掉了，应用程序不太方便干预，也看不到 RPC 过程中的具体异常，尽管这些异常对于问题的排
斥非常有帮助；在使用第二种方式的回退工厂类时，应用程序可以通过 Java 代码对 RPC 异常进行拦
截和处理，包括进行日志输出。

#### 3. 5. 2 分布式系统面临的雪崩难题

###### 在分布式系统中，一个服务可能会依赖很多其他的服务，并且这些服务都不可避免地有失效的

可能。假如一个应用运行 30 个 Provider 实例，每个实例 99. 99 %的时间处于正常服务状态，即使只有
0. 01 %的失败率，每个月仍然有几个小时不可用。另外，还有一个大的问题是：流量洪峰过来时，
服务有可能被其他服务依赖，如果这个 Provider 实例出现延迟响应，就会导致其他 Provider 发生更多
的级联故障，从而导致这个分布式系统都不可用。
举个简单的例子，在一个秒杀系统中，商品（good-provider）、订单（order-provider）、秒杀
（seckill-provider） 3 个 Provider 都会通过 RPC 远程调用到用户账号与认证（uaa-provider）的相关接
口，查询用户的相关信息，如图 3 - 18 所示。
如果在流量洪峰过来之时，假设 uaa-provider 出现响应迟钝（甚至宕机），则商品、订单、秒
杀 3 个 Provider 都会出现等待超时而导致响应缓慢，由于排队的请求越来越多、单个请求时间都变得
很长（因为内部都有超时等待），因此各服务节点的系统资源（CPU、内存等）很快会耗尽，最后
进入系统性雪崩状态，具体如图 3 - 19 所示。


```
图 3 - 18 秒杀系统中商品、订单、秒杀、用户 4 个 Provider 之间的依赖示意图
```
图^3 -^19 流量洪峰过来时因 uaa-provider 响应缓慢导致整体雪崩
总体来说，在微服务架构中根据业务拆分成的一个个的 Provider 微服务，由于网络原因或者自
身的原因，服务并不能保证 100 %可用，为了保证微服务提供者高可用，单个 Provider 服务通常会多
体部署。由于 Provider 与 Provider 之间的依赖性，故障或者不可用会沿请求调用链向上传递，会对整
个系统造成瘫痪，这就是故障的“雪崩”效应。


###### 引发雪崩效应的原因比较多，下面是常见的几种：

###### 1 ）硬件故障：如服务器宕机、机房断电、光纤被挖断等。

###### 2 ）流量激增：如异常流量、巨量请求瞬时涌入（如秒杀）等。

###### 3 ）缓存穿透：一般发生在系统重启所有缓存失效时，或者发生在短时间内大量缓存失效时，

###### 前端过来的大量请求没有命中缓存，直击后端服务和数据库，造成微服务提供者和数据库超负荷运

###### 行，引起整体瘫痪。

###### 4 ）程序 BUG：如程序逻辑 BUG 导致内存泄漏等原因引发的整体瘫痪。

5 ）JVM 卡顿：JVM 的 FullGC 时间较长，极端的情况长达数十秒，这段时间内 JVM 不能提供任
何服务。

为了解决雪崩效应，业界提出了熔断器模型。通过熔断器，当一些非核心服务出现响应迟缓或
者宕机等异常时，对服务进行降级并提供有损服务，以保证服务的柔性可用，避免引起雪崩效应。

#### 3. 5. 3 SpringCloudHystrix 熔断器

###### 在物理学上，熔断器本身是一个开关装置，用在电路上保护线路过载，当线路中有电器发生短

###### 路时，熔断器能够及时切断故障，防止发生过载、发热甚至起火等严重后果。分布式架构中熔断器

###### 主要用于 RPC 接口上，为接口安装上“保险丝”，以防止 RPC 接口出现拥塞导致系统压力过大而引

起系统瘫痪，当 RPC 接口流量过大或者目标 Provider 出现异常时，熔断器及时切断故障可以起到自
我保护的作用。
为什么说熔断器非常重要呢？如果没有过载保护，在分布式系统中，当被调用的远程服务无法
使用时，就会导致请求的资源阻塞在远程服务器上耗尽资源。很多时候刚开始可能只是出现了局部
小规模的故障，然而由于种种原因，故障影响范围越来越大，最终导致全局性的后果。
熔断器通常也叫作断路器，其具体的工作机制：统计最近 RPC 调用发生错误的次数，然后根据
统计值中的失败比例等信息决定是否允许后面的 RPC 调用继续或者快速地失败回退。
熔断器的 3 种状态如下：
1 ）关闭（closed）：熔断器关闭状态，这也是熔断器的初始状态，此状态下 RPC 调用正常放行。
2 ）开启（open）：失败比例到一定的阈值之后，熔断器进入开启状态。此状态下 RPC 将会快
速失败，执行失败回退逻辑。
3 ）半开启（half-open）：在打开一定时间之后（睡眠窗口结束），熔断器进入半开启状态，
小流量尝试进行 RPC 调用放行。如果尝试成功则熔断器变为关闭状态，RPC 调用正常；如果尝试失
败则熔断器变为开启状态，RPC 调用快速失败。

熔断器状态之间相互转换的逻辑关系如图 3 - 20 所示。
下面重点介绍熔断器的半开启状态。在半开启状态下，允许进行一次 RPC 调用的尝试，如果实
际调用成功，熔断器将复位到关闭状态，回归正常的模式；但是如果这次 RPC 调用的尝试失败，熔
断器就会返回到开启状态，一直需要等待到下次半开启状态。


图 3 - 20 熔断器状态之间的相互转换关系
SpringCloudHystrix 中的熔断器默认是开启的，但是可以通过配置熔断器的参数进行定制。下
面是 demo-provider 微服务中熔断器示例的相关配置：
hystrix:
...
command:
default:
...
circuitBreaker: #熔断器相关配置
enabled: true #是否使用熔断器 ，默认为 true
requestVolumeThreshold: 20 #窗口时间内的最小请求数
sleepWindowInMilliseconds: 5000 #打开后允许一次尝试的睡眠时间 ，默认配置为 5 秒
errorThresholdPercentage: 50 #窗口时间内熔断器开启的错误比例 , 默认配置为 50
metrics:
rollingStats:
timeInMilliseconds: 10000 #滑动窗口时间
numBuckets: 10 #滑动窗口的时间桶数
以上用到的 Hystrix 熔断器相关参数分为两类：熔断器的相关参数和滑动窗口的相关参数。对示
例中使用到的熔断器的相关参数大致介绍如下：

（ 1 ）hystrix. command. default. circuitBreaker. enabled
该配置用来确定熔断器是否用于跟踪 RPC 请求的运行状态，或者说用于配置是否启用熔断器，
默认值为 true。

（ 2 ）hystrix. command. default. circuitBreaker. requestVolumeThreshold
该配置用于设置熔断器触发熔断的最少请求次数。如果设置为 20 ，那么当一个滑动窗口时间内
（比如 10 秒）收到 19 个请求，即使 19 个请求都失败，熔断器也不会打开变成 open 状态。默认值为 20 。

（ 3 ）hystrix. command. default. circuitBreaker. errorThresholdPercentage
该配置用于设置错误率阈值，滑动窗口时间内当错误率超过此值时，熔断器进入 open 状态，所
有请求都会触发失败回退，错误率阈值百分比的默认值为 50 。


（ 4 ）hystrix. command. default. circuitBreaker. sleepWindowInMilliseconds
该配置用于设置熔断器的睡眠窗口，具体指的是熔断器打开之后过多长时间才允许一次请求尝
试执行，默认值为 5000 毫秒，表示当熔断器打开后， 5000 毫秒内会拒绝所有请求， 5000 毫秒后熔断
器才会进行入 half-open 状态。

（ 5 ）hystrix. command. default. circuitBreaker. forceOpen
如果配置为 true，熔断器将被强制打开，所有请求将触发失败回退。此配置的默认值为 false。
熔断器的状态转换与 Hystrix 的滑动窗口的健康统计值（比如失败比例）相关，对示例中使用到
的 Hystrix 的滑动窗口的健康统计相关配置大致介绍如下：

（ 1 ）hystrix. command. default. metrics. rollingStats. timeInMilliseconds
设置统计滑动窗口的持续时间（以毫秒为单位），默认值为 10000 毫秒。熔断器的打开会根据
一个滑动窗口的统计值来计算，若滑动窗口时间内的错误率超过阈值，则熔断器进入开启状态。滑
动窗口将被进一步细分为时间桶（bucket），滑动窗口的统计值等于窗口内所有时间桶的统计信息
的累加，每个时间桶的统计信息包含请求的成功（success）、失败（failure）、超时（timeout）、
被拒（rejection）的次数。

（ 2 ）hystrix. command. default. metrics. rollingStats. numBuckets
设置一个滑动窗口被划分的时间桶数量，默认值为 10 。若滑动窗口的持续时间为 10000 毫秒，
并且一个滑动窗口被划为 10 个时间桶，那么一个时间桶的时间即 1 秒。所设置的 numBuckets（时间
桶数量）值和 timeInMilliseconds（滑动窗口时长）值有一定关系，必须符合 timeInMilliseconds%
numberBuckets== 0 的规则，否则会抛出异常，例如 70000 （ 70 秒）% 700 （桶数）== 0 是可以的，
但是 70000 （滑动窗口 70 秒）% 600 （桶数）== 400 将抛出异常。

以上有关 Hystrix 熔断器的配置选项使用的是 hystrix. command. default 前缀，这些默认配置项将对
项目中所有 FeignRPC 接口生效，除非某个 FeignRPC 接口进行单独配置。如果需要对某个 FeignRPC
调用做特殊的配置，配置项前缀的格式如下：

hystrix. command. 类名 #方法名 （参数类型列表）
来看一个对单个接口做特殊配置的例子。以对 UserClient 类中 FeignRPC 接口/detail/v 1 做特殊配
置为例，该接口的功能是从 user-provider 服务获取用户信息。在配置之前，先看一下 UserClient 接口
的代码，具体如下：
packagecom. crazymaker. SpringCloud. user. info. remote. client;
...
@FeignClient (value="uaa-provider",
configuration=FeignConfiguration. class,
fallback=UserClientFallback. class,
path="/uaa-provider/api/user")
publicinterfaceUserClient
{
/**
*远程调用 RPC 方法：获取用户详细信息
*@paramuserId 用户 id
*@return 用户详细信息
*/
@RequestMapping (value="/detail/v 1 ", method=RequestMethod. GET)


RestOut<UserDTO>detail (@RequestParam (value="userId") LonguserId);
}
在 demo-provider 中，如果要对 UserClient. detail 接口的 RPC 调用的熔断器参数做特殊的配置，则
不使用 hystrix. command. default 默认前缀，而是使用 hystrix. command. FeignClient #Method格式的前缀 ，
具体的配置项为：
hystrix:
...
command:
UserClient #detail (Long): #格式为 ：类名 #方法名 （参数类型列表）
...
circuitBreaker: #熔断器相关配置
enabled: true #是否使用熔断器 ，默认为 true
requestVolumeThreshold: 20 #至少有 20 个请求，熔断器才达到熔断触发的次数阈值
sleepWindowInMilliseconds: 5000 #打开后允许一次尝试的睡眠时间 ，默认配置为 5 秒
errorThresholdPercentage: 50 #窗口时间内熔断器开启的错误比例 ，默认配置为 50
metrics:
rollingPercentile:
timeInMilliseconds: 60000 #滑动窗口时间
numBuckets: 600 #滑动窗口的时间桶数
bucketSize: 200 #时间桶内的统计次数
除了熔断器 circuitBreaker 相关参数和 metrics 滑动窗口相关参数之外，其他的很多 Hystrix
command 参数也可以对特定的 FeignRPC 接口做特殊配置，配置时仍然使用“类名 #方法名 （形参类
型列表）”格式。
对于初学者来说，有关滑动窗口的概念和配置理解起来还是比较费劲的。对于 Hystrix 的基础原
理（包含滑动窗口），本书将在第 6 章进行详细介绍。


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
Springcloud 配置| 史上最全, 一文全懂
https://www.cnblogs.com/crazymakercircle/p/ 11674597 .html
SpringClouddemo（极速-入门）
https://www.cnblogs.com/crazymakercircle/p/ 14017169 .html
springcloudConfig 入门，带视频
https://www.cnblogs.com/crazymakercircle/p/ 12043604 .html

##### 3. 相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题 ：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 4 章 Spring Cloud RPC 远程调用核心

# 原理

如果不了解 SpringCloud 中的 Feign 核心原理，就不会真正地了解 SpringCloud 的性能优化和配
置优化，也就不可能做到真正掌握 SpringCloud。
本章从 Feign 远程调用的重要组件开始，图文并茂地介绍 Feign 本地 JDKProxy 实例的创建流程
以及 Feign 远程调用的执行流程，彻底地为大家解读 SpringCloud 的核心知识，使得广大的工程师不
光做到知其然，更能知其所以然。

4. (^1) 代理模式与 RPC 客户端实现类

###### 本节首先介绍一下客户端 RPC 远程调用实现类的职责，然后从基础原理讲起，依次介绍代理

###### 模式的原理、使用静态代理模式实现 RPC 客户端类、使用动态代理模式实现 RPC 客户端类，一步一

步地不断接近 FeignRPC 的核心原理知识。

#### 4. 1. 1 客户端 RPC 远程调用实现类的职责

客户端 RPC 实现类位于远程调用 Java 接口和微服务提供者 Provider 之间，承担了以下职责：
1 ）拼装 REST 请求：根据 Java 接口的参数拼装目标 REST 接口的 URL。
2 ）发送请求和获取结果：通过 JavaHTTP 组件（如 HttpClient）调用微服务提供者 Provider 的
REST 接口，并且获取 REST 响应。
3 ）结果解码：解析 REST 接口的响应结果，封装成目标 POJO 对象（Java 接口的返回类型），
并且返回。

```
RPC 远程调用客户端实现类的职责，具体如图 4 - 1 所示。
```

图 4 - 1 RPC 远程调用客户端实现类的职责
使用 Feign 进行 RPC 远程调用时，对每一个 Java 远程调用接口，Feign 都会生成了一个 RPC 远程
调用客户端实现类，只是对于开发者来说该实现类是透明的，开发者感觉不到这个类的存在。

```
Feign 为 DemoClient 接口生成的 RPC 客户端实现类，大致如图 4 - 2 所示。
```
```
图 4 - 2 Feign 为 DemoClient 接口生成的 RPC 客户端实现类参考图
由于看不到 Feign 的 RPC 客户端实现类的任何源码，初学者会感觉到很神奇，感觉这就是一个
```

黑盒子。这里，从最原始的、最简单的 RPC 远程调用客户端实现类开始，为大家逐步地揭开 Feign
的 RPC 客户端实现类的神秘面纱。
在一点点揭开 RPC 远程调用客户端实现类的面纱之前，先模拟一个 Feign 远程调用 Java 接口，
对应于 demo-provider 服务的两个 REST 接口。
模拟的远程调用 Java 接口名字叫作 MockDemoClient，其代码如下：
packagecom. crazymaker. demo. proxy. FeignMock;
...
@RestController (value=TestConstants. DEMO_CLIENT_PATH)
publicinterfaceMockDemoClient
{
/**
*远程调用接口的方法，完成 REST 接口 api/demo/hello/v 1 的远程调用
*REST 接口功能：返回 helloworld
*@returnJSON 响应实例
*/
@GetMapping (name="api/demo/hello/v 1 ")
RestOut<JSONObject>hello ();

/**
*远程调用接口的方法，完成 REST 接口 api/demo/echo/{ 0 }/v 1 的远程调用
*REST 接口功能：回显输入的信息
*@returnecho 回显消息 JSON 响应实例
*/
@GetMapping (name="api/demo/echo/{ 0 }/v 1 ")
RestOut<JSONObject>echo (Stringword);
}
本书层层递减，为大家演示以下三种 RPC 远程调用客户端：
1 ）简单的 RPC 客户端实现类。
2 ）静态代理模式的 RPC 客户端实现类。
3 ）动态代理模式的 RPC 客户端实现类。
最后的动态代理模式的 RPC 客户端实现类在实现原理上已经非常接近于 Feign 的 RPC 客户端实
现类了。

#### 4. 1. 2 简单的 RPC 客户端实现类

###### 最简单的 RPC 客户端实现类的主要工作如下：

###### 1 ）组装 REST 接口 URL。

```
2 ）通过 HttpClient 组件调用 REST 接口，并获得响应结果。
3 ）解析 REST 接口的响应结果，封装成 JSON 对象，并且返回给调用者。
最简单的 RPC 客户端实现类的参考代码大致如下：
packagecom. crazymaker. demo. proxy. basic;
//省略 import
@AllArgsConstructor
@Slf 4 j
classRealRpcDemoClientImplimplementsMockDemoClient
```

{
finalStringcontextPath=TestConstants. DEMO_CLIENT_PATH;
//完成对 REST 接口 api/demo/hello/v 1 的调用
publicRestOut<JSONObject>hello ()
{
/**
*远程调用接口的方法，完成 demo-provider 的 RESTAPI 远程调用
*RESTAPI 功能：返回 helloworld
*/
Stringuri="api/demo/hello/v 1 ";
/**
*组装 REST 接口 URL
*/
StringrestUrl=contextPath+uri;
log.info ("restUrl={}", restUrl);
/**
*通过 HttpClient 组件调用 REST 接口
*/
StringresponseData=null;
try
{
responseData=HttpRequestUtil.simpleGet (restUrl);
}catch (IOExceptione)
{
e.printStackTrace ();
}
/**
*解析 REST 接口的响应结果，解析成 JSON 对象，并且返回
*/
RestOut<JSONObject>result=JsonUtil.jsonToPojo (responseData,
new TypeReference<RestOut<JSONObject>>(){});
returnresult;
}
//完成对 REST 接口 api/demo/echo/{ 0 }/v 1 的调用
publicRestOut<JSONObject>echo (Stringword)
{
/**
*远程调用接口的方法，完成 demo-provider 的 RESTAPI 远程调用
*RESTAPI 功能：回显输入的信息
*/
Stringuri="api/demo/echo/{ 0 }/v 1 ";
/**
*组装 REST 接口 URL
*/
StringrestUrl=contextPath+MessageFormat.format (uri, word);
log.info ("restUrl={}", restUrl);
/**
*通过 HttpClient 组件调用 REST 接口
*/
StringresponseData=null;
try
{
responseData=HttpRequestUtil.simpleGet (restUrl);
}catch (IOExceptione)
{


```
e.printStackTrace ();
}
/**
*解析 REST 接口的响应结果，解析成 JSON 对象，并且返回给调用者
*/
RestOut<JSONObject>result=JsonUtil.jsonToPojo (responseData,
newTypeReference<RestOut<JSONObject>>() {});
returnresult;
}
}
以上简单 RPC 实现类 RealRpcDemoClientImpl 的测试用例大致如下：
packagecom. crazymaker. demo. proxy. basic;
...
/**
*测试用例
*/
@Slf 4 j
publicclassProxyTester
{
/**
*不用代理，进行简单的远程调用
*/
@Test
publicvoidsimpleRPCTest ()
{
/**
*简单的 RPC 调用类
*/
MockDemoClientrealObject=newRealRpcDemoClientImpl ();
/**
*调用 demo-provider 的 REST 接口 api/demo/hello/v 1
*/
RestOut<JSONObject>result 1 =realObject.hello ();
log.info ("result 1 ={}", result 1 .toString ());
```
/**
*调用 demo-provider 的 REST 接口 api/demo/echo/{ 0 }/v 1
*/
RestOut<JSONObject>result 2 =realObject.echo ("回显内容");
log.info ("result 2 ={}", result 2 .toString ());
}
}
运行测试用例前，需要提前启动 demo-provider 微服务实例，然后将主机名称 crazydemo. com 通
过 hosts 文件绑定到 demo-provider 实例所在机器的 IP（这里为 127. 0. 0. 1 ），并且需要确保两个 REST
接口/api/demo/hello/v 1 和/api/demo/echo/{word}/v 1 可以正常访问。
运行测试用例，部分输出结果如下：
[main]INFO c.c.d.p.b.RealRpcDemoClientImpl-
restUrl=http://crazydemo.com: 7700 /demo-provider/api/demo/hello/v 1
[main]INFO c.c.d.proxy. basic. ProxyTester-result 1 =RestOut{datas={"hello": "world"},
respCode= 0 ,respMsg='操作成功}


[main]INFO c.c.d.p.b.RealRpcDemoClientImpl-
restUrl=http://crazydemo.com: 7700 /demo-provider/api/demo/echo/回显内容/v 1
[main]INFO c.c.d.proxy. basic. ProxyTester-result 2 =RestOut{datas={"echo": "回显内容"},
respCode= 0 ,respMsg='操作成功}
以上的 RPC 客户端实现类很简单，但是实际开发中不可能这样为每一个远程调用 Java 接口都编
写一个 RPC 客户端实现类。如何自动生成 RPC 客户端实现类呢？这就需要用到代理模式，接下来首
先为大家介绍简单一点的代理模式实现类——静态代理模式的 RPC 客户端实现类。

#### 4. 1. 3 从基础原理讲起：代理模式与 RPC 客户端实现类

###### 首先来看一下代理模式的基本概念。代理模式的定义：为委托对象提供一种代理，以控制对

###### 委托对象的访问。在某些情况下，一个对象不适合或者不能直接引用另一个目标对象，而代理对象

###### 可以作为目标对象的委托，在客户端和目标对象之间起到中介的作用。

###### 代理模式包含了三个角色：抽象角色、委托角色、代理角色，如图 4 - 3 所示。

图 4 - 3 代理模式角色之间的关系图
1 ）抽象角色：通过接口或抽象类的方式，声明委托角色所提供的业务方法。
2 ）代理角色：实现抽象角色的接口，通过调用委托角色的业务逻辑方法来实现抽象方法，并
可以附加自己的操作。
3 ）委托角色：实现抽象角色，定义真实角色所要实现的业务逻辑，供代理角色调用。
代理模式分为静态代理、动态代理。
1 ）静态代理是在代码编写阶段由工程师提供代理类的源码，再编译成代理类。所谓静态也就
是在程序运行前就已经存在代理类的字节码文件，代理类和委托类的关系在运行前就确定了。
2 ）动态代理是在代码编写阶段不用关心具体的代理实现类，而是在运行阶段直接获取具体的
代理对象，代理实现类由 JDK 负责生成。

```
静态代理模式的实现，主要涉及 3 个组件：
```

（ 1 ）抽象接口类（abstractsubject）
该类的主要职责是声明目标类与代理类的共同接口方法，该类既可以是一个抽象类也可以是
一个接口。

（ 2 ）真实目标类（realsubject）
该类也称为被委托类或被代理类，该类定义了代理所表示的真实对象，执行具体业务逻辑方
法，而客户端通过代理类间接地调用真实目标类中定义的方法。

（ 3 ）代理类（proxysubject）
该类也称为委托类或代理类，该类持有一个对真实目标类的引用，在其抽象接口方法的实现
中，需要调用真实目标类中相应的接口实现方法，以此起到代理的作用。

使用静态代理模式实现 RPC 远程接口调用，大致涉及以下 3 个类：
1 ）一个远程接口，比如前面介绍的模拟的远程调用 Java 接口 MockDemoClient。
2 ）一个真实委托类，比如前面介绍的 RealRpcDemoClientImpl，负责完成真正的 RPC 调用。
3 ）一个代理类，比如本节的 DemoClientStaticProxy，通过调用真实目标类（委托类）负责完
成 RPC 调用。

```
通过静态代理模式，实现 MockDemoClient 接口的 RPC 调用实现类，类之间的关系如图 4 - 4 所示。
```
```
图 4 - 4 静态代理模式的 RPC 调用 UML 类图
静态代理模式的 RPC 实现类 DemoClientStaticProxy 的代码如下：
packagecom. crazymaker. demo. proxy. basic;
//省略 import
@AllArgsConstructor
@Slf 4 j
classDemoClientStaticProxyimplementsDemoClient
{
/**
*被代理的真正实例
*/
```

privateMockDemoClientrealClient;
@Override
publicRestOut<JSONObject>hello ()
{
log.info ("hello 方法被调用");
returnrealClient.hello ();
}
@Override
publicRestOut<JSONObject>echo (Stringword)
{
log.info ("echo 方法被调用");
returnrealClient.echo (word);
}
}
在静态代理类 DemoClientStaticProxy 的 hello () 和 echo () 两个方法中，调用真实委托类实例
realClient 的两个对应的委托方法，完成对远程 REST 接口的请求。
以上静态代理类 DemoClientStaticProxy 的使用代码（测试用例）大致如下：
packagecom. crazymaker. demo. proxy. basic;
//省略 import
/**
*静态代理和动态代理，测试用例
*/
@Slf 4 j
publicclassProxyTester
{
/**
*静态代理测试
*/
@Test
publicvoidstaticProxyTest ()
{
/**
*被代理的真实 RPC 调用类
*/
MockDemoClientrealObject=newRealRpcDemoClientImpl ();
/**
* 静态的代理类
*/
DemoClientproxy=newDemoClientStaticProxy (realObject);
RestOut<JSONObject>result 1 =proxy.hello ();
log.info ("result 1 ={}", result 1 .toString ());
RestOut<JSONObject>result 2 =proxy.echo ("回显内容");
log.info ("result 2 ={}", result 2 .toString ());
}
}
在运行测试用例前，需要提前启动 demo-provider 微服务实例，并且需要将主机名称
crazydemo. com 通过 hosts 文件绑定到 demo-provider 实例所在机器的 IP（这里为 127. 0. 0. 1 ），并且需要
确保两个 REST 接口/api/demo/hello/v 1 、/api/demo/echo/{word}/v 1 可以正常访问。
一切准备妥当，运行测试用例，大致的结果输出如下：
[main]INFO c.c.d.p.b.DemoClientStaticProxy-hello 方法被调用


[main]INFO c.c.d.p.b.RealRpcDemoClientImpl-restUrl=
[http://crazydemo.com:](http://crazydemo.com:) 7700 /demo-provider/api/demo/hello/v 1
[main]INFO c.c.d.proxy. basic. ProxyTester-result 1 =RestOut{datas={"hello": "world"},
respCode= 0 ,respMsg='操作成功}
[main]INFO c.c.d.p.b.DemoClientStaticProxy-echo 方法被调用
[main]INFO c.c.d.p.b.RealRpcDemoClientImpl-
restUrl=http://crazydemo.com: 7700 /demo-provider/api/demo/echo/回显内容/v 1
[main]INFO c.c.d.proxy. basic. ProxyTester-result 2 =RestOut{datas={"echo": "回显内容"},
respCode= 0 ,respMsg='操作成功}
静态代理的 RPC 实现类，看上去是一堆冗余代码，发挥不了什么价值。那么，为什么在这里
一定要先介绍静态代理模式的 RPC 实现类呢？原因有以下两点：

1 ）上面的 RPC 实现类出于演示目的做了简化，对委托类并没有做任何的扩展。而实际的远程
调用代理类会对委托类进行很多扩展，比如远程调用时的负载均衡、熔断、重试等。
2 ）上面的 RPC 实现类是动态代理实现类的学习铺垫，因为 Feign 的 RPC 客户端实现类是一个
JDK 动态代理类，是在运行过程中动态生成的。动态代理的知识对于很多的读者来说不是太好理解，
所以先介绍一下代理模式和静态代理的基础知识，作为下一步的学习铺垫。

#### 4. 1. 4 使用动态代理模式实现 RPC 客户端类

###### 为什么需要动态代理呢？这需要从静态代理的缺陷说起。静态代理实现类在编译期就已经写

###### 好，代码清晰可读，缺点也很明显：

###### 1 ）手工编写代理实现类会占用时间，如果需要实现代理的类很多，一个一个地手工编码代理

###### 类根本写不过来。

###### 2 ）如果更改了抽象接口，还得去维护这些代理类，维护上容易出纰漏。

###### 动态代理与静态代理相反，不需要手工实现代理类，而是由 JDK 通过反射技术在执行阶段动态

###### 生成动态代理类，所以也叫动态代理。使用的时候，可以直接获取动态代理的实例。获取动态代理

###### 实例大致需要如下 3 步：

###### 1 ）需要明确代理类和委托类共同的抽象接口，由 JDK 生成的动态代理类会实现该接口。

2 ）构造一个调用处理器对象，该调用处理器要实现 InvocationHandler 接口，实现其唯一的抽
象方法 invoke（...）。而 InvocationHandler 接口由 JDK 定义，位于 java. lang. reflect 包中。
3 ）通过 java. lang. reflect. Proxy 类的 newProxyInstance (...) 方法在运行阶段获取 JDK 生成的动态代
理类的实例。注意，这一步获取的是对象而不是类。该方法需要 3 个参数，其中的第二个参数为抽
象接口的 class 对象，第三个参数为调用处理器对象。

```
举一个例子，创建抽象接口 MockDemoClient 的一个动态代理实例，大致的代码如下：
//参数 1 ：类装载器
ClassLoaderclassLoader=ProxyTester.class.getClassLoader ();
//参数 2 ：代理类和委托类共同的抽象接口
Class[]clazz=newClass[]{MockDemoClient. class};
//参数 3 ：动态代理的调用处理器
InvocationHandlerinvocationHandler=newDemoClientInocationHandler (realObject);
/**
*使用以上 3 个参数，创建 JDK 动态代理类
```

*/
MockDemoClientproxy=(MockDemoClient)
Proxy.newProxyInstance (classLoader, clazz, invocationHandler);
创建动态代理实例的核心是创建一个 JDK 调用处理器 InvocationHandler 的实现类。该实现类需
要实现其唯一的抽象方法 invoke（...），并且在该方法中调用真实委托类的方法。一般情况下，调
用处理器需要能够访问到真实委托类，一般的做法是将真实委托类实例作为其内部的成员。
例子中获取的动态代理实例所涉及的 3 个类具体如下：
1 ）一个远程接口，使用前面介绍的模拟的远程调用 Java 接口 MockDemoClient。
2 ）一个真实目标类，使用前面介绍的 RealRpcDemoClientImpl 类，该类负责完成真正的 RPC 调
用，作为动态代理的委托类。
3 ）一个 InvocationHandler 的实现类，本小节将实现 DemoClientInocationHandler 调用处理器类，
该类通过调用内部成员委托类的对应方法完成 RPC 调用。

```
模拟远程接口 MockDemoClient 的 RPC 动态代理模式实现，类之间的关系如图 4 - 5 所示。
```
图 4 - 5 动态代理模式实现 RPC 远程调用 UML 类图
通过动态代理模式实现模拟远程接口 MockDemoClient 的 RPC 调用，关键的类为调用处理器，
调用处理器 DemoClientInocationHandler 的代码如下：
packagecom. crazymaker. demo. proxy. basic;
//省略 import
/**


*动态代理的调用处理器
*/
@Slf 4 j
publicclassDemoClientInocationHandlerimplementsInvocationHandler
{
/**
*被代理的委托类实例
*/
privateMockDemoClientrealClient;
publicDemoClientInocationHandler (MockDemoClientrealClient)
{
this. realClient=realClient;
}
publicObjectinvoke (Objectproxy, Methodmethod, Object[]args) throwsThrowable
{
Stringname=method.getName ();
log.info ("{}方法被调用",method.getName ());
/**
*直接调用委托类的方法：调用其 hello 方法
*/
if (name.equals ("hello"))
{
returnrealClient.hello ();
}
/**
*通过 Java 反射调用委托类的方法：调用其 echo 方法
*/
if (name.equals ("echo"))
{
returnmethod.invoke (realClient, args);
}
/**
*通过 Java 反射调用委托类的方法
*/
Objectresult=method.invoke (realClient, args);
returnresult;
}
}
调用处理器 DemoClientInocationHandler 既实现了 InvocationHandler 接口，又拥有一个内部委托
类成员，负责完成实际的 RPC 请求。调用处理器有点儿像静态代理模式中的代理角色，但是在这里
却不是，它仅仅是 JDK 所生成的代理类的内部成员。
以上调用处理器 DemoClientInocationHandler 的使用代码（测试用例）大致如下：
packagecom. crazymaker. demo. proxy. basic;
//省略 import
@Slf 4 j
publicclassStaticProxyTester{
/**
*动态代理测试
*/
@Test
publicvoiddynamicProxyTest (){
DemoClientclient=newDemoClientImpl ();


//参数 1 ：类装载器
ClassLoaderclassLoader=StaticProxyTester.class.getClassLoader ();
//参数 2 ：被代理的实例类型
Class[]clazz=newClass[]{DemoClient. class};
//参数 3 ：调用处理器
InvocationHandlerinvocationHandler=newDemoClientInocationHandler (client);
//获取动态代理实例
DemoClientproxy=(DemoClient)
Proxy.newProxyInstance (classLoader, clazz, invocationHandler);
//执行 RPC 远程调用方法
Result<JSONObject>result 1 =proxy.hello ();
log.info ("result 1 ={}", result 1 .toString ());
Result<JSONObject>result 2 =proxy.echo ("回显内容");
log.info ("result 2 ={}", result 2 .toString ());
}
}
在运行测试用例前，需要提前启动 demo-provider 微服务实例，并且要确保两个 REST 接口
/api/demo/hello/v 1 和/api/demo/echo/{word}/v 1 可以正常访问。
一切准备妥当，运行测试用例，大致的结果输出如下：
18 : 36 : 32. 499 [main]INFO c.c.d.p.b.DemoClientInocationHandler-hello 方法被调用
18 : 36 : 32. 621 [main]INFO c.c.d.p.b.StaticProxyTester-
result 1 =Result{data={"hello": "world"}, status= 200 ,msg='操作成功, requesttime='null'}
18 : 36 : 32. 622 [main]INFO c.c.d.p.b.DemoClientInocationHandler-echo 方法被调用
18 : 36 : 32. 622 [main]INFO c.c.d.p.b.StaticProxyTester-result 2 =Result{data={"echo": "
回显内容"}, status= 200 ,msg='操作成功, requesttime='null'}

#### 4. 1. 5 JDK 动态代理机制的原理

动态代理实质是通过 java. lang. reflect. Proxy 的 newProxyInstance（...）方法生成一个动态代理类
的实例。该方法比较重要，这里对其做一个详细的介绍，其定义如下：
publicstaticObjectnewProxyInstance (ClassLoaderloader，//类加载器
Class<?>[]interfaces，//动态代理类需要实现的接口
InvocationHandlerh)//调用处理器
throwsIllegalArgumentException
{
...
}
此方法的三个参数介绍如下：
 第一个参数为 ClassLoader 类加载器类型，此处的类加载器和委托类的类加载器相同即可。
 第二个参数为 Class[]类型，代表动态代理类将会实现的抽象接口，此接口也是委托类所实
现的接口。
 第三个参数为 InvocationHandler 类型，其调用处理器实例将作为 JDK 生成的动态代理对象的
内部成员，在对动态代理对象进行方法调用时，该处理器的 invoke（...）方法都会被执行。
InvocationHandler 处理器的 invoke（...）方法如何实现由大家自己决定。委托类（真实目标类）
的扩展或者定制逻辑，一般都会定义在此 InvocationHandler 处理器的 invoke（...）方法中。
JVM 在调用 Proxy. newProxyInstance（...）方法时，会自动为动态代理对象生成一个内部的代
理类，那么是否能看到该动态代理类的 class 字节码呢？答案是肯定的，可以通过如下的方式获取其


###### 字节码，并且保存到文件中：

/**
*获取动态代理类的 class 字节码
*/
byte[]classFile=ProxyGenerator.generateProxyClass ("Proxy 0 ",
RealRpcDemoClientImpl.class.getInterfaces ());
/**
*在当前的工程目录下保存文件
*/
FileOutputStreamfos=newFileOutputStream (newFile ("Proxy 0 .class"));
fos.write (classFile);
fos.flush ();
fos.close ();
运行上一个小节的 dynamicProxyTest () 测试用例，在 demo-provider 模块的根路径可以发现那个
被新创建的 Proxy 0 .class 字节码文件。如果 IDE 有反编译的能力，可以在 IDE 中将该文件打开，然后
可以看到其反编译的源码，大致如下：
importcom. crazymaker. demo. proxy. MockDemoClient;
importcom. crazymaker. SpringCloud. common. result. RestOut;
importjava. lang. reflect. InvocationHandler;
importjava. lang. reflect. Method;
importjava. lang. reflect. Proxy;
importjava. lang. reflect. UndeclaredThrowableException;
publicfinalclassProxy 0 extendsProxyimplementsMockDemoClient{
privatestaticMethodm 1 ;
privatestaticMethodm 4 ;
privatestaticMethodm 3 ;
privatestaticMethodm 2 ;
privatestaticMethodm 0 ;
publicProxy 0 (InvocationHandlervar 1 ) throws {
super (var 1 );
}
...
publicfinalRestOutecho (Stringvar 1 ) throws {
try{
return (RestOut) super.h.invoke (this, m 4 ,newObject[]{var 1 });
}catch (RuntimeException|Errorvar 3 ){
throwvar 3 ;
}catch (Throwablevar 4 ){
thrownewUndeclaredThrowableException (var 4 );
}
}
publicfinalRestOuthello () throws {
try{
return (RestOut) super.h.invoke (this, m 3 ,(Object[]) null);
}catch (RuntimeException|Errorvar 2 ){
throwvar 2 ;
}catch (Throwablevar 3 ){
thrownewUndeclaredThrowableException (var 3 );
}
}
publicfinalStringtoString () throws {
try{
return (String) super.h.invoke (this, m 2 ,(Object[]) null);
}catch (RuntimeException|Errorvar 2 ){
throwvar 2 ;


}catch (Throwablevar 3 ){
thrownewUndeclaredThrowableException (var 3 );
}
}
...
static{
try{
m 1 =Class.forName ("java. lang. Object"). getMethod ("equals", Class. forName
("java. lang. Object"));
m 4 =Class.forName ("com. crazymaker. demo. proxy. MockDemoClient").
getMethod ("echo",Class.forName ("java. lang. String"));
m 3 =Class.forName ("com. crazymaker. demo. proxy. MockDemoClient").
getMethod ("hello");
m 2 =Class.forName ("java. lang. Object"). getMethod ("toString");
m 0 =Class.forName ("java. lang. Object"). getMethod ("hashCode");
}catch (NoSuchMethodExceptionvar 2 ){
thrownewNoSuchMethodError (var 2 .getMessage ());
}catch (ClassNotFoundExceptionvar 3 ){
thrownewNoClassDefFoundError (var 3 .getMessage ());
}
}
}
通过代码可以看出，这个动态代理类其实只做了两件简单的事情：
（ 1 ）该动态代理类实现了接口类的抽象方法
以上动态代理类 Proxy 0 实现了 MockDemoClient 接口的 echo (String)、hello () 两个方法。此外，
Proxy 0 还继承了 java. lang. Object 的 equals ()、hashCode ()、toString () 方法。

（ 2 ）该动态代理类将对自己的方法调用委托给了 InvocationHandler 调用处理器内部成员
以上动态代理类 Proxy 0 的每一个方法实现，其代码其实非常简单，并且逻辑都大致一样：将
方法自己的 Method 反射对象和调用参数进行了二次委托，委托给内部成员 InvocationHandler 调用处
理器的 invoke (...) 方法。至于该内部 InvocationHandler 调用处理器实例则由读者自己编写，在调用
java. lang. reflect. Proxy 的 newProxyInstance (...) 创建动态代理对象时作为第三个参数传入。

至此，JDK 动态代理机制的核心原理和动态代理类的神秘面纱已经彻底揭开了。Feign 的 RPC
客户端正是通过 JDK 的动态代理机制实现的，Feign 对 RPC 调用的各种增强处理主要通过调用处理器
InvocationHandler 实现。

4. (^2) 模拟的 FeignRPC 动态代理实现
由于 Feign 的组件依赖多，其 InvocationHandler 调用处理器的内部实现比较复杂，为了便于大家
理解，这里模拟 Feign 远程调用的动态代理模式设计了一个参考实例，作为正式学习的铺垫。
模拟的 FeignRPC 代理模式所涉及的类具体如图 4 - 6 所示。


```
图 4 - 6 模拟的 FeignRPC 代理模式的 UML 类图
```
#### 4. 2. 1 模拟 Feign 的 MethodHandler 方法处理器

由于每个 RPC 客户端类一般会包含多个远程调用方法，所以 Feign 为远程调用方法封装了一个
专门的接口——MethodHandler，此接口很简单，仅仅包含了一个 invoke (...) 抽象方法。
这里，首先对 Feign 的方法处理器 MethodHandler 进行模拟，模拟的 RPC 方法处理器接口如下：
packagecom. crazymaker. demo. proxy. FeignMock;
/**
*RPC 方法处理器
*/
interfaceRpcMethodHandler
{
/**
*功能：组装 URL，完成 RESTRPC 远程调用，并且返回 JSON 结果
*
*@paramargvRPC 方法的参数
*@returnREST 接口的响应结果
*@throwsThrowable 异常
*/
Objectinvoke (Object[]argv) throwsThrowable;
}
模拟的 RPC 方法处理器只有一个抽象方法 invoke (Object[])，该方法在进行 RPC 调用时需要完成
URL 的组装、执行 RPC 请求并且将响应封装成 JavaPOJO 实例然后返回。
模拟方法处理器 RpcMethodHandler 接口的实现类大致如下：
packagecom. crazymaker. demo. proxy. FeignMock;
//省略 import


@Slf 4 j
publicclassMockRpcMethodHandlerimplementsRpcMethodHandler
{
/**
* RESTURL 的前面部分，一般来自于 Feign 远程调用接口的类级别注解
* 如"http://crazydemo.com: 7700 /demo-provider/";
*/
finalStringcontextPath;
/**
* RESTURL 的前面部分，来自于远程调用 Feign 接口的方法级别的注解
* 如"api/demo/hello/v 1 ";
*/
finalStringurl;
publicMockRpcMethodHandler (StringcontextPath, Stringurl)
{
this. contextPath=contextPath;
this. url=url;
}
/**
*功能：组装 URL，完成 RESTRPC 远程调用，并且返回 JSON 结果
*
*@paramargvRPC 方法的参数
*@returnREST 接口的响应结果
*@throwsThrowable 异常
*/
@Override
publicObjectinvoke (Object[]argv) throwsThrowable
{
/**
*组装 REST 接口 URL
*/
StringrestUrl=contextPath+MessageFormat.format (url, argv);
log.info ("restUrl={}", restUrl);
/**
*通过 HttpClient 组件调用 REST 接口
*/
StringresponseData=HttpRequestUtil.simpleGet (restUrl);
/**
*解析 REST 接口的响应结果，解析成 JSON 对象并且返回
*/
RestOut<JSONObject>result=JsonUtil.jsonToPojo (responseData,
newTypeReference<RestOut<JSONObject>>(){});
returnresult;
}
}
模拟方法处理器实现类 MockRpcMethodHandler 的 invoke (Object[]) 大致完成了以下三个工作：
1 ）组装 URL，将来自于 RPC 的请求上下文路径（一般来自于 RPC 客户端类级别注解）和远程
调用的方法级别的 URI 路径拼接在一起，组成完整的 URL 路径。
2 ）通过 HttpClient 组件（也可以是其他组件）发起 HTTP 请求，调用服务端的 REST 接口。
3 ）解析 REST 接口的响应结果，解析成 POJO 对象（这里是 JSON 对象）并且返回。


#### 4. 2. 2 模拟 Feign 的 InvokeHandler 调用处理器

调用处理器 FeignInvocationHandler 是一个相对简单的类，拥有一个非常重要的 Map 类型的成员
dispatch，用于保存 RPC 方法反射实例到其 MethodHandler 方法处理器的映射。
这里设计了一个模拟调用处理器 MockInvocationHandler，用于模拟 FeignInvocationHandler 调用
处理器，模拟调用处理器同样拥有一个 Map 类型的成员 dispatch，负责保存 RPC 方法反射实例到模
拟方法处理器 MockRpcMethodHandler 的映射。一个运行时 MockInvocationHandler 模拟调用处理器
实例的 dispatch 成员的内存结构图大致如图 4 - 7 所示。

图 4 - 7 一个运行时 MockInvocationHandler 的 dispatch 成员的内存结构
MockInvocationHandler 通过 Java 反射，扫描模拟 RPC 远程调用接口 MockDemoClient 中的每一个
方法的反射注解，组装出一个对应的 Map 映射实例，它的键（Key）为 RPC 方法的反射实例，它的
值（Value）为 MockRpcMethodHandler 方法处理器实例。
MockInvocationHandler 的源码如下：
packagecom. crazymaker. demo. proxy. FeignMock;
//省略 import
classMockInvocationHandler implements InvocationHandler
{
/**
*远程调用的分发映射：根据方法名称分发方法处理器
*key：远程调用接口的方法反射实例
*value：模拟的方法处理器实例
*/
privateMap<Method,RpcMethodHandler>dispatch;
/**
*功能：代理对象的创建
*@paramclazz 被代理的接口类型
*@return 代理对象
*/
publicstatic<T>TnewInstance (Class<T>clazz)
{

```
一个 MockInvocationHandler 模拟调用处理器实例
的 dispatch 成员
```
```
value：模拟调用处理器实例
```

/**
*从远程调用接口的类级别注解中获取 REST 地址的 contextPath 部分
*/
AnnotationcontrollerAnno=clazz.getAnnotation (RestController. class);
if (controllerAnno==null)
{
returnnull;
}
StringcontextPath=((RestController) controllerAnno). value ();
//创建一个调用处理器实例
MockInvocationHandlerinvokeHandler=newMockInvocationHandler ();
invokeHandler. dispatch=newLinkedHashMap<>();
/**
* 通过反射迭代远程调用接口的每一个方法，组装 MockRpcMethodHandler 处理器
*/
for (Methodmethod: clazz.getMethods ())
{
AnnotationmethodAnnotation=method.getAnnotation (GetMapping. class);
if (methodAnnotation==null)
{
continue;
}
/**
*从远程调用接口的方法级别注解中获取 REST 地址的 uri 部分地址
*/
Stringuri=((GetMapping) methodAnnotation). name ();
/**
*组装 MockRpcMethodHandler 模拟方法处理器
*注入 REST 地址的 contextPath 部分和 uri 部分
*/
MockRpcMethodHandlerhandler=newMockRpcMethodHandler (contextPath, uri);
/**
*将模拟方法处理器 handler 实例缓存到 dispatch 映射中
*key 为方法反射实例，value 为方法处理器
*/
invokeHandler.dispatch.put (method, handler);
}
//创建代理对象
Tproxy=(T) Proxy.newProxyInstance (clazz.getClassLoader (),
newClass<?>[]{clazz}, invokeHandler);
returnproxy;
}
/**
*功能：动态代理实例的方法调用
*@paramproxy 动态代理实例
*@parammethod 待调用的方法
*@paramargs 方法实参
*@return 返回值
*@throwsThrowable 抛出的异常
*/
@Override
publicObjectinvoke (Objectproxy,
Methodmethod, Object[]args) throwsThrowable
{
if ("equals".equals (method.getName ()))
{
Objectother=args. length> 0 &&args[ 0 ]!=null? args[ 0 ]: null;


```
returnequals (other);
}elseif ("hashCode".equals (method.getName ()))
{
returnhashCode ();
}elseif ("toString".equals (method.getName ()))
{
returntoString ();
}
/**
* 从 dispatch 映射中根据方法反射实例获取方法处理器
*/
RpcMethodHandlerrpcMethodHandler=dispatch.get (method);
/**
*方法处理器组装 URL，完成 RESTRPC 远程调用，并且返回 JSON 结果
*/
returnrpcMethodHandler.invoke (args);
}
}
```
#### 4. 2. 3 模拟 Feign 动态代理 RPC 的执行流程

模拟调用处理器 MockInvocationHandler 的 newInstance (...) 方法创建一个调用处理器实例，该方
法与 JDK 的动态代理机制没有任何关系，是一个自定义的业务方法。该方法的逻辑如下：

1 ）从 RPC 远程调用接口的类级别注解中获取请求 URL 地址的 contextPath 上下文根路径部分，
如实例中的http://crazydemo.com: 7700 /demo-provider/。
2 ）通过迭代扫描 RPC 接口的每一个方法，组装出对应的 MockRpcMethodHandler 模拟方法处理
器，并且缓存到 dispatch 映射中。

模拟方法处理器 MockRpcMethodHandler 实例的创建过程，大致如下：
1 ）从对应的 RPC 远程调用方法的注解中取得 URL 地址的 URI 部分，如 hello () 方法的注解中的
URI 地址为 api/demo/hello/v 1 。
2 ）新建 MockRpcMethodHandler 模拟方法处理器，注入 URL 地址的 contextPath 上下文根路径部
分和 URI 部分。
3 ）将新建的方法处理器实例作为 value 缓存到调用处理器 MockInvocationHandler 的 dispatch 映射
中，其键为对应的 RPC 远程调用方法的 Method 反射实例。

模拟 Feign 的调用处理器 MockInvocationHandler 的 invoke (...) 方法用于完成方法处理器实例的调
用，该 invoke (...) 方法是 JDK 的 InvocationHandler 的 invoke (...) 抽象方法的具体实现。
当动态代理实例的 RPC 方法（如 hello 方法）被调用时，MockInvocationHandler 的 invoke (...) 方法
会根据 RPC 方法的反射实例，从 dispatch 映射中取出对应的 MockRpcMethodHandler 方法处理器实例，
由该方法处理器完成对远程服务的 RPC 调用。
模拟 Feign 动态代理 RPC 调用（以 hello 方法为例）的执行流程如图 4 - 8 所示。


```
图 4 - 8 模拟 Feign 动态代理的 RPC 执行流程（以 hello 方法为例）
```
4. 2. (^4) 模拟动态代理 RPC 远程调用的测试
以下为模拟 Feign 动态代理 RPC 的调用处理器、方法处理器的测试用例，代码如下：
packagecom. crazymaker. demo. proxy. FeignMock;
//省略 import
@Slf 4 j
publicclassFeignProxyMockTester
{
/***测试用例*/
@Test
publicvoidtest ()
{
/**
*创建远程调用接口的本地 JDKProxy 代理实例
*/
MockDemoClientproxy=
MockInvocationHandler.newInstance (MockDemoClient. class);
/**
*通过模拟接口完成远程调用
*/
RestOut<JSONObject>responseData=proxy.hello ();
log.info (responseData.toString ());
/**
*通过模拟接口完成远程调用
*/
RestOut<JSONObject>echo=proxy.echo ("proxyTest");
log.info (echo.toString ());
}
}
在运行测试用例前，需要提前启动 demo-provider 微服务实例，并且确保两个 REST 接口
/api/demo/hello/v 1 和/api/demo/echo/{word}/v 1 可以正常访问。一切准备妥当，运行测试用例，大致
的结果输出如下：
[main]INFO c.c.d.p.F.MockInvocationHandler-远程方法 hello 被调用
[main]INFO c.c.d.p.F.MockRpcMethodHandler-
restUrl=http://crazydemo.com: 7700 /demo-provider/api/demo/hello/v 1


[main]INFO c.c.d.p.F.FeignProxyMockTester-RestOut{datas={"hello": "world"},
respCode= 0 ,respMsg='操作成功}
[main]INFO c.c.d.p.F.MockInvocationHandler-远程方法 echo 被调用
[main]INFO c.c.d.p.F.MockRpcMethodHandler-
restUrl=http://crazydemo.com: 7700 /demo-provider/api/demo/echo/proxyTest/v 1
[main]INFO c.c.d.p.F.FeignProxyMockTester-RestOut{datas={"echo": "proxyTest"},
respCode= 0 ,respMsg='操作成功}
本小节模拟的调用处理器、方法处理器在大致架构设计、执行流程上，与实际的 Feign 已经非
常类似了。但是实际的 Feign 的调用处理器、方法处理器在 RPC 远程调用的保护机制、编码解码流
程等方面，比模拟的组件要复杂太多。

#### 4. 2. 5 Feign 弹性 RPC 客户端实现类

在本章的开头笔者演示了简单的 RPC 客户端实现类 RealRpcDemoClientImpl，直接通过
HttpClient 组件完成了对 demo-provider 服务的远程调用。
首先，Feign 的 RPC 客户端实现类是一种 JDK 动态代理类，能完成对简单 RPC 类（类似本章前面
的 RealRpcDemoClientImpl）的动态代理；其次，Feign 通过调用处理器、方法处理器对 RPC 委托类进
行了增强，其调用处理器 InvokeHandler 通过对第三方组件如 Ribbon、Hystrix 的使用，使得 Feign 动态
代理 RPC 客户端类具备了客户端负载均衡、失败回退、熔断器、舱壁隔离等一系列的 RPC 保护能力。
总体来说，Feign 通过调用处理器 InvokeHandler 增强了其动态代理类，使之变成了一个弹性 RPC
客户端实现类。Feign 弹性 RPC 客户端实现类大致的功能如图 4 - 9 所示。

```
图 4 - 9 Feign 弹性 RPC 客户端实现类
```
```
熔断器
```

Feign 弹性 RPC 客户端实现类的大致功能介绍如下：
（ 1 ）失败回退
当 RPC 远程调用失败时将执行回退代码，尝试通过其他方式来规避处理而不是产生一个异常。
（ 2 ）熔断器熔断
当 RPC 远程服务被调用时，熔断器将监视这个调用。如果调用时间太长，熔断器将介入并中
断调用。如果 RPC 调用失败次数达到某个阈值，将会采取快速失败策略，终止持续的调用失败。

（ 3 ）舱壁隔离
如果所有 RPC 调用都使用的是同一个线程池，那么很有可能一个缓慢的远程服务将拖垮整个应用
程序。弹性客户端应该能够隔离每个远程资源，并分配各自的舱壁线程池，使之相互隔离互不影响。

（ 4 ）客户端负载均衡
RPC 客户端可以在微服务提供者的多个实例之间实现多种方式的负载均衡，比如轮询、随机、
权重等。

弹性 RPC 客户端的作用除了是对 RPC 调用的本地保护之外，也是对远程服务的一种保护。当远
程服务发生错误或者表现不佳时，弹性 RPC 客户端能“快速失败”，不消耗诸如数据库连接、线程
池之类的资源，能保护远程服务（微服务 Provider 实例或者数据库服务等）免于崩溃。
总之，弹性 RPC 客户端可以避免某个 Provider 实例的单点问题或者故障，在整个微服务节点之
间传播，从而避免雪崩效应的发生。

### 4. 3 Feign 弹性 RPC 客户端的重要组件

在微服务启动时，Feign 会进行包扫描，对添加@FeignClient 注解的 RPC 接口创建远程接口的本
地 Proxy 动态代理实例。之后这些本地 Proxy 动态代理实例会注入 SpringIOC 容器中。当远程接口的
方法被调用时，由 Proxy 动态代理实例负责完成真正的远程访问并且返回结果。

#### 4. 3. 1 演示用例说明

为了演示 Feign 的远程调用动态代理类，本章接下来的演示用例是从 uaa-provider 服务实例向
demo-provider 服务实例发起 RPC 远程调用，大致的调用流程如图 4 - 10 所示。
uaa-provider 服务中的 DemoRPCController 类的代码如下：
packagecom. crazymaker. SpringCloud. user. info. controller;
//省略 import
@RestController
@RequestMapping ("/api/call/demo/")
@Api (tags="演示 demo-provider 远程调用")
publicclassDemoRPCController
{
//注入@FeignClient 注解所配置的 demo-provider 远程客户端动态代理实例
@Resource
DemoClientdemoClient;


```
@GetMapping ("/hello/v 1 ")
@ApiOperation (value="hello 远程调用")
publicRestOut<JSONObject>remoteHello ()
{
/**
*调用 demo-provider 的 REST 接口 api/demo/hello/v 1
*/
RestOut<JSONObject>result=demoClient.hello ();
JSONObjectdata=newJSONObject ();
data.put ("demo-data", result);
returnRestOut.success (data). setRespMsg ("操作成功");
}
@GetMapping ("/echo/{word}/v 1 ")
@ApiOperation (value="echo 远程调用")
publicRestOut<JSONObject>remoteEcho (
@PathVariable (value="word") Stringword)
{
/**
*调用 demo-provider 的 REST 接口 api/demo/echo/{ 0 }/v 1
*/
RestOut<JSONObject>result=demoClient.echo (word);
JSONObjectdata=newJSONObject ();
data.put ("demo-data", result);
returnRestOut.success (data). setRespMsg ("操作成功");
}
}
```
图 4 - 10 从 uaa-provider 实例向 demo-provider 实例发起远程调用
启动 uaa-provider 服务后，访问其 swagger-ui 接口，可以看到新增了两个对 demo-provider 实例进
行 RPC 调用的 REST 接口，具体如图 4 - 11 所示。
本章后面的 Feign 动态代理 RPC 客户端类的知识，都是基于此演示用例进行介绍，特殊情况下，
还需要在 uua-provider 的方法执行时进行单步调试，以查看 Feign 在执行过程中的相关变量和属性的
值。当然，在演示 uaa-provider 之前，需要启动好 demo-provider 服务。
基于以上演示用例，下面开始梳理 Feign 中涉及 RPC 远程调用的几个重要组件。


```
图 4 - 11 uaa-provider 新增的对 demo-provider 实例进行 RPC 调用的两个接口
```
#### 4. 3. 2 Feign 的动态代理 RPC 客户端实例

由于 uua-provider 服务需要对 demo-provider 服务进行 FeignRPC 调用，因此 uua-provider 需要依赖
DemoClient 远程调用接口，该接口的代码大家都非常熟悉了，如下所示：
packagecom. crazymaker. SpringCloud. demo. contract. client;
//省略 import
@FeignClient (
value="seckill-provider", path="/api/demo/",
fallback=DemoDefaultFallback. class)
publicinterfaceDemoClient
{
/**
*远程调用接口的方法:
*调用 demo-provider 的 REST 接口 api/demo/hello/v 1
*REST 接口功能：返回 helloworld
*@returnJSON 响应实例
*/
@GetMapping ("/hello/v 1 ")
RestOut<JSONObject>hello ();
/**
*远程调用接口的方法:
*调用 demo-provider 的 REST 接口 api/demo/echo/{ 0 }/v 1
*REST 接口功能：回显输入的信息
*@returnecho 回显消息 JSON 响应实例
*/
@RequestMapping (value="/echo/{word}/v 1 ",
method=RequestMethod. GET)
RestOut<JSONObject>echo (
@PathVariable (value="word") Stringword);
}


注意，DemoClient 远程调用接口加有@FeignClient 注解，Feign 在启动时会为带有@FeignClient
注解的接口创建一个动态代理 RPC 客户端实例，并注册到 SpringIOC 容器，如图 4 - 12 所示。

图 4 - 12 远程调用接口 DemoClient 的动态代理 RPC 客户端实例
DemoClient 的本地 JDK 动态代理实例的创建过程比较复杂，稍后将重点介绍。先来看另外两个
重要的 Feign 逻辑组件——调用处理器和方法处理器。

#### 4. 3. 3 Feign 的调用处理器 InvocationHandler

大家知道，通过 JDKProxy 生成动态代理类，核心步骤就是定制一个调用处理器。调用处理器
实现类需要实现 JDK 中位于 java. lang. reflect 包中的 InvocationHandler 调用处理器接口，并且实现该接
口的 invoke (...) 抽象方法。
Feign 提供了一个默认的调用处理器，叫作 FeignInvocationHandler 类，该类完成基本的调用处
理逻辑，处于 feign-core 核心 JAR 包中。当然，Feign 的调用处理器可以进行替换，如果 Feign 是与 Hystrix
结合使用，则会被替换成 HystrixInvocationHandler 调用处理器类，而该类处于 feign-hystrix 的 JAR 包中。
以上两个 Feign 调用处理器都实现了 JDK 的 InvocationHandler 接口，具体如图 4 - 13 所示。

图 4 - 13 两个 Feign 的 InvocationHandler 调用处理器示意图
默认的调用处理器 FeignInvocationHandler 是一个相对简单的类，有一个非常重要的 Map 类型的
成员 dispatch 映射，用于保存 RPC 方法反射实例到 Feign 的方法处理器 MethodHandler 实例的映射。


演示示例中，DemoClient 接口的 JDK 动态代理实现类的调用处理器 FeignInvocationHandler 实例
的 dispatch 成员的内存结构图，大致如图 4 - 14 所示。

图 4 - 14 内存结构图
DemoClient 的动态代理实例的调用处理器 FeignInvocationHandler 的 dispatch 成员中，有两个键
－值对（Key-ValuePair），一个键－值对缓存的是 hello 方法的方法处理器实例，一个键－值对缓
存的是 echo 方法的方法处理器实例。
在处理远程方法调用的时候，调用处理器 FeignInvocationHandle 会根据被调远程方法的 Java 反
射实例，在 dispatch 映射中找到对应的 MethodHandler 方法处理器，然后交给 MethodHandler 去完成实
际的 HTTP 请求和结果的处理。
Feign 的调用处理器 FeignInvocationHandler 的关键源码，节选如下：
packagefeign;
//省略 import
publicclassReflectiveFeignextendsFeign{
...
//内部类：默认的 Feign 调用处理器 FeignInvocationHandler
staticclassFeignInvocationHandlerimplementsInvocationHandler{
privatefinalTargettarget;
//RPC 方法反射实例和方法处理器的映射
privatefinalMap<Method,MethodHandler>dispatch;
//构造函数
FeignInvocationHandler (Targettarget, Map<Method,MethodHandler>dispatch){
this. target=checkNotNull (target,"target");
this. dispatch=checkNotNull (dispatch,"dispatchfor%s", target);
}
//默认 Feign 调用的处理
@Override
publicObjectinvoke (Objectproxy, Methodmethod, Object[]args) throwsThrowable{
...
//首先，根据方法反射实例从 dispatch 中取得 MethodHandler 方法处理器实例
//然后，调用方法处理器的 invoke (...) 方法
returndispatch.get (method). invoke (args);
}
...
}

```
一个 FeignInvocationHandler 调用处理器实例
的 dispatch 成员
```

以上源码很简单，重点在于 invoke (...) 方法，虽然核心代码只有一行，但有两个功能：
1 ）根据被调 RPC 方法的 Java 反射实例，在 dispatch 中找到对应的 MethodHandler 方法处理器。
2 ）调用 MethodHandler 方法处理器的 invoke (...) 方法完成实际的 RPC 远程调用，包括 HTTP 请求
的发送和响应的解码。

#### 4. 3. 4 Feign 的方法处理器 MethodHandler

Feign 的方法处理器 MethodHandler 接口和 JDK 动态代理机制中的 InvocationHandler 调用处理器
接口没有任何的继承和实现关系。
Feign 的 MethodHandler 接口是 Feign 自定义接口，是一个非常简单的接口，只有一个 invoke (...)
方法，并且定义在 InvocationHandlerFactory 工厂接口的内部，MethodHandler 接口源码如下：
//定义在 InvocationHandlerFactory 接口中
publicinterfaceInvocationHandlerFactory{
...
//方法处理器接口，仅拥有一个 invoke (...) 方法
interfaceMethodHandler{
//完成远程 URL 请求
Objectinvoke (Object[]argv) throwsThrowable;
}
...
}
MethodHandler 的 invoke (...) 方法的主要职责为完成实际远程 URL 请求，然后返回解码后的远程
URL 的响应结果。Feign 内置了 SynchronousMethodHandler、DefaultMethodHandler 两种方法处理器
的实现类，具体如图 4 - 15 所示。

图 4 - 15 Feign 的 MethodHandler 方法处理器及其实现类
内置的 SynchronousMethodHandler 同步方法处理实现类是 Feign 的一个重要类，提供了基本的远
程 URL 的同步请求响应处理。SynchronousMethodHandler 方法处理器的源码大致如下：
packagefeign;
//省略 import
finalclassSynchronousMethodHandlerimplementsMethodHandler{
...
privatestaticfinallongMAX_RESPONSE_BUFFER_SIZE= 8192 L;


privatefinalMethodMetadatametadata; //RPC 远程调用方法的元数据
privatefinalTarget<?>target; //RPC 远程调用 Java 接口的元数据
privatefinalClientclient; //Feign 客户端实例：执行 REST 请求和处理响应
privatefinalRetryerretryer;
privatefinalList<RequestInterceptor>requestInterceptors; //请求拦截器
...
privatefinalDecoderdecoder; //结果解码器
privatefinalErrorDecodererrorDecoder;
privatefinalbooleandecode 404 ; //是否反编码 404
privatefinalbooleancloseAfterDecode;
//执行 Handler 的处理
publicObjectinvoke (Object[]argv) throwsThrowable{
RequestTemplaterequestTemplate=this.buildTemplateFromArgs.create (argv);
...
while (true){
try{
returnthis.executeAndDecode (requestTemplate); //执行 REST 请求和处理响应
}catch (RetryableExceptionvar 5 ){
//省略不相干代码
}
}
}
//执行 RPC 远程调用，然后解码结果
ObjectexecuteAndDecode (RequestTemplatetemplate) throwsThrowable{
Requestrequest=this.targetRequest (template);
longstart=System.nanoTime ();
Responseresponse;
try{
response=this.client.execute (request, this. options);
response.toBuilder (). request (request). build ();
}
}
}
SynchronousMethodHandler 的 invoke (...) 方法首先生成请求模板 requestTemplate 实例，然后调用
内部成员方法 executeAndDecode () 执行 RPC 远程调用。
SynchronousMethodHandler 的成员方法 executeAndDecode () 执行流程如下：
1 ）通过请求模板 requestTemplate 实例生成目标 request 请求实例，主要完成请求的 URL、请求
参数、请求头等内容的封装。
2 ）通过 client（Feign 客户端）成员发起真正的 RPC 远程调用。
3 ）获取 response 响应，对结果进行解码。
SynchronousMethodHandler 的主要成员如下：
（ 1 ）Target<?>target
RPC 远程调用 Java 接口的元数据。保存了 RPC 接口的类名称、服务名称等信息，换句话说，远
程调用 Java 接口的@FeignClient 注解中配置的主要属性值都保存在 target 实例中。

（ 2 ）MethodMetadatametadata
RPC 方法的元数据，该元数据首先保存了 RPC 方法的配置键，格式为“接口名 #方法名 （形参
表）”；其次保存了 RPC 方法的请求模板（包括 URL、请求方法等）；再次保存了 RPC 方法的 returnType
返回类型；另外还保存了 RPC 方法的一些其他的属性。


（ 3 ）Clientclient
Feign 客户端实例是真正执行 RPC 请求和处理响应的组件。默认实现类为 Client. Default，通过
JDK 的基础连接类 HttpURLConnection 发起 HTTP 请求。Feign 客户端有多种实现类，比如封装了
ApacheHttpClient 组件的 feign. httpclient. HttpClient 客户端实现类，稍后详细介绍。

（ 4 ）List<RequestInterceptor>requestInterceptors
为每个请求在执行前加入拦截器的逻辑。
（ 5 ）Decoderdecoder
HTTP 响应的解码器。
同步方法处理器 SynchronousMethodHandler 的属性较多，这里就不一一介绍了。其内部有一个
Factory 工厂类，负责实例的创建。创建一个 SynchronousMethodHandler 实例的源码如下：
packagefeign;
...
//同步方法调用器
finalclassSynchronousMethodHandlerimplementsMethodHandler{
...
//方法调用器创建工厂
staticclassFactory{
privatefinalClientclient; //Feign 客户端：负责 RPC 请求和处理响应
privatefinalRetryerretryer;
privatefinalList<RequestInterceptor>requestInterceptors; //请求拦截器
privatefinalLoggerlogger;
privatefinalLevellogLevel;
privatefinalbooleandecode 404 ; //是否解码 404 错误响应
privatefinalbooleancloseAfterDecode;
//省略 Factory 创建工厂的全参构造器
//工厂的默认创建方法：创建一个方法调用器
publicMethodHandlercreate (Target<?>target, MethodMetadatamd,
feign. RequestTemplate. FactorybuildTemplateFromArgs,
Optionsoptions, Decoderdecoder, ErrorDecodererrorDecoder){
//返回一个新的同步方法调用器
returnnewSynchronousMethodHandler (target, this. client, this. retryer,
this. requestInterceptors, this. logger, this. logLevel, md,
buildTemplateFromArgs, options, decoder,
errorDecoder, this. decode 404 ,this. closeAfterDecode);
}
}
}

#### 4. 3. 5 Feign 的客户端组件

客户端组件是 Feign 中一个非常重要的组件，负责最终的 HTTP（包括 REST）请求的执行。其
核心逻辑：发送 Request 请求到服务器，在接收到 Response 响应后进行解码，并返回结果。
feign. Client 接口是客户端的顶层接口，只有一个抽象方法，源码如下：
packagefeign;
/**客户端接口
* SubmitsHTTP{@linkRequestrequests}.
*Implementationsareexpectedtobethread-safe.
*/


publicinterfaceClient{
//提交 HTTP 请求，并且接收 response 响应后进行解码
Responseexecute (Requestrequest, Optionsoptions) throwsIOException;
}
对于不同的 feign. Client 客户端实现类，它们内部提交 HTTP 请求的技术是不同的。常用的 Feign
客户端实现类如下：

1 ）Client. Default 实现类：默认的实现类，使用 JDK 的 HttpURLConnnection 类提交 HTTP 请求。
2 ）ApacheHttpClient 实现类：该客户端实现类在内部使用 ApacheHttpClient 开源组件提交 HTTP
请求。
3 ）OkHttpClient 实现类：该客户端实现类在内部使用 OkHttp 3 开源组件提交 HTTP 请求。
4 ）LoadBalancerFeignClient 实现类：内部使用 Ribbon 负载均衡技术完成 HTTP 请求处理。
Feign 客户端组件的 UML 类图大致如图 4 - 16 所示。

图 4 - 16 Feign 客户端组件的 UML 类图
下面对上面 4 个常见的客户端实现类进行简要介绍。
（ 1 ）Client. Default 实现类
作为默认的 Client 接口的实现类，Client. Default 内部使用 JDK 自带的 HttpURLConnnection 类去提
交 HTTP 请求。
Client. Default 实现类的方法大致如图 4 - 17 所示。

```
图 4 - 17 Client. Default 实现类的方法
```

在 JKD 1. 8 中，虽然 HttpURLConnnection 底层使用了非常简单的 HTTP 连接池技术，但是其 HTTP
连接的复用能力实际是非常弱的，所以其性能也比较低，生产环境下不建议使用。

（ 2 ）ApacheHttpClient 实现类
ApacheHttpClient 客户端实现类的内部使用 ApacheHttpClient 开源组件提交 HTTP 请求。和 JDK 自
带的 HttpURLConnnection 连接类比，ApacheHttpClient 更加易用和灵活，它不仅使客户端发送 HTTP
请求变得容易，而且也方便开发人员测试接口。既提高了开发的效率，也提高了代码的健壮性。从
性能的角度而言，ApacheHttpClient 带有连接池的功能，具备了优秀的 HTTP 连接的复用能力。
客户端实现类 ApacheHttpClient 处于 feign-httpclient 独立 JAR 包中，如果使用，还需引入配套版
本的 JAR 包依赖。疯狂创客圈的脚手架 crazy-SpringCloud 使用了 ApacheHttpClient 客户端，在各
Provider 微服务提供者模块中加入了 feign-httpclient 和 httpclient 两个组件的依赖坐标，具体如下：
<dependency>
<groupId>io. github. openfeign</groupId>
<artifactId>feign-httpclient</artifactId>
<version>${feign-httpclient. version}</version>
</dependency>
<!--https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient-->
<dependency>
<groupId>org. apache. httpcomponents</groupId>
<artifactId>httpclient</artifactId>
<version>${httpclient. version}</version>
</dependency>
另外，在配置文件中将配置项 feign. httpclient. enabled 的值设置为 true，表示需要启动
ApacheHttpClient。

（ 3 ）OkHttpClient 实现类
OkHttpClient 客户端内部使用了开源组件 OkHttp 3 提交 HTTP 请求。OkHttp 3 组件是 Square 公司开
发的，用于替代 HttpUrlConnection 和 ApacheHttpClient 的高性能 HTTP 组件。由于 OkHttp 3 较好地支
持 SPDY 协议（SPDY 是 Google 开发的基于 TCP 的传输层协议，用以最小化网络延迟，提升网络速度，
优化用户的网络使用体验），并且从 Android 4. 4 开始，Google 将 Android 源码中的 JDK 连接类
HttpURLConnection 使用 OkHttp 进行了替换。

（ 4 ）LoadBalancerFeignClient 负载均衡客户端实现类
该客户端实现类处于 Feign 核心 JAR 包中，在内部使用 Ribbon 开源组件实现多个 Provider 实例之
间的负载均衡。其内部有一个封装的 delegate 委托客户端成员，该成员才是最终的 HTTP 请求提交者。
Ribbon 负载均衡组件计算出合适的服务端 Provider 实例之后，由 delegate 委托客户端完成到 Provider
服务端的 HTTP 请求。
LoadBalancerFeignClient 封装的 delegate 委托客户端的类型可以是 Client. Default 默认客户端，也
可以是 ApacheHttpClient 客户端类或 OkHttpClient 客户端类，或者其他的定制类。
LoadBalancerFeignClient 负载均衡客户端实现类的 UML 类图如图 4 - 18 所示。
此外，除了以上 4 个 feign. Client 客户端实现类，还可以定制自己的 feign. Client 实现类。


```
图 4 - 18 LoadBalancerFeignClient 负载均衡客户端实现类的 UML 类图
```
### 4. 4 Feign 的 RPC 动态代理实例的创建流程

```
在介绍 Feign 远程代理实例的创建流程之前，本节先总结一下 Feign 的整体执行流程。
```
#### 4. 4. 1 Feign 的整体运作流程

首先回顾一下 Feign 的整体运作流程。Feign 英文直译为“假装”或“装作”，也就是说 Feign
是一个伪客户端，即它不做任何的 HTTP 请求处理。在应用启动的初始化过程中，Feign 完成了以下
两项工作：

1 ）对于每一个 RPC 远程调用 Java 接口，Feign 根据@FeignClient 注解生成本地 JDK 动态代理
实例。
2 ）对于 Java 接口中的每一个 RPC 远程调用方法，Feign 首先根据 SpringMVC（如@GetMapping）
类型注解生成方法处理器实例，该实例内部包含了一个请求模板 RequestTemplate 实例。

在远程调用 REST 请求执行过程中，Feign 完成了以下两项工作：
1 ）Feign 使用远程方法调用中的实际参数替换掉 RequestTemplate 模板实例中的参数，生成最终
的 HTTP 请求。
2 ）将 HTTP 请求通过 feign. Client 客户端发送到 Provider 服务端。
总之，Feign 根据注解生成动态代理 RPC 客户端实例和 HTTPRequest 请求，大大简化了 HTTP 远
程 API 的调用。
使用 Feign 进行开发，开发人员既可以使用注解的方式定制本地 JDK 动态代理实例，也可以通
过注解的方式调整 Request 请求模板，结合起来，使得整个远程 RPC 调用的工作变得非常轻松和容易。
总结来说，Feign 的整体运作流程大致如下：


（ 1 ）通过应用启动类上的@EnableFeignClients 注解，开启 Feign 的装配和远程代理实例创建
在@EnableFeignClients 注解源码中，可以看到导入了 FeignClientsRegistrar 类，该类用于扫描
@FeignClient 注解过的 RPC 接口。

（ 2 ）通过对@FeignClient 注解 RPC 接口扫描，创建远程调用的动态代理实例
FeignClientsRegistrar 类会进行包扫描，扫描所有包下所有@FeignClient 注解过的接口，并创建
RPC 接口的 FactoryBean 工厂类实例，并将这些 FactoryBean 注入 SpringIOC 容器中。
如果应用某些地方需要注入 RPC 接口的实例（比如被@Resource 引用），则 Spring 会通过注册
的 FactoryBean 工厂类实例的 getObject () 方法获取 RPC 接口的动态代理实例。
在创建 RPC 接口的动态代理实例时，Feign 会为每一个 RPC 接口创建一个调用处理器，也会为接
口的每一个 RPC 方法创建一个方法处理器，并且将方法处理器缓存在调用处理器的 dispatch 成员中。
在创建动态代理实例时，Feign 也会通过 RPC 方法的注解为每一个 RPC 方法生成一个
RequesTemplate 请求模板实例，RequestTemplate 中包含请求的所有信息，如请求 URL、请求类型（如
GET）、请求参数等。

（ 3 ）发生 RPC 调用时，通过动态代理实例类完成远程 Provider 的 HTTP 调用
当动态代理实例类的方法被调用时，Feign 会根据 RPC 方法的反射实例，从调用处理器的
dispatch 成员中取得方法处理器，然后由 MethodHandler 方法处理器开始 HTTP 请求处理。
MethodHandler 会结合实际的调用参数，通过 RequesTemplate 模板实例去生成 Request 请求实例。
最后，将 Request 请求实例交给 feign. Client 客户端实例去进一步完成 HTTP 请求处理。

（ 4 ）在完成远程 HTTP 调用前，需要进行客户端负载均衡的处理环节
在 SpringCloud 微服务架构中，同一个 Provider 微服务一般都会运行多个实例，所以说客户端的
负载均衡能力其实是必选项，而不是可选项。
生产环境下 Feign 必须和 Ribbon 结合在一起使用，所以方法处理器 MethodHandler 的客户端 client 成
员，必须是具备负载均衡能力的 LoadBalancerFeignClient 类型，而不会是完成 HTTP 请求提交的
ApacheHttpClient 等类型。只有在负载均衡计算出最佳的 Provider 实例之后，才能开始 HTTP 请求的提交。
在 LoadBalancerFeignClient 内部，有一个 delegate 委托成员，其类型可能为 feign. client. Default、
ApacheHttpClient、OkHttpClient 等，最终由该 delegate 客户端委托成员完成 HTTP 请求的提交。

至此，整体的 Feign 运作流程大家应该都比较熟悉了。其实，上面介绍的大致逻辑和前面介绍
的模拟的 FeignRPC 执行流程是类似的，只是 Feign 实际的工作流程的每一个环节更加的细致和复杂。

#### 4. 4. 2 RPC 动态代理容器实例的 FactoryBean 工厂类

为了方便 Feign 的 PRC 客户端动态代理实例的使用，还需要将其注册到 SpringIOC 容器，以方便
使用者通过@Resource 或@Autoware 注解将其注入其他的依赖属性。
一般情况下，Spring 通过@Service 等注解进行 Bean 实例化的配置，但是在某些情况下（比如在
Bean 实例化时）需要大量的配置信息，默认的 Bean 实例化机制是无能为力的。为此 Spring 提供了一
个 org. springframework. bean. factory. FactoryBean 工厂接口，用户可以通过实现该接口在 Java 代码中定
制 Bean 实例化的逻辑。
FactoryBean 在 Spring 框架中占用重要的地位，Spring 自身就提供了 70 多个 FactoryBean 的实现。


它们隐藏了一些复杂的 Bean 实例化的细节，给上层应用带来了便利。FactoryBean 注册到容器之后，
从 Spring 上下文通过 ID 或者类型获取 IOC 容器 Bean 时，获取的实际上是 FactoryBean 的 getObject () 返
回的对象，而不是 FactoryBean 本身。
Feign 的 RPC 客户端动态代理 IOC 容器实例，只能通过 FactoryBean 方式创建，原因有两点：
1 ）代理对象为通过 JDK 反射机制动态创建 Bean，不是直接定义的普通实现类。
2 ）其配置的属性比较多，而且是通过@FeignClient 注解配置完成的。
所以，Feign 提供了一个用于获取 RPC 容器实例的工厂类，叫作 FeignClientFactoryBean 类。
FeignClientFactoryBean 类的部分源码如下：
packageorg. springframework. cloud. openfeign;
...
classFeignClientFactoryBeanimplementsFactoryBean<Object>, InitializingBean,
ApplicationContextAware{
privateClass<?>type;//RPC 接口的 class 对象
privateStringname; //RPC 接口配置的远程 Provider 微服务名称，如 demo-provider
privateStringurl; //RPC 接口配置的 url 值，由@FeignClient 注解负责配置
privateStringpath; //RPC 接口配置的 path 值，由@FeignClient 注解负责配置
privatebooleandecode 404 ;
privateApplicationContextapplicationContext;
privateClass<?>fallback;
privateClass<?>fallbackFactory;
...
//获取 IOC 容器的 Feign. Builder 建造者 Bean
protectedBuilderfeign (FeignContextcontext){
FeignLoggerFactoryloggerFactory=this.get (context, FeignLoggerFactory. class);
Loggerlogger=loggerFactory.create (this. type);
//从 IOC 容器获取 Feign. Builder 实例
//并且设置编码器、解码器、日志器、方法解析器
Builderbuilder=((Builder) this.get (context, Builder. class))
.logger (logger)
.encoder ((Encoder) this.get (context, Encoder. class))
.decoder ((Decoder) this.get (context, Decoder. class))
.contract ((Contract) this.get (context, Contract. class));
this.configureFeign (context, builder);
returnbuilder;
}
//通过 ID 或者类型获取 IOC 容器 Bean 时调用
publicObjectgetObject () throwsException{
returnthis.getTarget ();
}
//委托方法：获取 RPC 动态代理 Bean
<T>TgetTarget (){
FeignContextcontext=(FeignContext) this. applicationContext. getBean
(FeignContext. class);
//获取 Feign. Builder 建造者实例
Builderbuilder=this.feign (context);
Stringurl;
...
}
...
}
前面讲到，FeignClientsRegistrar 类会进行包扫描，扫描所有包下所有@FeignClient 注解过的接
口，创建 RPC 接口的 FactoryBean 工厂类实例，并将这些 FactoryBean 注入 SpringIOC 容器中。


FeignClientsRegistrar 类的 RPC 接口的 FactoryBean 工厂类实例的注册源码节选如下：
classFeignClientsRegistrarimplementsImportBeanDefinitionRegistrar,...{
...
//为每一个 RPC 客户端接口注册一个 beanDefinition，其 beanClass 为 FeignClientFactoryBean
privatevoidregisterFeignClient (BeanDefinitionRegistryregistry,
AnnotationMetadata annotationMetadata,
Map<String,Object>attributes){
StringclassName=annotationMetadata.getClassName ();
BeanDefinitionBuilderdefinition=
BeanDefinitionBuilder.genericBeanDefinition (FeignClientFactoryBean. class);
this.validate (attributes);
definition.addPropertyValue ("url",this.getUrl (attributes)); //RPC 接口配置的
url 值
definition.addPropertyValue ("path",this.getPath (attributes));//RPC 接口配置的
path 值
Stringname=this.getName (attributes);
definition.addPropertyValue ("name", name); //RPC 接口配置的远程 provider 名称
definition.addPropertyValue ("type", className); //RPC 接口的全路径类名
definition.addPropertyValue ("decode 404 ",attributes.get ("decode 404 "));
definition.addPropertyValue ("fallback",attributes.get ("fallback"));
definition.addPropertyValue ("fallbackFactory",
attributes.get ("fallbackFactory"));
definition.setAutowireMode ( 2 );
Stringalias=name+"FeignClient";
AbstractBeanDefinitionbeanDefinition=definition.getBeanDefinition ();
booleanprimary=(Boolean) attributes.get ("primary");//RPC 接口配置的 primary 值
beanDefinition.setPrimary (primary);
Stringqualifier=this.getQualifier (attributes);
if (StringUtils.hasText (qualifier)){
alias=qualifier;
}
BeanDefinitionHolderholder=newBeanDefinitionHolder (beanDefinition, className, new
String[]{alias});
BeanDefinitionReaderUtils.registerBeanDefinition (holder, registry);
}
}
FeignClientsRegistrar 类的 registerFeignClient () 方法为扫描到的每一个 RPC 客户端接口注册一个
beanDefinition 实例（Bean 的），其中的 beanClass 为 FeignClientFactoryBean。
registerFeignClient () 方法的 attributes 参数值来自于 RPC 客户端接口@FeignClient 注解所配置的
值，将该方法打上断点，在 uaa-provider 启动时可以看到 attributes 参数的具体信息，如图 4 - 19 所示。

```
图 4 - 19 registerFeignClient () 方法的 attributes 参数值 ()
```

#### 4. 4. 3 Feign. Builder 建造者容器实例

当从 SpringIOC 容器获取 RPC 接口的动态代理实例，也就是当 FeignClientFactoryBean 的
getObject () 方法被调用时，其调用的 getTarget () 方法首先从 IOC 容器获取配置好的 Feign. Builder 建造
者容器实例，然后通过 Feign. Builder 建造者容器实例的 target () 方法完成 RPC 动态代理实例的创建。

```
这里将 Builder 翻译为建造者，以便同构造器进行区分。
```
Feign. Builder 建造者容器实例在自动配置类 FeignClientsConfiguration 中完成配置，通过其源码可
以看到，配置类的 feignBuilder (...) 方法通过调用 Feign.builder () 静态方法创建了一个建造者容器实例。
自动配置类 FeignClientsConfiguration 的部分源码如下：
packageorg. springframework. cloud. openfeign;
//省略 import
//Feign 客户端的配置类
@Configuration
publicclassFeignClientsConfiguration{
//容器实例：请求结果解码器
@Bean
@ConditionalOnMissingBean
publicDecoderfeignDecoder (){
returnnewOptionalDecoder (newResponseEntityDecoder (
newSpringDecoder (this. messageConverters)));
}
//容器实例：请求编码器
@Bean
@ConditionalOnMissingBean
publicEncoderfeignEncoder (){
returnnewSpringEncoder (this. messageConverters);
}
//容器实例：请求重试实例，如果没有定制，默认返回 NEVER_RETRY (不重试) 实例
@Bean
@ConditionalOnMissingBean
publicRetryerfeignRetryer (){
returnRetryer. NEVER_RETRY;
}
//容器实例：Feign. Builder 客户端建造者实例，以“请求重试实例”作为参数进行初始化
@Bean
@Scope ("prototype")
@ConditionalOnMissingBean
publicBuilderfeignBuilder (Retryerretryer){
returnFeign.builder (). retryer (retryer);
}
...
}
Feign. Builder 类是 feign. Feign 抽象类的一个内部类，作为 Feign 默认的建造者。Feign. Builder 类
的部分源码节选如下：
packagefeign;
...
publicabstractclassFeign{
...


//建造者方法
publicstaticBuilderbuilder (){
returnnewBuilder ();
}
//内部类：建造者类
publicstaticclassBuilder{
...
//创建 RPC 客户端的动态代理实例
public<T>Ttarget (Target<T>target){
returnbuild (). newInstance (target);
}
//建造方法
public Feign build (){
//方法处理器工厂的实例
SynchronousMethodHandler. Factory synchronousMethodHandlerFactory=
newSynchronousMethodHandler.Factory (client,
retryer,
requestInterceptors,
logger,
logLevel, decode 404 );
//RPC 方法解析器
ParseHandlersByName handlersByName=
newParseHandlersByName (contract, options, encoder, decoder,
errorDecoder, synchronousMethodHandlerFactory);
//反射式 Feign 实例
returnnewReflectiveFeign (handlersByName, invocationHandlerFactory);
}
}
当 FeignClientFactoryBean 工厂类的 getObject () 方法被调用后，通过 Feign. Builder 容器实例的
target () 方法完成 RPC 动态代理实例的创建。
Feign. Builder 的 target () 实例方法首先调用内部 build () 方法创建一个 Feign 实例，然后通过该实例
的 newInstance (...) 方法创建最终的 RPC 动态代理实例。默认情况下，所创建的 Feign 实例为
ReflectiveFeign 类型，二者的关系如图 4 - 20 所示。

图 4 - 20 Feign 和 ReflectiveFeign 二者之间的关系
这里通过单步断点演示一下。首先，通过开发调试工具（如 IDEA）在 Feign. Builder 的 target (...)
方法唯一的一行代码上打上一个断点。然后，以调试模式启动 uaa-provider 服务，在工程启动的过
程中可以看到断点所在的语句会被执行到。


断点被执行到之后，通过 IDEA 的 Evaluate 工具计算 target () 方法运行时的 target 实参值，可以看
到，它的实参值就是对 DemoClient 远程接口信息的一种二次封装，具体如图 4 - 21 所示。

图 4 - 21 DemoClient 动态代理实例创建时的 target () 方法处的断点信息
总结一下，当从 Spring 容器获取 RPC 接口的动态代理实例时，对应的 FeignClientFactoryBean 的
getObject () 方法会被调用到，然后通过 Feign. Builder 建造者容器实例的 target () 方法创建 RPC 接口的动
态代理实例，并缓存到 SpringIOC 容器中。

4. 4. (^4) 默认的 RPC 动态代理实例创建流程
默认情况下，Feign. Builder 建造者实例的 target () 方法会调用自身的 build () 方法创建一个
ReflectiveFeign（反射式 Feign）实例，然后调用该实例的 newInstance () 方法创建远程接口的最终的
JDK 动态代理实例。
通过 ReflectiveFeign（反射式 Feign）实例的 newInstance () 方法创建 RPC 动态代理实例的具体步
骤是什么呢？先看看 ReflectiveFeign 的源码，具体如下：
packagefeign;
//省略 import
publicclassReflectiveFeignextendsFeign{
//方法解析器
privatefinalParseHandlersByNametargetToHandlersByName;
//调用处理器工厂
privatefinalInvocationHandlerFactoryfactory;
...
//创建 RPC 客户端动态代理实例
public<T>TnewInstance (Target<T>target){
//方法解析: 方法名和方法处理器的映射
Map<String,MethodHandler>nameToHandler=targetToHandlersByName.apply (target);
//方法反射对象和方法处理器的映射
Map<Method,MethodHandler>methodToHandler=newLinkedHashMap<Method,
MethodHandler>();


...
//创建一个 InvocationHandler 调用处理器
InvocationHandlerhandler=factory.create (target, methodToHandler);
//最后调用 JDK 的 Proxy. newProxyInstance 创建代理对象
T proxy=(T) Proxy.newProxyInstance (
target.type (). getClassLoader (), newClass<?>[]{target.type ()}, handler);
...
//返回代理对象
returnproxy;
}
终于看到 Feign 动态代理类实例的创建逻辑了，以上默认的 FeignRPC 动态代理客户端实例的创
建流程和前面介绍的模拟动态代理 RPC 客户端实例的创建流程，大致是相同的。
简单来说，默认的 FeignRPC 动态代理客户端实例的创建流程大致为以下 4 步：
（ 1 ）方法解析
解析远程接口中的所有方法，为每一个方法创建一个 MethodHandler 方法处理器，然后进行方
法名称和方法处理器的键－值（Key-Value）映射 nameToHandler。

（ 2 ）创建方法反射实例和方法处理器的映射
通过方法名称和方法处理器的映射 nameToHandler，创建一个方法反射实例到方法处理器的
Key-Value 映射，叫作 methodToHandler，作为远程方法调用时的分发处理器。

（ 3 ）创建一个 JDK 调用处理器
主要以 methodToHandler 为参数创建一个 InvocationHandler 调用处理器实例。
（ 4 ）最后创建一个动态代理对象
调用 JDK 的 Proxy.newProxyInstance () 方法创建一个动态代理实例，其参数有三个：RPC 远程接
口的类装载器、RPC 远程接口的 Class 实例以及上一步创建的 InvocationHandler 调用处理器实例。
远程接口的 RPC 动态代理实例的创建流程大致如图 4 - 22 所示。

```
图 4 - 22 远程接口的 RPC 动态代理实例的创建流程
```

###### 以上 RPC 动态代理客户端实例的创建流程的 4 个步骤是需要理解和掌握的重点内容，后面的介

###### 绍还会反复围绕这 4 个步骤展开。

在 ReflectiveFeign.newInstance () 方法中，首先调用了 ParseHandlersByName.apply () 方法，解析
RPC 接口中的所有 RPC 方法配置（通过 Contract 解析），然后为每个 RPC 方法创建一个对应的
MethodHandler 方法处理器。
默认的 ParseHandlersByName 方法解析器是 ReflectiveFeign（反射式 Feign）类的一个内部类，
大致的源码如下：
packagefeign;
//省略 import
publicclassReflectiveFeignextendsFeign{
...
//内部类：方法解析器
staticfinalclassParseHandlersByName{
//同步方法处理器工厂
privatefinalSynchronousMethodHandler. Factoryfactory;
...
//RPC 接口元数据解析
publicMap<String,MethodHandler>apply (Target key){
//解析 RPC 方法元数据，返回一个方法元数据列表
List<MethodMetadata>metadata=contract.parseAndValidatateMetadata (key.type ());
Map<String,MethodHandler>result=newLinkedHashMap<String,MethodHandler>();
//迭代 RPC 方法元数据列表
for (MethodMetadatamd:metadata){
...
//通过方法处理器工厂 factory 创建 SynchronousMethodHandler 同步方法处理实例
result.put (md.configKey (),
factory.create (key, md, buildTemplate, options, decoder, errorDecoder));
}
returnresult;
}
}
通过以上源码可知，方法解析器 ParseHandlersByName 创建方法处理器是通过方法处理器工厂
类实例 factory 的 create () 方法完成的。而默认的方法处理器工厂类 Factory 定义在
SynchronousMethodHandler 类中，其代码如下：
packagefeign;
//省略 import
finalclassSynchronousMethodHandlerimplementsMethodHandler{
//省略不相干的代码
staticclassFactory{
publicMethodHandlercreate (
Target<?>target, MethodMetadatamd,
feign. RequestTemplate. FactorybuildTemplateFromArgs,
Optionsoptions, Decoderdecoder, ErrorDecodererrorDecoder)
{
returnnewSynchronousMethodHandler (
target, this. client, this. retryer, this. requestInterceptors,
this. logger, this. logLevel, md,
buildTemplateFromArgs, options, decoder, errorDecoder, this. decode 404 );
}
...
}
通过以上源码可知，在默认方法处理器工厂类 Factory 的 create () 方法中创建的正是同步方法处
理器 SynchronousMethodHandler 的实例。


接下来，简单介绍一下 FeignInvocationHandler 调用处理器的创建。和方法处理器类似，它的创
建也是通过工厂模式完成的。默认的 InvocationHandler 实例是通过 InvocationHandlerFactory 工厂类
完成的。该工厂类的源码大致如下：
packagefeign;
//调用处理器工厂接口
publicinterfaceInvocationHandlerFactory{
InvocationHandlercreate (Targettarget, Map<Method,MethodHandler>dispatch);
...
//默认实现类
staticfinalclassDefaultimplementsInvocationHandlerFactory{
//通过内部类 FeignInvocationHandler 构造一个默认的调用处理器
@Override
publicInvocationHandlercreate (Targettarget, Map<Method,MethodHandler>dispatch){
returnnewReflectiveFeign.FeignInvocationHandler (target, dispatch);
}
}
}
上面的源码中调用处理器工厂 InvocationHandlerFactory 仅是一个接口，只定义了一个唯一的
create () 方法，用于创建 InvocationHandler 调用处理器实例。
InvocationHandlerFactory 工厂类提供了一个默认的实现类——Default 内部类，其 create () 方法所
创建的调用处理器实例就是前文反复提及的，也做过重点介绍的 Feign 的默认调用处理器类
FeignInvocationHandler 类的实例。

#### 4. 4. 5 Contract 远程调用协议规则类

在通过 ReflectiveFeign.newInstance () 方法创建本地 JDKProxy 实例时，首先需要调用方法解析器
ParseHandlersByName 的 apply () 方法，获取方法名和方法处理器的映射。
而在 ParseHandlersByName.apply () 方法中，需要通过 Contract 协议规则类将远程调用 Feign 接口
中的所有方法配置和注解，解析成一个 List<MethodMetadata>方法元数据列表。
Contract 协议规则类与方法解析器、调用处理器的关系如图 4 - 23 所示。

```
图 4 - 23 Contract 协议规则类与方法解析器、调用处理器的关系
```

关于 RPC 接口的配置解析类，SpringCloudFeign 中有两个协议规则解析类，一个为 Feign 默认
的协议规则解析类（DefaultContact），一个为 SpringMvcContact 协议规则解析类，后者用于解析使
用了 SpringMVC 规则配置的 RPC 方法。
SpringCloudFeign 的协议规则解析大致如图 4 - 24 所示。

图 4 - 24 Feign 的 Contact 的协议规则解析示意图
Feign 有一套自己的默认协议规则，定义了一系列的 RPC 方法的配置注解，用于 RPC 方法所对
应的 HTTP 请求相关的参数，下面是一个官方的简单实例：
publicinterfaceGitHub{
@RequestLine ("GET/repos/{owner}/{repo}/contributors")
List<Contributor>getContributors (@Param ("owner") Stringowner,@Param ("repo")
Stringrepository);
classContributor{
Stringlogin;
intcontributions;
}
}
实例中的@RequestLine 注解是一个 Feign 默认的配置注解，用于配置 HTTP 的 Method 请求类型和
URI 请求路径。
为了降低学习成本，SpringCloud 并没有推荐采用 Feign 自己的协议规则注解来进行 RPC 接口配
置，而是推荐部分的 SpringMVC 协议规则注解来进行 RPC 接口的配置，并且通过 SpringMvcContact
协议规则解析类进行解析。
采用 SpringMVC 协议规则注解来进行 RPC 接口的配置的好处：对开发人员来说，远程调用 RPC
方法的注解配置和对应的服务端 REST 接口的注解配置可以保持基本一致，这样就降低了开发人员
的学习成本和维护成本。

### 4. 5 Feigh 远程调用的执行流程

由于 Feign 中生成 RPC 接口 JDK 动态代理实例涉及的 InvocationHandler 调用处理器有多种，导致
Feign 远程调用的执行流程也稍微有所区别，但是远程调用执行流程的主要步骤是一致的。这里主
要介绍与两类 InvocationHandler 调用处理器相关的 RPC 执行流程：


1 ）与默认的调用处理器 FeignInvocationHandler 相关的 RPC 执行流程。
2 ）与 Hystrix 调用处理器 HystrixInvocationHandler 相关的 RPC 执行流程。
还是以 uaa-provider 启动过程中的 DemoClient 接口的动态代理实例的执行过程为例，演示和分
析远程调用的执行流程。

#### 4. 5. 1 与 FeignInvocationHandler 相关的远程调用执行流程

FeignInvocationHandler 是默认的调用处理器，如果不进行特殊的配置，那么 Feign 将默认使用
此调用处理器。
这里结合 uaa-provider 服务中 DemoClient 的动态代理实例的 hello () 方法远程调用执行过程，详细
介绍一下与 FeignInvocationHandler 相关的远程调用执行流程，大致如图 4 - 25 所示。
整体的远程调用执行流程大致分为 4 步，具体如下：
（ 1 ）通过 SpringIOC 容器实例完成动态代理实例的装配
前文讲到，Feign 在启动时会为加上了@FeignClient 注解的所有远程接口（包括 DemoClient 接口）
创建一个 FactoryBean 工厂实例，并注册到 SpringIOC 容器。
然后在 uaa-provider 的 DemoRPCController 控制层类中，通过@Resource 注解从 SpringIOC 容器找
到 FactoryBean 工厂实例，通过其 getObject () 方法获取动态代理实例并装配给 DemoRPCController 实
例的成员变量 demoClient。
在需要进行 hello () 远程调用时，直接通过 demoClient 成员变量调用 JDK 动态代理实例的 hello ()
方法。

图 4 - 25 与 FeignInvocationHandler 相关的远程调用执行流程
（ 2 ）执行 InvocationHandler 调用处理器的 invoke (...) 方法
前面讲到，JDK 动态代理实例的方法调用过程是通过委托给 InvocationHandler 调用处理器完成
的，故在调用 demoClient 的 hello () 方法时，会调用其调用处理器 FeignInvocationHandler 实例的
invoke (...) 方法。
大家知道，FeignInvocationHandler 实例内部保持了一个远程调用方法反射实例和方法处理器的
一个 dispatch 映射。FeignInvocationHandle 在其 invoke (...) 方法中会根据 hello () 方法的 Java 反射实例，


在 dispatch 映射对象中找到对应的 MethodHandler 方法处理器，然后由后者完成实际的 HTTP 请求和
结果的处理。

（ 3 ）执行 MethodHandler 方法处理器的 invoke (...) 方法
通过前面关于 MethodHandler 方法处理器的组件介绍可知，Feign 默认的方法处理器为
SynchronousMethodHandler 同步调用处理器，其 invoke (...) 方法主要是通过内部 feign. client 类型的
client 成员实例完成远程 URL 请求执行和获取远程结果。
feign. Client 客户端成员有多种类型，不同的类型完成 URL 请求处理的具体方式不同。
（ 4 ）通过 feign. Client 客户端成员完成远程 URL 请求执行和获取远程结果
如果 MethodHandler 方法处理器实例的 client 成员实例是默认的 feign. Client. Default 实现类，就通
过 JDK 自带的 HttpURLConnnection 类完成远程 URL 请求执行和获取远程结果。
如果 MethodHandler 方法处理器实例的 client 客户端是 ApacheHttpClient 客户端实现类，就使用
Apachehttpclient 开源组件完成远程 URL 请求执行和获取远程结果。
如果 MethodHandler 方法处理器实例的 client 客户端是 LoadBalancerFeignClient 负载均衡客户端
实现类，就使用 Ribbon 结算出最佳的 provider 节点，然后由内部的 delegate 委托客户端成员去请求
provider 服务，完成 URL 请求处理。

以上 4 步基本上就是 SpringCloud 中的 Feign 远程调用的执行流程。
然而，默认的基于 FeignInvocationHandler 调用处理器的执行流程在运行机制以及调用性能上都
满足不了生产环境的要求，大致原因有以下两点：

1 ）远程调用过程中没有异常的熔断监测和恢复机制。
2 ）没有用到高性能的 HTTP 连接池技术。
接下来的内容将为大家介绍一种结合 Hystrix 进行 RPC 保护的远程调用处理流程。该流程所使
用的 InvocationHandler 调用处理器叫作 HystrixInvocationHandler 调用处理器。
这里作为铺垫，首先为大家介绍一下 HystrixInvocationHandler 调用处理器本身的具体实现。

#### 4. 5. 2 与 HystrixInvocationHandler 相关的远程调用执行流程

HystrixInvocationHandler 调用处理器类位于 feign. hystrix 包中，其字节码文件不在 Feign 核心包
“feign-core-*. jar”中，而是在扩展包“feign-hystrix-*. jar”中。这里的*号表示的是与 SpringCloud
版本配套的版本号，当 SpringCloud 的版本为 Finchley. RELEASE 时，feign-core 和 feign-hystrix 两个
JAR 包的版本号都为“ 9. 5. 1 ”。
HystrixInvocationHandler 是具备 RPC 保护能力的调用处理器，实现了 InvocationHandler 接口，
对接口的 invoke (...) 抽象方法的实现如下：
packagefeign. hystrix;
//省略 import
finalclassHystrixInvocationHandlerimplementsInvocationHandler{
...
//Map 映射：key 为 RPC 方法的反射实例，value 为方法处理器
privatefinalMap<Method,MethodHandler>dispatch;
//...


publicObjectinvoke (Objectproxy, finalMethodmethod, finalObject[]args) throws
Throwable{
//创建一个 HystrixCommand 命令，对同步方法调用器进行封装
HystrixCommand<Object>hystrixCommand=
newHystrixCommand<Object>((Setter) this.setterMethodMap.get (method))
{
protectedObjectrun () throwsException{
try{
SynchronousMethodHandlerhandler=
HystrixInvocationHandler.this.dispatch.get (method)；
returnhandler.invoke (args);
}catch (Exceptionvar 2 ){
throwvar 2 ;
}catch (Throwablevar 3 ){
throw (Error) var 3 ;
}
}
protectedObjectgetFallback (){
//省略 HystrixCommand 的异常回调
}
};
//根据 method 的返回值的类型，或返回 hystrixCommand，或则直接执行
if (this.isReturnsHystrixCommand (method)){
returnhystrixCommand;
}elseif (this.isReturnsObservable (method)){
returnhystrixCommand.toObservable ();
}elseif (this.isReturnsSingle (method)){
returnhystrixCommand.toObservable (). toSingle ();
}else{
//直接执行
returnthis.isReturnsCompletable (method)?
hystrixCommand.toObservable (). toCompletable (): hystrixCommand.execute ();
}
...
}
HystrixInvocationHandler 调用处理器与默认调用处理器 FeignInvocationHandler 有一个共同点：
都有一个非常重要的 Map 类型的成员 dispatch 映射，用于保存 RPC 方法反射实例到 MethodHandler 方
法处理器的映射。
在源码中，HystrixInvocationHandler 的 invoke (...) 方法会创建 hystrixCommand 命令实例，对从
dispatch 获取的 SynchronousMethodHandler 实例进行封装，然后对 RPC 方法实例 method 进行判断，判
断是直接返回 hystrixCommand 命令实例还是立即执行其 execute () 方法。默认情况下，都是立即执行
其 execute () 方法。
HystrixCommand 具备熔断、隔离、回退等能力，如果其 run () 方法执行时发生异常，会执行
getFallback () 失败回调方法，这一点后面会详细介绍。
回到 uaa-provider 服务中 DemoClient 的动态代理实例的 hello () 方法具体执行过程，在执行命令处
理器 hystrixCommand 实例的 run () 方法时，步骤如下：

1 ）根据 RPC 方法 DemoClient.hello () 的反射实例，在 dispatch 映射对象中找到对应的方法处理器
MethodHandler 实例。
2 ）调用 MethodHandler 方法处理器的 invoke (...) 方法完成实际的 hello () 方法所配置的远程 URL 的
HTTP 请求和结果的处理。


如果 MethodHandler 内的 RPC 调用出现异常了，比如远程服务器宕机、网络延迟太大而导致请
求超时、远程服务器来不及响应等，hystrixCommand 命令器将调用失败回调方法 getFallback () 返回
回退结果。而 hystrixCommand 的 getFallback () 方法最终会调用配置在 RPC 接口@FeignClient 注解的
fallback 属性上的失败回退类中的对应的回退方法，执行业务级别的失败回退处理。
使用 HystrixInvocationHandler 方法处理器进行远程调用，总体流程上与使用默认的方法处理器
FeignInvocationHandler 进行远程调用是大致相同的。
还是以 uaa-provider 模块中的 DemoClient 中 hello () 方法的远程调用执行过程为例，进行整体流程
的展示，具体的时序图如图 4 - 26 所示。

图 4 - 26 与 HystrixInvocationHandler 相关的远程调用执行流程
总体来说，使用 HystrixInvocationHandler 处理器的执行流程与使用 FeignInvocationHandler 默认
的调用处理器大致是相同的，不同的是，HystrixInvocationHandler 增加了 RPC 的保护机制。

#### 4. 5. 3 Feign 远程调用的完整流程及其特性

Feign 是一个声明式的 RPC 调用组件，它整合了 Ribbon 和 Hystrix，使得服务调用更加简单。Feign
提供了 HTTP 请求的模板，通过编写简单的接口和方法注解就可以定义 HTTP 请求的参数、格式、地
址等信息。Feign 极大地简化了 RPC 远程调用，可以像调用普通方法那样完成 RPC 远程调用。
Feign 远程调用的核心就是通过一系列的封装和处理，将以 Java 注解方式定义的 RPC 方法最终
转换成 HTTP 请求，然后将 HTTP 请求的响应结果解码成 POJO 对象，返回给调用者。
Feign 远程调用的完整流程大致如图 4 - 27 所示。
从图 4 - 27 可以看到，Feign 通过对 RPC 注解的解析将请求模板化。在实际调用时传入参数，根
据参数再应用到请求模板上，进而转化成真正的 Request 请求。
有了 Feign 以及动态代理机制，Java 开发人员不用再通过使用 HTTP 框架封装 HTTP 请求报文的
方式完成远程服务的 HTTP 调用。


图 4 - 27 Feign 远程调用的完整流程
SpringCloudFeign 具有如下特性：
1 ）可插拔的注解支持，包括 Feign 注解和 SpringMvc 注解。
2 ）支持可插拔的 HTTP 编码器和解码器。
3 ）支持 Hystrix 和它的 RPC 保护机制。
4 ）支持 Ribbon 的负载均衡。
5 ）支持 HTTP 请求和响应的压缩。
总体来说，SpringCloudFeign 组件本身整合了 Ribbon 和 Hystrix，使用它可以设计一套稳定可靠
的弹性客户端调用方案，避免整个系统出现雪崩效应。

### 4. 6 HystrixFeign 动态代理实例的创建流程

SpringCloud 中使用 Hystrix 进行 RPC 保护基本是必选项，所以这里专门用一个小节重点介绍一
下 HystrixFeign 相关的动态代理实例的创建流程。
HystrixInvocationHandler 具体的替换过程是通过 HystrixFeign. Builder 建造者容器实例的 build ()
方法完成的。


#### 4. 6. 1 HystrixFeign. Builder 建造者容器实例

首先，复习一下 Feign 中 JDK 代理实例创建的整体流程。前面讲到，Feign 中默认的远程接口的
JDK 动态代理实例的创建是通过 Feign. Builder 建造者容器实例的 target (...) 方法完成的。而 target (...)
方法的第一步是通过自身的 build () 方法来构造一个 ReflectiveFeign（反射式 Feign）实例；第二步是
通过反射式 Feign 实例的 newInstance () 方法创建真正的 JDKProxy 代理实例。
HystrixFeign 有自己的建造者类，即 HystrixFeign. Builder 类，该类继承了 feign. Feign. Builder 默认
建造者，重写了其获得 Feign 实例的 build () 方法。
HystrixFeign 的关键源码如下：
packagefeign. hystrix;
//省略 import
publicfinalclassHystrixFeign{
publicHystrixFeign (){
}
//创建一个新的 HystrixFeign. Builder 实例
publicstaticHystrixFeign.Builderbuilder (){
returnnewHystrixFeign.Builder ();
}
//HystrixFeign 的建造者类
//继承了 Feign 默认的建造者，重写了 build () 方法
publicstaticfinalclassBuilderextendsfeign. Feign. Builder{
publicFeignbuild (){
returnthis.build ((FallbackFactory) null);
}
//重载的 build 方法替换了基类的 invocationHandlerFactory
//然后调用基类的 build () 方法，建造一个 ReflectiveFeign（反射式 Feign）实例
Feignbuild (finalFallbackFactory<?>nullableFallbackFactory){
super.invocationHandlerFactory (newInvocationHandlerFactory (){
//实现 InvocationHandlerFactory 的 create 方法
publicInvocationHandlercreate (Targettarget, Map<Method,MethodHandler>
dispatch)
{
//返回的是 HystrixInvocationHandler
returnnewHystrixInvocationHandler (
target, dispatch, Builder. this. setterFactory,
nullableFallbackFactory);
}
});
super.contract (newHystrixDelegatingContract (this. contract));
returnsuper.build ();
}
}
}
HystrixFeign. Builder 类继承了默认的 feign. Feign. Builder 建造者类，创建一个匿名的调用处理器
工厂实例，该工厂在创建调用处理器时，使用 HystrixInvocationHandler 替换基类中用到的默认调用
处理器 FeignInvocationHandler。
另外，在 HystrixFeign. Builder 重载的 build () 方法中最终返回的仍然是基类的 build () 方法，当然
返回的还是一个 ReflectiveFeign（反射式 Feign）实例。
注意，HystrixFeign 并不是 Feign 的子类，这一点不像 Feign 的子类 ReflectiveFeign，所以在创建
RPC 动态代理实例时，仍然会用到 ReflectiveFeign.newInstance () 方法。


在 ReflectiveFeign.newInstance () 方法创建 RPC 动态代理实例时，会通过调用处理器工厂的
create () 方法创建 InvocationHandler 调用处理器实例。而此时，被替换过的处理器工厂将创建带 RPC
保护功能的 HystrixInvocationHandler 类型的调用处理器。

#### 4. 6. 2 配置 HystrixFeign. Builder 建造者容器实例

HystrixFeign. Builder 实例替换 feign. Feign. Builder 实例，编写完成 FeignClientsConfiguration 自动
配置类的源码。相关的自动配置类 FeignClientsConfiguration 的部分源码如下：
packageorg. springframework. cloud. openfeign;
//省略 import
@Configuration
publicclassFeignClientsConfiguration{
//省略其他代码
@Configuration
@ConditionalOnClass ({HystrixCommand. class, HystrixFeign. class})
protectedstaticclassHystrixFeignConfiguration{
protectedHystrixFeignConfiguration (){
}
//创建了一个 HystrixFeign. Builder 类型的 SpringIOC 实例
@Bean
@Scope ("prototype")
@ConditionalOnMissingBean
@ConditionalOnProperty (
name={"feign. hystrix. enabled"}
)
publicBuilderfeignHystrixBuilder (){
returnHystrixFeign.builder ();
}
}
}
通过上面的源码可以看出，创建一个 HystrixFeign. Builder 类型的 SpringIOC 实例，实质上必须
同时满足以下两个条件：

1 ）在类路径中同时存在 HystrixCommand. class、HystrixFeign. class 两个类。
2 ）应用配置文件中存在着 feign. hystrix. enabled 的配置项。
满足以上条件，feignHystrixBuilder () 会调用 HystrixFeign.builder () 静态方法创建一个新的
HystrixFeign. Builder 类型的 SpringIOC 实例。
HystrixFeign. Builder 容器实例注册之后，在创建 JDK 动态代理实例时，基类 Feign. Builder 建造
者的 target () 方法会调用子类 HystrixFeign. Builder 实例的 build () 方法，完成调用处理器工厂
InvocationHandlerFactory 实例的替换。

### 4. 7 feign. Client 客户端容器实例

```
前面介绍到了常用的 Feign 客户端实现类，大致如下：
1 ）Client. Default 实现类：默认的实现类，使用 JDK 的 HttpURLConnnection 类提交 HTTP 请求。
2 ）ApacheHttpClient 实现类：该客户端实现类在内部使用 ApacheHttpClient 开源组件提交 HTTP
```

###### 请求。

3 ）OkHttpClient 实现类：该客户端实现类在内部使用 OkHttp 3 开源组件提交 HTTP 请求。
4 ）LoadBalancerFeignClient 实现类：内部使用 Ribbon 负载均衡技术完成 HTTP 请求处理。
Feign 在启动时，有两个与 feign. Client 客户端实例相关的自动配置类，根据多种条件组合去装
配不同类型的 feign. Client 客户端实例到 SpringIOC 容器，这两个自动配置类为：

```
1 ）FeignRibbonClientAutoConfiguration。
2 ）FeignAutoConfiguration。
```
#### 4. 7. 1 装配 LoadBalancerFeignClient 负载均衡容器实例

详细来看，Feign 涉及的与 Client 相关的两个自动配置类具体如下：
（ 1 ）org. springframework. cloud. openfeign. ribbon. FeignRibbonClientAutoConfiguration
此自动配置类能够配置具有负载均衡能力的 FeignClient 容器实例。
（ 2 ）org. springframework. cloud. openfeign. FeignAutoConfiguration
此自动配置类只能配置最原始的 FeignClient 客户端容器实例。
事实上，第一个自动配置类 FeignRibbonClientAutoConfiguration，在容器的装配次序上是优先
于第二个自动配置类 FeignAutoConfiguration 的。
为了达到高可用，SpringCloud 中一个微服务提供者至少应该部署两个以上节点，从这个角度
来说，LoadBalancerFeignClient 容器实例已经成为事实上的标配。
具体可以参见 FeignRibbonClientAutoConfiguration 源码，节选如下：
importcom. netflix. loadbalancer. ILoadBalancer;
...
@ConditionalOnClass ({ILoadBalancer. class, Feign. class})
@Configuration
@AutoConfigureBefore ({FeignAutoConfiguration. class}) //本配置类具备优先权
@EnableConfigurationProperties ({FeignHttpClientProperties. class})
@Import ({
HttpClientFeignLoadBalancedConfiguration. class, //配置：包装 ApacheHttpClient 实例的负载
均衡客户端
OkHttpFeignLoadBalancedConfiguration. class,//配置：包装 OkHttpClient 实例的负载均衡客户端
DefaultFeignLoadBalancedConfiguration. class//配置：包装 Client. Default 实例的负载均衡客户端
})
publicclassFeignRibbonClientAutoConfiguration{
//空的构造器
publicFeignRibbonClientAutoConfiguration (){
}
...
}
从源码中可以看到，FeignRibbonClientAutoConfiguration 的自动配置有两个前提条件：
1 ）当前的类路径中存在 ILoadBalancer. class 接口。
2 ）当前的类路径中存在 Feign. class 接口。
在这里，重点说一下 ILoadBalancer. class 接口，它处于 Ribbon 的 JAR 包中。如果需要在类路径中
导入该 JAR 包，则需要在 Maven 的 pom. xml 文件中增加 Ribbon 的相关依赖，具体如下：


<!--ribbon-->
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-netflix-ribbon</artifactId>
</dependency>
为了加深大家对客户端负载均衡的理解，这里将 ILoadBalancer. class 接口的两个重要的抽象方
法列出来，具体如下：
packagecom. netflix. loadbalancer;
importjava. util. List;
publicinterfaceILoadBalancer{
//通过负载均衡算法计算服务器
ServerchooseServer (Objectvar 1 );
//取得全部的服务器
List<Server>getAllServers ();
...
}
FeignRibbonClientAutoConfiguration 自动配置类并没有直接配置 LoadBalancerFeignClient 容器
实例，而是使用@Import 注解。通过导入其他配置类的方式完成 LoadBalancerFeignClient 客户端容器
实例的配置。
分别导入了以下三个自动配置类：
（ 1 ）HttpClientFeignLoadBalancedConfiguration. class
该配置类负责配置一个包装 ApacheHttpClient 实例的 LoadBalancerFeignClient 负载均衡客户端
容器实例。

（ 2 ）OkHttpFeignLoadBalancedConfiguration. class
该配置类负责配置一个包装 OkHttpClient 实例的 LoadBalancerFeignClient 负载均衡客户端容器
实例。

（ 3 ）DefaultFeignLoadBalancedConfiguration. class
该配置类负责配置一个包装 Client. Default 实例的 LoadBalancerFeignClient 负载均衡客户端容器
实例。

#### 4. 7. 2 包装 ApacheHttpClient 实例的负载均衡客户端装配

首先来看如何配置一个包装 ApacheHttpClient 实例的负载均衡客户端容器实例。这个 IOC 实例
的配置由 HttpClientFeignLoadBalancedConfiguration 自动配置类完成，其源码节选如下：
@Configuration
@ConditionalOnClass ({ApacheHttpClient. class})
@ConditionalOnProperty (
value={"feign. httpclient. enabled"},
matchIfMissing=true
)
classHttpClientFeignLoadBalancedConfiguration{
//空的构造器
HttpClientFeignLoadBalancedConfiguration (){
}
@Bean
@ConditionalOnMissingBean ({Client. class})


publicClientfeignClient (
CachingSpringLoadBalancerFactorycachingFactory,
SpringClientFactoryclientFactory, HttpClienthttpClient)
{
ApacheHttpClientdelegate=newApacheHttpClient (httpClient);
returnnewLoadBalancerFeignClient (delegate, cachingFactory, clientFactory);//
进行包装
}
//省略不相干的代码
}
首先来看源码中的 feignClient () 方法，分为两步：
1 ）创建一个 ApacheHttpClient 类型的 feign. Client 客户端实例，该实例的内部使用 ApacheHttpclient
开源组件完成 HTTP 请求处理。
2 ）创建一个 LoadBalancerFeignClient 负载均衡客户端实例，将 ApacheHttpClient 实例包装起来，
然后返回该包装实例，作为 feign. Client 类型的 SpringIOC 容器实例。

接下来，介绍一下 HttpClientFeignLoadBalancedConfiguration 类上的两个重要的注解：
1 ）@ConditionalOnClass (ApacheHttpClient. class)。
2 ）@ConditionalOnProperty (value="feign. httpclient. enabled", matchIfMissing=true)。
这两个注解包含以下两个条件：
1 ）必须满足 ApacheHttpClient. class 在当前的类路径中存在。
2 ）必须满足工程配置文件中 feign. httpclient. enabled 配置项的值为 true。
如果以上两个条件同时满足，HttpClientFeignLoadBalancedConfiguration 自动配置工作就会启动。
具体如何验证呢？首先在应用配置文件中将配置项 feign. httpclient. enabled 的值设置为 false，然
后在 HttpClientFeignLoadBalancedConfiguration 的 feignClient () 方法内的某行上打上断点，重新启动
项目，注意观察，会发现整个启动过程中断点没有被命中。
接下来，将配置项 feign. httpclient. enabled 的值设置为 true，再一次启动项目，发现断点被命中。
由此可见，验证 HttpClientFeignLoadBalancedConfiguration 自动配置类被启动。
为了满足@ConditionalOnClass (ApacheHttpClient. class) 的条件要求，需要在 pom 文件加上
feign-httpclient 以及 httpclient 组件相关的 Maven 依赖，具体如下：
<dependency>
<groupId>io. github. openfeign</groupId>
<artifactId>feign-httpclient</artifactId>
<version> 9. 5. 1 </version>
<!--<version>${feign-httpclient. version}</version>-->
</dependency>
<dependency>
<groupId>org. apache. httpcomponents</groupId>
<artifactId>httpclient</artifactId>
<version>${httpclient. version}</version>
</dependency>
对于 feign. httpclient. enabled 配置项来说，@ConditionalOnProperty 的 matchIfMissing 属性值默认
为 true，也就是说，这个属性在默认的情况下就为 true。


#### 4. 7. 3 包装 OkHttpClient 实例的负载均衡客户端实例

接下来看如何配置一个包装 OkHttpClient 实例的负载均衡客户端容器实例。这个 IOC 实例的配
置由 OkHttpFeignLoadBalancedConfiguration 自动配置类负责完成，其源码节选如下：
@Configuration
@ConditionalOnClass ({OkHttpClient. class})
@ConditionalOnProperty ("feign. okhttp. enabled")
classOkHttpFeignLoadBalancedConfiguration{
//空的构造器
OkHttpFeignLoadBalancedConfiguration (){
}
@Bean
@ConditionalOnMissingBean ({Client. class})
publicClientfeignClient (
CachingSpringLoadBalancerFactorycachingFactory,
SpringClientFactoryclientFactory, HttpClienthttpClient)
{
OkHttpClientdelegate=newOkHttpClient (httpClient);
returnnewLoadBalancerFeignClient (delegate, cachingFactory, clientFactory);//
进行包装
}
//省略不相干的代码
}
首先来看源码中的 feignClient () 方法，分为两步：
1 ）创建一个 OkHttpClient 类型的客户端实例，该实例的内部使用 OkHttp 3 开源组件完成 HTTP
请求处理。
2 ）创建一个 LoadBalancerFeignClient 负载均衡客户端实例，将 OkHttpClient 实例包装起来，然
后返回 LoadBalancerFeignClient 客户端实例、feign. Client 客户端 IOC 容器实例。

接下来，介绍一下 OkHttpFeignLoadBalancedConfiguration 类上的两个重要的注解：
1 ）@ConditionalOnClass (OkHttpClient. class)。
2 ）@ConditionalOnProperty ("feign. okhttp. enabled")。
这两个注解包含以下两个条件：
1 ）必须满足 OkHttpClient. class 在当前类路径中存在。
2 ）必须满足工程配置文件中 feign. okhttp. enabled 配置项的值为 true。
如果以上两个条件同时满足，则 OkHttpFeignLoadBalancedConfiguration 自动配置工作就会启动。
为了满足@ConditionalOnClass (OkHttpClient. class) 的条件要求，并且由于 OkHttpClient. class 类
的位置处于 feign-okhttp 相关的 JAR 包中，因此需要在 pom 文件加上 feign-okhttp 以及 OkHttp 3 相关的
Maven 依赖，具体如下：
<!--OkHttp-->
<dependency>
<groupId>com. squareup. okhttp 3 </groupId>
<artifactId>okhttp</artifactId>
</dependency>


<!--feign-okhttp-->
<dependency>
<groupId>io. github. openfeign</groupId>
<artifactId>feign-okhttp</artifactId>
</dependency>
对于 feign. okhttp. enabled 配置项设置，在默认的情况下就为 false。也就是说，如果需要使用
feign-okhttp，则一定需要做特别的配置，工程配置文件的配置项大致如下：
feign. httpclient. enabled=false
feign. okhttp. enabled=true

#### 4. 7. 4 包装 Client. Default 实例的负载均衡客户端实例

最后来看如何配置一个包装 Client. Default 客户端实例的负载均衡客户端容器实例。这个 IOC 实
例的配置由 DefaultFeignLoadBalancedConfiguration 自动配置类负责完成。该配置类其实就是
FeignRibbonClientAutoConfiguration 配置类通过@import 注解导入的第 3 个配置类。
DefaultFeignLoadBalancedConfiguration 的源码节选如下：
packageorg. springframework. cloud. openfeign. ribbon;
//省略 import
@Configuration
classDefaultFeignLoadBalancedConfiguration{
DefaultFeignLoadBalancedConfiguration (){
}
@Bean
@ConditionalOnMissingBean
publicClientfeignClient (CachingSpringLoadBalancerFactorycachingFactory,
SpringClientFactoryclientFactory)
{
returnnewLoadBalancerFeignClient (newDefault ((SSLSocketFactory) null,
(HostnameVerifier) null), cachingFactory,
clientFactory);
}
}
通过源码可以看出，如果前面的两个客户端自动配置类的条件没有满足，IOC 容器中没有
feign. Client 客户端容器实例，则创建一个默认的客户端实例：

1 ）创建一个 Client. Default 默认客户端实例，该实例将使用 HttpURLConnnection 完成请求处理。
2 ）创建一个 LoadBalancerFeignClient 负载均衡客户端实例，将 Client. Default 实例包装起来，然
后返回 LoadBalancerFeignClient 客户端实例，作为 feign. Client 类型的 SpringIOC 容器实例。

最后小结一下本章的内容。本章通过对 SpringCloud 中 Feign 核心原理和实现机制的解读，帮助
读者深入彻底地了解 SpringCloud 底层原理。
本章层层递进，抽丝剥茧，着重介绍了远程接口的 JDKProxy 代理实例的创建、Feign 远程接口
调用的两大执行流程。
本章虽然借助了 SpringCloud 的源码，但并没有迷失在源码中，更加注重的还是原理的分析和
阐述。最终的结果是让大家既学习了 SpringCloud 的原理，也阅读了 SpringCloud 的源码，并且通过
源码的学习，领悟一些 Java 高手编程时所用到的设计模式和代码组织方式。


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
Feign 原理（图解）
https://www.cnblogs.com/crazymakercircle/p/ 11965726 .html
Feign-独立使用-实战
https://www.cnblogs.com/crazymakercircle/p/ 10243749 .html
Feign、httpclient、OkHttp 3 结合使用
https://www.cnblogs.com/crazymakercircle/p/ 11968479 .html
Feign RibbonHystrix 三者关系 |史上最全, 深度解析
https://www.cnblogs.com/crazymakercircle/p/ 11664812 .html

##### 3. 相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题 ：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 5 章 RxJava 响应式编程框架

在 SpringCloud 框架中涉及的 Ribbon 和 Hystrix 这两个重要的组件都用到了 RxJava，RxJava 是响
应式编程框架，作为重要的编程基础知识特开辟一章对 RxJava 的使用做详细的介绍。
Hystrix 和 Ribbon 的代码中大量运用了 RxJava 的 API，对于有 RxJava 基础的同学，学习 Hystrix 和
Ribbon 并不是一件难事。如果不懂 RxJava，对于 Hystrix 和 Ribbon 的学习会令人头疼不已。

5. (^1) 从基础原理讲起：观察者模式

###### 本书的重要特色是从基础原理讲起。只有了解基础原理之后，大家对新的知识特别是复杂的

###### 知识才能做到更加容易地理解和掌握。

RxJava 是基于观察者模式实现的，这里先带领大家复习一下观察者模式的基础原理和经典实
现。当然，这也是 Java 工程师面试必备的一个重要知识点。

#### 5. 1. 1 观察者模式的基础原理

观察者模式是常用的设计模式之一，是所有 Java 工程师必须掌握的设计模式。观察者模式也叫
发布订阅模式。
此模式的角色中，首先有一个可观察的主题对象 Subject，然后有多个观察者 Observer 去关注它。
当 Subject 的状态发生变化时，它会自动通知这些 Observer 订阅者，令 Observer 做出响应。
在整个观察者模式中，一共有四个角色：Subject（抽象主题、抽象被观察者）、ConcreteSubject
（具体主题、具体被观察者）、Observer（抽象观察者）以及 ConcreteObserver（具体观察者）。
观察者模式的四个角色以及它们之间的关系具体如图 5 - 1 所示。

```
图^5 -^1 观察者模式的四个角色以及它们之间的关系
观察者模式中 4 个角色的介绍如下：
```

（ 1 ）Subject（抽象主题）
Subject 抽象主题的主要职责之一是维护 Observer 观察者对象的集合，集合里的所有观察者都订
阅了该主题。Subject 抽象主题负责提供一些接口，可以增加、删除和更新观察者对象。

（ 2 ）ConcreteSubject（具体主题）
ConcreteSubject 用于保持主题的状态，并且在主题的状态发生变化时，给所有注册过的观察者
发出通知。具体来说，ConcreteSubject 需要调用 Subject（抽象主题）基类的通知方法，给所有注册
过的观察者发出通知。

（ 3 ）Observer（抽象观察者）
观察者的抽象类定义更新接口，使得被观察者可以在收到主题通知的时候，更新自己的状态。
（ 4 ）ConcreteObserver（具体观察者）
实现抽象观察者 Observer 所定义的更新接口，以便在收到主题通知时完成自己的状态的真正
更新。

#### 5. 1. 2 观察者模式的经典实现

首先来看 Subject 主题类的代码实现：它将所有订阅过自己的 Observer 观察者对象保存在一个集
合中，然后提供一组方法完成 Observer 观察者的新增、删除和通知。
Subject 主题类的参考实现代码大致如下：
packagecom. crazymaker. demo. observerPattern;
importlombok. extern. slf 4 j.Slf 4 j;
importjava. util. ArrayList;
importjava. util. List;
@Slf 4 j
publicclassSubject{
//保存订阅过自己的观察者对象
privateList<Observer>observers=newArrayList<>();
//观察者对象订阅
publicvoidadd (Observerobserver){
observers.add (observer);
log.info ("addanobserver");
}
//观察者对象注销
publicvoidremove (Observerobserver){
observers.remove (observer);
log.info ("removeanobserver");
}
//通知所有注册的观察者对象
publicvoidnotifyObservers (StringnewState){
for (Observer:observers){
observer.update (newState);
}
}
}
接着来看看 ConcreteSubject 具体主题类：它首先拥有一个成员用于保持主题的状态，并且在主
题的状态发生变化时去调用基类 Subject（抽象主题）的通知方法，给所有注册过的观察者发出通知。


packagecom. crazymaker. demo. observerPattern;
importlombok. extern. slf 4 j.Slf 4 j;
@Data
@Slf 4 j
publicclassConcreteSubjectextendsSubject{
privateStringstate;//保持主题的状态
publicvoidchange (StringnewState){
state=newState;
log.info ("changestate: "+newState);
//状态发生改变，通知观察者
notifyObservers (newState);
}
}
然后来看一下观察者 Observer 接口，它抽象出了一个观察者自身的状态更新方法。
packagecom. crazymaker. demo. observerPattern;
publicinterfaceObserver{
voidupdate (StringnewState); //状态更新的方法
}
最后来看看 ConcreteObserver 具体观察者类：它首先接收主题的通知，实现抽象观察者 Observer
所定义的 update 更新接口，以便在接收到主题的状态变化时完成自己的状态更新。
packagecom. crazymaker. demo. observerPattern;
importlombok. extern. slf 4 j.Slf 4 j;
@Slf 4 j
publicclassObserverAimplementsObserver{
//观察者状态
privateStringobserverState;
@Override
publicvoidupdate (StringnewState){
//更新观察者状态，让它与主题的状态一致
observerState=newState;
log.info ("目前的观察者的状态为："+observerState);
}
}
4 个角色的实现代码已经介绍完了。如何使用观察者模式呢？步骤如下：
packagecom. crazymaker. demo. observerPattern;
publicclassObserverPatternDemo{
publicstaticvoidmain (String[]args){
//第一步：创建主题
ConcreteSubjectmConcreteSubject=newConcreteSubject ();
//第二步：创建观察者
ObserverobserverA=newObserverA ();
ObserverObserverB=newObserverA ();
//第三步：主题订阅
mConcreteSubject.add (observerA);
mConcreteSubject.add (ObserverB);
//第四步：主题状态变更
mConcreteSubject.change ("倒计时结束，开始秒杀");
}
}
运行示例程序，结果如下：


```
22 : 46 : 03. 548 [main]INFO c.c.d.o.ConcreteSubject-changestate: 倒计时结束，开始秒杀
22 : 46 : 03. 548 [main]INFO c.c.d.o.ObserverA-目前的观察者的状态为：倒计时结束，开始秒杀！
22 : 46 : 03. 548 [main]INFO c.c.d.o.ObserverA-目前的观察者的状态为：倒计时结束，开始秒杀！
```
#### 5. 1. 3 Rxjava 中的观察者模式

RxJava 是基于观察者模式设计的。RxJava 中的 Observable 类、Subscriber 类分别对应于观察者模
式中的 Subject（抽象主题）和 Observer（抽象观察者）两个角色。
RxJava 中，Observable 和 Subscriber 通过 subscribe () 方法实现订阅关系，具体如图 5 - 2 所示。

图 5 - 2 RxJava 通过 subscribe () 方法实现订阅关系
RxJava 中，Observable 和 Subscriber 之间通过 emitter.onNext (...) 弹射的方式实现主题的消息发布，
具体如图 5 - 3 所示。

图 5 - 3 RxJava 通过 emitter.onNext () 弹射主题消息
RxJava 中主题的消息发布方式之一是通过内部的弹射器（Emitter）完成。弹射器除了调用
onNext () 方法弹射消息之外，还定义了两个特殊的通知方法：onCompleted () 和 onError ()。具体介绍
如下：

（ 1 ）onCompleted ()：表示消息序列弹射完结
RxJava 中主题（可观察者）中的弹射器可以不止发布（弹射）一个消息，可以重复调用它的
onNext () 方法弹射一系列的消息（或事件），这一系列的消息组成一个序列；在绝大部分场景下，
Observable 内部有一个专门的队列来负责缓存消息序列。当弹射器明确不会再有新的消息弹射出来
时，需要触发 onCompleted () 方法，作为消息序列的结束标志。
主题（可观察者）的弹射器所弹出的消息序列，也可以称之为消息流。
（ 2 ）onError ()：表示主题的消息序列异常终止
如果 Observable 在事件处理过程中出异常时，弹射器的 onError () 方法会被触发，同时消息序列
自动终止，不允许再有消息弹射出来。

```
RxJava 的一个简单的示例代码如下：
packagecom. crazymaker. demo. observerPattern;
//省略 import
```

@Slf 4 j
publicclassRxJavaObserverDemo{
/**
*演示 RxJava 中的 Observer 模式
*/
@Test
publicvoidrxJavaBaseUse (){
//被观察者（主题）
Observableobservable=Observable.create (
newAction 1 <Emitter<String>>(){
@Override
publicvoidcall (Emitter<String>emitter){
emitter.onNext ("apple");
emitter.onNext ("banana");
emitter.onNext ("pear");
emitter.onCompleted ();
}
}, Emitter. BackpressureMode. NONE);
//订阅者（观察者）
Subscriber<String>subscriber=newSubscriber<String>(){
@Override
publicvoidonNext (Strings){
log.info ("onNext:{}", s);
}
@Override
publicvoidonCompleted (){
log.info ("onCompleted");
}
@Override
publicvoidonError (Throwablee){
log.info ("onError");
}
};
//订阅：Observable 与 Subscriber 之间依然通过 subscribe () 进行关联
observable.subscribe (subscriber);
}
}
运行程序，结果如下：
11 : 29 : 07. 555 [main]INFO c.c.d.o.RxJavaObserverDemo-onNext:apple
11 : 29 : 07. 564 [main]INFO c.c.d.o.RxJavaObserverDemo-onNext:banana
11 : 29 : 07. 564 [main]INFO c.c.d.o.RxJavaObserverDemo-onNext:pear
11 : 29 : 07. 564 [main]INFO c.c.d.o.RxJavaObserverDemo-onCompleted
通过代码和运行接口可以看出：被观察者 Observable 与观察者 Subscriber 通过 subscribe () 方法产
生关联。当订阅开始时，Observable 主题便开始发送事件。
通过代码还可以看出：Subscriber 有 3 个回调方法。其中，onNext (Strings) 回调方法用于响应
Observable 主题的正常的弹射消息；onCompleted () 回调方法用于响应 Observable 主题的结束消息；
onError (Throwablee) 回调方法用于响应 Observable 主题的异常消息。
在一个消息序列中，弹射器的 onCompleted () 正常结束和 onError () 异常终止只能调用一个，并且
必须是消息序列中的最后一个被发送的消息。换句话说，弹射器的 onCompleted () 和 onError () 两个方
法是互斥的，在消息序列中调用了其中一个，就不可以再调用另一个。
通过示例可以看出 RxJava 与经典的观察者模式的不同。RxJava 中，在主题内部有一个弹射器


###### 的角色；而经典的观察者模式，主题所发送的是单个的消息，并不是一个消息序列。

在 RxJava 中，Observable 主题还会负责消息序列缓存，这一点像经典的生产者消费者模式。在
经典的生产者消费者模式中，生产者生产数据后放入缓存队列，自己不去做处理，而消费者从缓存
队列里拿到所要处理的数据，完成逻辑处理。从这一点来说，RxJava 同时也借鉴了生产者消费者模
式的思想。

#### 5. 1. 4 RxJava 的不完整回调

Java 8 引入函数式编程方式大大地提高了编码效率。但是，Java 8 的函数式编程有一个非常重
要的要求：需要函数式接口作为支撑。什么是函数式接口呢？它指的是有且只有一个抽象方法的接
口，比如 Java 中内置的 Runnable 接口。
RxJava 的一大特色就是支持函数式的编程。由于标准的 Subscriber 观察者接口有 3 个抽象方法，
当然就不是一个函数式接口，因此直接使用 Subscriber 观察者接口是不支持函数式编程的。
RxJava 为了支持函数式编程，另外定义几个函数式接口。比较重要的有 Action 0 、Action 1 。
（ 1 ）Action 0 回调接口
这是一个无参数的、无返回值的函数式接口。源码如下：
packagerx. functions;
/**
*Azero-argumentaction.
*/
publicinterfaceAction 0 extendsAction{
voidcall ();
}
Action 0 接口的 call () 方法无参数、无返回值。其具体的使用场景可以对应于 Subscriber 观察者中
的 onCompleted () 回调方法的使用场景，因为 Subscriber 的 onCompleted () 回调方法也是无参数、无返
回值的。

（ 2 ）Action 1 回调接口
这是一个有 1 个参数、泛型、无返回值的函数式接口。源码如下：
packagerx. functions;
/**
*Aone-argumentaction.
*@param<T>thefirstargumenttype
*/
publicinterfaceAction 1 <T>extendsAction{
voidcall (Tt);
}
Action 1 回调接口的具体实现主要有以下两种用途：
1 ）作为函数式编程去替代使用 Subscriber 的 onNext () 方法的传统编程，前提是 Action 1 回调接口
的泛型类型与 Subscriber 的 onNext () 回调方法的参数类型一致。
2 ）作为函数式编程去替代使用 Subscriber 的 onErrorAction（Throwablee）方法的传统编程，前
提是 Action 1 回调接口的泛型类型与 Subscriber 的 onErrorAction () 回调方法的参数类型一致。


Action 1 接口所承担的主要是观察者（订阅者）角色，所以 RxJava 也为主题类提供了重载的
subscribe (Action 1 action) 订阅方法，可以接收一个 Action 1 回调接口的实现对象作为弹射消息序列的
订阅者。
下面使用不完整回调实现前一个小节的例子，大家可以对比一下。具体的源码如下：
packagecom. crazymaker. demo. observerPattern;
//省略 import
@Slf 4 j
publicclassRxJavaObserverDemo{
/**
*演示 RxJava 中的不完整观察者
*/
@Test
publicvoidrxJavaActionDemo (){
//被观察者（主题）
Observableobservable=Observable.create (
newAction 1 <Emitter<String>>(){
@Override
publicvoidcall (Emitter<String>emitter){
emitter.onNext ("apple");
emitter.onNext ("banana");
emitter.onNext ("pear");
emitter.onCompleted ();
}
}, Emitter. BackpressureMode. NONE);
Action 1 <String>onNextAction=newAction 1 <String>(){
@Override
publicvoidcall (Strings){
log.info (s);
}
};
Action 1 <Throwable>onErrorAction=newAction 1 <Throwable>(){
@Override
publicvoidcall (Throwablethrowable){
log.info ("onError, ErrorInfois: "+throwable.getMessage ());
}
};
Action 0 onCompletedAction=newAction 0 (){
@Override
publicvoidcall (){
log.info ("onCompleted");
}
};
log.info ("第 1 次订阅：");
//根据 onNextAction 来定义 onNext ()
observable.subscribe (onNextAction);
log.info ("第 2 次订阅：");
//根据 onNextAction 来定义 onNext ()、根据 onErrorAction 来定义 onError ()
observable.subscribe (onNextAction, onErrorAction);
log.info ("第 3 次订阅：");
//根据 onNextAction 来定义 onNext ()、根据 onErrorAction 来定义 onError ()
//根据 onCompletedAction 来定义 onCompleted ()
observable.subscribe (onNextAction, onErrorAction, onCompletedAction);
}
}


###### 运行程序，结果如下：

11 : 06 : 22. 015 [main]INFO c.c.d.o.RxJavaObserverDemo-第 1 次订阅：
11 : 06 : 22. 015 [main]INFO c.c.d.o.RxJavaObserverDemo-apple
11 : 06 : 22. 015 [main]INFO c.c.d.o.RxJavaObserverDemo-banana
11 : 06 : 22. 015 [main]INFO c.c.d.o.RxJavaObserverDemo-pear
11 : 06 : 22. 015 [main]INFO c.c.d.o.RxJavaObserverDemo-第 2 次订阅：
11 : 06 : 22. 015 [main]INFO c.c.d.o.RxJavaObserverDemo-apple
11 : 06 : 22. 016 [main]INFO c.c.d.o.RxJavaObserverDemo-banana
11 : 06 : 22. 016 [main]INFO c.c.d.o.RxJavaObserverDemo-pear
11 : 06 : 22. 016 [main]INFO c.c.d.o.RxJavaObserverDemo-第 3 次订阅：
11 : 06 : 22. 016 [main]INFO c.c.d.o.RxJavaObserverDemo-apple
11 : 06 : 22. 016 [main]INFO c.c.d.o.RxJavaObserverDemo-banana
11 : 06 : 22. 016 [main]INFO c.c.d.o.RxJavaObserverDemo-pear
11 : 06 : 22. 016 [main]INFO c.c.d.o.RxJavaObserverDemo–onCompleted
在上面的代码中，Observableobservable 被订阅了 3 次，由于没有异常消息，因此从输出中只能
看到正常消息和结束消息。
总之，RxJava 提供的 Action 0 回调接口、Action 1 回调接口可以看作是 Subscriber 观察者接口的
阉割版和函数式编程版本。使用 RxJava 的不完整回调观察者接口，结合 Java 8 的函数式编程能够编
写出更为简洁和灵动的代码。

#### 5. 1. 5 RxJava 函数式编程

有了 Action 0 和 Action 1 这两个函数式接口，就可以使用 RxJava 进行函数式编程了。下面使用函
数式编程的风格实现前一个小节的例子，大家对比一下。
publicclassRxJavaObserverDemo{
...
/**
*演示 RxJava 中的 Lambda 表达式实现
*/
@Test
publicvoidrxJavaActionLambda (){
Observable<String>observable=Observable.just ("apple","banana","pear");
log.info ("第 1 次订阅：");
//使用 Action 1 函数式实现来定义 onNext 回调
observable.subscribe (s->log.info (s));
log.info ("第 2 次订阅：");
//使用 Action 1 函数式实现来定义 onNext 回调
//使用 Action 1 函数式实现来定义 onError 回调
observable.subscribe (
s->log.info (s),
e->log.info ("ErrorInfois: "+e.getMessage ()));
log.info ("第 3 次订阅：");
//使用 Action 1 函数式实现来定义 onNext 回调
//使用 Action 1 函数式实现来定义 onError 回调
//使用 Action 0 函数式实现来定义 onCompleted 回调
observable.subscribe (
s->log.info (s),
e->log.info ("ErrorInfois: "+e.getMessage ()),
()->log.info ("onCompleted 弹射结束"));
}
}


###### 运行这个示例程序，输出的结果和 5. 1. 4 节的用例程序的输出结果是一致的，所以这里不再赘

述。对比 5. 1. 4 节的程序可以看出，RxJava 的函数式编程比起普通的 Java 编程，会简洁很多。
实际上，RxJava 源码中，在 Observable 类的 subscribe () 订阅方法的重载版本中使用的是一个
ActionSubscriber 包装类实例，对 3 个函数式接口实例进行包装，所以最终的消息订阅者还是一个
Subscriber 类型的实例。
下面是 Observable 类的一个重载的 subscribe (...) 订阅方法的源码，具体如下：
publicfinalSubscriptionsubscribe (finalAction 1 <?superT>onNext,
finalAction 1 <Throwable>onError, finalAction 0 onCompleted)
{
if (onNext==null){
thrownewIllegalArgumentException ("onNextcannotbenull");
}
if (onError==null){
thrownewIllegalArgumentException ("onErrorcannotbenull");
}
if (onCompleted==null){
thrownewIllegalArgumentException ("onCompletecannotbenull");
}
//通过包装类进行包装
returnsubscribe (newActionSubscriber<T>(onNext, onError, onCompleted));
}
上面源码中用到的 ActionSubscriber 类是 Subscriber 接口的一个实现类，主要用于包装三个函数
式接口的实现。

#### 5. 1. 6 RxJava 的 Operators 操作符

RxJava 的 Operators 操作符实质是为了方便数据流的操作，是 RxJava 为 Observable 主题所定义的
一系列函数。
RxJava 的操作符按照其作用具体可分为以下 5 类：
（ 1 ）创建型操作符
创建一个可观察对象 Observable 主题对象，并根据输入参数弹射数据。
（ 2 ）过滤型操作符
从 Observable 弹射的消息流中过滤出满足条件的消息。
（ 3 ）转换型操作符
对 Observable 弹射的消息执行转换操作。
（ 4 ）聚合型操作符
对 Observable 弹射的消息流进行聚合操作，比如统计数量等。

### 5. 2 创建型操作符

创建型操作符用于创建一个可观察对象 Observable 主题对象并弹出数据。RxJava 的创建型操作
符比较多，大致如下：


```
（ 1 ）create ()
使用一个函数从头创建一个 Observable 主题对象。
（ 2 ）defer ()
只有当订阅者订阅时才创建 Observable，为每个订阅创建一个新的 Observable 主题对象。
（ 3 ）range ()
创建一个弹射指定范围的整数序列的 Observable 主题对象。
（ 4 ）interval ()
创建一个按照给定的时间间隔弹射整数序列的 Observable 主题对象。
（ 5 ）timer ()
创建一个在给定的延时之后弹射单个数据的 Observable 主题对象。
（ 6 ）empty ()
创建一个什么都不做直接通知完成的 Observable 主题对象。
（ 7 ）error ()
创建一个什么都不做直接通知错误的 Observable 主题对象。
（ 8 ）never ()
创建一个不弹射任何数据的 Observable 主题对象。
接下来以 just、from、range、interval、defer 五个操作符为例进行介绍。
```
#### 5. 2. 1 just 操作符

Observable 的 just 操作符用于创建一个 Observable 主题，并且会将实参数据弹射出来。just 操作
符可接收多个实参，所有实参都将被逐一弹射。
just 操作符的演示代码如下：
packagecom. crazymaker. demo. rxJava. basic;
importlombok. extern. slf 4 j.Slf 4 j;
importorg. junit. Test;
importrx. Observable;
@Slf 4 j
publicclassCreaterOperatorDemo{
/**
*演示 just 的基本使用
*/
@Test
publicvoidjustDemo (){
//发送一个字符串"helloworld"
Observable.just ("helloworld")
.subscribe (s->log.info ("juststring->"+s));
//逐一发送 1 ， 2 ， 3 ， 4 这四个整数
Observable.just ( 1 , 2 , 3 , 4 )
.subscribe (i->log.info ("justint->"+i));
}
}
运行之后的结果大致如下：


```
20 : 53 : 17. 653 [main]INFO c.c.d.r.b.CreaterOperatorDemo-juststring->helloworld
20 : 53 : 17. 658 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 1
20 : 53 : 17. 659 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 2
20 : 53 : 17. 659 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 3
20 : 53 : 17. 659 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 4
```
```
just 操作符只是简单的原样弹射，如果实参是数组或者 Iterable 迭代器对象，则数
组或 Iterable 会被当作单个数据弹射。
```
```
虽然 just 操作符可以弹射多个数据，但是最多为 9 个。
```
#### 5. 2. 2 from 操作符

from 操作符以数组、Iterable 迭代器等对象作为输入，创建一个 Observable 主题对象，然后将实
参（如数组、Iterable 迭代器等）中的数据元素逐一弹射出去。
from 操作符的演示代码如下：
...
@Slf 4 j
publicclassCreaterOperatorDemo{
/***演示 from 的基本使用 */
@Test
publicvoidfromDemo (){
//逐一发送一个数组中的每一个元素
String[]items={"a","b","c","d","e","f"};
Observable.from (items)
.subscribe (s->log.info ("juststring->"+s));
//逐一发送迭代器中的每一个元素
Integer[]array={ 1 , 2 , 3 , 4 };
List<Integer>list=Arrays.asList (array);
Observable.from (list)
.subscribe (i->log.info ("justint->"+i));
}
...
}
运行测试代码，结果如下：
21 : 10 : 18. 537 [main]INFO c.c.d.r.b.CreaterOperatorDemo-juststring->a
21 : 10 : 18. 540 [main]INFO c.c.d.r.b.CreaterOperatorDemo-juststring->b
21 : 10 : 18. 540 [main]INFO c.c.d.r.b.CreaterOperatorDemo-juststring->c
21 : 10 : 18. 540 [main]INFO c.c.d.r.b.CreaterOperatorDemo-juststring->d
21 : 10 : 18. 540 [main]INFO c.c.d.r.b.CreaterOperatorDemo-juststring->e
21 : 10 : 18. 541 [main]INFO c.c.d.r.b.CreaterOperatorDemo-juststring->f
21 : 10 : 18. 543 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 1
21 : 10 : 18. 544 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 2
21 : 10 : 18. 544 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 3
21 : 10 : 18. 545 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 4
从以上的输出可以看出，from () 操作将传入的数组或 Iterable 拆分成单个元素依次弹射出去。

#### 5. 2. 3 range 操作符

range 操作符以一组整数范围作为输入，创建一个 Observable 主题对象并弹射该整数范围内所包
含的所有整数。


```
range 操作符的演示代码如下：
packagecom. crazymaker. demo. rxJava. basic;
...
@Slf 4 j
publicclassCreaterOperatorDemo{
/**演示 range 的基本使用*/
@Test
publicvoidrangeDemo (){
//逐一发送一组范围内的整数序列
Observable.range ( 1 , 10 )
.subscribe (i->log.info ("justint->"+i));
}
}
输出的结果如下：
21 : 24 : 50. 507 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 1
21 : 24 : 50. 513 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 2
21 : 24 : 50. 513 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 3
21 : 24 : 50. 513 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 4
21 : 24 : 50. 513 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 5
21 : 24 : 50. 513 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 6
21 : 24 : 50. 513 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 7
21 : 24 : 50. 513 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 8
21 : 24 : 50. 514 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 9
21 : 24 : 50. 514 [main]INFO c.c.d.r.b.CreaterOperatorDemo-justint-> 10
Observable.range ( 1 , 10 ) 表示弹射在区间[ 1 , 10 ]范围内的数据，其范围包含区间的上限和下限。
```
#### 5. 2. 4 interval 操作符

interval 操作符创建一个 Observable 主题对象（消息流），该流会按照固定时间间隔发射整数序
列。interval 操作符的演示代码如下：
packagecom. crazymaker. demo. rxJava. basic;
...
@Slf 4 j
publicclassOtherOperatorDemo
{
/**
*演示 interval 转换
*/
@Test
publicvoidintervalDemo () throwsInterruptedException
{
Observable
.interval ( 100 ,TimeUnit. MILLISECONDS)
.subscribe (aLong->log.info (aLong.toString ()));
Thread.sleep (Integer. MAX_VALUE);
}
//...
}
演示代码中的 interval 操作符的弹射间隔时间为 100 毫秒。运行程序，输出的结果如下：
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 0
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 1


```
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 2
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 3
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 4
...
```
#### 5. 2. 5 defer 延迟创建操作符

just、from、range 以及其他创建操作符都是在创建主题时弹射数据，而不是在被订阅的时候。
而 defer 创建操作符所创建的主题，在创建主题时并不弹射数据，它会一直等待直到有观察者订阅
它才弹射数据。
defer 操作符的演示代码如下：
packagecom. crazymaker. demo. rxJava. defer;
...
@Slf 4 j
publicclassSimpleDeferDemo
{
/**
*演示 defer 延迟创建操作符
*/
@Test
publicvoiddeferDemo ()
{
AtomicIntegerfoo=newAtomicInteger ( 100 );
Observableobservable=Observable.just (foo.get ());
/**
*延迟创建
*/
ObservabledObservable=Observable.defer (()->Observable.just (foo.get ()));
/**
*修改对象的值
*/
foo.set ( 200 );
/**
*有观察者订阅
*/
observable.subscribe (integer->log.info ("justemit{}",
String.valueOf (integer)));
/**
*有观察者订阅
*/
dObservable.subscribe (integer->log.info ("deferjustemit{}",
String.valueOf (integer)));
}
}
[main]INFO c.c.d.r.defer. SimpleDeferDemo-justemit 100
[main]INFO c.c.d.r.defer. SimpleDeferDemo-deferjustemit 200
实质上通过 defer 创建的主题在观察者订阅时会创建一个新的 Observable 主题，因此，尽管每个
订阅者都以为自己订阅的是同一个 Observable，事实上每个订阅者获取的是独立的消息序列。


### 5. 3 过滤型操作符

```
本节介绍一下 RxJava 的两个过滤型操作符：filter 操作符和 distinct 操作符。
```
#### 5. 3. 1 filter 操作符

filter 操作符用于判断 Observable 弹射的每一个消息是否满足条件。如果满足条件，则继续向下
游的观察者传递；如果不满足条件则过滤掉。filter 操作符的处理流程大致如图 5 - 4 所示。
filter 操作符使用 Func 1 函数式接口传入判断条件，
其演示代码如下：
packagecom. crazymaker. demo. rxJava. basic;
...
@Slf 4 j
publicclassFilterOperatorDemo{
/**
*演示 filter 的基本使用
*/
@Test
publicvoidfilterDemo (){
//通过 filter 过滤能被 5 整除的数
Observable.range ( 1 , 20 )
.filter (newFunc 1 <Integer,Boolean>(){
@Override
publicBooleancall (Integerinteger){
returninteger% 5 == 0 ;
}
})
.subscribe (i->log.info ("filterint->"+i));
}
}
演示代码首先通过 rang 操作符弹射一个范围为[ 1 , 20 ]的整数序列；然后，通过 filter 操作符对弹
射的数据进行过滤，过滤能被 5 整除的数。
运行程序，控制台打印如下：
21 : 45 : 40. 579 [main]INFO c.c.d.r.b.FilterOperatorDemo-filterint-> 5
21 : 45 : 40. 584 [main]INFO c.c.d.r.b.FilterOperatorDemo-filterint-> 10
21 : 45 : 40. 584 [main]INFO c.c.d.r.b.FilterOperatorDemo-filterint-> 15
21 : 45 : 40. 585 [main]INFO c.c.d.r.b.FilterOperatorDemo-filterint-> 20
如果使用 Lambda 表达式对上面代码进行改写，则代码如下：
//演示 filter 的基本使用，使用 Lambda 形式
@Test
publicvoidfilterDemoLambda (){
//通过 filter 过滤能被 5 整除的数
Observable.range ( 1 , 20 )
.filter (integer->integer% 5 == 0 )
.subscribe (i->log.info ("filterint->"+i));
}

```
图 5 - 4 filter 操作符的处理流程
```

#### 5. 3. 2 distinct 操作符

distinct 操作符用于在消息流中过滤掉重复的元
素。过滤规则为：只允许还没有被弹射过的元素弹
射出去。distinct 操作符大致的处理流程如图 5 - 5 所示。
下面是一个简单的 distinct 操作符的使用实例：
packagecom. crazymaker. demo. rxJava. basic;
...
@Slf 4 j
publicclassFilterOperatorDemo{
/**
*演示 distinct 的基本使用
*/
@Test
publicvoiddistinctDemo (){
Observable.just ("apple","pair","banana","apple","pair")
.distinct () //使用 disctinct 过滤重复元素
.subscribe (s->log.info ("distincts->"+s));
}
}
运行程序，结果大致如下：
15 : 05 : 32. 229 [main]INFO c.c.d.r.b.FilterOperatorDemo-distincts->apple
15 : 05 : 32. 234 [main]INFO c.c.d.r.b.FilterOperatorDemo-distincts->pair
15 : 05 : 32. 234 [main]INFO c.c.d.r.b.FilterOperatorDemo-distincts->banana
从输出结果可以看出，消息流中后面的"apple"、"pair" 两个元素由于前面已经被弹射过，所
以被过滤了。

### 5. 4 转换型操作符

```
本节介绍 RxJava 的 3 个转换型操作符：map 操作符、flatMap 操作符和 scan 操作符。
```
#### 5. 4. 1 map 操作符

map 操作符接收一个转换函数，对 Observable 弹射的消息流的每一个元素都应用该转换函数，
转换之后的结果从消息流弹出。map 操作符返回的消息流由转换函数执行转换之后的结果组成。
map 操作符大致的处理流程如图 5 - 6 所示。
map 操作符需要接收一个函数式接口 Function<T,R>
的对象，该对象实现了接口的 apply (T) 方法，此方法负责
对接收到的实参进行转换，返回转换之后的新值。
map 操作符的使用实例如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassTransformationDemo
{
/***演示 map 转换*/

```
图 5 - 5 distinct 操作符的处理流程
```
```
图 5 - 6 map 操作符大致的处理流程
```

@Test
publicvoidmapDemo ()
{
Observable.range ( 1 , 4 )
.map (i->i*i)
.subscribe (i->log.info (i.toString ()));
}
...
}
运行程序，结果大致如下：
[main]INFO c.c.d.r.b.TransformationDemo- 1
[main]INFO c.c.d.r.b.TransformationDemo- 4
[main]INFO c.c.d.r.b.TransformationDemo- 9
[main]INFO c.c.d.r.b.TransformationDemo- 16
map 操作符从消息流中取一个值，然后返回另一个值，转换的逻辑是一对一的，而 flatMap 的
逻辑并不是如此。

#### 5. 4. 2 flatMap 操作符

flatMap 操作符将输入消息流的任意数量的元素（零
项或无穷项）打包成一个新的 Observable 主题然后弹出。
flatMap 操作符的处理流程如图 5 - 7 所示。
flatMap 操作符将一个弹射数据的 Observable 流，变换
成弹射 Observable 主题对象的新流，新流所弹出的主题对
象（元素）会包含原流中的一个或者多个数据元素，其
特点如下：

1 ）flatmap 转换是一对一或者一对多类型的，原来弹射了几个数据，转换之后可以是更多个。
2 ）flatMap 转换同样可以改变弹射的数据类型。
3 ）flatMap 转换后的数据，还是会逐个发射给下游的 Subscriber 来接收，表面上这些数据是由
一个 Observable 发射的，其实是多个 Observable 发射然后合并的。

```
一个简单的 flatMap 操作符使用实例的代码如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassTransformationDemo
{
...
/**
*演示 flapMap 转换
*/
@Test
publicvoidflapMapDemo ()
{
/**
*注意，flatMap 中的 just 所创建的是一个新的流
*/
Observable.range ( 1 , 4 )
.flatMap (i->Observable.just (i*i, i*i+ 1 ))
.subscribe (i->log.info (i.toString ()));
}
```
```
图 5 - 7 flatMap 操作符的处理流程
```

}
运行程序，结果大致如下：
[main]INFO c.c.d.r.b.TransformationDemo- 1
[main]INFO c.c.d.r.b.TransformationDemo- 2
[main]INFO c.c.d.r.b.TransformationDemo- 4
[main]INFO c.c.d.r.b.TransformationDemo- 5
[main]INFO c.c.d.r.b.TransformationDemo- 9
[main]INFO c.c.d.r.b.TransformationDemo- 10
[main]INFO c.c.d.r.b.TransformationDemo- 16
[main]INFO c.c.d.r.b.TransformationDemo- 17
由于在转换的过程中 flatMap 操作符创建了新的 Observable 主题对象，因此它也可以被归类为创
建型操作符。一个更加复杂一点的 flatMap 操作符的使用实例代码如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassTransformationDemo
{
...
/**
*演示一个稍微复杂的 flapMap 转换
*/
@Test
publicvoidflapMapDemo 2 ()
{
Observable.range ( 1 , 4 )
.flatMap (i->Observable.range ( 1 ,i). toList ())
.subscribe (list->log.info (list.toString ()));
}
}
实例中 flatMap 把输入流的元素通过 range 创建型操作符转换成一个 Observable 对象，然后再调
用它的 toList () 方法转换成包装单个 List 元素的新 Observable 主题对象弹出。运行这个演示程序，输
出的结果如下：
[main]INFO c.c.d.r.b.TransformationDemo-[ 1 ]
[main]INFO c.c.d.r.b.TransformationDemo-[ 1 , 2 ]
[main]INFO c.c.d.r.b.TransformationDemo-[ 1 , 2 , 3 ]
[main]INFO c.c.d.r.b.TransformationDemo-[ 1 , 2 , 3 , 4 ]


#### 5. 4. 3 scan 操作符

scan 操作符对一个 Observable 流序列的每一项数据应用一个累积函数，然后将这个函数的累积
结果弹射出去。除了第一项之外，scan 操作符会将上一个数据项的累积结果作为下一个数据项在应
用累积函数时的输入，所以 scan 操作符有点类似于递归操作。
现假定累积函数为一个简单的累加函数，使用 scan 操作符对 1 到 5 的数据序列进行扫描，具体的
执行流程如图 5 - 8 所示。

```
图 5 - 8 使用 scan 操作符对 1 到 5 的数据流序列进行累加扫描
参考的实现代码如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassTransformationDemo
{
/**演示 scan 扫描操作符*/
@Test
publicvoidscanDemo ()
{
/**定义一个 accumulator 累积函数*/
Func 2 <Integer,Integer,Integer>accumulator=newFunc 2 <Integer, Integer,
Integer>()
{
@Override
publicIntegercall (Integerinput 1 ,Integerinput 2 )
{
log.info ("{}+{}={} ", input 1 ,input 2 ,input 1 +input 2 );
returninput 1 +input 2 ;
}
};
/**
*使用 scan 进行流扫描
*/
Observable.range ( 1 , 5 )
.scan (accumulator)
.subscribe (newAction 1 <Integer>()
{
@Override
publicvoidcall (Integersum)
{
log.info ("累加的结果:{}", sum);
}
});
}
}
```

###### 运行以上参考代码，结果部分节选如下：

[main]INFO c.c.d.r.b.TransformationDemo- 累加的结果: 1
[main]INFO c.c.d.r.b.TransformationDemo- 1 + 2 = 3
[main]INFO c.c.d.r.b.TransformationDemo- 累加的结果: 3
[main]INFO c.c.d.r.b.TransformationDemo- 3 + 3 = 6
[main]INFO c.c.d.r.b.TransformationDemo- 累加的结果: 6
[main]INFO c.c.d.r.b.TransformationDemo- 6 + 4 = 10
[main]INFO c.c.d.r.b.TransformationDemo- 累加的结果: 10
[main]INFO c.c.d.r.b.TransformationDemo- 10 + 5 = 15
[main]INFO c.c.d.r.b.TransformationDemo- 累加的结果: 15
在以上实例中，scan 操作符对原 Observable 流所弹射的第一项数据 1 应用了 accumulator 累积函数，
然后将累积函数的结果 1 作为输出流的第一项数据弹射出去；接下来，它将第一个结果连同原
Observable 流的第二项数据 2 一起，再填充给 accumulator 累积函数，之后将累积结果 3 作为输出流的
第二项数据弹射出去。scan 操作符持续重复这个过程，不断对原流进行累积，直到最后一个数据项
的累积结果从输出流弹射出去为止。

5. (^5) 聚合操作符
本节介绍 RxJava 的两个聚合型操作符：count 操作符和 reduce 操作符。

#### 5. 5. 1 count 操作符

count 操作符用来对原 Observable 流的数据项进行计数，最后将总数给弹射出去。如果原流弹射
错误，则会将错误直接报出来。在原 Observable 流没有终止前，count 操作符是不会弹射统计数据的。
使用 scan 操作符对数据流序列进行计数，具体的执行流程如图 5 - 9 所示。

```
图 5 - 9 使用 count 操作符对数据流序列进行计数
下面是一个使用 count 操作符的简单例子，代码如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassAggregateDemo
{
/**
*演示 count 计数操作符
*/
```

@Test
publicvoidcountDemo ()
{
String[]items={"one","two","three","four"};
Integercount=Observable
.from (items)
.count ()
.toBlocking (). single ();
log.info ("计数的结果为{}", count);
}
}
运行以上代码，输出的结果部分节选如下：
[main]INFO c.c.d.r.basic. AggregateDemo-计数的结果为 4
可以看出，count 操作符将一个原 Observable 流转换成一个弹射单个值的 Observable 输出流，输
出流的唯一数据项的值为原 Observable 流所弹射的数据项数量。
在上面的代码中，为了获取 count 输出流中的数据项，使用了 Observable.toBlocking (). single () 两
个操作符。其中，Observable.toBlocking () 操作返回了一个 BlockingObservable 阻塞型实例，该类型
不是一种新的数据流，仅仅是对原 Observable 流的包装，只是该类型会阻塞当前线程，等待直到内
部的原 Observable 流弹射了自己想要的数据；BlockingObservable.single () 方法，表示阻塞当前线程，
直到从封装的原 Observable 流获取到唯一的弹射数据元素项，如果原 Observable 流弹射出的数据元
素不止一个，single () 方法会抛出异常。

#### 5. 5. 2 reduce 操作符

reduce 操作符对一个 Observable 流序列的每一项应用一个规约函数，最后将流的最终规约计算结
果弹射出去。除了第一项之外，reduce 操作符会将上一个数据项的应用规约函数的结果作为下一个数
据项在应用规约函数时的输入，所以，和 scan 操作符一样，reduce 操作符也有点类似于递归操作。
现假定规约函数为一个简单的累加函数，使用 reduce 操作符对 1 到 5 的数据序列流进行规约，其
具体的规约流程如图 5 - 10 所示。

```
图 5 - 10 reduce 操作符对 1 到 5 的数据序列流的规约流程
参考的实现代码如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassAggregateDemo
{
/**
```

*演示 reduce 规约操作符
*/
@Test
publicvoidreduceDemo ()
{
/**
*定义一个 accumulator 规约函数
*/
Func 2 <Integer,Integer,Integer>accumulator=newFunc 2 <Integer, Integer,
Integer>()
{
@Override
publicIntegercall (Integerinput 1 ,Integerinput 2 )
{
log.info ("{}+{}={} ", input 1 ,input 2 ,input 1 +input 2 );
returninput 1 +input 2 ;
}
};
/**
*使用 reduce 进行流规约
*/
Observable.range ( 1 , 5 )
.reduce (accumulator)
.subscribe (newAction 1 <Integer>()
{
@Override
publicvoidcall (Integersum)
{
log.info ("规约的结果:{}", sum);
}
});
}
}
运行以上参考代码，输出的结果节选如下：
[main]INFO c.c.d.r.basic. AggregateDemo- 1 + 2 = 3
[main]INFO c.c.d.r.basic. AggregateDemo- 3 + 3 = 6
[main]INFO c.c.d.r.basic. AggregateDemo- 6 + 4 = 10
[main]INFO c.c.d.r.basic. AggregateDemo- 10 + 5 = 15
[main]INFO c.c.d.r.basic. AggregateDemo- 规约的结果: 15
以上实例代码中，reduce 操作符对原 Observable 流所弹射的第一项数据 1 应用规约函数，得到中
间结果 1 ；然后它将第一个中间结果 1 连同原流的第二项数据 2 一起，再填充给 accumulator 规约函数，
得到中间结果 3 。reduce 持续对原流进行迭代，一直到原流的最后一个数据项 5 ，reduce 将 5 连同中间
结果 10 一起填充给 accumulator 规约函数，得到最终结果 15 。最后，reduce 会将最终结果 15 作为输出
流的数据项弹射出去。
reduce 操作符与前面介绍的 scan 扫描操作符很类似，只是 scan 会弹出每次计算的中间结果，而
reduce 只会弹出最后的结果。

### 5. 6 其他操作符

```
本节介绍 RxJava 的其他比较常用的操作符：take 操作符和 window 操作符。
```

#### 5. 6. 1 take 操作符

take 操作符用于根据索引在原流上进行元素的挑
选操作，挑选原流上的 n 个元素。如果原流序列中的
项少于指定索引，则抛出错误。
take 操作符大致的处理流程如图 5 - 11 所示。
下面是一个使用 take 操作符完成 10 秒倒计时的演
示实例，代码如下：
packagecom. crazymaker. demo. rxJava. basic;
...
@Slf 4 j
publicclassOtherOperatorDemo
{
...
/**
*演示 take 操作符
*这是一个 10 秒倒计时实例
*/
@Test
publicvoidtakeDemo () throwsInterruptedException
{
Observable.interval ( 1 ,TimeUnit. SECONDS) //设置执行间隔
.take ( 10 )// 10 秒倒计时
.map (aLong-> 10 - aLong)
.subscribe (aLong->log.info (aLong.toString ()));
Thread.sleep (Integer. MAX_VALUE);
}
}
运行这个演示程序，输出的结果大致如下：
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 10
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 9
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 8
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 7
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 6
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 5
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 4
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 3
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 2
[RxComputationScheduler- 1 ]INFO c.c.d.r.b.OtherOperatorDemo- 1
skip 操作符与 take 操作符类似，也是用于根据索引在原流上进行元素的挑选操作，只是 take 是
取前 n 个元素，而 skip 是跳过前 n 个元素。注意，如果序列中的项少于指定索引，则两个函数都将抛
出错误。

#### 5. 6. 2 window 操作符

RxJava 的窗口可以理解为固定数量（或者固定时间间隔）的元素分组。假定通过 window 操作
符以固定数量 n 进行窗口划分，一旦流上弹射的元素的数量大于一个窗口的数量 n，则输出流上将弹
出一个新的元素，输出元素是一个 Observable 主题对象，该主题包含了原流的窗口之内的 n 个元素。

```
图 5 - 11 take 操作符大致的处理流程
```

使用 window 操作符创建固定数量窗口（滚动窗口）
的大致处理流程如图 5 - 12 所示。
一个使用 window 操作符以固定数量进行元素分
组的示例如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassWindowDemo
{
/**
*演示 window 创建操作符创建滑动窗口
*/
@Test
publicvoidsimpleWindowObserverDemo ()
{
List<Integer>srcList=Arrays.asList ( 10 , 11 , 20 , 21 , 30 , 31 );
Observable.from (srcList)
.window ( 3 )//以固定数量分组
.flatMap (o->o.toList ())
.subscribe (list->log.info (list.toString ()));
}
...
}
运行这个演示程序，输出的结果如下：
[main]INFO c.c.d.rxJava. basic. WindowDemo-[ 10 , 11 , 20 ]
[main]INFO c.c.d.rxJava. basic. WindowDemo-[ 21 , 30 , 31 ]
在使用 window 进行分组时，不同窗口的元素还
可以重叠，可以理解成滑动窗口。
创建重叠窗口使用函数 window（intcount，int
skip），其中第一个参数为窗口的元素个数，第二个
参数为下一个窗口跳过的元素个数。使用 window 操作
符创建重叠窗口的处理流程如图 5 - 13 所示。
使用 window 操作符以固定数量创建重叠窗口的
示例如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassWindowDemo
{
...
/**
*window 创建操作符创建滑动窗口
*演示 window 创建操作符创建滑动窗口
*/
@Test
publicvoidwindowObserverDemo ()
{

```
图 5 - 12 使用 window 操作符创建
固定数量窗口（滚动窗口）
```
```
图 5 - 13 使用 window 操作符
创建重叠窗口（滑动窗口）
```

List<Integer>srcList=Arrays.asList ( 10 , 11 , 20 , 21 , 30 , 31 );
Observable.from (srcList)
.window ( 3 , 1 )
.flatMap (o->o.toList ())
.subscribe (list->log.info (list.toString ()));
}
...
}
运行这个演示程序，输出的结果如下：
[main]INFO c.c.d.rxJava. basic. WindowDemo-[ 10 , 11 , 20 ]
[main]INFO c.c.d.rxJava. basic. WindowDemo-[ 11 , 20 , 21 ]
[main]INFO c.c.d.rxJava. basic. WindowDemo-[ 20 , 21 , 30 ]
[main]INFO c.c.d.rxJava. basic. WindowDemo-[ 21 , 30 , 31 ]
[main]INFO c.c.d.rxJava. basic. WindowDemo-[ 30 , 31 ]
[main]INFO c.c.d.rxJava. basic. WindowDemo-[ 31 ]
RxJava 的窗口还可以按照固定时间间隔进行分组。一个使用 window 操作符以固定时间间隔创
建不重叠窗口的示例如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassWindowDemo
{
...
/**
*window 创建操作符创建时间窗口
*演示 window 创建操作符创建时间窗口
*/
@Test
publicvoidtimeWindowObserverDemo () throwsInterruptedException
{
ObservableeventStream=Observable
.interval ( 100 ,TimeUnit. MILLISECONDS);
eventStream.window ( 300 ,TimeUnit. MILLISECONDS)
.flatMap (o->((Observable<Integer>) o). toList ())
.subscribe (list->log.info (list.toString ()));
Thread.sleep (Integer. MAX_VALUE);
}
...
}
在此示例中，window 操作符以 300 毫秒的固定间隔划分出非重叠窗口，每个窗口保持 300 毫秒
的时间，从而确保输入流 eventStream 接收到 3 个值，直到停止。
运行这个演示程序，输出的结果如下：
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 0 , 1 ]
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 2 , 3 , 4 ]
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 5 , 6 , 7 ]
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 8 , 9 , 10 ]
...


### 5. 7 RxJava 的 Scheduler 调度器

顾名思义，Scheduler 是一种用来对 RxJava 流操作进行调度的类，从 Scheduler 的工厂方法可以
获取现有调度器实现，获取方法如下：

1 ）Schedulers.io ()：用于获取内部的 ioScheduler 调度器实例。
2 ）Schedulers.newThread ()：用于获取内部的 newThreadScheduler 调度器实现，该调度器为 RxJava
流操作创建一个新线程。
3 ）Schedulers.computation ()：用于获取内部的 computationScheduler 调度器实例。
4 ）Schedulers.trampoline ()：使用当前线程立即执行 RxJava 流操作。
5 ）Schedulers.single ()：使用 RxJava 内置的单例线程执行 RxJava 流操作。
以上 5 个获取调度器的方法具体介绍如下：
（ 1 ）Schedulers.io ()
获取内部的 ioScheduler 调度器实例，主要用于 IO 密集型的流操作，例如读写 SD 卡文件、查询
数据库、访问网络等。此调度器具有线程缓存机制，在接收到任务后，先检查线程缓存池中是否有
空闲的线程，如果有则复用，如果没有则创建新的线程并加入 IO 专用线程池中，如果专用线程池
每次都没有空闲线程使用则可以无上限地创建新线程。

（ 2 ）Schedulers.newThread ()
每执行一个 RxJava 流操作就创建一个新的线程，不具有线程缓存机制，因为创建一个新的线
程比复用一个线程更耗时耗力，Schedulers.newThread () 的效率没有 Schedulers.io () 高。

（ 3 ）Schedulers.computation ()
返回具有固定线程池的内部 computationScheduler 调度器实例，用于执行 CPU 密集型的流操作，
线程数大小为 CPU 的核数。不可以用于 I/O 操作，例如不能用于 XML/JSON 文件的解析、Bitmap 图
片的压缩取样等，因为 I/O 操作会浪费 CPU 时间。

（ 4 ）Schedulers.trampoline ()
在当前线程立即执行流操作，如果当前线程有流操作在执行，则立即暂停，等插入进来的任
务执行完之后，再将未完成的任务接着执行。

（ 5 ）Schedulers.single ()
RxJava 拥有一个专用的线程单例，此调度器负责的所有的流操作都在这一个线程中执行，当
此线程中有任务执行时，其他任务将会按照先进先出的顺序依次排队。

```
一个简单的调度器使用实例的代码如下：
packagecom. crazymaker. demo. rxJava. basic;
importlombok. extern. slf 4 j.Slf 4 j;
importorg. junit. Test;
importrx. Observable;
importrx. Subscriber;
```

importrx. schedulers. Schedulers;
@Slf 4 j
publicclassSchedulerDemo{
/**
*演示 Schedulers 的基本使用
*/
@Test
publicvoidtestScheduler () throwsInterruptedException{
//被观察者
Observableobservable=Observable.create (
newObservable. OnSubscribe<String>(){
@Override
publicvoidcall (Subscriber<?superString>subscriber){
for (inti= 0 ;i< 5 ;i++){
log.info ("produce->"+i);
subscriber.onNext (String.valueOf (i));
}
subscriber.onCompleted ();
}
});
//订阅 Observable 与 Subscriber 之间依然通过 subscribe () 进行关联。
observable
//使用具有线程缓存机制的可复用线程
.subscribeOn (Schedulers.io ())
//每执行一个任务就创建一个新的线程
.observeOn (Schedulers.newThread ())
.subscribe (s->{
log.info ("consumer->"+s);
});
Thread.sleep (Integer. MAX_VALUE);
}
}
运行这个演示程序，输出的部分结果如下：
17 : 04 : 17. 922 [RxIoScheduler- 2 ]INFO c.c.d.r.b.SchedulerDemo-produce-> 0
17 : 04 : 17. 932 [RxIoScheduler- 2 ]INFO c.c.d.r.b.SchedulerDemo-produce-> 1
17 : 04 : 17. 932 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.SchedulerDemo-consumer-> 0
17 : 04 : 17. 933 [RxIoScheduler- 2 ]INFO c.c.d.r.b.SchedulerDemo-produce-> 2
17 : 04 : 17. 933 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.SchedulerDemo-consumer-> 1
17 : 04 : 17. 933 [RxIoScheduler- 2 ]INFO c.c.d.r.b.SchedulerDemo-produce-> 3
17 : 04 : 17. 933 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.SchedulerDemo-consumer-> 2
17 : 04 : 17. 933 [RxIoScheduler- 2 ]INFO c.c.d.r.b.SchedulerDemo-produce-> 4
17 : 04 : 17. 933 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.SchedulerDemo-consumer-> 3
17 : 04 : 17. 933 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.SchedulerDemo-consumer-> 4
通过上面的代码可以看出，RxJava 提供了两个方法来改变流操作的调度器：
1 ）subscribeOn ()：主要改变的是弹射的线程。
2 ）observeOn ()：主要改变的是订阅的线程。
在 RxJava 中，创建操作符创建的 Observable 主题的弹射任务将由其后最近的 subscribeOn () 设置
的调度器负责。
在 RxJava 中，Observable 主题的下游消费型操作（如流转换等）的线程调度将由其前面的最近
的 observeOn () 设置的调度器负责。observeOn () 可以多次设定，每一次设定都对处于下一次
observeOn () 设定之前的流操作产生作用。


### 5. 8 背压

```
本节首先介绍什么是背压（backpressure）问题，然后介绍一下背压问题的几种应对模式。
```
#### 5. 8. 1 什么是背压问题

###### 当上下游的流操作处于不同的线程时，如果上游弹射数据的速度快于下游接收处理数据的速

###### 度，这样对于那些没来得及处理的数据就会造成积压，这些数据既不会丢失，也不会被垃圾回收机

###### 制回收，而是存放在一个异步缓存池中，如果缓存池中的数据一直得不到处理，越积越多，最后就

###### 会造成内存溢出，这便是响应式编程中的背压问题。

###### 一个存在背压问题的演示实例代码如下：

```
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassBackpressureDemo{
/**
*演示不使用背压
*/
@Test
publicvoidtestNoBackpressure () throwsInterruptedException{
//被观察者（主题）
Observableobservable=Observable.create (
newObservable. OnSubscribe<String>(){
@Override
publicvoidcall (Subscriber<?superString>subscriber){
//循环 10 次
for (inti= 0 ;i< 10 ;i++){
log.info ("produce->"+i);
subscriber.onNext (String.valueOf (i));
}
}
});
//观察者
Action 1 <String>subscriber=newAction 1 <String>(){
public voidcall (Strings){
try{
//每消费一次，间隔 50 毫秒
Thread.sleep ( 50 );
}catch (InterruptedExceptione){
e.printStackTrace ();
}
log.info ("consumer->"+s);
}
};
//订阅: observable 与 subscriber 之间依然通过 subscribe () 进行关联。
observable
.subscribeOn (Schedulers.io ())
.observeOn (Schedulers.newThread ())
.subscribe (subscriber);
```

Thread.sleep (Integer. MAX_VALUE);
}
}
在实例代码中，observable 发射操作执行在一条通过 Schedulers.io () 调度器获取的 io 线程上，而
观察者 Subscriber 的消费操作执行在另一条通过 Schedulers.newThread () 调度器获取的新线程上。
Observable 流不断发送数据，累积发送 10 次；观察者 Subscriber 每隔 50 毫秒接收一条数据。
运行上面的演示程序，输出的结果如下：
17 : 56 : 17. 719 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 0
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 1
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 2
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 3
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 4
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 5
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 6
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 7
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 8
17 : 56 : 17. 723 [RxIoScheduler- 2 ]INFO c.c.d.r.b.BackpressureDemo-produce-> 9
17 : 56 : 17. 774 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 0
17 : 56 : 17. 824 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 1
17 : 56 : 17. 875 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 2
17 : 56 : 17. 925 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 3
17 : 56 : 17. 976 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 4
17 : 56 : 18. 027 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 5
17 : 56 : 18. 078 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 6
17 : 56 : 18. 129 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 7
17 : 56 : 18. 179 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 8
17 : 56 : 18. 230 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 9
上面的程序有一个特点：生产者 Observable 弹射数据的速度大于下游消费者 Subscriber 接收处理
数据的速度，但是由于数据量小，上面的程序运行起来也没有出现问题。
简单修改一下生产者，将原来的弹射 10 条改成无限制地弹射，代码如下：
//被观察者（主题）
Observableobservable=Observable.create (
newObservable. OnSubscribe<String>(){
@Override
publicvoidcall (Subscriber<?superString>subscriber){
//无限制地循环
for (inti= 0 ; ;i++){
//log.info ("produce->"+i);
subscriber.onNext (String.valueOf (i));
}
}
});
再次运行演示程序，抛出的异常如下：
Causedby: rx. exceptions. MissingBackpressureException
atrx. internal. operators. OperatorObserveOn$ObserveOnSubscriber. onNext
(OperatorObserveOn. java: 160 )
atrx. internal. operators. OperatorSubscribeOn$SubscribeOnSubscriber. onNext
(OperatorSubscribeOn. java: 74 )
atcom. crazymaker. demo. rxJava. basic. BackpressureDemo$ 1 .call
(BackpressureDemo. java: 24 )
atcom. crazymaker. demo. rxJava. basic. BackpressureDemo$ 1 .call
(BackpressureDemo. java: 19 )


atrx.Observable.unsafeSubscribe (Observable. java: 10327 )
atrx. internal. operators. OperatorSubscribeOn$SubscribeOnSubscriber. call
(OperatorSubscribeOn. java: 100 )
atrx. internal. schedulers. CachedThreadScheduler$EventLoopWorker$ 1 .call
(CachedThreadScheduler. java: 230 )
... 9 more
异常原因：由于上游 Observable 弹射数据的速度远远大于下游通过 Subscriber 接收的速度，导致
observable 用于暂存弹射数据的队列空间耗尽，造成上游数据积压。

#### 5. 8. 2 背压问题的几种应对模式

如何应对背压问题呢？在创建主题时，可以使用 Observable 类的一个重载的 create 方法设置具
体的背压模式，具体如下：
publicstatic<T>Observable<T>create (Action 1 <Emitter<T>>emitter,
Emitter. BackpressureModebackpressure){
returnunsafeCreate (newOnSubscribeCreate<T>(emitter, backpressure));
}
此方法的第二个参数用于指定一种背压模式。背压的模式有多种，比较常用的有“最近模式”
Emitter. BackpressureMode. LATEST。这种模式的含义为：如果消费跟不上，则仅缓存最近弹射出来
的数据，将老旧一点的数据直接丢弃。
使用“最近模式”背压，改写上一小节的测试用例，具体如下：
/**
*演示使用“最近模式”背压
*/
@Test
publicvoidtestBackpressure () throwsInterruptedException{
//主题实例，使用背压
Observableobservable=Observable.create (
newAction 1 <Emitter<String>>(){
@Override
publicvoidcall (Emitter<String>emitter){
////无限循环
for (inti= 0 ;; i++){
//log.info ("produce->"+i);
emitter.onNext (String.valueOf (i));
}
}
}, Emitter. BackpressureMode. LATEST);
//订阅者（观察者）
Action 1 <String>subscriber=newAction 1 <String>(){
publicvoidcall (Strings){
try{
//每消费一次，间隔 50 毫秒
Thread.sleep ( 3 );
}catch (InterruptedExceptione){
e.printStackTrace ();
}
log.info ("consumer->"+s);
}
};
//订阅: observable 与 subscriber 之间依然通过 subscribe () 进行关联


```
observable
.subscribeOn (Schedulers.io ())
.observeOn (Schedulers.newThread ())
.subscribe (subscriber);
Thread.sleep (Integer. MAX_VALUE);
}
运行这个演示程序，输出的结果节选如下：
18 : 51 : 54. 736 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 0
18 : 51 : 54. 745 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer-> 1
//省略部分输出
18 : 51 : 55. 217 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
```
- > 123
    18 : 51 : 55. 220 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 124
    18 : 51 : 55. 224 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 125
    18 : 51 : 55. 228 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 126
    18 : 51 : 55. 232 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 127
    18 : 51 : 55. 236 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 7337652
    18 : 51 : 55. 240 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 7337653
    18 : 51 : 55. 244 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 7337654
    //省略部分输出
    18 : 51 : 55. 595 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 7337747
    18 : 51 : 55. 598 [RxNewThreadScheduler- 1 ]INFO c.c.d.r.b.BackpressureDemo-consumer
- > 14161628
    从输出可以看到，上游主题连续不断地弹射，下游订阅者在接收完 127 后，直接跳到了 7337652
中间弹射出来的几百万条旧一点的数据直接被丢弃了。
除了“最近模式”Emitter. BackpressureMode. LATEST，RxJava 在 Emitter<T>接口中通过一个枚
举常量定义了以下几种背压模式：
enumBackpressureMode{
/**
*Nobackpressureisapplied。无背压模式
*可能导致 rx. exceptions. MissingBackpressureException 异常
*或者 IllegalStateException 异常
*/
NONE,
/**
*抛出 rx. exceptions. MissingBackpressureException 异常，如果消费者跟不上时
*/
ERROR,
/**
*缓存所有的 onNext 方法弹射出来的消息，等待消费者慢慢地消费
*/
BUFFER,
/**
*如果下游消费跟不上，丢弃 onNext 方法弹射出来的新消息
*/
DROP,
/**
*如果消费者跟不上，丢掉旧的消息，缓存 onNext 方法弹射出来的新消息


*/
LATEST
}
对于以上的 RxJava 背压模式，具体的介绍如下：
（ 1 ）BackpressureMode. NONE 和 BackpressureMode. ERROR
在这两种模式中发送的数据，不使用背压。当上游 Observable 主题弹射数据的速度大于下游通
过 Subscriber 接收的速度，造成上游数据积压时，会抛出 MissingBackpressureException 异常。

（ 2 ）BackpressureMode. BUFFER
在这种模式下，有一个无限的缓冲区（初始化时是 128 ）。下游消费不了的元素，统统都会放
到缓冲区中。如果缓冲区中持续地积累，会导致内存耗尽，最终抛出 OutOfMemoryException 异常。

（ 3 ）BackpressureMode. DROP
这种模式下 Observable 主题使用固定大小为 1 的缓冲区。如果下游订阅者无法处理，则流的第
一个元素会缓存下来，后续的会被丢弃。

（ 4 ）BackpressureMode. LATEST
这种模式与 BackpressureMode. DROP 类似，并且 Observable 主题也使用固定大小为 1 的缓冲区。
BackpressureMode. LATEST 的缓存策略是使用最新的弹出元素替换缓冲区缓存的元素。当消费者可
以处理下一个元素时，它收到的是 Observable 最近一次弹出的元素。


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
反应器模式 vs 观察者模式
https://www.cnblogs.com/crazymakercircle/p/ 9902589 .html

##### 3. 相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题 ：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 6 章 Hystrix RPC 保护的原理

本章从 SpringCloud 架构中 RPC 保护的目标开始介绍，为大家揭开 HystrixRPC 的核心原理的神
秘面纱，让大家在使用 Hystrix 和对它进行配置时，做到知其然也能知其所以然。

## 6. 1 RPC 保护的目标

###### 在分布式多节点集群架构系统内部的节点之间进行 RPC 保护的目标大致如下：

###### （ 1 ）最为重要的目标，避免整个系统出现级联失败而雪崩

###### 在 RPC 调用过程中，需要防止由于单个服务的故障而耗尽整个服务集群的线程资源，避免分

###### 布式环境里大量级联失败。

###### （ 2 ）RPC 调用能够相互隔离

###### 为每一个目标服务维护一个线程池（或信号量），即使其中某个目标服务的调用资源被耗尽，

###### 也不会影响到对其他服务的 RPC 调用。当目标服务的线程池（或信号量）被耗尽时，则拒绝 RPC 调用。

###### （ 3 ）能够快速的降级和恢复

###### 当 RPC 目标服务故障时，能够快速和优雅地降级；当 RPC 目标服务失效后又恢复正常时，能快

###### 速恢复。

###### （ 4 ）能够对 RPC 调用提供接近实时的监控和警报

###### 监控信息包括请求成功、请求失败、请求超时和线程拒绝。如果对特定服务 RPC 调用的错误

###### 百分比超过阈值，则后续的 RPC 调用自动失败，一段时间内停止对该服务的所有请求。

前面第 3 章内容介绍过 SpringCloud 在调用处理器中使用了 HystrixCommand 命令封装 RPC 调用，
从而实现 RPC 保护。


### 6. 2 HystrixCommand 简介

Hystrix 使用命令模式，并结合 RxJava 的响应式编程和滑动窗口技术，实现了对外部服务 RPC
调用的保护。
Hystrix 实现了 HystrixCommand 或 HystrixObservableCommand 两个命令类，用于封装需要保护
的 RPC 调用。由于其中的 HystrixObservableCommand 命令不具备同步执行的能力、只具备异步执行
能力，而 HystrixCommand 命令却都具备，并且 SpringCloud 中重点使用了 HystrixCommand，因此本
章将以 HystrixCommand 命令为着重点介绍 Hystrix 的原理和使用。

#### 6. 2. 1 HystrixCommand 的使用

如果不是在 SpringCloud 的开发环境中使用 HystrixCommand 命令，则需要增加其 Maven 的依赖
坐标，大致如下：
<dependency>
<groupId>com. netflix. hystrix</groupId>
<artifactId>hystrix-core</artifactId>
</dependency>
独立使用 HystrixCommand 命令，主要涉及以下两个步骤：
继承 HystrixCommand 类，将正常的业务逻辑实现在继承的 run 方法中，将回退的业
务逻辑实现在继承的 getFallback 方法中。
使用 HystrixCommand 类提供的启动方法，执行启动命令。
HystrixCommand 命令的 run 方法是异步调用（或者同步调用）时被调度执行的方法，getFallback
方法是当 run 执行异常（或超时等）时的失败回退方法。
使用 HystrixCommand 命令时，需要通过它的启动方法（如 execute）去启动 HystrixCommand 执
行。这个过程有点像使用 Thread 时通过 start 去启动 run 的执行。
HystrixCommand 命令的完整执行过程比较复杂，简化版本的 HystrixCommand 命令的执行过程
大致如图 6 - 1 所示。

```
图 6 - 1 简化版本的 HystrixCommand 命令的执行过程
```

下面通过继承 HystrixCommand 创建一个简单的 HTTP 请求命令，并且对 HTTP 请求过程中执行
的总次数、失败的总次数进行了统计，具体的代码如下：
packagecom. crazymaker. demo. hystrix;
//省略 import
@Slf 4 j
publicclassHttpGetterCommandextendsHystrixCommand<String>
{
privateStringurl;
//run 方法是否执行
privatebooleanhasRun=false;
//执行的次序
privateintindex;
//执行的总次数，线程安全
privatestaticAtomicIntegertotal=newAtomicInteger ( 0 );
//失败的总次数，线程安全
privatestaticAtomicIntegerfailed=newAtomicInteger ( 0 );
publicHttpGetterCommand (Stringurl, Settersetter)
{
super (setter);
this. url=url;
}
@Override
protectedStringrun () throwsException
{
hasRun=true;
index=total.incrementAndGet ();
log.info ("req{}begin...", index);
StringresponseData=HttpRequestUtil.simpleGet (url);
log.info ("req{}end:{}", index, responseData);
return"req"+index+": "+responseData;
}
@Override
protectedStringgetFallback ()
{
//是否直接失败
booleanisFastFall=! hasRun;
if (isFastFall)
{
index=total.incrementAndGet ();
}
if (super.isCircuitBreakerOpen ())
{
HystrixCommandMetrics. HealthCountshc=super.getMetrics (). getHealthCounts ();
log.info ("windowtotalRequests：{}, errorPercentage:{}",
hc.getTotalRequests (),//滑动窗口总的请求数
hc.getErrorPercentage ());//滑动窗口出错比例
}
//熔断器是否打开
booleanisCircuitBreakerOpen=isCircuitBreakerOpen ();
log.info ("req{}fallback: 熔断{}, 直接失败{}，失败次数{}",
index,
isCircuitBreakerOpen,
isFastFall,
failed.incrementAndGet ());


return"req"+index+": 调用失败";
}
}
以上自定义的 HTTP 请求命令 HttpGetterCommand 继承了 HystrixCommand，并且实现了该基类
的 run 和 getFallback 两个方法。在构造函数中，使用 HystrixCommand. Setter 配置实例对该基类的实例
进行了初始化。
HttpGetterCommand 的测试用例代码大致如下：
packagecom. crazymaker. demo. hystrix;
...
@Slf 4 j
publicclassHystryxCommandExcecuteDemo
{
/***测试 HttpGetterCommand */
@Test
publicvoidtestHttpGetterCommand () throwsException
{
/**
* 构造配置实例
*/
HystrixCommand. Settersetter=HystrixCommand. Setter
.withGroupKey (HystrixCommandGroupKey.Factory.asKey ("group- 1 "))
.andCommandKey (HystrixCommandKey.Factory.asKey ("command- 1 "))
.andThreadPoolKey (HystrixThreadPoolKey.Factory.asKey ("threadPool- 1 "));
/**测试 HttpGetterCommand*/
Stringresult=newHttpGetterCommand (HELLO_TEST_URL, setter). execute ();
log.info ("result={}", result);
}
}
用例中首先构造一个配置实例 setter，配置了非常基础的命令组 Key（GroupKey）、命令 Key
（CommandKey）、线程池 Key（ThreadPoolKey）三个配置项，然后创建 HttpGetterCommand 实例
并使用 execute () 执行该命令，执行的结果大致如下：
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand-req 1 begin...
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand-req 1 fallback: 熔断 false,
直接失败 false，失败次数 1
[main]INFO c.c.d.h.HystryxCommandExcecuteDemo-result=req 1 : 调用失败
这里的 HttpGetterCommand 实例所请求的地址是一个常量，其值如下：
/**
*演示用地址：demo-provider 的 REST 接口 /api/demo/hello/v 1
*根据实际的地址调整
*/
publicstaticfinalStringHELLO_TEST_URL=
"http://crazydemo.com: 7700 /demo-provider/api/demo/hello/v 1 ";
为了演示启动请求失败的过程，这里特意没有启动 demo-provider 服务，所以从上面的执行结
果中可以看到由于 HTTP 请求失败所以回退方法被成功地执行到了。

#### 6. 2. 2 HystrixCommand 的配置内容和方式

HystrixCommand 命令的配置方式之一是使用 HystrixCommand. Setter 配置实例进行配置，最简
单的配置实例如下：


HystrixCommand. Settersetter=HystrixCommand. Setter
.withGroupKey (HystrixCommandGroupKey.Factory.asKey ("group- 1 "))
.andCommandKey (HystrixCommandKey.Factory.asKey ("command- 1 "))
.andThreadPoolKey (HystrixThreadPoolKey.Factory.asKey ("threadPool- 1 "));
其中涉及以下 3 个配置项：
1 ）CommandKey：该命令的名称。
2 ）GroupKey：该命令属于哪一个组，以帮助我们更好地组织命令。
3 ）ThreadPoolKey：该命令所属线程池的名称，相同的线程池名称会共享同一线程池，若不做
配置，会默认使用 GroupKey 作为线程池名称。

除此之外，还可以通过 HystrixCommand. Setter 配置实例整体设置一些其他的属性集合，大致有：
1 ）CommandProperties：与命令执行相关的一些属性集，包括降级设置、熔断器的配置、隔离
策略以及一些监控指标配置项等。
2 ）ThreadPoolProerties：与线程池相关的一些属性集，包括线程池大小、排队队列的大小等。
由于本书的很多的用例要用到 HystrixCommand. Setter 配置实例，因此专门写了一个方法获取配
置实例，其源码如下：
packagecom. crazymaker. demo. hystrix;
...
@Slf 4 j
publicclassSetterDemo
{
publicstaticHystrixCommand.SetterbuildSetter (
StringgroupKey,
StringcommandKey,
StringthreadPoolKey)
{
/**
*与命令执行相关的一些属性集
*/
HystrixCommandProperties. SettercommandSetter=
HystrixCommandProperties.Setter ()
//至少有 3 个请求，熔断器才达到熔断触发的次数阈值
.withCircuitBreakerRequestVolumeThreshold ( 3 )
//熔断器中断请求 5 秒后会进入 half-open 状态，尝试放行
.withCircuitBreakerSleepWindowInMilliseconds ( 5000 )
//错误率超过 60 %，快速失败
.withCircuitBreakerErrorThresholdPercentage ( 60 )
//启用超时
.withExecutionTimeoutEnabled (true)
//执行的超时时间，默认为 1000 毫秒
.withExecutionTimeoutInMilliseconds ( 5000 )
//可统计的滑动窗口内的 buckets 数量，用于熔断器和指标发布
.withMetricsRollingStatisticalWindowBuckets ( 10 )
//可统计的滑动窗口的时间长度
//这段时间内的执行数据用于熔断器和指标发布
.withMetricsRollingStatisticalWindowInMilliseconds ( 10000 );
/**
*线程池配置
*/
HystrixThreadPoolProperties. SetterpoolSetter=


HystrixThreadPoolProperties.Setter ()
//这里设置线程池大小为 5
.withCoreSize ( 5 )
.withMaximumSize ( 5 );
/**
*与线程池相关的一些属性集
*/
HystrixCommandGroupKeyhGroupKey=
HystrixCommandGroupKey.Factory.asKey (groupKey);
HystrixCommandKeyhCommondKey=HystrixCommandKey.Factory.asKey (commandKey);
HystrixThreadPoolKeyhThreadPoolKey=
HystrixThreadPoolKey.Factory.asKey (threadPoolKey);
HystrixCommand. SetterouterSetter=HystrixCommand. Setter
.withGroupKey (hGroupKey)
.andCommandKey (hCommondKey)
.andThreadPoolKey (hThreadPoolKey)
.andCommandPropertiesDefaults (commandSetter)
.andThreadPoolPropertiesDefaults (poolSetter);
returnouterSetter;
}
}
以上代码中涉及的配置项比较多，后文都会一一介绍。
命令的配置方式之二是使用 Hystrix 提供的 ConfigurationManager 配置管理类的工厂实例对
HystrixCommand 命令的执行参数进行配置。下面是一个简单的实例：
//熔断器的请求次数阈值：大于 3 次请求
ConfigurationManager
.getConfigInstance ()
.setProperty (
"hystrix. command. default. circuitBreaker. requestVolumeThreshold", 3 );
SpringCloudHystrix 所使用的就是这种配置方法。

### 6. 3 HystrixCommand 命令执行的方法

```
前面讲到，独立使用 HystrixCommand 命令主要涉及以下两个步骤：
继承 HystrixCommand 类，将正常的业务逻辑实现在继承的 run 方法中，将回退的业
务逻辑实现在继承的 getFallback 方法中。
调用 HystrixCommand 类提供的启动方法，执行启动命令。
HystrixCommand 提供了 4 个启动方法：execute ()、queue ()、observe ()、toObservable ()。
```
#### 6. 3. 1 execute () 方法

HystrixCommand 的 execute () 方法以同步堵塞方式执行 run ()。一旦开始执行该命令，当前线程
会阻塞，直到该命令返回结果，然后才能继续执行下面的逻辑。
HystrixCommand 的 execute () 方法的使用示例具体如下：
packagecom. crazymaker. demo. hystrix;


...
@Slf 4 j
publicclassHystryxCommandExcecuteDemo
{
publicstaticfinalintCOUNT= 5 ;
/**
*测试同步执行
*/
@Test
publicvoidtestExecute () throwsException
{
/**
*使用统一配置类
*/
HystrixCommand. Settersetter=SetterDemo.buildSetter (
"group- 1 ",
"testCommand",
"testThreadPool");
/**
*循环 5 次
*/
for (inti= 0 ;i<COUNT; i++)
{
Stringresult=
newHttpGetterCommand (HELLO_TEST_URL, setter). execute ();
log.info ("result={}", result);
}
Thread.sleep (Integer. MAX_VALUE);
}
}
运行测试示例前需要启动 demo-provider 实例，确保其 REST 接口 /api/demo/hello/v 1 可以正常
访问。执行上面的程序，输出的主要结果如下：
08 : 20 : 05. 488 [hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand- 第 1 次请求->
begin...
08 : 20 : 08. 698 [hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand- 第 1 次请求->
end！
08 : 20 : 08. 708 [main]INFO c.c.d.h.CommandTester-第 1 次请求的结果：{"status": 200 ,"msg": "
操作成功","data":{"hello": "world"}}
08 : 20 : 08. 710 [hystrix-testThreadPool- 2 ]INFO c.c.d.h.HttpGetterCommand- 第 2 次请求->
begin...
08 : 20 : 10. 741 [hystrix-testThreadPool- 2 ]INFO c.c.d.h.HttpGetterCommand- 第 2 次请求->
end！
08 : 20 : 10. 744 [main]INFO c.c.d.h.CommandTester-第 2 次请求的结果：{"status": 200 ,"msg": "
操作成功","data":{"hello": "world"}}
08 : 20 : 10. 751 [hystrix-testThreadPool- 3 ]INFO c.c.d.h.HttpGetterCommand- 第 3 次请求->
begin...
08 : 20 : 12. 766 [hystrix-testThreadPool- 3 ]INFO c.c.d.h.HttpGetterCommand- 第 3 次请求->
end！
08 : 20 : 12. 767 [main]INFO c.c.d.h.CommandTester-第 3 次请求的结果：{"status": 200 ,"msg": "
操作成功","data":{"hello": "world"}}
//省略后面的重复请求的输出
从结果中可以看出，Hystrix 会从线程池中取一个线程来执行 HttpGetterCommand 命令的 run ()
方法，命令执行过程中，main 线程一直在等待 run () 的返回值。


#### 6. 3. 2 queue () 方法

HystrixCommand 的 queue () 方法以异步非阻塞方式执行 run () 方法，该方法直接返回一个 Future
对象。可通过 Future.get () 拿到 run () 的返回结果，但 Future.get () 是阻塞执行的。
HystrixCommand 的 queue () 方法的使用示例代码如下：
packagecom. crazymaker. demo. hystrix;
...
@Slf 4 j
publicclassHystryxCommandExcecuteDemo
{
@Test
publicvoidtestQueue () throwsException{
/**
*使用统一配置
*/
HystrixCommand. Settersetter=getSetter (
"group- 1 ",
"testCommand",
"testThreadPool");
List<Future<String>>flist=newLinkedList<>();
/**
*同时发起 5 个异步请求
*/
for (inti= 0 ;i<COUNT; i++){
Future<String>future=newHttpGetterCommand (TEST_URL, setter). queue ();
flist.add (future);
}
/**
*统一获取异步请求的结果
*/
Iterator<Future<String>>it=flist.iterator ();
intcount= 1 ;
while (it.hasNext ()){
Future<String>future=it.next ();
Stringresult=future.get ( 10 ,TimeUnit. SECONDS);
log.info ("第{}次请求的结果：{}", count++, result);
}
Thread.sleep (Integer. MAX_VALUE);
}
}
运行测试这个示例程序前需要启动 demo-provider 实例，确保其 REST 接口/api/demo/hello/v 1 可
以正常访问。执行上面的程序，主要的输出结果如下：
08 : 30 : 54. 618 [hystrix-testThreadPool- 2 ]INFO c.c.d.h.HttpGetterCommand- 第 3 次请求->
begin...
08 : 30 : 54. 618 [hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand- 第 4 次请求->
begin...
08 : 30 : 54. 618 [hystrix-testThreadPool- 4 ]INFO c.c.d.h.HttpGetterCommand- 第 5 次请求->
begin...
08 : 30 : 54. 618 [hystrix-testThreadPool- 3 ]INFO c.c.d.h.HttpGetterCommand- 第 2 次请求->
begin...
08 : 30 : 54. 618 [hystrix-testThreadPool- 5 ]INFO c.c.d.h.HttpGetterCommand- 第 1 次请求->
begin...
08 : 30 : 58. 358 [hystrix-testThreadPool- 2 ]INFO c.c.d.h.HttpGetterCommand- 第 3 次请求->end！
08 : 30 : 58. 358 [hystrix-testThreadPool- 3 ]INFO c.c.d.h.HttpGetterCommand- 第 2 次请求->end！
08 : 30 : 58. 358 [hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand- 第 4 次请求->end！


08 : 30 : 58. 358 [hystrix-testThreadPool- 4 ]INFO c.c.d.h.HttpGetterCommand- 第 5 次请求->end！
08 : 30 : 58. 358 [hystrix-testThreadPool- 5 ]INFO c.c.d.h.HttpGetterCommand- 第 1 次请求->end！
08 : 30 : 58. 364 [main]INFO c.c.d.h.CommandTester-第 1 次请求的结果：{"status": 200 ,"msg": "
操作成功","data":{"hello": "world"}}
08 : 30 : 58. 365 [main]INFO c.c.d.h.CommandTester-第 2 次请求的结果：{"status": 200 ,"msg": "
操作成功","data":{"hello": "world"}}
08 : 30 : 58. 365 [main]INFO c.c.d.h.CommandTester-第 3 次请求的结果：{"status": 200 ,"msg": "
操作成功","data":{"hello": "world"}}
08 : 30 : 58. 365 [main]INFO c.c.d.h.CommandTester-第 4 次请求的结果：{"status": 200 ,"msg": "
操作成功","data":{"hello": "world"}}
08 : 30 : 58. 365 [main]INFO c.c.d.h.CommandTester-第 5 次请求的结果：{"status": 200 ,"msg": "
操作成功","data":{"hello": "world"}}
实际上，前面介绍的 HystrixCommand 的 execute () 方法是在内部使用 queue (). get () 的方式完成同
步调用的。

#### 6. 3. 3 observe () 方法

HystrixCommand 的 observe () 方法会返回一个响应式编程 Observable 主题。可以为该主题对象注
册 Subscriber 观察者回调实例，或者注册 Action 1 不完全回调实例来响应式处理命令的执行结果。
HystrixCommand 的 observe () 方法的使用示例代码如下：
packagecom. crazymaker. demo. hystrix;
...
@Slf 4 j
publicclassHystryxCommandExcecuteDemo
{
@Test
publicvoidtestObserve () throwsException
{
/**
*使用统一配置类
*/
HystrixCommand. Settersetter=SetterDemo.buildSetter (
"group- 1 ",
"testCommand",
"testThreadPool");
Observable<String>observe=newHttpGetterCommand (HELLO_TEST_URL, setter)
.observe ();
Thread.sleep ( 1000 );
log.info ("订阅尚未开始！");
//订阅 3 次
observe.subscribe (result->log.info ("onNextresult={}", result),
error->log.error ("onErrorerror={}", error));
observe.subscribe (result->log.info ("onNextresult={}", result),
error->log.error ("onErrorerror={}", error));
observe.subscribe (
result->log.info ("onNextresult={}", result),
error->log.error ("onErrorerror={}", error),
()->log.info ("onCompletedcalled") );
Thread.sleep (Integer. MAX_VALUE);
}
}
运行测试示例前，需要启动 demo-provider 实例，确保其 REST 接口/api/demo/hello/v 1 可以正常
访问。执行上面的程序，主要的结果如下：


[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand-req 1 begin...
[main]INFO c.c.d.h.HystryxCommandExcecuteDemo-订阅尚未开始！
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand- req 1 end:
{"respCode": 0 ,"respMsg": "操作成功","datas":{"hello": "world"}}
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-onNext
result=req 1 :{"respCode": 0 ,"respMsg": "操作成功","datas":{"hello": "world"}}
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-onNextresult
=req 1 :{"respCode": 0 ,"respMsg": "操作成功","datas":{"hello": "world"}}
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-onNext
result=req 1 :{"respCode": 0 ,"respMsg": "操作成功","datas":{"hello": "world"}}
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-onCompleted
called
从执行结果可知，如果 HystrixCommand 的 run () 执行成功则触发订阅者的 onNext () 和
onCompleted () 回调方法，如果执行异常则触发订阅者的 onError () 回调方法。
调用 HystrixCommand 的 observe () 方法会返回一个 hotObservable（热主题）。什么叫作热主题
呢？就是无论主题是否存在观察者订阅，都会自动触发执行其 run () 方法。另外还有一点，observe ()
方法所返回的主题可以重复订阅。

#### 6. 3. 4 toObservable () 方法

HystrixCommand 的 toObservable () 方法也会返回一个响应式编程 Observable 主题。同样可以为该
主题对象注册 Subscriber 观察者回调实例，或者注册 Action 1 不完全回调实例来响应式处理命令的执
行结果。不过，和 observe () 返回的主题不同，Observable 主题返回的是 coldObservable（冷主题），
并且只能被订阅一次。
HystrixCommand 的 toObservable () 方法的使用示例代码如下：
packagecom. crazymaker. demo. hystrix;
...
@Slf 4 j
publicclassHystryxCommandExcecuteDemo
{
@Test
publicvoidtestToObservable () throwsException
{
/**
*使用统一配置类
*/
HystrixCommand. Settersetter=SetterDemo.buildSetter (
"group- 1 ",
"testCommand",
"testThreadPool");
for (inti= 0 ;i<COUNT; i++)
{
Thread.sleep ( 2 );
newHttpGetterCommand (HELLO_TEST_URL, setter)
.toObservable ()
.subscribe (result->log.info ("result={}", result),
error->log.error ("error={}", error)
);
}
Thread.sleep (Integer. MAX_VALUE);
}
}


在运行测试示例前，需要启动 demo-provider 实例，确保其 REST 接口/api/demo/hello/v 1 可以正
常访问。执行上面的程序，主要的输出结果如下：
[hystrix-testThreadPool- 5 ]INFO c.c.d.h.HttpGetterCommand-req 3 begin...
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand-req 2 begin...
[hystrix-testThreadPool- 3 ]INFO c.c.d.h.HttpGetterCommand-req 4 begin...
[hystrix-testThreadPool- 2 ]INFO c.c.d.h.HttpGetterCommand-req 1 begin...
[hystrix-testThreadPool- 4 ]INFO c.c.d.h.HttpGetterCommand-req 5 begin...
[hystrix-testThreadPool- 4 ]INFO c.c.d.h.HttpGetterCommand- req 5 end:
{"respCode": 0 ,...}
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HttpGetterCommand- req 2 end:
{"respCode": 0 ,...}
[hystrix-testThreadPool- 3 ]INFO c.c.d.h.HttpGetterCommand- req 4 end:
{"respCode": 0 ,...}
[hystrix-testThreadPool- 2 ]INFO c.c.d.h.HttpGetterCommand- req 1 end:
{"respCode": 0 ,...}
[hystrix-testThreadPool- 5 ]INFO c.c.d.h.HttpGetterCommand- req 3 end:
{"respCode": 0 ,...}
[hystrix-testThreadPool- 1 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-
result=req 2 :{...}
[hystrix-testThreadPool- 3 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-
result=req 4 :{...}
[hystrix-testThreadPool- 5 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-
result=req 3 :{...}
[hystrix-testThreadPool- 4 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-
result=req 5 :{...}
[hystrix-testThreadPool- 2 ]INFO c.c.d.h.HystryxCommandExcecuteDemo-
result=req 1 :{...}
什么是 coldObservable（冷主题）？就是在进行主题获取的时候，不会立即触发执行，只有在
观察者订阅时才会执行内部的 HystrixCommand 命令的 run () 方法。
对比起来，toObservable () 方法和 observe () 方法之间的区别如下：
1 ）observe () 和 toObservable () 虽然都返回了 Observable 主题，但是 observe () 返回的是热主题，
toObservable () 返回的是冷主题。
2 ）observe () 返回的主题可以被多次订阅，而 toObservable () 返回的主题只能被单次订阅。
在使用@HystrixCommand 注解时，observe () 方法对应的执行模式为 EAGER，toObservable () 方
法对应的执行模式为 LAZY，具体如下：
//此注解使用 observe () 方法来获取主题
@HystrixCommand (observableExecutionMode=ObservableExecutionMode. EAGER)
//此注解使用 toObservable () 方法来获取冷主题
@HystrixCommand (observableExecutionMode=ObservableExecutionMode. LAZY)
由于本书仅结合 SpringCloud 介绍 Hystrix 核心原理，并没有涉及@HystrixCommand 注解的单独
使用，因此也不对@HystrixCommand 注解进行详细介绍。

#### 6. 3. 5 HystrixCommand 的执行方法之间的关系

实际上，Hystrix 内部总是以 Observable 的形式作为响应式的调用，不同执行命令方法只是进行
了相应 Observable 转换。Hystrix 的核心类 HystrixCommand 尽管只返回单个结果，但也确实是基于
RxJava 的 Observable 主题类实现的。


前面介绍到，需获取 HystrixCommand 命令的结果，可以调用 execute ()、queue ()、observe () 和
toObservable () 这 4 种方法，它们之间的关系如图 6 - 2 所示。

图 6 - 2 4 种方法之间的关系
execute ()、queue ()、observe () 和 toObservable () 这 4 种方法之间的调用关系如下：
1 ）toObservable () 返回一个冷主题，订阅者可以进行结果订阅。
2 ）observe () 首先调用 toObservable () 获得一个冷主题，再创建一个 ReplaySubject 重复主题去订
阅该冷主题，然后将重复主题转化为热主题。因此调用 observe () 会自动触发执行 run ()/construct () 方法。
3 ）queue () 调用了 toObservable (). toBlocking (). toFuture ()。详细来说，queue () 首先通过
toObservable () 来获得一个冷主题，然后通过 toBlocking () 将该冷主题转换成 BlockingObservable 阻塞
主题，该主题可以把数据以阻塞的方式发送出来，最后通过 toFuture 方法是把 BlockingObservable 阻
塞主题转换成一个 Future 异步回调实例，并且返回该 Future 实例。但是，queue () 自身并不会阻塞，
消费者可以自己决定如何处理 Future 的异步回调操作。
4 ）execute () 调用了 queue (). get ()，阻塞消费者的线程，同步获取 Future 异步回调实例的结果。
除了定义了 HystrixCommand 这个具备同步获取结果的命令处理器之外，Hystrix 还定义了另一
个只具备响应式编程能力的命令处理器 HystrixObservableCommand，该命令没有实现 execute () 和
queue () 两种方法，仅实现了 observe () 和 toObservable () 两种方法，如图 6 - 3 所示。

```
图 6 - 3 HystrixObservableCommand 纯响应式命令处理器的执行方法
```

### 6. 4 RPC 保护之舱壁模式

本节为大家介绍 RPC 保护的重要方法——舱壁模式，并且重点介绍 Hystrix 线程池隔离、信号
量隔离的具体配置方式。

#### 6. 4. 1 什么是舱壁模式

###### 船舶工业上为了使船不容易沉没，使用舱壁将船舶划分为几个部分，以便在船体遭到破坏的

###### 情况下可以将船舶各个部件密封起来。泰坦尼克号沉没的主要原因之一就是其舱壁设计不合理，水

###### 可以通过上面的甲板进入舱壁的顶部，导致整个船体淹没。

###### 在 RPC 调用过程中，使用舱壁模式可以保护有限的系统资源不被耗尽。在一个基于微服务的应

###### 用程序中，通常需要调用多个微服务提供者的接口才能完成一个特定任务。不使用舱壁模式，所有

###### 的 RPC 调用都从同一个线程池中获取线程，一个具体的实例如图 6 - 4 所示。在该实例中，微服务提供

者 ProviderA 对依赖 ProviderB、ProviderC、ProviderD 的所有 RPC 调用都从公共的线程池中获取线程。

图 6 - 4 公共的 RPC 线程池
在高服务器请求的情况下，对某个性能较低的微服务提供者的 RPC 调用很容易“霸占”整个
公共的 RPC 线程池，对其他性能正常的微服务提供者的 RPC 调用往往需要等待线程资源的释放。最
后，整个 Web 容器（Tomcat）会崩溃。现在假定 ProviderA 的 RPC 线程个数为 1000 ，而并发量非常
大，其中有 500 个线程来执行 ProviderB 的 RPC 调用，如果 ProviderB 不小心宕机了，那么这 500 个线
程都会超时，此时剩下的服务 ProviderC、ProviderD 的总共可用的线程为 500 个，随着并发量的增
大，剩余的 500 个线程估计也会被 ProviderB 的 RPC 耗尽，然后 ProviderA 进入瘫痪，最后导致整个
系统的所有服务都不可用，这就是服务的雪崩效应。
为了最大限度地减少 Provider 之间的相互影响，一个很好的做法是对于不同的微服务提供者设
置不同的 RPC 调用线程池，让不同 RPC 通过专门的线程池请求到各自的 Provider 微服务提供者，像
舱壁一样对 Provider 进行隔离。对于不同的微服务提供者设置不同的 RPC 调用线程池，这种模式就
叫作舱壁模式，如图 6 - 5 所示。


图 6 - 5 舱壁模式的 RPC 线程池
使用舱壁模式可以避免对单个 Provider 的 RPC 消耗掉所有资源，从而防止由于某一个服务性能
底而引起的级联故障和雪崩效应。在 ProviderA 中，假定对服务 ProviderB 的 RPC 调用分配专门的线
程池，该线程池叫作 ThreadPoolB，其中有 10 个线程，只要对 ProviderB 的 RPC 并发量超过了 10 ，
后续的 RPC 就走降级服务，就算服务的 ProviderB 挂了，最多也就导致 ThreadPoolB 不可用，而不
会影响系统中的其他服务的 RPC。
一般来说，RPC 线程与 Web 容器的 IO 线程也是需要隔离的。如图 6 - 6 所示，当 ProviderA 的用户
请求涉及 ProviderB 和 ProviderC 的 RPC 的时候，ProviderA 的 IO 线程会将任务交给对应的 RPC 线程池
里面的 RPC 线程来执行，ProviderA 的 IO 线程就可以去干别的事情去了，当 RPC 线程执行完远程调
用的任务之后，就会将调用的结果返回给 IO 线程。如果 RPC 线程池耗尽了，IO 线程池也不会受到影
响，从而实现 RPC 线程与 Web 容器的 IO 线程的相互隔离。

```
图 6 - 6 RPC 线程与 Web 容器的 IO 线程相互隔离
```

Hystrix 提供了两种 RPC 隔离方式：线程池隔离和信号量隔离。由于信号量隔离不太适合使用
在 RPC 调用的场景，所以这里重点介绍线程池隔离。虽然线程在就绪状态、运行状态、阻塞状态、
终止状态间转变时需要由操作系统调度，这会带来一定的性能消耗，但是 Netflix 详细评估了使用异
步线程和同步线程带来的性能差异，结果表明在 99 %的情况下异步线程带来的延迟仅为几毫秒，这
种性能的损耗对于用户程序来说是完全可以接受的。

#### 6. 4. 2 Hystrix 线程池隔离

Hystrix 既可以为 HystrixCommand 命令默认创建一个线程池，也可以关联上一个指定的线程池。
每一个线程池都有一个 Key，叫作 ThreadPoolKey（线程池名）。如果没有为 HystrixCommand 指定
线程池，Hystrix 会为 HystrixCommand 创建一个与 GroupKey（命令组 Key）同名的线程池，当然，
如果与 GroupKey 同名的线程池已经存在，则直接进行关联。也就是说，默认情况下，
HystrixCommand 命令的 ThreadPoolKey 与 GroupKey 是相同的。总体来说，线程池就是隔离的关键，
所有的监控、调用、缓存等都围绕线程池展开。
如果要指定线程池，可以通过如下代码在 Setter 中定制线程池的 Key 和属性：
/**
*在 Setter 实例中指定线程池的 Key 和属性
*/
HystrixCommand. SetterrpcPool 1 _setter=HystrixCommand. Setter
.withGroupKey (HystrixCommandGroupKey.Factory.asKey ("group 1 "))
.andCommandKey (HystrixCommandKey.Factory.asKey ("command 1 "))
.andThreadPoolKey (HystrixThreadPoolKey.Factory.asKey ("threadPool 1 "))
.andThreadPoolPropertiesDefaults (
HystrixThreadPoolProperties.Setter ()
.withCoreSize ( 10 ) //配置线程池里的线程数
.withMaximumSize ( 10 )
);
然后，可以通过 HystrixCommand 或者 HystrixObservableCommand 的构造函数传入 Setter 配置
实例：
@Slf 4 j
publicclassHttpGetterCommandextendsHystrixCommand<String>
{
privateStringurl;
...
publicHttpGetterCommand (Stringurl, Settersetter)
{
super (setter);
this. url=url;
}
...
}
HystrixThreadPoolKey 是一个接口，它有一个辅助工厂类 Factory，它的 asKey（String）方法专
门用于创建一个线程池的 Key，示例代码如下：
HystrixThreadPoolKey.Factory.asKey ("threadPoolN")
下面是一个完整的线程池隔离演示例子：创建了两个线程池 threadPool 1 和 threadPool 2 ，然后通
过这两个线程池发起简单的 RPC 远程调用，其中，通过 threadPool 1 线程池访问一个错误连接


ERROR_URL，通过 threadPool 2 访问一个正常连接 HELLO_TEST_URL。在实验过程中，可以通过
调整 RPC 的次数多次运行程序，然后通过结果查看线程池的具体隔离效果。
线程池隔离实例的代码如下：
packagecom. crazymaker. demo. hystrix;
//省略 import
@Slf 4 j
publicclassIsolationStrategyDemo
{
/**
*测试: 线程池隔离
*/
@Test
publicvoidtestThreadPoolIsolationStrategy () throwsException
{
/**
*RPC 线程池 1
*/
HystrixCommand. SetterrpcPool 1 _Setter=HystrixCommand. Setter
.withGroupKey (HystrixCommandGroupKey.Factory.asKey ("group 1 "))
.andCommandKey (HystrixCommandKey.Factory.asKey ("command 1 "))
.andThreadPoolKey (HystrixThreadPoolKey.Factory.asKey ("threadPool 1 "))
.andCommandPropertiesDefaults (HystrixCommandProperties.Setter ()
.withExecutionTimeoutInMilliseconds ( 5000 ) //配置执行时间上限
). andThreadPoolPropertiesDefaults (
HystrixThreadPoolProperties.Setter ()
.withCoreSize ( 10 ) //配置线程池里的线程数
.withMaximumSize ( 10 )
);
/**
*RPC 线程池 2
*/
HystrixCommand. SetterrpcPool 2 _Setter=HystrixCommand. Setter
.withGroupKey (HystrixCommandGroupKey.Factory.asKey ("group 2 "))
.andCommandKey (HystrixCommandKey.Factory.asKey ("command 2 "))
.andThreadPoolKey (HystrixThreadPoolKey.Factory.asKey ("threadPool 2 "))
.andCommandPropertiesDefaults (HystrixCommandProperties.Setter ()
.withExecutionTimeoutInMilliseconds ( 5000 ) //配置执行时间上限
). andThreadPoolPropertiesDefaults (
HystrixThreadPoolProperties.Setter ()
.withCoreSize ( 10 ) //配置线程池里的线程数
.withMaximumSize ( 10 )
);
/**
*访问一个错误连接，让 threadpool 1 耗尽
*/
for (intj= 1 ;j<= 5 ;j++)
{
newHttpGetterCommand (ERROR_URL, rpcPool 1 _Setter)
.toObservable ()
.subscribe (s->log.info ("result:{}", s));
}
/**
*访问一个正确连接，观察 threadpool 2 是否正常
*/


for (intj= 1 ;j<= 5 ;j++)
{
newHttpGetterCommand (HELLO_TEST_URL, rpcPool 2 _Setter)
.toObservable ()
.subscribe (s->log.info ("result:{}", s));
}
Thread.sleep (Integer. MAX_VALUE);
}
}
运行这个演示程序，输出的结果节选如下：
[hystrix-threadPool 1 - 4 ]INFO c.c.d.h.HttpGetterCommand-req 1 begin...
[hystrix-threadPool 1 - 3 ]INFO c.c.d.h.HttpGetterCommand-req 4 begin...
[hystrix-threadPool 2 - 3 ]INFO c.c.d.h.HttpGetterCommand-req 10 begin...
[hystrix-threadPool 2 - 5 ]INFO c.c.d.h.HttpGetterCommand-req 7 begin...
[hystrix-threadPool 1 - 5 ]INFO c.c.d.h.HttpGetterCommand-req 9 begin...
[hystrix-threadPool 2 - 1 ]INFO c.c.d.h.HttpGetterCommand-req 6 begin...
[hystrix-threadPool 1 - 1 ]INFO c.c.d.h.HttpGetterCommand-req 8 begin...
[hystrix-threadPool 1 - 2 ]INFO c.c.d.h.HttpGetterCommand-req 2 begin...
[hystrix-threadPool 2 - 4 ]INFO c.c.d.h.HttpGetterCommand-req 5 begin...
[hystrix-threadPool 2 - 2 ]INFO c.c.d.h.HttpGetterCommand-req 3 begin...
[hystrix-threadPool 1 - 1 ]INFO c.c.d.h.HttpGetterCommand-req 8 fallback: 熔断 false, 直
接失败 false
[hystrix-threadPool 1 - 4 ]INFO c.c.d.h.HttpGetterCommand-req 1 fallback: 熔断 false, 直
接失败 false
[hystrix-threadPool 1 - 2 ]INFO c.c.d.h.HttpGetterCommand-req 2 fallback: 熔断 false, 直
接失败 false
[hystrix-threadPool 1 - 3 ]INFO c.c.d.h.HttpGetterCommand-req 4 fallback: 熔断 false, 直
接失败 false
[hystrix-threadPool 1 - 5 ]INFO c.c.d.h.HttpGetterCommand-req 9 fallback: 熔断 false, 直
接失败 false
...
[hystrix-threadPool 2 - 4 ]INFO c.c.d.h.HttpGetterCommand- req 5 end:
{"respCode": 0 ,"respMsg": "操作成功...}
[hystrix-threadPool 2 - 2 ]INFO c.c.d.h.HttpGetterCommand- req 3 end:
{"respCode": 0 ,"respMsg": "操作成功...}
[hystrix-threadPool 2 - 3 ]INFO c.c.d.h.HttpGetterCommand- req 10 end:
{"respCode": 0 ,"respMsg": "操作成功...}
[hystrix-threadPool 2 - 1 ]INFO c.c.d.h.HttpGetterCommand- req 6 end:
{"respCode": 0 ,"respMsg": "操作成功...}
[hystrix-threadPool 2 - 5 ]INFO c.c.d.h.HttpGetterCommand- req 7 end:
{"respCode": 0 ,"respMsg": "操作成功...}
...
从上面的结果可知：threadPool 1 的线程使用和 threadPool 2 的线程使用是完全地相互独立和相互
隔离的，无论 threadPool 1 是否耗尽，threadPool 2 的线程都可以正常发起 RPC 请求。
默认情况下，在 SpringCloud 中，Hystrix 会为每一个 CommandGroupKey（命令组 Key）自动
创建一个同名的线程池。而在 Hystrix 客户端，每一个 RPC 目标 Provider 的 CommandGroupKey（命
令组 Key）的默认值为它的应用名称（applicationname）。比如，demo-provider 服务的 CommandGroup
Key 默认值为其名称“demo-provider”。所以，如果某个 Provider（如 uaa-provider）需发起对
demo-Provider 的远程调用，则 Hystrix 为该 Provider 创建的 RPC 线程池的名称默认为“demo-provider”，
专用于对 demo-provider 的 REST 服务进行 RPC 调用和隔离，如图 6 - 7 所示。


```
图 6 - 7 对 demo-provider 服务进行 RPC 调用的专用线程池
```
#### 6. 4. 3 Hystrix 线程池隔离配置

在 SpringCloud 微服务提供者中，如果需使用 Hystrix 线程池进行 RPC 隔离，可以在应用配置文
件中进行相应配置。下面是 demo-provider 的 RPC 线程池配置的实例：
hystrix:
threadpool:
default:
coreSize: 10 #线程池核心线程数
maximumSize: 20 #线程池最大线程数
allowMaximumSizeToDivergeFromCoreSize: true #线程池maximumSize最大线程数是否生效
keepAliveTimeMinutes： 10 #设置可空闲时间 ，单位为分钟
command:
default: #全局默认配置
execution: #RPC隔离的相关配置
isolation:
strategy: THREAD #配置请求隔离的方式 ，这里采用线程池方式
thread:
timeoutInMilliseconds: 100000 #RPC执行的超时时间 ，默认为 1000 毫秒
interruptOnTimeout: true #发生超时后是否中断方法的执行 ，默认值为 true
对上面实例中用到的与 Hystrix 线程池有关的配置项介绍如下：
（ 1 ）hystrix. threadpool. default. coreSize
设值线程池的核心线程数。
（ 2 ）hystrix. threadpool. default. maximumSize
设值线程池的最大线程数，起作用的前提是 allowMaximumSizeToDrivergeFromCoreSize 的属性
值为 true。maximumSize 属性值可以等于或者大于 coreSize 值，当线程池的线程不够用时，Hystrix 会
创建新的线程，直到线程数达到 maximumSize 的值，创建的线程为非核心线程。

```
（ 3 ）hystrix. threadpool. default. allowMaximumSizeToDivergeFromCoreSize
该属性允许 maximumSize 起作用。
```

（ 4 ）hystrix. threadpool. default. keepAliveTimeMinutes
该属性设置非核心线程的存活时间，如果某个非核心线程的空闲时间超过 keepAliveTimeMinutes 设
置的时间，非核心线程将被释放。其单位为分钟，默认值为 1 ，默认情况下非核心线程空闲 1 分钟后释放。

（ 5 ）hystrix. command. default. execution. isolation. strategy
该属性设置完成 RPC 远程调用 HystrixCommand 命令的隔离策略。它有两个可选值：THREAD、
SEMAPHORE，默认值为 THREAD。THREAD 表示使用线程池进行 RPC 隔离，SEMAPHORE 表示
通过信号量来进行 RPC 隔离和限制并发量。

（ 6 ）hystrix. command. default. execution. isolation. thread. timeoutInMilliseconds
设置调用者等待 HystrixCommand 命令执行的超时限制，超过此时间，HystrixCommand 被标记
为 TIMEOUT，并执行回退逻辑。超时会作用在 HystrixCommand.queue ()，即使调用者没有调用 get ()
去获得 Future 对象。

以上的配置是 application 应用级别的默认线程池配置，覆盖的范围为系统中的所有 RPC 线程池。
有时，需要为特定的 Provider 微服务提供者做特殊的配置，比如当某一个 Provider 的接口访问的并发
量非常大，是其他 Provider 的几十倍时，则其远程调用需要更多的 RPC 线程，这时候，可以单独为
它进行专门的 RPC 线程池配置。作为示例，在 demo-Provider 中对 uaa-provider 的 RPC 线程池配置如下：
hystrix:
threadpool:
default:
coreSize: 10 #线程池核心线程数
maximumSize: 20 #线程池最大线程数
allowMaximumSizeToDivergeFromCoreSize: true #线程池最大线程数是否有效
uaa-provider:
coreSize: 20 #线程池核心线程数
maximumSize: 100 #线程池最大线程数
allowMaximumSizeToDivergeFromCoreSize: true #线程池最大线程数是否有效
上面的配置中使用了 hystrix. threadpool. uaa-provider 配置项前缀，其中 uaa-provider 部分为 RPC 线
程池的 ThreadPoolKey（线程池名称），也就是默认的 CommandGroupKey（命令组名）。
在调用处理器 HystrixInvocationHandler 的 invoke (...) 方法内打上断点，在调试时，通过查看
hystrixCommand 对象的值可以看出，demo-provider 中针对微服务提供者 uaa-provider 的 RPC 线程池配
置已经生效，如图 6 - 8 所示。

```
图 6 - 8 针对 uaa-provider 的 RPC 线程池配置已经生效
```

#### 6. 4. 4 Hystrix 信号量隔离

除了使用线程池进行资源隔离之外，Hystrix 还可以使用信号量机制完成资源隔离。信号量所
起到的作用就像一个开关，而信号量的值就是每个命令的并发执行数量，当并发数高于信号量的值
时，就不再执行命令。比如，如果 ProviderA 的 RPC 信号量大小为 10 ，那么它同时只允许有 10 个 RPC
线程来访问 ProviderA，其他的请求都会被拒绝，从而达到资源隔离和限流保护的作用。
Hystrix 信号量机制不提供专用的线程池，也不提供额外的线程，在获取信号量之后，执行
HystrixCommand 命令逻辑的线程还是之前 Web 容器的 IO 线程。
信号量可以细分为 run 执行信号量和 fallback 回退信号量。
IO 线程在执行 HystrixCommand 命令之前，需要抢到 run 执行信号量，成功之后才允许执行
HystrixCommand.run () 方法。如果争抢失败，就准备回退，但是在执行 HystrixCommand.getFallback ()
回退方法之前，还需要争抢 fallback 回退信号量，成功之后才允许执行 HystrixCommand.getFallback ()
回退方法。如果都获取失败，则操作直接终止。
在如图 6 - 9 所示的例子中，假设有 5 个 Web 容器的 IO 线程并发进行 RPC 远程调用，但是执行信号
量的大小为 3 ，也就是只有 3 个 IO 线程能够真正地抢到 run 执行信号量，争抢成功后这些线程才能发
起 RPC 调用。剩下的 2 个 IO 线程准备回退，去抢 fallback 回退信号量，争抢成功后执行
HystrixCommand.getFallback () 回退方法。

图 6 - 9 5 个 Web 容器的 IO 线程争抢信号量
下面是一个模拟 Web 容器进行 RPC 调用的演示程序，使用一个拥有 50 个线程的线程池模拟 Web
容器的 IO 线程池，并使用随书编写的 HttpGetterCommand 命令模拟 RPC 调用。实验之前，需要提前
启动的 demo-provider 服务的 REST 接口/api/demo/hello/v 1 。
为了演示信号量隔离，演示程序所设置的 run 执行信号量和 fallback 回退信号量都为 4 ，并且通
过 IO 线程池同时提交了 50 个模拟的 RPC 调用去争抢这些信号量，具体的演示程序如下：
packagecom. crazymaker. demo. hystrix;
//省略 import
@Slf 4 j


publicclassIsolationStrategyDemo
{
/**
*测试: 信号量隔离
*/
@Test
publicvoidtestSemaphoreIsolationStrategy () throwsException
{
/**
*命令属性实例
*/
HystrixCommandProperties. SettercommandProperties=
HystrixCommandProperties.Setter ()
.withExecutionTimeoutInMilliseconds ( 5000 ) //配置时间上限
.withExecutionIsolationStrategy (
//隔离策略为信号量隔离
HystrixCommandProperties. ExecutionIsolationStrategy. SEMAPHORE
)
//HystrixCommand.run () 方法允许的最大请求数
.withExecutionIsolationSemaphoreMaxConcurrentRequests ( 4 )
//HystrixCommand.getFallback () 方法允许的最大请求数
.withFallbackIsolationSemaphoreMaxConcurrentRequests ( 4 );
/**
*命令的配置实例
*/
HystrixCommand. Settersetter=HystrixCommand. Setter
.withGroupKey (HystrixCommandGroupKey.Factory.asKey ("group 1 "))
.andCommandKey (HystrixCommandKey.Factory.asKey ("command 1 "))
.andCommandPropertiesDefaults (commandProperties);
/**
*模拟 Web 容器的 IO 线程池
*/
ExecutorServicemock_IO_threadPool=Executors.newFixedThreadPool ( 50 );
/**
* 模拟 Web 容器的并发 50
*/
for (intj= 1 ;j<= 50 ;j++)
{
mock_IO_threadPool.submit (()->
{
/**
*RPC 调用
*/
newHttpGetterCommand (HELLO_TEST_URL, setter)
.toObservable ()
.subscribe (s->log.info ("result:{}", s));
});
}
Thread.sleep (Integer. MAX_VALUE);
}
}
在执行此演示程序之前，需要启动 crazydemo. com（指向 127. 0. 0. 1 ）主机上的 demo-provider 微
服务提供者。demo-provider 启动之后，再执行上面的演示程序，运行的结果节选如下：
[pool- 2 - thread- 35 ]INFO c.c.d.h.HttpGetterCommand-req 3 fallback: 熔断 false, 直接失败
true，失败次数 3
[pool- 2 - thread- 45 ]INFO c.c.d.h.HttpGetterCommand-req 4 fallback: 熔断 false, 直接失败
true，失败次数 4


[pool- 2 - thread- 7 ]INFO c.c.d.h.HttpGetterCommand-req 2 fallback: 熔断 false, 直接失败
true，失败次数 2
[pool- 2 - thread- 15 ]INFO c.c.d.h.HttpGetterCommand-req 1 fallback: 熔断 false, 直接失败
true，失败次数 1
[pool- 2 - thread- 35 ]INFO c.c.d.h.IsolationStrategyDemo- result: req 3 : 调用失败
...
[pool- 2 - thread- 27 ]INFO c.c.d.h.HttpGetterCommand-req 7 begin...
[pool- 2 - thread- 18 ]INFO c.c.d.h.HttpGetterCommand-req 6 begin...
[pool- 2 - thread- 13 ]INFO c.c.d.h.HttpGetterCommand-req 5 begin...
[pool- 2 - thread- 48 ]INFO c.c.d.h.HttpGetterCommand-req 8 begin...
[pool- 2 - thread- 18 ]INFO c.c.d.h.HttpGetterCommand- req 6 end:
{"respCode": 0 ,"respMsg": "操作成功...}
[pool- 2 - thread- 48 ]INFO c.c.d.h.HttpGetterCommand- req 8 end:
{"respCode": 0 ,"respMsg": "操作成功...}
[pool- 2 - thread- 27 ]INFO c.c.d.h.HttpGetterCommand- req 7 end:
{"respCode": 0 ,"respMsg": "操作成功...}
[pool- 2 - thread- 13 ]INFO c.c.d.h.HttpGetterCommand- req 5 end:
{"respCode": 0 ,"respMsg": "操作成功...}
[pool- 2 - thread- 13 ]INFO c.c.d.h.IsolationStrategyDemo-
result: req 5 :{"respCode": 0 ,"respMsg": "操作成...}
...
通过结果可以看出：
1 ）执行 RPC 远程调用的线程就是模拟 IO 线程池中的线程。
2 ）虽然提交了 50 个 RPC 调用，但是只有 4 个 RPC 调用抢到了执行信号量，分别为 req 5 、req 6 、
req 7 、req 8 。
3 ）虽然失败了 46 个 RPC 调用，但是只有 4 个 RPC 调用抢到了回退信号量，分别为 req 1 、req 2 、
req 3 、req 4 。

使用信号量进行 RPC 隔离时，是有自身弱点的。由于最终 Web 容器的 IO 线程完成实际 RPC 远程
调用，这样就带来了一个问题：由于 RPC 远程调用是一种耗时的操作，如果 IO 线程被长时间占用，
将导致 Web 容器请求处理能力下降，甚至可能会在一段时间内由于 IO 线程被占满而造成 Web 容器无
法对新的用户请求及时响应，最终导致 Web 容器崩溃。因此，信号量隔离机制不适用于 RPC 隔离。
但是，对于一些非网络的 API 调用或者耗时很小的 API 调用，信号量隔离机制比线程池隔离机制的
效率更高。
再来看信号量的配置，这一次使用代码的方式进行命令属性配置，涉及 Hystrix 命令属性配置
器 HystrixCommandProperties.Setter () 的以下实例方法：

（ 1 ）withExecutionIsolationSemaphoreMaxConcurrentRequests (int)
此方法设置使用执行信号量的大小，也就是 HystrixCommand.run () 方法允许的最大请求数。如
果达到最大请求数，则后续的请求会被拒绝。
在 Web 容器中，抢占信号量的线程应该是容器（比如 Tomcat）IO 线程池的一小部分，所以信
号量的数量不能大于容器线程池大小，否则起不到保护作用。执行信号量大小的默认值为 10 。
如果使用属性配置而不是代码方式进行配置，则以上代码配置所对应的配置项为：
hystrix. command. default. execution. isolation. semaphore. maxConcurrentRequests
（ 2 ）withFallbackIsolationSemaphoreMaxConcurrentRequests (int)
此方法设置使用回退信号量的大小，也就是 HystrixCommand.getFallback () 方法允许的最大请求
数。如果达到最大请求数，则后续的回退请求会被拒绝。


###### 如果使用属性配置而不是代码方式进行配置，则以上代码配置所对应的配置项为：

hystrix. command. default. fallback. isolation. semaphore. maxConcurrentRequests
最后，介绍一下信号量隔离与线程池隔离的区别，分别从调用线程、开销、异步、并发量 4 个
维度进行对比，具体如表 6 - 1 所示。

```
表 6 - 1 调用线程、开销、异步、并发量 4 个维度的对比
线程池隔离信号量隔离
调用线程 RPC 线程与 Web 容器 IO 线程相互隔离 RPC 线程与 Web 容器 IO 线程相同
开销存等在开请销求排队、线程调度、线程上下文切换无线程切换，开销低
异步支持不支持
并发量最大线程池大小最线大程信数号量上限，且最大信号量需要小于 IO
```
### 6. 5 RPC 保护之熔断器模式

###### 熔断器的工作机制为：统计最近 RPC 调用发生错误的次数，然后根据统计值中的失败比例等

###### 信息，决定是否允许后面的 RPC 调用继续，或者快速地失败回退。熔断器的 3 种状态如下：

1 ）closed：熔断器关闭状态，这也是熔断器的初始状态，此状态下 RPC 调用正常放行。
2 ）open：失败比例到一定的阈值之后，熔断器进入开启状态，此状态下 RPC 将会快速失败，
执行失败回退逻辑。
3 ）half-open：在打开一定时间之后（睡眠窗口结束），熔断器进入半开启状态，小流量尝试
进行 RPC 调用放行。如果尝试成功则熔断器变为 closed 状态，RPC 调用正常；如果尝试失败则熔断
器变为 open 状态，RPC 调用快速失败。

```
熔断器状态之间相互转换的逻辑关系如图 6 - 10 所示。
```
```
图 6 - 10 熔断器状态之间的转换关系详细图
```

#### 6. 5. 1 熔断器状态变化的演示实例

为了观察熔断器的状态变化，通过继承 HystrixCommand 类，这里特别设计了一个能够设置运
行时长的自定义命令类 TakeTimeDemoCommand，通过设置其运行占用时间 takeTime 成员的值，可
以控制其运行过程中是否超时。演示实例的代码如下：
packagecom. crazymaker. demo. hystrix;
//省略 import
@Slf 4 j
publicclassCircuitBreakerDemo
{
//执行的总次数，线程安全
privatestaticAtomicIntegertotal=newAtomicInteger ( 0 );
/**
*内部类：一个能够设置运行时长的自定义命令类
*/
staticclassTakeTimeDemoCommandextendsHystrixCommand<String>
{
//run 方法是否执行
privatebooleanhasRun=false;
//执行的次序
privateintindex;
//运行的占用时间
longtakeTime;
publicTakeTimeDemoCommand (longtakeTime, Settersetter)
{
super (setter);
this. takeTime=takeTime;
}
@Override
protectedStringrun () throwsException
{
hasRun=true;
index=total.incrementAndGet ();
Thread.sleep (takeTime);
HystrixCommandMetrics. HealthCountshc=super.getMetrics (). getHealthCounts ();
log.info ("succeed-req{}: 熔断器状态：{}, 失败率：{}%",
index,super.isCircuitBreakerOpen (), hc.getErrorPercentage ());
return"req"+index+": succeed";
}
@Override
protectedStringgetFallback ()
{
//是否直接失败
booleanisFastFall=! hasRun;
if (isFastFall)
{
index=total.incrementAndGet ();
}
HystrixCommandMetrics. HealthCountshc=super.getMetrics (). getHealthCounts ();
log.info ("fallback-req{}: 熔断器状态：{}, 失败率：{}%",
index,super.isCircuitBreakerOpen (), hc.getErrorPercentage ());
return"req"+index+": failed";
}


}
/**
*测试用例：熔断器熔断
*/
@Test
publicvoidtestCircuitBreaker () throwsException
{
/**
*命令参数配置
*/
HystrixCommandProperties. SetterpropertiesSetter=
HystrixCommandProperties.Setter ()
//至少有 3 个请求，熔断器才达到熔断触发的次数阈值
.withCircuitBreakerRequestVolumeThreshold ( 3 )
//熔断器中断请求 5 秒后会进入 half-open 状态, 尝试放行
.withCircuitBreakerSleepWindowInMilliseconds ( 5000 )
//错误率超过 60 %，快速失败
.withCircuitBreakerErrorThresholdPercentage ( 60 )
//启用超时
.withExecutionTimeoutEnabled (true)
//执行的超时时间，默认为 1000 毫秒，这里设置为 500 毫秒
.withExecutionTimeoutInMilliseconds ( 500 )
//可统计的滑动窗口内的 buckets 数量，用于熔断器和指标发布
.withMetricsRollingStatisticalWindowBuckets ( 10 )
//可统计的滑动窗口的时间长度
//这段时间内的执行数据用于熔断器和指标发布
.withMetricsRollingStatisticalWindowInMilliseconds ( 10000 );
HystrixCommand. SetterrpcPool=HystrixCommand. Setter
.withGroupKey (HystrixCommandGroupKey.Factory.asKey ("group- 1 "))
.andCommandKey (HystrixCommandKey.Factory.asKey ("command- 1 "))
.andThreadPoolKey (HystrixThreadPoolKey.Factory.asKey ("threadPool- 1 "))
.andCommandPropertiesDefaults (propertiesSetter);
/**
*首先设置运行时间为 800 毫秒，大于命令的超时限制 500 毫秒
*/
longtakeTime= 800 ;
for (inti= 1 ;i<= 10 ;i++)
{
TakeTimeDemoCommandcommand=newTakeTimeDemoCommand (takeTime, rpcPool);
command.execute ();
//健康信息
HystrixCommandMetrics. HealthCountshc=
command.getMetrics (). getHealthCounts ();
if (command.isCircuitBreakerOpen ())
{
/**
*熔断之后，设置运行时间为 300 毫秒，小于命令的超时限制 500 毫秒
*/
takeTime= 300 ;
log.info ("============ 熔断器打开了，等待休眠期（默认 5 秒）结束");
/**
*等待 7 秒之后，再一次发起请求
*/
Thread.sleep ( 7000 );
}
}


Thread.sleep (Integer. MAX_VALUE);
}
}
在上面的演示中，有以下配置器的配置命令需要重点说明：
1 ）通过 withExecutionTimeoutInMilliseconds（int）方法将默认为 1000 毫秒的执行超时上限设置
为 500 毫秒，也就是说，只要 TakeTimeDemoCommand.run () 的执行超过 500 毫秒就会触发 Hystrix 超
时回退。
2 ）通过 withCircuitBreakerRequestVolumeThreshold（int）方法将熔断器触发熔断的最少请求次
数的默认值 20 次改为了 3 次，这样更容易测试。
3 ）通过 withCircuitBreakerErrorThresholdPercentage（int）方法设置错误率阈值百分比的值为 60 ，
滑动窗口时间内当错误率超过此值时，熔断器进入 open 开启状态，所有请求都会触发失败回退
（fallback），错误率阈值百分比的默认值为 50 。

执行上面的演示实例，运行的结果节选如下：
[HystrixTimer- 1 ]INFO c.c.d.h.CircuitBreakerDemo-fallback-req 1 : 熔断器状态：false, 失
败率： 0 %
[HystrixTimer- 1 ]INFO c.c.d.h.CircuitBreakerDemo-fallback-req 2 : 熔断器状态：false, 失
败率： 100 %
[HystrixTimer- 2 ]INFO c.c.d.h.CircuitBreakerDemo-fallback-req 3 : 熔断器状态：false, 失
败率： 100 %
[HystrixTimer- 1 ]INFO c.c.d.h.CircuitBreakerDemo-fallback-req 4 : 熔断器状态：true, 失
败率： 100 %
[main]INFO c.c.d.h.CircuitBreakerDemo-============ 熔断器打开了，等待休眠期（默认 5 秒）
结束
[hystrix-threadPool- 1 - 5 ]INFO c.c.d.h.CircuitBreakerDemo-succeed-req 5 : 熔断器状态：
true, 失败率： 100 %
[hystrix-threadPool- 1 - 6 ]INFO c.c.d.h.CircuitBreakerDemo-succeed-req 6 : 熔断器状态：
false, 失败率： 0 %
[hystrix-threadPool- 1 - 7 ]INFO c.c.d.h.CircuitBreakerDemo-succeed-req 7 : 熔断器状态：
false, 失败率： 0 %
[hystrix-threadPool- 1 - 8 ]INFO c.c.d.h.CircuitBreakerDemo-succeed-req 8 : 熔断器状态：
false, 失败率： 0 %
[hystrix-threadPool- 1 - 9 ]INFO c.c.d.h.CircuitBreakerDemo-succeed-req 9 : 熔断器状态：
false, 失败率： 0 %
[hystrix-threadPool- 1 - 10 ]INFO c.c.d.h.CircuitBreakerDemo-succeed-req 10 : 熔断器状态：
false, 失败率： 0 %
从上面的执行结果可知，在第四次请求 req 4 时，熔断器才达到熔断触发的次数阈值 3 ，由于前 3 次
皆为超时失败，失败率大于阈值 60 %，因此第四次请求执行之后，熔断器状态为 open 熔断状态。
在命令的熔断器打开后，熔断器默认会有 5 秒的睡眠等待时间，在这段时间内的所有请求直接
执行回退方法； 5 秒之后，熔断器会进入 half-open 状态, 尝试放行一次命令执行，如果成功则关闭
熔断器，状态转成 closed，否则，熔断器回到 open 状态。
在上面的程序中，在熔断器熔断之后，演示程序将命令的运行时间 takeTime 改成了 300 毫秒，
小于命令的超时限制 500 毫秒。在等待 7 秒之后，演示程序再一次发起请求，从运行结果可以看到，
第 5 次请求 req 5 执行成功了，这是一次 half-open 状态的尝试放行，请求成功之后，熔断器的状态转
成了 open，后续请求将继续放行。注意，演示程序第 5 次请求 req 5 后的熔断器状态值反映在第 6 次请
求 req 6 的执行输出中。


#### 6. 5. 2 熔断器和滑动窗口的配置属性

熔断器的配置包含了滑动窗口的配置和熔断器自身的配置。Hystrix 的健康统计是通过滑动窗
口来完成的，其熔断器的状态也是依据滑动窗口的统计数据来变化的，所以这里先介绍滑动窗口的
配置。先看看两个概念：滑动窗口和时间桶。

1 .滑动窗口
可以这么来理解滑动窗口：一位乘客坐在正在行驶的列车的靠窗座位上，列车行驶的公路两
侧种着一排挺拔的白杨树，随着列车的前进，路边的白杨树迅速从窗口滑过，我们用每棵树来代表
一个请求，用列车的行驶代表时间的流逝，那么，列车上的这个窗口就是一个典型的滑动窗口，这
个乘客能通过窗口看到的白杨树的数量，就是滑动窗口要统计的数据。

2 .时间桶
时间桶是统计滑动窗口数据时的最小单位。同样类比列车窗口，在列车速度非常快时，如果
每掠过一棵树就统计一次窗口内树的数据，显然开销非常大，如果乘客将窗口分成 N 份，前进行时
列车每掠过窗口的 N 分之一就统计一次数据，开销就大大地减小了。简单来说，时间桶也就是滑动
窗口的 N 分之一。
代码方式下熔断器的设置可以使用 HystrixCommandProperties.Setter () 配置器来完成，参考 6. 5. 1
节的实例，把自定义的 TakeTimeDemoCommand 中 Setter () 配置器的相关参数配置如下：
/**
*命令参数配置
*/
HystrixCommandProperties. SetterpropertiesSetter=
HystrixCommandProperties.Setter ()
//至少有 3 个请求，熔断器才达到熔断触发的次数阈值
.withCircuitBreakerRequestVolumeThreshold ( 3 )
//熔断器中断请求 5 秒后会进入 half-open 状态，进行尝试放行
.withCircuitBreakerSleepWindowInMilliseconds ( 5000 )
//错误率超过 60 %，快速失败
.withCircuitBreakerErrorThresholdPercentage ( 60 )
//启用超时
.withExecutionTimeoutEnabled (true)
//执行的超时时间，默认为 1000 毫秒，这里设置为 500 毫秒
.withExecutionTimeoutInMilliseconds ( 500 )
//可统计的滑动窗口内的 buckets 数量，用于熔断器和指标发布
.withMetricsRollingStatisticalWindowBuckets ( 10 )
//可统计的滑动窗口的时间长度
//这段时间内的执行数据用于熔断器和指标发布
.withMetricsRollingStatisticalWindowInMilliseconds ( 10000 );
在以上配置中，与熔断器的滑动窗口相关的配置的具体含义为：
1 ）滑动窗口中，最少 3 个请求才会触发断路，默认值为 20 个。
2 ）错误率达到 60 %时才可能触发断路，默认值为 50 %。
3 ）断路之后的 5000 毫秒内，所有请求都直接调用 getFallback () 进行回退降级，不会调用 run ()
方法； 5000 毫秒过后，熔断器变为 half-open 状态。


```
以上 TakeTimeDemoCommand 的熔断器滑动窗口的状态转换关系如图 6 - 11 所示。
```
图 6 - 11 TakeTimeDemoCommand 的熔断器健康统计滑动窗口的状态转换关系图
大家已经知道，Hystrix 熔断器的配置除了代码方式，还有 properties 文本属性配置的方式；另
外 Hystrix 熔断器相关的滑动窗口不止一个基础的健康统计滑动窗口，还包含一个百分比命令执行时
间统计滑动窗口，两个窗口都可以进行配置。
下面以文本属性配置方式为主，详细介绍 Hystrix 基础健康统计滑动窗口的配置：
（ 1 ）hystrix. command. default. metrics. rollingStats. timeInMilliseconds
设置健康统计滑动窗口的持续时间（以毫秒为单位），默认值为 10000 毫秒。熔断器的状态
会根据滑动窗口的统计值来计算，若滑动窗口时间内的错误率超过阈值，熔断器将进入 open 开启状
态，滑动窗口将被进一步细分为时间桶，滑动窗口的统计值等于窗口内所有时间桶的统计信息的累
加，每个时间桶的统计信息包含请求的成功（success）、失败（failure）、超时（timeout）、被拒
（rejection）的次数。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withMetricsRollingStatisticalWindowInMilliseconds
(int)
（ 2 ）hystrix. command. default. metrics. rollingStats. numBuckets
设置健康统计滑动窗口被划分为时间桶的数量，默认值为 10 。若滑动窗口的持续时间为默认
的 10000 毫秒，则一个时间桶（bucket）的时间即 1 秒。如果要做定制化的配置，则所设置的 numBuckets
（时间桶数量）值和 timeInMilliseconds（滑动窗口时长）值有关联关系，必须符合
timeInMilliseconds%numberBuckets== 0 的规则，否则会抛出异常。例如二者的关联关系为 70000
（滑动窗口 70 秒）% 700 （桶数）== 0 是可以的，但是 70000 （ 70 秒）% 600 （桶数）== 400 将抛出
异常。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withMetricsRollingStatisticalWindowBuckets (int)
（ 3 ）hystrix. command. default. metrics. healthSnapshot. intervalInMilliseconds
设置健康统计滑动窗口拍摄运行状况统计指标的快照的时间间隔。什么是拍摄运行状况统计


###### 指标的快照呢？就是计算成功和错误百分比这些影响熔断器状态的统计数据。

###### 拍摄快照的时间间隔的单位为毫秒，默认值为 500 毫秒。由于统计指标的计算是一个耗 CPU

###### 的操作（CPU 密集型操作），也就是说，高频率地计算错误百分比等健康统计数据会占用很多 CPU

###### 资源，所以，在高并发 RPC 流量大的场景下，可以适当调大拍摄快照的时间间隔。

###### 此选项通过代码方式配置时所对应的函数如下：

HystrixCommandProperties.Setter (). withMetricsHealthSnapshotIntervalInMilliseconds
(int)
Hystrix 熔断器相关的滑动窗口不止一个基础的健康统计滑动窗口，还包含一个“百分比命令
执行时间”统计滑动窗口。什么是“百分比命令执行时间”统计滑动窗口呢？该滑动窗口主要用于
统计 1 %、 10 %、 50 %、 90 %、 99 %等一系列比例的命令执行平均耗时，主要用以生成统计图表。
带 hystrix. command. default. metrics. rollingPercentile 前缀的配置项，专门用于配置百分比命令执
行时间统计窗口。下面以文本属性配置方式为主，详细介绍 Hystrix 执行时间百分比统计滑动窗口的
配置：

（ 1 ）hystrix. command. default. metrics. rollingPercentile. enabled：
该配置项用于设置百分比命令执行时间统计窗口是否生效，命令的执行时间是否被跟踪，并
且计算各个百分比如 1 %、 10 %、 50 %、 90 %、 99. 5 %等的平均时间。该配置项默认为 true。

（ 2 ）hystrix. command. default. metrics. rollingPercentile. timeInMilliseconds
设置百分比命令执行时间统计窗口的持续时间（以毫秒为单位），默认值为 60000 毫秒，当
然，此滑动窗口也会被进一步细分为时间桶，以便提高统计的效率。
本选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withMetricsRollingPercentileWindowInMilliseconds (
int)
（ 3 ）hystrix. command. default. metrics. rollingPercentile. numBuckets
设置百分比命令执行时间统计窗口被划分为时间桶的数量，默认值为 6 。此滑动窗口的默认
持续时间为默认的 60000 毫秒，即默认情况下，一个时间桶的时间为 10 秒。如果要做定制化的配置，
此窗口所设置的 numBuckets（时间桶数量）值和 timeInMilliseconds（滑动窗口时长）值有关联关系，
必须符合 timeInMilliseconds（滑动窗口时长）%numberBuckets== 0 的规则，否则将抛出异常。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withMetricsRollingPercentileWindowBuckets (int)
（ 4 ）hystrix. command. default. metrics. rollingPercentile. bucketSize
设置百分比命令执行时间统计窗口的时间桶内最大的统计次数，如果 bucketSize 为 100 ，而桶
的时长为 1 秒，若这 1 秒里有 500 次执行，则只有最后 100 次执行的信息会被统计到桶里去。增加此配
置项的值会导致内存开销及其他计算开销的上升，该配置项的默认值为 100 。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withMetricsRollingPercentileBucketSize (int)
以上是 Hystrix 熔断器相关的滑动窗口的配置，接下来是熔断器本身的配置。
带 hystrix. command. default. circuitBreaker 前缀的配置项专门用于对熔断器本身进行配置。下面


以文本属性配置方式为主，对 Hystrix 熔断器的配置进行一下详细介绍：

（ 1 ）hystrix. command. default. circuitBreaker. enabled
该配置用来确定是否启用熔断器，默认值为 true。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withCircuitBreakerEnabled (boolean)
（ 2 ）hystrix. command. default. circuitBreaker. requestVolumeThreshold
该配置用于设置熔断器触发熔断的最少请求次数。如果设为 20 ，那么当一个滑动窗口时间内
（比如 10 秒）收到 19 个请求，即使 19 个请求都失败，熔断器也不会打开变成 open 状态。默认值为 20 。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withCircuitBreakerRequestVolumeThreshold (int)
（ 3 ）hystrix. command. default. circuitBreaker. errorThresholdPercentage
该配置用于设置错误率阈值，当健康统计滑动窗口的错误率超过此值时，熔断器进入 open 开
启状态，所有请求都会触发失败回退（fallback）。错误率阈值百分比的默认值为 50 。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withCircuitBreakerErrorThresholdPercentage (int)
（ 4 ）hystrix. command. default. circuitBreaker. sleepWindowInMilliseconds
此配置项指定了熔断器打开后经过多长时间允许一次请求尝试执行。熔断器打开时，Hystrix
会在经过一段时间后就放行一条请求，如果这条请求执行成功了，说明此时服务很可能已经恢复了
正常，那么就会关闭熔断器；如果此请求执行失败，则认为目标服务依然不可用，熔断器继续保持
打开状态。
该配置用于配置熔断器的睡眠窗口，具体指的是熔断器打开之后过多长时间才允许一次请求
尝试执行，默认值为 5000 毫秒，表示当熔断器开启（open）后， 5000 毫秒内会拒绝所有的请求， 5000
毫秒之后，熔断器才会进行入 half-open 状态。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withCircuitBreakerSleepWindowInMilliseconds (int)
（ 5 ）hystrix. command. default. circuitBreaker. forceOpen
如果配置为 true，则熔断器将被强制打开，所有请求将被触发失败回退（fallback）。此配置的
默认值为 false。
此选项通过代码方式配置时所对应的函数如下：
HystrixCommandProperties.Setter (). withCircuitBreakerForceOpen (boolean)
下面是本书随书实例中 demo-provider 中的有关熔断器的配置，节选如下：
hystrix:
...
command:
...
default: #全局默认配置
circuitBreaker: #熔断器相关配置
enabled: true #是否启动熔断器 ，默认为 true
requestVolumeThreshold: 20 #启用熔断器功能窗口时间内的最小请求数


sleepWindowInMilliseconds: 5000 #指定熔断器打开后多长时间内允许一次请求尝试执行
errorThresholdPercentage: 50 #窗口时间内超过 50 %的请求失败后就会打开熔断器
metrics:
rollingStats:
timeInMilliseconds: 6000
numBuckets: 10
UserClient #detail (Long): #独立接口配置 ，格式为：类名 #方法名 （参数类型列表）
circuitBreaker: #熔断器相关配置
enabled: true #是否使用熔断器 ，默认为 true
requestVolumeThreshold: 20 #窗口时间内的最小请求数
sleepWindowInMilliseconds: 5000 #打开后允许一次尝试的睡眠时间 ，默认配置为 5 秒
errorThresholdPercentage: 50 #窗口时间内熔断器开启的错误比例 ，默认配置为 50
metrics:
rollingStats:
timeInMilliseconds: 10000 #滑动窗口时间
numBuckets: 10 #滑动窗口的时间桶数
使用文本格式配置时，可以对熔断器的参数值做默认配置，也可以对特定的 RPC 接口做个性化
配置。对熔断器的参数值做默认配置时，使用 hystrix. command. default 默认前缀；对特定的 RPC 接口做
个性化配置时，使用 hystrix. command. FeignClient #Method格式的前缀 。上面的演示例子中，对远程客
户端 Feign 接口 UserClient 中的 detail (Long) 方法做了个性化的熔断器配置，其配置项的前缀为：
hystrix. command. UserClient #detail (Long)

#### 6. 5. 3 Hystrix 命令的执行流程

在获取 HystrixCommand 命令的执行结果时，无论是调用 execute () 和 toObservable () 方法，还是
调用 observe () 方法，最终都会通过 HystrixCommand.toObservable () 订阅执行结果和返回。在 Hystrix
内部，调用 toObservable () 方法返回一个观察的主题，当 Subscriber 订阅者订阅主题后，
HystrixCommand 会弹射一个事件，然后通过一系列的判断（顺序依次是缓存是否命中、熔断器是
否打开、线程池是否占满），开始执行实际的 HystrixCommand.run () 方法，该方法的实现主要为异
步处理的业务逻辑，如果这其中任何一个环节出现错误或者抛出异常，它都会回退到 getFallback ()
方法进行服务降级处理，当降级处理完成之后，它会将结果返回给实际的调用者。
HystrixCommand 的工作流程，总结起来大致如下：
1 ）判断是否使用缓存响应请求，若启用了缓存，且缓存可用，则直接使用缓存响应请求。Hystrix
支持请求缓存，但需要用户自定义启动。
2 ）判断熔断器是否开启，如果熔断器处于 open 状态，则跳到第 5 步。
3 ）如果使用线程池进行请求隔离，则判断线程池是否已满，已满则跳到第 5 步；如果使用信
号量进行请求隔离，则判断信号量是否耗尽，耗尽则跳到第 5 步。
4 ）执行 HystrixCommand.run () 方法执行具体业务逻辑，如果执行失败或者超时，则跳到第 5 步，
否则跳到第 6 步。
5 ）执行 HystrixCommand.getFallback () 服务降级处理逻辑。
6 ）返回请求响应。
以上流程如图 6 - 12 所示。


```
图 6 - 12 HystrixCommand 的执行流程示意图
什么场景下会触发 fallback 方法呢？请见表 6 - 2 。
表 6 - 2 触发 fallback 方法的场景
名字说明触发 fallback
EMIT 值传递 NO
SUCCESS 执行完成，没有错误 NO
FAILURE 执行抛出异常 YES
TIMEOUT 执行开始，但没有在允许的时间内完成 YES
BAD_REQUEST 执行抛出 HystrixBadRequestException NO
SHORT_CIRCUITED 熔断器打开，不尝试执行 YES
THREAD_POOL_REJECTED 线程池拒绝，不尝试执行 YES
SEMAPHORE_REJECTED 信号量拒绝，不尝试执行 YES
```
### 6. 6 RPC 监控之滑动窗口实现原理

Hystrix 通过滑动窗口的数据结构来统计调用的指标数据，并且大量使用了 RxJava 响应式编程
操作符。滑动窗口本质就是不断变换的数据流，因此滑动窗口的实现非常适合使用观察者模式以及
响应式编程模式去完成。最终，RxJava 便成了 Hystrix 滑动窗口实现的最佳的框架选择。Hystrix 滑动
窗口的核心实现是使用 RxJava 的 window 操作符（算子）完成的。使用 RxJava 实现滑动窗口还有一
大好处就是可以依赖 RxJava 的线程模型来保证数据写入和聚合的线程安全。
Hystrix 滑动窗口的原理和实现逻辑非常复杂，所以在深入学习之前先看一个 Hystrix 滑动窗口
模拟实现示例。


#### 6. 6. 1 Hystrix 健康统计滑动窗口模拟实现

先总体介绍一下 Hystrix 健康统计滑动窗口的执行流程：
1 ）HystrixCommand 命令器的执行结果（失败、成功）会以事件的形式通过 RxJava 事件流弹射
出去，形成执行完成事件流。
2 ）桶计数流以事件流作为来源，将事件流中的事件按照固定时间长度（桶时间间隔）划分成
滚动窗口，并对时间桶滚动窗口内的事件按照类型进行累积，完成之后将桶数据弹射出去，形成桶
计数流。
3 ）桶滑动统计流以桶计数流作为来源，按照步长为 1 、长度为设定的桶数（配置的滑动窗口
桶数）的规则划分滑动窗口，并对滑动窗口内的所有的桶数据按照各事件类型进行汇总，汇总成最终
的窗口健康数据并弹射出去，形成最终的桶滑动统计流，作为 Hystrix 熔断器进行状态转换的数据支撑。

```
以上介绍的 Hystrix 健康统计滑动窗口的执行流程具体如图 6 - 13 所示。
```
图 6 - 13 Hystrix 健康统计滑动窗口的执行流程
为了帮助大家学习 Hystrix 滑动窗口的执行流程，这里设计一个简单的 Hystrix 滑动窗口模拟实
现用例，对 Hystrix 滑动窗口数据流处理过程进行简化，只留下核心部分，简化的模拟执行流程如下：

1 ）模拟 HystrixCommand 的事件发送机制，每 100 ms 发送一个随机值（ 0 或 1 ），随机值为 0 代表
失败、 1 代表成功，模拟执行完成事件流。
2 ）模拟 HystrixCommand 的桶计数流，以事件流作为来源，将事件流中的事件按照固定时间长
度（ 300 ms）划分成时间桶滚动窗口，并对时间桶滚动窗口内值为 0 的事件进行累积，完成之后将
累积数据弹射出去，形成桶计数流。
3 ）模拟桶计数流作为来源，按照步长为 1 、长度为设定的桶数（ 3 ）的规则划分滑动窗口，并
对滑动窗口内的所有的桶数据进行汇总，汇总成最终的失败汇总数据并弹射出去，形成最终的桶滑
动统计流。

```
以上的模拟 Hystrix 健康统计滑动窗口的执行流程具体如图 6 - 14 所示。
```

图 6 - 14 模拟的 Hystrix 健康统计滑动窗口简化版执行流程
简化的模拟 Hystrix 健康统计滑动窗口执行流程的参考实现代码如下：
packagecom. crazymaker. demo. rxJava. basic;
//省略 import
@Slf 4 j
publicclassWindowDemo
{
/**
*演示模拟 hystrix 的健康统计 metric
*/
@Test
publicvoidhystrixTimewindowDemo () throwsInterruptedException
{
//创建 Random 类对象
Randomrandom=newRandom ();
//模拟 Hystrixevent 事件流，每 100 毫秒发送一个 0 或 1 的随机值
//随机值为 0 代表失败，随机值为 1 代表成功
ObservableeventStream=Observable
.interval ( 100 ,TimeUnit. MILLISECONDS)
.map (i->random.nextInt ( 2 ));
/**
*完成桶内 0 值计数的聚合函数
*/
Func 1 reduceBucketToSummary=newFunc 1 <Observable<Integer>,
Observable<Long>>()
{
@Override
publicObservable<Long>call (Observable<Integer>eventBucket)
{
Observable<List<Integer>>olist=eventBucket.toList ();
Observable<Long>countValue=olist.map (list->
{
longcount=list.stream (). filter (i->i== 0 ). count ();
log.info ("{}' 0 count:{}",list.toString (), count);
returncount;
});
returncountValue;
}


};
/**
*桶计数流
*/
Observable<Long>bucketedCounterStream=eventStream
.window ( 300 ,TimeUnit. MILLISECONDS)
.flatMap (reduceBucketToSummary); //将时间桶进行聚合，统计事件值为 0 的个数
/**
*滑动窗口聚合函数
*/
Func 1 reduceWindowToSummary=newFunc 1 <Observable<Long>, Observable<Long>>()
{
@Override
publicObservable<Long>call (Observable<Long>eventBucket)
{
returneventBucket.reduce (newFunc 2 <Long,Long,Long>()
{
@Override
publicLongcall (Longbucket 1 ,Longbucket 2 )
{
/**
*对窗口内的桶进行累加
*/
returnbucket 1 +bucket 2 ;
}
});
}
};
/**
*桶滑动统计流
*/
ObservablebucketedRollingCounterStream=bucketedCounterStream
.window ( 3 , 1 )
.flatMap (reduceWindowToSummary);//将滑动窗口进行聚合
bucketedRollingCounterStream.subscribe (sum->log.info ("滑动窗口的和：{}", sum));
Thread.sleep (Integer. MAX_VALUE);
}
}

运行程序，输出的结果节选如下：
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 0 , 0 , 0 ]' 0 count: 3
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 0 , 1 , 1 ]' 0 count: 1
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 1 , 0 , 1 ]' 0 count: 1
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 5
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 0 , 1 , 0 ]' 0 count: 2
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 4
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 0 , 1 , 0 ]' 0 count: 2
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 5
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 1 , 1 , 1 ]' 0 count: 0
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 4
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 0 , 1 , 1 ]' 0 count: 1
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 3
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 1 , 0 , 0 ]' 0 count: 2
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 3
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 1 , 1 , 1 ]' 0 count: 0
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 3
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 1 , 1 , 0 ]' 0 count: 1
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 3


[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-[ 1 , 1 , 1 ]' 0 count: 0
[RxComputationScheduler- 1 ]INFO c.c.d.rxJava. basic. WindowDemo-滑动窗口的和： 1
在上面的代码中，eventStream 流通过 interval 操作符每 100 毫秒（ms）发送一个随机值（ 0 或 1 ），
随机值为 0 代表失败、 1 代表成功，模拟 HystrixCommand 的事件发送机制。
桶计数流 bucketedCounterStream 使用 window 操作符以 300 毫秒为一个时间桶窗口，将原始的事
件流进行拆分，每个时间桶窗口的 3 事件聚集起来，输出一个新的 Observable（子流）。然后，
bucketedCounterStream 通过 flapMap 操作将每一个 Observable（子流）进行扁平化。
桶计数流 bucketedCounterStream 的处理过程大致如图 6 - 15 所示。

图^6 -^15 模拟的桶计数流 bucketedCounterStream 的处理过程
bucketedCounterStream 的 flapMap 扁平化操作是通过调用 reduceBucketToSummary 方法完成的，
该方法首先将每一个时间桶窗口内的 Observable 子流内的元素序列转成一个列表，然后进行过滤
（留下值为 0 事件）和统计，返回值为 0 的元素统计数量（失败数）。
接下来，需要对 bucketedCounterStream 桶计数进行汇总统计，形成滑动窗口的统计数据，这个
工作由 bucketedRollingCounterStream 桶滑动统计流完成。
桶滑动统计流仍然使用 window 和 flatMap 两个操作符，先将输入流中通过 window 操作符按照步
长为 1 、长度为 3 的规则划分滑动窗口，每个滑动窗口的 3 统计数据被聚集起来，输出一个新的
Observable（子流）。然后通过 flatMap 扁平化操作符对每一个 Observable（子流）进行聚合，计算
出各元素的累加值。
模拟的桶滑动统计流 bucketedRollingCounterStream 的处理过程具体如图 6 - 16 所示。
bucketedRollingCounterStream 的 flapMap 扁平化操作是通过调用 reduceWindowToSummary 方法
完成的，该方法通过 RxJava 的 reduce 操作符进行“聚合”操作，将 Observable 子流中的 3 事件的累加
结果计算出来。


```
图 6 - 16 桶滑动统计流 bucketedRollingCounterStream 的处理过程
```
#### 6. 6. 2 Hystrix 滑动窗口核心实现原理

在 Hystrix 中，业务逻辑以命令模式封装成了一个一个命令（HystrixCommand），每个命令执
行完成后，都会发送完成事件（HystrixCommandCompletion）到 HystrixCommandCompletionStream
命令完成事件流。HystrixCommandCompletion 是 Hystrix 中的核心事件，它可以代表某个命令执行成
功、超时、异常等的各种状态，与 Hystrix 熔断器的状态转换息息相关。
桶计数流 BucketedCounterStream 是一个抽象类，提供了基本的桶计数器实现。用户在使用
Hystrix 的时候一般都要配两个值：timeInMilliseconds 滑动窗口的长度（时间间隔）和 numBuckets
滑动窗口中桶数。每个桶对应的时间长度就是 bucketSizeInMs=timeInMilliseconds/numBuckets，
该时间长度可以记为一个时间桶窗口。BucketedCounterStream 每隔一个时间桶窗口就把这段时间内
的所有调用事件聚合到一个累积桶内。下面来看一下它的实现：
protectedBucketedCounterStream (finalHystrixEventStream<Event>inputEventStream,
finalintnumBuckets, finalintbucketSizeInMs,
finalFunc 2 <Bucket,Event,Bucket>appendRawEventToBucket){
this. numBuckets=numBuckets;
this. reduceBucketToSummary=newFunc 1 <Observable<Event>, Observable<Bucket>>(){
@Override
publicObservable<Bucket>call (Observable<Event>eventBucket){
returneventBucket.reduce (getEmptyBucketSummary (),
appendRawEventToBucket);
}
};
...
this. bucketedStream=Observable.defer (newFunc 0 <Observable<Bucket>>(){
@Override
publicObservable<Bucket>call (){
returninputEventStream
.observe ()
.window (bucketSizeInMs, TimeUnit. MILLISECONDS)
.flatMap (reduceBucketToSummary)


.startWith (emptyEventCountsToStart);
}
});
}
BucketedCounterStream 的构造函数里接收四个参数，其中第一个参数 inputEventStream 是一个
HystrixCommandCompletionStream 命令完成事件流。每个 HystrixCommand 命令执行完成后，执行完
成事件都通过 inputEventStream 弹射出来。第二个参数 numBuckets 为滑动窗口中的桶数量，第三个
参数 bucketSizeInMs 为每个桶对应的时间长度，第四个参数为将原始事件统计到累积桶（Bucket）
的回调函数。
BucketedCounterStream 的核心是 window 操作符，它可以将原始的完成事件流按照时间桶的长
度 bucketSizeInMs 进行拆分，并将这个时间段内的事件聚集起来，输出一个 Observable 子流，然后
通过 flapMap 操作将每一个 Observable 子流进行扁平化。
具体的 flapMap 扁平化操作是通过调用 reduceBucketToSummary 方法完成的，该方法通过 RxJava
的 reduce 操作符进行“聚合”操作，将 Observable 子流中的一串事件归纳成一个累积桶 Bucket。
桶计数流 BucketedCounterStream 的处理过程大致如图 6 - 17 所示。

图 6 - 17 桶计数流 BucketedCounterStream 的处理过程
什么是累积桶 Bucket 呢？它是一个整型数组，数组中的每一个元素用于存放对应类型的事件的
总数，具体如图 6 - 18 所示。
累积桶 Bucket 的数组元素所保存的各类事件总数，是通过聚合函数 appendRawEventToBucket
进行累加得到的。累加的方式是：将数组元素的位置与事件类型对应，将相同类型的事件总数累加
到对应的数组位置上，从而统计出一个 Bucket 桶内的 SUCCESS 总数、FAILURE 总数等。
最原始的累积桶 Bucket 是一个空桶，每一个元素的值为 0 。获取原始桶的方法与具体的统计流
子类相关，子类 HealthCountsStream 健康统计流获取原始空桶的函数如下：


图 6 - 18 累积桶 Bucket 示意图
publicclassHealthCountsStream...{
//获取初始桶，返回一个全零数组，长度为事件类型总数。
//数组中的每一个元素用于存放对应类型的事件数量
@Override
long[]getEmptyBucketSummary (){
returnnewlong[HystrixEventType.values (). length];
}
}
桶计数流 BucketedCounterStream 将时间桶类的同类型的事件总数（如 FAILURE、SUCCESS 总数）
聚合到累积桶 Bucket 中，处理的最终的结果是源源不断的汇总数据组成了最终的桶计数流。
接下来，需要对熔断器的滑动窗口内的所有的累积桶 Bucket 进行汇总统计，形成滑动窗口的统
计数据，作为熔断器状态转换的依据，这个工作由 BucketedRollingCounterStream 桶滑动统计流完成。
BucketedRollingCounterStream 桶滑动统计流的数据来源正好是 BucketedCounterStream 桶计数
流。桶滑动统计流仍然使用 window 和 flatMap 两个操作符，先通过滑动窗口将一定数量的数据聚集
成一个集合流，然后对每一个集合流进行聚合，如图 6 - 19 所示。

```
图 6 - 19 桶滑动统计流 BucketedRollingCounterStream 的处理过程
```

桶滑动统计流 BucketedRollingCounterStream 的核心源码大致如下：
publicabstractclassBucketedRollingCounterStream...{
privateObservable<Output>sourceStream;
privatefinalAtomicBooleanisSourceCurrentlySubscribed=newAtomicBoolean (false);
protectedBucketedRollingCounterStream (
HystrixEventStream<Event>stream, finalintnumBuckets, intbucketSizeInMs,
finalFunc 2 <Bucket,Event,Bucket>appendRawEventToBucket,
finalFunc 2 <Output,Bucket,Output>reduceBucket)
{
super (stream, numBuckets, bucketSizeInMs, appendRawEventToBucket);
Func 1 <Observable<Bucket>, Observable<Output>> reduceWindowToSummary=
newFunc 1 <Observable<Bucket>, Observable<Output>>(){
@Override
publicObservable<Output>call (Observable<Bucket>window){
returnwindow.scan (getEmptyOutputValue (), reduceBucket). skip (numBuckets);
}
};
this. sourceStream=bucketedStream.window (numBuckets, 1 )
.flatMap (reduceWindowToSummary)
.doOnSubscribe (newAction 0 (){...})
.share ()
.onBackpressureDrop ();
}
...
}
桶滑动统计流 BucketedRollingCounterStream 中的 window 操作符与 BucketedCounterStream 中的
window 操作符相比，在版本上有所不同，它的第二个参数 skip= 1 的意思就是按照步长为 1 的间
隔在输入数据流中持续滑动，不断聚集出 numBuckets 数量的输入对象，输出一个个 Observable 子流，
这才是滑动窗口的真正含义。而 BucketedCounterStream 中所用的 window 操作符，窗口与窗口之间
没有重叠，严格来说，这才叫作滑动窗口算子。
BucketedRollingCounterStream 流通过 window 操作符滑动生成一个个 Observable 子流后，再通过
flapMap 操作将每一个 Observable 子流进行扁平化，具体的 flapMap 扁平化操作通过调用自定义的窗
口规约方法 reduceWindowToSummary 完成。注意，该窗口规约方法没有用 reduce 操作符，而是用了
scan+skip (numBuckets) 的组合。scan 和 reduce 一样都是聚合操作符，但是 scan 会将所有的中间结果
弹出，而 reduce 仅弹出最终结果。在 scan 弹出所有的中间结果和最终统计结果之后，后面的
skip (numBuckets) 操作将跳过所有的中间结果，剩下最终结果。这样做的好处是，如果桶子里的元
素个数不满足 numBuckets，就把这个不完整的窗口给过滤掉。
最后，总结一下 Hystrix 各大流之间存在的继承关系，具体如下：
1 ）最顶层的 BucketedCounterStream 桶计数流是一个抽象类，提供了基本的桶计数器实现，按
计算出来的 bucketSizeInMs 时间间隔将各种类型的事件数量聚合成桶。
2 ）BucketedRollingCounterStream 抽象类在桶计数流的基础上，实现滑动窗口内 numBuckets 个
Bucket（累积桶）的相同类型事件数的汇总，并聚合成指标数据。
3 ）最底下一层的类则是各种具体的实现，比如 HealthCountsStream 最终会聚合成健康检查数据
（HystrixCommandMetrics. HealthCounts），比如统计命令执行成功和失败的次数供熔断器
HystrixCircuitBreaker 使用。


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html

**3.** 相关的面试题，请参考 (^3000) 页《尼恩 **Java** 面试宝典》的 (^35) 个面试专题 ：
https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 7 章微服务网关与用户身份识别

在微服务分布式架构下，客户端（如浏览器）直接访问 Provider 微服务提供者，会存在以下问题：
1 ）客户端需要进行负载均衡，从多个 Provider 之间挑选出最合适的微服务提供者。
2 ）存在跨域请求时，服务端需要进行额外处理。
3 ）每个服务需要进行独立的用户认证。
解决以上问题的手段就是使用微服务网关。微服务网关是微服务架构中不可或缺的部分，统
一提供 Provider 路由、均衡负载、权限控制等功能。微服务网关的功能大致如图 7 - 1 所示。

图 7 - 1 微服务网关的功能
微服务网关的实现框架有多种，SpringCloud 全家桶中比较常用的有 Zuul 和 SpringCloud
Gateway 两大框架。虽然 SpringCloud 官方推荐自家的 SpringCloudGateway 框架，但是，由于 Zuul
使用非常广泛而且文档更加丰富，所以，本书优先推荐使用 Zuul 作为生产场景的微服务网关。
在高并发的使用场景下，则推荐使用 SpringCloudGateway 框架作为网关。疯狂创客圈社群将
以图文博客的方式对 SpringCloudGateway 网关进行详细介绍。

## 7. 1 Zuul 的基础使用

```
Zuul 是 Netflix 公司的开源网关产品，可以和 Eureka、Ribbon、Hystrix 等组件配合使用。Zuul 的
```

规则引擎允许规则和过滤器以任何 JVM 语言来编写，内置支持 Java 和 Groovy。
在 SpringCloud 框架中，Zuul 的角色是网关，负责接收所有的 REST 请求（如网页端、APP 端等），
然后进行内部转发，是微服务提供者集群的流量入口。本书将 Zuul 称为内部网关，以便和 Nginx 外
部网关作区分。
Zuul 的功能大致有：
1 ）路由：将不同 REST 请求转发至不同的微服务提供者，其作用类似于 Nginx 的反向代理。同
时，也起到了统一端口的作用，将很多的微服务提供者（Provider）的不同端口统一到了 Zuul 的服
务端口。
2 ）认证：网关直接暴露在公网上时，终端要调用某个服务通常会把登录后的令牌（token）传
过来，网关层对令牌进行有效性验证。如果令牌无效（或没令牌），则不允许访问 REST 服务。可
以结合 SpringSecurity 中的认证机制完成 Zuul 网关的安全认证。。
3 ）限流：高并发场景下瞬时流量不可预估，为了保证服务对外的稳定性，限流成为每个应用
必备的一道安全防火墙。如果没有这道安全防火墙，当请求的流量超过服务的负载能力时，很容易
造成整个服务的瘫痪。
4 ）负载均衡：在多个微服务提供者（Provider）之间按照多种策略实现负载均衡。

### 7. 2 创建 Zuul 网关服务

SpringCloud 对 Zuul 进行了整合与增强。Zuul 作为网关层，自身也是一个微服务，与其他微服
务提供者一样都注册在 Eureka 服务器上，可以相互发现，Zuul 能感知到哪些服务提供在线，同时通
过配置路由规则可以将 REST 请求自动转发到指定的后端微服务提供者（Provider）。
新建 Zuul 网关服务项目时，需要在启动类中添加注解@EnableZuulProxy，声明这是一个网关微
服务提供者。当然，也需要在 pom. xml 文件中手动添加如下依赖：
<dependency>
<groupId>org. springframework. cloud</groupId>
<artifactId>spring-cloud-starter-netflix-zuul</artifactId>
</dependency>
启动类的代码如下：
packagecom. crazymaker. SpringCloud. cloud. center. zuul;
...
@EnableAutoConfiguration (exclude={SecurityAutoConfiguration. class})
@SpringBootApplication (scanBasePackages=
{"com. crazymaker. SpringCloud. cloud. center. zuul",
"com. crazymaker. SpringCloud. standard",
"com. crazymaker. SpringCloud. user. info. contract"
})
@EnableScheduling
@EnableHystrix
@EnableDiscoveryClient
//开启网关服务
@EnableZuulProxy
@EnableCircuitBreaker


```
publicclassZuulServerApplication{
publicstaticvoidmain (String[]args){
SpringApplication.run (ZuulServerApplication. class, args);
}
}
```
#### 7. 2. 1 Zuul 路由规则配置

作为反向代理，Zuul 需要通过路由规则将 REST 请求转发到上游的微服务提供者。作为示例，
以下列出的是 crazy-SpringCloud 脚手架中的 Zuul 网关的路由规则配置：
#服务网关配置
zuul:
ribbonIsolationStrategy:THREAD
host:
connect-timeout-millis: 600000
socket-timeout-millis: 600000
#路由规则
routes:
seckill-provider:
path:/seckill-provider/**
serviceId: seckill-provider
strip-prefix:false
message-provider:
path:/message-provider/**
serviceId: message-provider
strip-prefix:false
user-provider:
path:/user-provider/**
serviceId: user-provider
strip-prefix:false
backend-provider:
path:/backend-provider/**
serviceId: backend-provider
strip-prefix:false
generate-provider:
path:/generate-provider/**
serviceId: generate-provider
strip-prefix:false
sensitiveHeaders: Cookie, Set-Cookie, token, backend, Authorization
demo-provider:
path:/demo-provider/**
serviceId: demo-provider
strip-prefix:false
urlDemo:
path:/blog/**
url:https://www.cnblogs.com
sensitiveHeaders: Cookie, Set-Cookie, token, backend, Authorization
以上示例中，有两种方式的路由规则的配置：
1 ）路由到直接 URL。
2 ）路由到微服务提供者。
先看第一种方式的路由规则的配置：路由到直接 URL。在上述示例中，有一条叫作 urlDemo 的
路由规则，该规则匹配到格式为“/blog/**”的所有 URL 请求，并直接转发到https://www.cnblogs.com
的地址上。


###### 比如，通过网关访问如下 URL：

[http://](http://) 127. 0. 0. 1 : 7799 /blog/crazymakercircle/p/ 9904544 .html
此 URL 满足/blog/**的匹配规则，将被 Zuul 直接转发到上游的 URL 地址：
https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
将上游地址改为疯狂创客圈的社群博客的实际地址，在浏览器看到的 Zuul 转发结果如图 7 - 2
所示。

图 7 - 2 Zuul 直接转发到上游的 URL 地址
再看第二种方式的路由规则的配置：路由到微服务提供者。在上述代码中，有一条叫作
user-provider 的路由规则，该规则将匹配到的“/user-provider/**”格式的所有 URL 请求直接路由到
名字叫作 user-provider 的某个微服务提供者。
两种方式的区别：
1 ）第一种方式使用 url 属性来指定直接的上游 URL 的前缀，第二种方式使用 serviceId 属性来指
定上游微服务提供者的名称。
2 ）第二种方式需要结合 Eureka 客户端来实现动态的路由转发功能，启动类需要加上注解
@EnableDiscoveryClient，只能用于 SpringCloud 架构中。其实该注解也可以不加，因为网关注解
@EnableZuulProxy 已经默认导入了。

```
使用第二种方式时，在配置文件中增加 Eureka 客户端的相关配置，大致如下：
eureka:
client:
serviceUrl:
defaultZone:http://${EUREKA_ZONE_HOST: localhost}: 7777 /eureka/
instance:
prefer-ip-address: true #访问路径可以显示IP地址
instance-id:${spring. cloud. client. ip-address}:${server. port}
ip-address:${spring. cloud. client. ip-address}
```
#### 7. 2. 2 过滤敏感请求头部

在同一个系统中的不同微服务提供者之间共享请求头是可行的，但是，如果 Zuul 需要将请求
转发到外部，可能不希望敏感的请求头泄露给外部的其他服务器。


防止请求头泄露的方式之一是在 Zuul 的路由配置中指定要忽略的请求头列表，并且多个敏感
头部之间可以用逗号分隔开。下面是一个简单的实例：
spring:
application:
name: cloud-zuul
zuul:
sensitiveHeaders: Cookie, Set-Cookie, token, backend, Authorization
大家知道，Cookies 经常用于在流量中缓存用户的会话、用户凭证等信息，对于外部系统而言
是需要保密的，所以应该设置为敏感标题，不应该带往系统外部。
默认情况下，Zuul 转发请求时会把 header 清空，如果在微服务集群内部转发请求，上游的提供
者会接收不到任何的头部。如果需要传递原始的 header 信息到最终的上游，则需要加上如下所示的
敏感头部设置：
zuul. sensitive-headers=
上面配置敏感头部为空，YML 格式的配置也需要进行空配置，表示没有需要屏蔽的头部。上
面的配置是全局的，也可用单个的路由规则进行局部配置，大致的格式如下：
zuul. routes. xxxapi-xxx. sensitiveHeaders=
比如 crazy-SpringCloud 脚手架中专门对外部的转发规则 urlDemo 进行了请求头的屏蔽，其配置
如下：
#服务网关路由规则
zuul:
routes:
urlDemo:
path:/blog/**
url:https://www.cnblogs.com
sensitiveHeaders: Cookie, Set-Cookie, token, backend, Authorization
单个路由规则的局部配置对于该规则自身而言，会覆盖全局的设置。

#### 7. 2. 3 路径前缀的处理

如果不做任何配置，默认情况下 Zuul 会去掉路由的路径前缀。例如，从客户端发起一个下面
的请求：

```
http://crazydemo.com: 7799 /demo-provider/api/demo/hello/v 1
```

在 Zuul 进行路由处理时，会去掉在路由规则清单中配置的路径前缀 demo-provider。处理之后，
转发到上游的微服务提供者的 URL 将变成下面的样子：

[http://{provider-ip}:{provider-port}/api/demo/hello/v](http://{provider-ip}:{provider-port}/api/demo/hello/v) 1
如果上游的微服务提供者也没有配置路径前缀，Zuul 的这种默认处理和转发是不会有问题的。
但是，如果上游微服务提供者配置了统一的路径前缀，如果前缀被去掉，则上游微服务提供者就会
报出 404 的错误，也就是找不到 URL 对应的资源。
比如，在 crazy-SpringCloud 脚手架中的所有微服务提供者都是配有 context-path 路径前缀的，如
此配置的优势之一是会使下游的 Nginx 外部网关做代理转发时更加灵活。
从微服务 demo-provider 的配置文件 src/main/resources/bootstrap. yml 可以看出，context-path 路径
前缀为/demo-provider，它的配置具体如下：
server:
port: 7700
servlet:
context-path:/demo-provider
在 Zuul 进行路由处理时，如何保留请求 URL 中的路径前缀呢？具体来说，可以把配置项
stripPrefix 的值设置为 false，确保路径前缀不会被截取掉。stripPrefix 的值默认为 true。
demo-provider 的路由规则具体如下：
#服务网关路由规则
zuul:
routes:
demo-provider:
path:/demo-provider/**
serviceId: demo-provider
strip-prefix:false

### 7. 3 Zuul 过滤器

SpringCloudZuul 除了实现请求的路由功能外，还有一个重要的功能就是过滤器。Zuul 可以通
过定义过滤器来实现请求的拦截和过滤，而它本身的大部分功能也是通过过滤器实现的。

#### 7. 3. 1 Zuul 网关的过滤器类型

Zuul 中定义了 4 种标准过滤器类型，它们分别是：
（ 1 ）pre 类型过滤器
此类型为请求路由之前调用的过滤器，可利用此类过滤器实现身份验证、记录调试信息等。
（ 2 ）route 类型过滤器
此类型为发送请求到上游服务的过滤器，比如使用 ApacheHttpClient 或 NetflixRibbon 请求上游
服务。


（ 3 ）post 类型过滤器
此类型为上游服务返回之后调用的过滤器，可用来为响应添加 HTTP 响应头、收集统计信息和
指标、将响应回复给客户端。

（ 4 ）error 类型过滤器
此类型为在其他阶段发生错误时执行的过滤器。
除了默认的过滤器类型，Zuul 还允许我们创建自定义的过滤器类型，例如，我们可以定制一
种 echo 类型的过滤器，直接在 Zuul 中生成响应，而不将请求转发给上游的服务。
Zuul 的请求处理流程为：
1 ）当外部请求到达 Zuul 网关时，首先会进入 pre 处理阶段，这个阶段请求将被 pre 类型的过滤
器处理，以完成再请求路由的前置过滤处理，比如请求的校验等。在完成了 pre 类型的过滤器处理
之后，请求进入第二个阶段——route 路由请求转发阶段。
2 ）route 路由请求转发阶段，请求将被 route 类型的过滤器处理，route 类型的过滤器将外部请求
转发给上游的服务。当服务实例的结果返回之后，route 阶段完成，请求进入第三个阶段——post
处理阶段。
3 ）post 处理阶段，请求将被 post 类型的过滤器处理，这些过滤器在处理的时候不仅可以获取请
求信息，还能获取服务实例的返回信息，所以 post 阶段可以对处理结果进行一些加工或转换等。
4 ）还有一个特殊的阶段——error，该阶段请求将被 error 类型的过滤器处理，该阶段在上述三
个阶段中发生异常时才会触发，但是 error 过滤器也能将最终结果返回给请求客户端。

```
Zuul 的请求处理流程具体如图 7 - 3 所示。
```
图 7 - 3 Zuul 的请求处理流程
Zuul 提供了一个动态读取、编译和运行这些过滤器的框架。过滤器不直接相互通信，而是通
过 RequestContext 共享状态，RequestContext 请求上下文实例对每个请求都是唯一的。


#### 7. 3. 2 实战：用户的黑名单过滤

Zuul 提供了一个过滤器 ZuulFilter 抽象基类，可以作为自定义过滤器的父类。定制一个过滤器
需要实现的父类方法有 4 个，具体如下：

1 .filterType 方法
该方法返回自定义过滤器的类型，以常量的形式定义在 FilterConstants 类中，具体如下：
packageorg. springframework. cloud. netflix. zuul. filters. support;
...
/**
*@authorSpencerGibb
*/
publicclassFilterConstants{
...
/**
*异常过滤
*/
publicstaticfinalStringERROR_TYPE="error";
/**
*后置过滤
*/
publicstaticfinalStringPOST_TYPE="post";
/**
*前置过滤
*/
publicstaticfinalStringPRE_TYPE="pre";
/**
*路由过滤
*/
publicstaticfinalStringROUTE_TYPE="route";
...
}
2 .filterOrder 方法
该方法返回过滤器顺序，值越小优先级越高。
3 .shouldFilter 方法
该方法返回过滤器是否生效的布尔值，返回 true 代表生效，返回 false 代表失效。比如，当请求
处理过程中需要根据请求中是否携带某个参数来判断是否需要过滤时，可以用 shouldFilter 方法对请
求进行参数判断，并返回一个相应的布尔值。
如果直接返回 true，则该过滤器总是生效。
4 .run 方法
过滤器的处理逻辑。在该方法中，或者进行当前的请求拦截和参数定制，或者进行后续的路
由定制，或者进行返回结果的定制等。
下面是根据请求参数 username 进行用户黑名单过滤的例子，如果 username 的参数值在黑名单中，
则对请求进行拦截。具体的代码如下：


packagecom. crazymaker. SpringCloud. cloud. center. zuul. filter;
//省略 import
/**
*演示过滤器：黑名单过滤
*/
@Slf 4 j
@Component
publicclassDemoFilterextendsZuulFilter
{
/**
*示例所使用的黑名单：实际使用场景，需要从数据库或者其他来源获取
*/
staticList<String>blackList=Arrays.asList ("foo","bar","test");
/**过滤的执行类型*/
@Override
publicStringfilterType ()
{
//pre：路由之前
//routing：路由之时
//post：路由之后
//error：发送错误调用
return"pre";
}
/**
*过滤的执行次序
*/
@Override
publicintfilterOrder ()
{
return 0 ;
}
/**
*这里是判断逻辑，是否要执行过滤，true 为跳过
*/
@Override
publicbooleanshouldFilter ()
{
/***获取上下文*/
RequestContextctx=RequestContext.getCurrentContext ();
/***如果请求已经被其他的过滤器终止，则本过滤器也不做处理*/
if (! ctx.sendZuulResponse ())
{
returnfalse;
}
/**
*获取请求
*/
HttpServletRequestrequest=ctx.getRequest ();
/**
*返回 true 表示需要执行过滤器的 run 方法
*/
if (request.getRequestURI (). startsWith ("/ZuulFilter/demo"))
{
returntrue;
}


/**
*返回 false 表示跳过此过滤器，不执行 run 方法
*/
returnfalse;
}
/**
*过滤器的具体逻辑
*通过请求中的用户名称参数，判断是否在黑名单中
*/
@Override
publicObjectrun ()
{
RequestContextctx=RequestContext.getCurrentContext ();
HttpServletRequestrequest=ctx.getRequest ();
/**
*对用户名称进行判断：
*如果用户名称在黑名单中，则不再转发给后端的微服务提供者
*/
Stringusername=request.getParameter ("username");
if (username!=null&&blackList.contains (username))
{
log.info (username+"isforbidden: "+request.getRequestURL (). toString ());
/**
*终止后续的访问流程
*/
ctx.setSendZuulResponse (false);
try
{
ctx.getResponse (). setContentType ("text/html; charset=utf- 8 ");
ctx.getResponse (). getWriter (). write ("对不起，您已经进入黑名单");
}catch (Exceptione)
{
e.printStackTrace ();
}
returnnull;
}
returnnull;
}
}
在上面的代码中，RequestContext. setSendZuulResponse（boolean）方法在请求上下文中设置标
志位 sendZuulResponse 的值为 false，表示不需要后续处理。上下文 setSendZuulResponse 标志位的值
通过 RequestContext.sendZuulResponse () 方法获取。
Zuul 内置的几乎所有的过滤器都会对该标志位进行判断，如果其值为 false，则将不用对请求进
行过滤处理。以非常重要的 route 类型 RibbonRoutingFilter 为例来看一下其 shouldFilter 方法的源码，
具体如下：
packageorg. springframework. cloud. netflix. zuul. filters. route;
...
publicclassRibbonRoutingFilterextendsZuulFilter{
...
@Override
publicbooleanshouldFilter (){
RequestContextctx=RequestContext.getCurrentContext ();
return (ctx.getRouteHost ()==null&&ctx.get (SERVICE_ID_KEY)!=null


&&ctx.sendZuulResponse ());
}
...
}
过滤器 RibbonRoutingFilter 的作用是通过结合 Ribbon 和 Hystrix 来向微服务提供者实例发起请求，
并返回请求结果。其判断条件中就有 sendZuulResponse 的标志位判断部分，如果该值为 false，则不
再发起请求。

### 7. 4 SpringSecurity 原理和实战

Web 微服务提供者的安全访问无疑是十分重要的，而 SpringSecurity 安全模块是保护 Web 应用
的一个非常好的选择。
SpringSecurity 是 Spring 应用项目中的一个安全模块，特别是在 SpringBoot 项目中，Spring
Security 默认为自动开启，可见其重要性。
在微服务架构下，建议仅将 SpringSecurity 组件应用于网关（例如 Zuul）上，对于集群内部的
微服务提供者，不建议启用 SpringSecurity 组件，因为重复的验证会降低请求处理的性能。本书配
套的 crazy-SpringCloud 微服务脚手架就是这样架构的。
如果需要为微服务提供者关闭 SpringSecurity 组件的自动启动，可以在启动类上添加以下注解：
@EnableEurekaClient
@SpringBootApplication (scanBasePackages={
...
}, exclude={SecurityAutoConfiguration. class})
或者可以在应用配置文件中将它的自动配置类排除，具体如下：
spring:
autoconfigure:
exclude: org. springframework. boot. autoconfigure. security. servlet.
SecurityAutoConfiguration

#### 7. 4. 1 SpringSecurity 核心组件

SpringSecurity 核心组件有：Authentication、AuthenticationProvider、AuthenticationManager 等，
下面分别介绍。

1 .SpringSecurity 核心组件之 Authentication
Authentication 直译是“凭证”的意思，在 SpringSecurity 中，Authentication 接口用来表示凭证
或者令牌，可以理解为用户的用户名、密码、权限等信息。Authentication 的代码如下：
publicinterfaceAuthenticationextendsPrincipal, Serializable{
//权限集合
//可使用 AuthorityUtils.commaSeparatedStringToAuthorityList ("admin, ROLE_ADMIN")
Collection<?extendsGrantedAuthority>getAuthorities ();
//用户名密码认证时，可以理解为密码
ObjectgetCredentials ();


//认证时包含的一些详细信息，可以是一个包含用户信息的 POJO 实例
ObjectgetDetails ();
//用户名和密码认证时，可理解为用户名
ObjectgetPrincipal ();
//是否通过认证，通过为 true
booleanisAuthenticated ();
//设置是否通过认证
voidsetAuthenticated (booleanisAuthenticated) throwsIllegalArgumentException;
}
下面对 Authentication 凭证/令牌接口的方法进行说明，具体如下：
（ 1 ）getPrincipal 方法
Principal 直译为“主要演员、主角”，用于获取用户身份信息，可以是用户名，也可以是用户
的 ID 等，其具体的值需要依据具体的认证令牌实现类确定。

（ 2 ）getAuthorities 方法
用于获取用户权限集合，一般情况下获取的是用户的权限信息。
（ 3 ）getCredentials 方法
直译为“获取资格证书”。用户名+密码认证时，通常情况下获取的是密码信息。
（ 4 ）getDetails 方法
用于获取用户的详细信息。用户名+密码认证时，这部分信息可以是用户的 POJO 实例。
（ 5 ）isAuthenticated 方法
判断当前 Authentication 凭证是否已验证通过。
（ 6 ）setAuthenticated 方法
设置当前 Authentication 凭证是否已验证通过（true 或 false）。
在 SpringSecurity 中，Authentication 凭证接口有很多内置的实现类，比如：
（ 1 ）UsernamePasswordAuthenticationToken 凭证
用于在用户名+密码认证的场景作为验证的凭证，该凭证（令牌）包含了用户名+密码信息。
（ 2 ）RememberMeAuthenticationToken 凭证
用于“记住我”的认证场景。如果在用户名+密码认证成功认证之后，在一定的时间内不需要再
输入用户名和密码进行认证，就可以使用 RememberMeAuthenticationToken（“记住我”）凭证。这通
常是通过服务端发送一个 Cookie 给客户端浏览器，下次浏览器再访问该服务端时，服务端能够自动检
测客户端的 Cookie，根据 Cookie 值自动触发 RememberMeAuthenticationToken 凭证/令牌的验证操作。

（ 3 ）AnonymousAuthenticationToken 匿名凭证
对于匿名访问的用户，SpringSecurity 支持为该用户建立一个 AnonymousAuthenticationToken
匿名凭证实例存放在 SecurityContextHolder 中。

```
除了以上内置凭证类，还可以通过实现 Authentication 凭证/令牌接口定制自己的凭证/令牌实现类。
```

2 .SpringSecurity 核心组件之 AuthenticationProvider
AuthenticationProvider 也是一个接口，包含两个函数，authenticate 和 supports，完成对凭证进行
验证的操作。
publicinterfaceAuthenticationProvider{
//对实参 authentication 凭证/令牌对象进行验证操作
Authenticationauthenticate (Authenticationauthentication)
throwsAuthenticationException;
//判断：是否支持该 authentication 凭证/令牌
booleansupports (Class<?>authentication);
}
AuthenticationProvider 接口有两个方法，具体如下：
（ 1 ）authenticate 方法
表示认证的动作，对 authentication 参数对象进行验证操作。如果验证通过，返回一个验证通过
的凭证/令牌。通过源码中的注释可以知道，如果认证失败，则抛出异常。

（ 2 ）supports 方法
判断实参 authentication 是否为当前认证提供者所能验证的令牌。
在 SpringSecurity 中，AuthenticationProvider 认证提供者接口有很多内置的实现类，比如：
（ 1 ）AbstractUserDetailsAuthenticationProvider 认证提供者实现类
这是一个对 UsernamePasswordAuthenticationToken 类型的凭证/令牌进行验证的认证提供者类，
用于用户名+密码验证的场景。

（ 2 ）RememberMeAuthenticationProvider 认证提供者实现类
这是一个对 RememberMeAuthenticationToken 类型的凭证/令牌进行验证的认证提供者类，用于
“记住我”的认证场景。

（ 3 ）AnonymousAuthenticationProvider
这是一个对 AnonymousAuthenticationToken 类型的凭证/令牌进行验证的认证提供者类，用于匿
名认证场景。

此外，如果自定义了凭证/令牌，并且 SpringSecurity 的默认认证提供者类不能支持该凭证/令牌，
则可以通过实现 AuthenticationProvider 接口来扩展自定义的认证提供者。

3 .SpringSecurity 核心组件之 AuthenticationManager
AuthenticationManager 还是一个接口，其唯一的 authenticate 验证方法是认证流程的入口，接收
一个 Authentication 令牌对象作为参数。
publicinterfaceAuthenticationManager{
//认证流程的入口
Authenticationauthenticate (Authenticationauthentication)
throwsAuthenticationException;
}
AuthenticationManager 的一个实现类叫作 ProviderManager，该类有一个 providers 成员变量，负
责管理一个提供者清单列表，其源码如下：


publicclassProviderManagerimplementsAuthenticationManager, MessageSourceAware,
InitializingBean{
...
//提供者清单
privateList<AuthenticationProvider>providers=Collections.emptyList ();
//迭代提供者清单，找出支持令牌的提供者，交给提供者去执行令牌验证
publicAuthenticationauthenticate (Authenticationauthentication)
throwsAuthenticationException{
...
}
}
认证管理者 ProviderManager 在进行令牌验证时，会对提供者列表清单进行迭代，找出支持令
牌的认证提供者，并交给认证提供者去执行令牌验证。如果该认证提供者的 supports 方法返回 true，
那么就会调用该提供者的 authenticate 方法，如果验证成功，则整个认证过程结束；如果不成功，则
继续处理列表清单中的下一个提供者。只要有一个验证成功，则为认证成功。

#### 7. 4. 2 SpringSecurity 的请求认证处理流程

一个基础、简单的 SpringSecurity 请求认证的处理流程，大致包括以下步骤：
定制一个凭证/令牌类。
定制一个认证提供者类，该认证提供者类需要和凭证/令牌类进行配套。
定制一个过滤器类，从请求中获取用户信息组装成定制凭证/令牌，交给认证管理者。
定制一个 HTTP 的安全认证配置类（AbstractHttpConfigurer 子类），将上一步定制
的过滤器加入请求的过滤处理责任链。
定义一个 SpringSecurity 安全配置类（WebSecurityConfigurerAdapter 子类），对 Web
容器的 HTTP 安全认证机制进行配置。
作为演示，这里实现一个非常简单的认证处理流程，具体的功能：当系统资源被访问时，过
滤器从 HTTP 的 token 请求头获取用户名和密码，然后与系统中的用户信息进行匹配，如果匹配成功，
则可以访问系统资源，否则返回 403 ——未授权的响应码。演示程序的代码位于本书配套源码的
demo-provider 模块中。

演示程序的第一步：定制一个凭证/令牌类，封装用户的用户名称和密码。所定制的 DemoToken
令牌的代码如下：
packagecom. crazymaker. SpringCloud. demo. security;
//省略 import
publicclassDemoTokenextendsAbstractAuthenticationToken
{
//用户名称
privateStringuserName;
//密码
privateStringpassword;
//...
}


###### 演示程序的第二步：定制一个认证提供者类和凭证/令牌类进行配套，并完成对自制凭证/令牌

实例的验证。所定制的 DemoAuthProvider 认证提供者类的代码如下：
publicclassDemoAuthProviderimplementsAuthenticationProvider
{
publicDemoAuthProvider ()
{
}
//模拟的数据源，实际场景从 DB 中获取
privateMap<String,String>map=newLinkedHashMap<>();
//初始化模拟的数据源，放入两个用户
{
map.put ("zhangsan"," 123456 ");
map.put ("lisi"," 123456 ");
}
//具体的验证令牌方法
@Override
publicAuthenticationauthenticate (Authenticationauthentication) throws
AuthenticationException
{
DemoTokentoken=(DemoToken) authentication;
//从数据源 map 中获取用户密码
StringrawPass=map.get (token.getUserName ());
//验证密码，如果不相等，就抛出异常
if (! token.getPassword (). equals (rawPass))
{
token.setAuthenticated (false);
thrownewBadCredentialsException ("认证有误：令牌校验失败");
}
//验证成功
token.setAuthenticated (true);
returntoken;
}
/**
*判断令牌是否被支持
*@paramauthentication 这里仅 DemoToken 令牌被支持
*@return
*/
@Override
publicbooleansupports (Class<?>authentication)
{
returnauthentication.isAssignableFrom (DemoToken. class);
}
}
DemoAuthProvider 模拟了一个简单的数据源并且加载了两个用户。在其 authenticate 验证方法中，将
入参 DemoToken 令牌中的用户名称和密码与模拟数据源中的用户信息进行匹配，匹配成功则验证成功。
演示程序的第三步：定制一个过滤器类，从请求中获取用户信息并组装成定制凭证/令牌，提
交给认证管理者。在生产场景中，认证信息一般为某个 HTTP 头部信息（如 Cookie 信息、令牌信息
等）。本演示过滤器类为 DemoAuthFilter，从请求头部中获取 token 字段，解析之后组装成 DemoToken
令牌实例，提交给 AuthenticationManager 认证管理者进行验证。DemoAuthFilter 的代码如下：
publicclassDemoAuthFilterextendsOncePerRequestFilter
{


//认证失败的处理器
privateAuthenticationFailureHandlerfailureHandler=newAuthFailureHandler ();
...
//authenticationManager 是认证流程的入口，接收一个 Authentication 令牌对象作为参数
privateAuthenticationManagerauthenticationManager;
@Override
protectedvoiddoFilterInternal (HttpServletRequestrequest,
HttpServletResponseresponse, FilterChainfilterChain) throwsServletException,
IOException
{
...
AuthenticationExceptionfailed=null;
try
{
AuthenticationreturnToken=null;
booleansucceed=false;
//从请求头中获取认证信息
Stringtoken=request.getHeader (SessionConstants. AUTHORIZATION_HEAD);
String[]parts=token.split (",");
//组装令牌
DemoTokendemoToken=newDemoToken (parts[ 0 ], parts[ 1 ]);
//提交给 AuthenticationManager 认证管理者进行令牌验证
returnToken=(DemoToken)
this.getAuthenticationManager (). authenticate (demoToken);
//获取认证成功标志
succeed=demoToken.isAuthenticated ();
if (succeed)
{
//认证成功，设置上下文令牌
SecurityContextHolder.getContext (). setAuthentication (returnToken);
//执行后续的操作
filterChain.doFilter (request, response);
return;
}
}catch (Exceptione)
{
logger.error ("认证有误", e);
failed=newAuthenticationServiceException ("请求头认证消息格式错误", e);
}
if (failed==null)
{
failed=newAuthenticationServiceException ("认证失败");
}
//认证失败了
SecurityContextHolder.clearContext ();
failureHandler.onAuthenticationFailure (request, response, failed);
}
...
}
为了使得过滤器能够生效，必须将过滤器加入 Web 容器的 HTTP 过滤处理责任链，此项工作可
以通过实现一个 AbstractHttpConfigurer 配置类来完成。

演示程序的第四步：定制一个 HTTP 的安全认证配置类（AbstractHttpConfigurer 子类），将上
一步定制的过滤器加入请求的过滤处理责任链。定制的 DemoAuthConfigurer 代码如下：


publicclassDemoAuthConfigurer<TextendsDemoAuthConfigurer<T,B>, B
extendsHttpSecurityBuilder<B>>extendsAbstractHttpConfigurer<T,B>
{
//创建认证过滤器
privateDemoAuthFilterauthFilter=newDemoAuthFilter ();
//将过滤器加入 HTTP 过滤处理责任链
@Override
publicvoidconfigure (Bhttp) throwsException
{
//获取 SpringSecurity 共享的 AuthenticationManager 认证提供者实例
//设置认证过滤器
authFilter.setAuthenticationManager (http. getSharedObject
(AuthenticationManager. class));
DemoAuthFilterfilter=postProcess (authFilter);
//将过滤器加入 HTTP 过滤处理责任链
http.addFilterBefore (filter, LogoutFilter. class);
}
}
演示程序的第五步：定义一个 SpringSecurity 安全配置类（WebSecurityConfigurerAdapter 子类），
对 Web 容器的 HTTP 安全认证机制进行配置。这一步有两个工作：一是应用 DemoAuthConfigurer 配置类，
二是构造 AuthenticationManagerBuilder 认证管理者实例。定制类 DemoWebSecurityConfig 的代码如下：
@EnableWebSecurity
publicclassDemoWebSecurityConfigextendsWebSecurityConfigurerAdapter
{
//配置 HTTP 请求的安全策略，应用 DemoAuthConfigurer 配置类实例
protectedvoidconfigure (HttpSecurityhttp) throwsException
{
http.csrf (). disable ()
...
.and ()
//应用 DemoAuthConfigurer 配置类
.apply (newDemoAuthConfigurer<>())
.and ()
.sessionManagement (). disable ();
}
//配置认证 Builder 建造者，由它负责建造 AuthenticationManager 认证管理者实例
//Builder 将建造 AuthenticationManager 管理者实例，并且将作为 HTTP 请求的共享对象存储在
//代码中，可以通过 http.getSharedObject (AuthenticationManager. class) 来获取管理者实例
@Override
protectedvoidconfigure (AuthenticationManagerBuilderauth) throwsException
{
//加入自定义的 Provider 认证提供者实例
auth.authenticationProvider (demoAuthProvider ());
}
//自定义的认证提供者实例
@Bean ("demoAuthProvider")
protectedDemoAuthProviderdemoAuthProvider ()
{
returnnewDemoAuthProvider ();
}
}
如何对以上自定义的安全认证机制进行验证呢？首先启动 demo-provider 服务，然后在浏览器
中访问 swagger-ui 界面，如图 7 - 4 所示。


```
图 7 - 4 demo-provider 服务的 swagger-ui 界面
接着在 swagger-ui 界面上访问/api/demo/hello/v 1 ，发现认证失败，如图 7 - 5 所示。
```
图 7 - 5 直接访问/api/demo/hello/v 1 返回认证失败
这是由于上面定义的 SpringSecurity 的请求认证处理流程已经生效，接下来在 swagger-ui 界面上再
一次访问/api/demo/hello/v 1 ，不过这一次给令牌请求头输入了正确的用户名和密码，如图 7 - 6 所示。

图 7 - 6 给令牌请求头输入了正确的用户名和密码
最后，再一次访问/api/demo/hello/v 1 ，发现请求的返回值已经正常，表明上面定义的 Spring
Security 的请求认证处理流程起到了对请求进行用户名和密码验证的作用。


#### 7. 4. 3 基于数据源的认证流程

###### 大多数的生产场景中，用户信息都存储在某个数据源（如数据库）中，认证过程都涉及从数

据源加载用户信息的环节。SpringSecurity 为这种场景内置了一套解决方案，主要包含几个内置类。

（ 1 ）UsernamePasswordAuthenticationToken
此凭证类实现了 Authentication 接口，主要封装用户输入的用户名和密码信息，并提供给支持
的认证提供者进行认证。

（ 2 ）AbstractUserDetailsAuthenticationProvider
此认证提供者类与 UsernamePasswordAuthenticationToken 凭证/令牌类配套，但这是一个抽象类，
具体的验证逻辑需要由子类完成。
此认证提供者类的常用子类为 DaoAuthenticationProvider 类，该类依赖一个 UserDetailsService
用户服务数据源，用于获取 UserDetails 用户信息，其中包括用户名、密码和所拥有的权限等。此认
证提供者子类从数据源 UserDetailsService 中加载用户信息后，将待认证的令牌中的“用户名+密码”
信息和所加载的数据源用户信息进行匹配和验证。

（ 3 ）UserDetailsService
UserDetailsService 有一个 loadUserByUsername 方法，其作用是根据用户名从数据源中查询用户
实体。一般情况下，可以实现一个定制的 UserDetailsService 接口的实现类来从特定的数据源获取用
户信息。用户信息服务接口的源码如下：
publicinterfaceUserDetailsService{
//通过用户名从数据源加载用户信息
UserDetailsloadUserByUsername (Stringusername) throwsUsernameNotFoundException;
}
（ 4 ）UserDetails
UserDetails 是一个接口，主要封装用户名、密码、是否过期、是否可用等信息。此接口的源码
如下：
publicinterfaceUserDetailsextendsSerializable{
//权限集合
Collection<?extendsGrantedAuthority>getAuthorities ();
//密码，一般为密文
StringgetPassword ();
//用户名
StringgetUsername ();
//用户名是否未过期
booleanisAccountNonExpired ();
//用户名是否未锁定
booleanisAccountNonLocked ();
//用户密码是否未过期
booleanisCredentialsNonExpired ();
//账号是否可用 (可理解为是否删除)
booleanisEnabled ();


}
UserDetails 接口的密码属性和 UsernamePasswordAuthenticationToken 的密码属性的区别在于：
前者的密码来自数据源，是密文；后者的密码来自用户请求，是明文。明文和密文的匹配工作由
PasswordEncoder 加密器完成。

（ 5 ）PasswordEncoder
PasswordEncoder 是一个负责明文加密、明文和密文匹配的接口，源码如下:
publicinterfacePasswordEncoder{
//对明文 rawPassword 加密
Stringencode (CharSequencerawPassword);
//判断 rawPassword 与 encodedPassword 是否匹配
booleanmatches (CharSequencerawPassword, StringencodedPassword);
}
DaoAuthenticationProvider 提供者在验证之前，会通过内部的 PasswordEncoder 加密器实例对令
牌中的密码明文和 UserDetails 中的密码密文进行匹配。如果匹配不成功，则令牌验证不通过。
PasswordEncoder 的内置实现类有多个，如 BCryptPasswordEncoder、Pbkdf 2 PasswordEncoder 等。
其中 BCryptPasswordEncoder 比较常用，其采用 SHA- 256 +密钥+盐的组合方式对密码明文进行 Hash
编码处理。注意，SHA- 256 是 Hash（哈希）编码算法，不是加密算法。这里是对明文编码而不是加
密，这是因为加密算法往往可以解密，只是解密的复杂度不同而已；而编码算法则不一样，其过程
是不可逆的。密码明文编码之后，只有用户知道密码，甚至后台管理员都无法直接看到用户的密码
明文。当用户忘记密码后，只能重置密码（通过手机验证码或者邮箱的形式）。所以，即使数据库
泄漏，黑客也很难破解密码。
推荐使用 BCryptPasswordEncoder 来进行密码明文的编码，本书配套的微服务脚手架中通过配
置类配置了一个全局的加密器 IOC 容器实例，参考代码如下：
packagecom. crazymaker. SpringCloud. standard. config;
//省略 import
/**
*密码加密器配置类
*/
@Configuration
publicclassDefaultPasswordConfig
{
/**
*装配一个全局的 Bean，用于密码加密和匹配
*
*@return BCryptPasswordEncoder 加密器实例
*/
@Bean
publicPasswordEncoderpasswordEncoder ()
{
returnnewBCryptPasswordEncoder ();
}
}
此类处于脚手架的 base-runtime 模块中，默认已经完成了 Bean 的装配，其他的模块直接通过
@Resource 注解装配即可。


###### 作为基于数据源的认证流程演示，这里简单改造 7. 4. 2 节的实例，使用基于数据源的请求认证

###### 方式完成认证处理，并且依据 7. 4. 2 节中验证流程的 5 个步骤进行说明。

###### 演示程序的第一步：定制一个凭证/令牌类。

本演示程序直接使用 SpringSecurity 提供的 UsernamePasswordAuthenticationToken 凭证类存放
用户名+密码信息。故这里不再定制自己的凭证/令牌类。

演示程序的第二步：定制一个认证提供者类与凭证/令牌类进行配套。
本演示程序直接使用 SpringSecurity 提供的提供者实现类 DaoAuthenticationProvider，并在项目
的 SpringSecurity 的启动配置类（本演示程序中为 DemoWebSecurityConfig 类）中创建该提供者的
Bean 实例。需要注意的是，该提供者有两个依赖：一个 UserDetailsService 类型的用户信息服务实例，
一个 PasswordEncoder 类型的加密器实例。
在项目的启动配置类中装配 DaoAuthenticationProvider 提供者容器实例的参考代码如下：
packagecom. crazymaker. SpringCloud. demo. config;
...
@EnableWebSecurity
publicclassDemoWebSecurityConfigextendsWebSecurityConfigurerAdapter
{
...
//注入全局 BCryptPasswordEncoder 加密器容器实例
@Resource
privatePasswordEncoder passwordEncoder;
//注入数据源服务容器实例
@Resource
privateDemoAuthUserService demoUserAuthService;
@Bean ("daoAuthenticationProvider")
protectedAuthenticationProviderdaoAuthenticationProvider () throwsException
{
//创建一个数据源提供者
DaoAuthenticationProviderdaoProvider=newDaoAuthenticationProvider ();
//设置加密器
daoProvider.setPasswordEncoder (passwordEncoder);
//设置用户数据源服务
daoProvider.setUserDetailsService (demoUserAuthService);
returndaoProvider;
}
}
代码中所依赖的 PasswordEncoder 类的加密器 IOC 实例会注入 base-runtime 模块所装配的全局的
BCryptPasswordEncoder 类的 passwordEncoderBean。代码中所依赖的数据源服务 IOC 实例的类是一
个自定义的数据源服务类，名为 DemoAuthUserService，核心代码如下：
packagecom. crazymaker. SpringCloud. demo. security;
//省略 import
@Slf 4 j
@Service
publicclassDemoAuthUserServiceimplementsUserDetailsService
{
//模拟的数据源，实际从数据库中获取
privateMap<String,String>map=newLinkedHashMap<>();
//初始化模拟的数据源，放入两个用户


{
map.put ("zhangsan"," 123456 ");
map.put ("lisi"," 123456 ");
}
/**
*装载系统配置的加密器
*/
@Resource
privatePasswordEncoderpasswordEncoder;
publicUserDetailsloadUserByUsername (Stringusername)
throwsUsernameNotFoundException
{
//实际场景中需要从数据库加载用户
//这里出于演示目的，用 map 模拟真实的数据源
Stringpassword=map.get (username);
if (password==null)
{
returnnull;
}
if (null==passwordEncoder)
{
passwordEncoder=CustomAppContext.getBean (PasswordEncoder. class);
}
/**
*返回一个用户详细实例，包含用户名、加密后的密码、用户权限清单、用户角色
*/
UserDetailsuserDetails=User.builder ()
.username (username)
.password (passwordEncoder.encode (password))
.authorities (SessionConstants. USER_INFO)
.roles ("USER")
.build ();
returnuserDetails;
}
}
SpringSecurity 的 DaoAuthenticationProvider 在验证令牌时，会将令牌中的密码明文和用户详细
实例 UserDetails 中的密码密文通过其内部的 PasswordEncoder 加密器实例进行匹配。所以，
UserDetails 中密文在加密时用的加密器和 DaoAuthenticationProvider 中的认证加密器是同一种类型，
需要使用同样的编码/加密算法，以保证能匹配成功。本演示程序中，由于二者使用的都是全局加
密器 IOC 容器实例，因此加密器的类型和算法自然是一致的。

演示程序的第三步：定制一个过滤器类，从请求中获取用户信息组装成定制凭证/令牌，交给
认证管理者。
这一步使用 7. 4. 2 节的 DemoAuthFilter 过滤器，仅进行简单的修改：从请求中获取令牌头部字段，
解析之后组装成 UserDetails 用户详情，然后构造一个“用户名+密码”类型的
UsernamePasswordAuthenticationToken 令牌实例，提交给 AuthenticationManager 认证管理者进行验证。
packagecom. crazymaker. SpringCloud. demo. security;
//省略 import
publicclassDemoAuthFilterextendsOncePerRequestFilter
{


...
@Override
protectedvoiddoFilterInternal (HttpServletRequestrequest,
HttpServletResponseresponse, FilterChainfilterChain) throwsServletException,
IOException
{
...
try
{
AuthenticationreturnToken=null;
booleansucceed=false;
Stringtoken=request.getHeader (SessionConstants. AUTHORIZATION_HEAD);
String[]parts=token.split (",");
//方式二: 数据源认证演示
UserDetailsuserDetails=User.builder ()
.username (parts[ 0 ])
.password (parts[ 1 ])
.authorities (SessionConstants. USER_INFO)
.build ();
//创建一个用户名+密码的凭证，一般情况下，令牌中的密码需要明文
AuthenticationuserPassToken=new
UsernamePasswordAuthenticationToken (userDetails,
userDetails.getPassword (),
userDetails.getAuthorities ());
//进入认证流程
returnToken=this.getAuthenticationManager (). authenticate (userPassToken);
succeed=userPassToken.isAuthenticated ();
if (succeed)
{
//认证成功, 设置上下文令牌
SecurityContextHolder.getContext (). setAuthentication (returnToken);
//执行后续的操作
filterChain.doFilter (request, response);
return;
}
}catch (Exceptione)
{
logger.error ("认证有误", e);
failed=newAuthenticationServiceException ("请求头认证消息格式错误", e);
}
...
}
...
}
以上的过滤器实现代码除了认证的令牌不同之外，其他的代码和 7. 4. 2 节中的代码基本一致。
演示程序的第四步：定制一个 HTTP 的安全认证配置类（AbstractHttpConfigurer 子类），将上
一步的定制的过滤器加入请求的过滤处理责任链。
演示程序的第五步：定义一个 SpringSecurity 安全启动配置类（WebSecurityConfigurerAdapter
子类），对 HTTP 的安全认证机制进行配置。

第四步、第五步的实现代码和 7. 4. 2 节中的第四步、第五步实现代码是完全一致的，这里不再
赘述。
完成以上五步后，一个基于数据源的认证流程也就实现了。重启项目后，可以参考 7. 4. 2 节的
自验证方法进行 SpringSecurity 的认证拦截验证。


### 7. 5 JWT+SpringSecurity 进行网关安全认证

JWT 和 SpringSecurity 相结合进行系统安全认证是目前用得最多的一种安全认证组合。疯狂创
客圈 crazy-SpringCloud 微服务开发脚手架也使用了 JWT 身份令牌结合 SpringSecurity 的安全认证机
制，完成用户请求的安全权限认证。整个用户认证的过程大致如下：

1 ）前台（如网页富客户端）通过 REST 接口将用户名和密码发送给 UAA 用户账号与认证微服
务进行登录。
2 ）UAA 服务在完成登录流程后，将 sessionid 作为 JWT 的 payload 负载，生成 JWT 身份令牌后发
送给前台。
3 ）前台可以将 JWT 令牌存到 localStorage 或者 sessionStorage 中，当然，退出登录时，前端必须
删除保存的 JWT 令牌。
4 ）前台每次在请求微服务提供者的 REST 资源时，将 JWT 令牌放到请求头中。crazy-SpringCloud
脚手架做了管理端和用户端的前台区分，管理端前台的令牌头为 Authorization，用户端前台的令牌
头为 token。
5 ）在请求到达 Zuul 网关时，Zuul 会结合 SpringSecurity 进行拦截，从而验证 JWT 的有效性。
6 ）Zuul 验证通过后，才可以访问微服务所提供的 REST 资源。
需要说明的是，在 crazy-SpringCloud 微服务开发脚手架中，微服务提供者自身不需要进行单独
的安全认证，微服务提供者之间的内部远程调用也是不需要安全认证的，安全认证全部由网关负责。
严格来说，这套安全机制是能够满足一般的生产场景安全认证要求的。如果觉得这个安全级别不是
太高，单个的微服务提供者也需要进行独立的安全认证，实现起来也很容易，只需要导入公共的安
全认证模块 base-auth 即可。实际上早期的 crazy-SpringCloud 脚手架也是这样架构的，后期发现这样
做纯属多虑，而且大大降低了微服务提供者模块的可复用性和可移植性（这是微服务架构的巨大优
势之一）。所以，crazy-SpringCloud 后来将整体架构调整为由网关（如 Zuul 或者 Nginx）负责安全
认证，去掉了微服务提供者的安全认证能力。

#### 7. 5. 1 JWT 安全令牌规范详解

###### JWT 是一种用户凭证的编码规范，是一种网络环境下编码用户凭证的 JSON 格式的开放标准

###### （RFC 7519 ）。JWT 令牌的格式被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）、

###### 用户身份认证等场景。

一个编码之后的 JWT 令牌字符串为三个部分：header+payload+signature，这三部分通过“.”连
接。第一部分常被称为头部（header），第二部分常被称为负载（payload），第三部分常被称为签
名（signature）。

```
1 .JWT 的头部
编码之前的 JWT 的头部（header 部分）是 JSON 格式，一个完整的头部如下：
{
"typ": "JWT",
```

"alg": "HS 256 "
}
其中，"typ"是 type 类型的简写，值为"JWT"代表是 JWT 类型；"alg"是加密算法的简写，值为
"HS 256 "代表加密方式是 HS 256 。
采用 JWT 令牌编码时，头部的 JSON 字符串将进行 Base 64 编码，编码之后的字符串构成 JWT 令
牌的第一部分。

2 .JWT 的负载
编码之前的 JWT 的负载（playload）部分也是 JSON 格式，负载部分是存放有效信息的部分，一
个简单的负载如下：
{
"sub": "sessionid",
"exp": 1579315717 ,
"iat": 1578451717
}
采用 JWT 令牌编码时，负载部分的 JSON 字符串将进行 Base 64 编码，编码之后的字符串构成 JWT
令牌第二部分。

3 .JWT 的 signature
JWT 的第三部分是一个签名字符串，这一部分是将头部的 Base 64 编码和负载的 Base 64 编码使用
点号“.”连接起来，然后通过头部声明的加密算法进行加密所得到的密文。为了保证安全，加密
时需要加入盐（salt）。
下面是一个演示用例：用 Java 代码生成 JWT 令牌，然后对令牌头部的字符串和负载部分的字符
串进行 Base 64 解码，并输出解码后的 JSON。
packagecom. crazymaker. demo. auth;
//省略 import
@Slf 4 j
publicclassJwtDemo
{
@Test
publicvoidtestBaseJWT ()
{
try
{
/**
*JWT 的演示内容
*/
Stringsubject="sessionid";
/**
*签名的加密盐
*/
Stringsalt="userpassword";
/**
*签名的加密算法
*/
Algorithmalgorithm=Algorithm. HMAC 256 (salt);
//签发时间


longstart=System.currentTimeMillis ()- 60000 ;
//过期时间，在签发时间的基础上加上一个有效时长
Dateend=newDate (start+SessionConstants. SESSION_TIME_OUT* 1000 );
/**
*获取编码后的 JWT 令牌
*/
Stringtoken=JWT.create ()
.withSubject (subject)
.withIssuedAt (newDate (start))
.withExpiresAt (end)
.sign (algorithm);
log.info ("token="+token);
//编码后输出 demo 为：
//token=eyJ 0 eXAiOiJKV 1 QiLCJhbGciOiJIUzI 1 NiJ 9 .eyJzdWIiOiJzZXNzaW 9 uIGlkIiwi
ZXhwIjoxNTc 5 MzE 1 NzE 3 LCJpYXQiOjE 1 Nzg 0 NTE 3 MTd 9 .iANh 9 Fa 0 B_ 6 H 5 TQ 11 bLCWcEpmWxuCwa 2 Rt 6 rnzBWteI
//以“·”分割令牌
String[]parts=token.split ("\\.");
/**
*然后对第一部分和第二部分进行解码
*解码后的第一部分：头部（header）
*/
StringheaderJson;
headerJson=StringUtils. newStringUtf 8 (Base 64 .decodeBase 64 (parts[ 0 ]));
log.info ("parts[ 0 ]="+headerJson);
//解码后的第一部分输出的示例为：
//parts[ 0 ]={"typ": "JWT","alg": "HS 256 "}
/**
*解码后的第二部分：负载（payload）
*/
StringpayloadJson;
payloadJson=StringUtils. newStringUtf 8 (Base 64 .decodeBase 64 (parts[ 1 ]));
log.info ("parts[ 1 ]="+payloadJson);
//输出的示例为：
//解码后第二部分 parts[ 1 ]={"sub": "sessionid","exp": 1579315535 ,
"iat": 1578451535 }
}catch (Exceptione)
{
e.printStackTrace ();
}
}
...
}
在编码前的 JWT 中，负载部分的 JSON 中的属性叫作 JWT 的声明（claim）。JWT 的声明被分为
两类：

```
1 ）公有的声明（如 iat 签发时间）。
2 ）私有的声明（自定义的 JSON 属性）。
公有的声明也就是 JWT 标准中注册的声明，主要为以下 JSON 属性：
1 ）iss：签发人。
2 ）sub：主题。
3 ）aud：用户。
```

4 ）iat：JWT 的签发时间。
5 ）exp：JWT 的过期时间，这个过期时间必须大于签发时间。
6 ）nbf：定义在什么时间之前，该 JWT 是不可用的。
私有的声明是除了公共声明之外的自定义 JSON 字段，私有的声明可以添加任何信息，一般添
加用户的相关信息或其他业务需要的必要信息。下面的 JSON 例子中的 uid、user_name、nick_name
等都是私有声明。
{
"uid": " 123 ...",
"sub": "sessionid",
"user_name": "admin",
"nick_name": "管理员",
"exp": 1579317358 ,
"iat": 1578453358
}
下面是一个向 JWT 令牌添加私有声明的实例，代码如下：
packagecom. crazymaker. demo. auth;
//省略 import
@Slf 4 j
publicclassJwtDemo
{
/**
*测试私有声明
*/
@Test
publicvoidtestJWTWithClaim ()
{
try
{
Stringsubject="sessionid";
Stringsalt="userpassword";
/**
*签名的加密算法
*/
Algorithmalgorithm=Algorithm. HMAC 256 (salt);
//签发时间
longstart=System.currentTimeMillis ()- 60000 ;
//过期时间，在签发时间的基础上加上一个有效时长
Dateend=newDate (start+SessionConstants. SESSION_TIME_OUT* 1000 );
/**
*JWT 建造者
*/
JWTCreator. Builderbuilder=JWT.create ();
/**
*增加私有声明
*/
builder.withClaim ("uid"," 123 ...");
builder.withClaim ("user_name","admin");
builder.withClaim ("nick_name","管理员");
/**
*获取编码后的 JWT 令牌
*/
Stringtoken=builder


```
.withSubject (subject)
.withIssuedAt (newDate (start))
.withExpiresAt (end)
.sign (algorithm);
log.info ("token="+token);
//以“.”分割，这里需要进行转义
String[]parts=token.split ("\\.");
StringpayloadJson;
/**
* 解码 payload
*/
payloadJson=StringUtils. newStringUtf 8 (Base 64 .decodeBase 64 (parts[ 1 ]));
log.info ("parts[ 1 ]="+payloadJson);
//输出 demo 为：parts[ 1 ]=
//{"uid": " 123 ...","sub": "sessionid","user_name": "admin","nick_name": "管理
员","exp": 1579317358 ,"iat": 1578453358 }
}catch (Exceptione)
{
e.printStackTrace ();
}
}
}
由于 JWT 的负载声明（JSON 属性）是可以解码的，属于明文信息，因此不建议添加敏感信息。
```
#### 7. 5. 2 JWT+SpringSecurity 认证处理流程

在实际开发中如何使用 JWT 进行用户认证呢？疯狂创客圈的 crazy-SpringCloud 开发脚手架将
JWT 令牌和 SpringSecurity 进行结合，设计了一个公有的、便于复用的用户认证模块 base-auth。一般
来说，在 Zuul 网关或者微服务提供者进行用户认证时导入这个公有的 base-auth 模块即可。
这里还是按照请求认证的处理流程的 5 个步骤介绍一下 base-auth 模块中 JWT 令牌的认证处理
流程。

```
第一步：定制一个凭证/令牌类，封装用户的用户信息和 JWT 认证信息。
packagecom. crazymaker. SpringCloud. base. security. token;
//省略 import
publicclassJwtAuthenticationTokenextendsAbstractAuthenticationToken
{
privatestaticfinallongserialVersionUID= 3981518947978158945 L;
//封装用户信息：用户 id、密码
privateUserDetailsuserDetails;
//封装的 JWT 认证信息
privateDecodedJWTdecodedJWT;
...
}
第二步：定制一个认证提供者类，和凭证/令牌类进行配套并完成对自制凭证/令牌实例的验证。
packagecom. crazymaker. SpringCloud. base. security. provider;
//省略 import
publicclassJwtAuthenticationProviderimplementsAuthenticationProvider
```

{
//用于通过 sessionid 查找用户信息
privateRedisOperationsSessionRepositorysessionRepository;
publicJwtAuthenticationProvider (RedisOperationsSessionRepository
sessionRepository)
{
this. sessionRepository=sessionRepository;
}
@Override
publicAuthenticationauthenticate (Authenticationauthentication) throws
AuthenticationException
{
//判断 JWT 令牌是否过期
JwtAuthenticationTokenjwtToken=(JwtAuthenticationToken) authentication;
DecodedJWTjwt=jwtToken.getDecodedJWT ();
if (jwt.getExpiresAt (). before (Calendar.getInstance (). getTime ()))
{
thrownewNonceExpiredException ("认证过期");
}
//获取 sessionid
Stringsid=jwt.getSubject ();
//获取令牌字符串，此变量将用于验证是否重复登录
StringnewToken=jwt.getToken ();
//获取会话
Sessionsession=null;
try
{
session=sessionRepository.findById (sid);
}catch (Exceptione)
{
e.printStackTrace ();
}
if (null==session)
{
thrownewNonceExpiredException ("还没有登录, 请登录系统！");
}
Stringjson=session.getAttribute (G_USER);
if (StringUtils.isBlank (json))
{
thrownewNonceExpiredException ("认证有误, 请重新登录");
}
//获取会话中的用户信息
UserDTOuserDTO=JsonUtil.jsonToPojo (json, UserDTO. class);
if (null==userDTO)
{
thrownewNonceExpiredException ("认证有误, 请重新登录");
}
//判断是否在其他地方已经登录
if (null==newToken||! newToken.equals (userDTO.getToken ()))
{
thrownewNonceExpiredException ("您已经在其他的地方登录!");
}
StringuserID=null;
if (null==userDTO.getUserId ())
{
userID=String.valueOf (userDTO.getId ());
}else


{
userID=String.valueOf (userDTO.getUserId ());
}
UserDetailsuserDetails=User.builder ()
.username (userID)
.password (userDTO.getPassword ())
.authorities (SessionConstants. USER_INFO)
.build ();
try
{
//用户密码的密文作为 JWT 的加密盐
StringencryptSalt=userDTO.getPassword ();
Algorithmalgorithm=Algorithm. HMAC 256 (encryptSalt);
//创建验证器
JWTVerifierverifier=JWT.require (algorithm)
.withSubject (sid)
.build ();
//进行 JWTtoken 进行验证
verifier.verify (newToken);
}catch (Exceptione)
{
thrownewBadCredentialsException ("认证有误：令牌校验失败，请重新登录", e);
}
//返回认证通过的令牌，包含用户信息，如 userid 等
JwtAuthenticationTokenpassedToken=
newJwtAuthenticationToken (userDetails, jwt,
userDetails.getAuthorities ());
passedToken.setAuthenticated (true);
returnpassedToken;
}
//支持自定义的令牌 JwtAuthenticationToken
@Override
publicbooleansupports (Class<?>authentication)
{
returnauthentication.isAssignableFrom (JwtAuthenticationToken. class);
}
}
JwtAuthenticationProvider 负责对传入的 JwtAuthenticationToken 凭证/令牌实例进行多方面的
验证：

1 ）验证解码后的 DecodedJWT 实例是否过期。
2 ）由于本演示中 JWT 的 subject 主题信息存放的是用户的 sessionid，因此还要判断会话（session）
是否存在。
3 ）使用会话中的用户密码作为盐，对 JWT 令牌进行安全性校验。
如果以上验证都顺利通过，则构建一个新的 JwtAuthenticationToken 令牌，将重要的用户信息
（用户 id）放入令牌并予以返回，供后续操作使用。

第三步：定制一个过滤器类，从请求中获取信息组装成 JwtAuthenticationToken 凭证/令牌并交给认
证管理者。在 crazy-SpringCloud 脚手架中，前台有用户端和管理端的两套界面，所以将认证头部信息
区分成管理端和用户端两类：管理端的头部字段为 Authorization，用户端的认证信息头部字段为 token。


过滤器从请求中获取认证的头部字段，解析之后组装成 JwtAuthenticationToken 令牌实例，提
交给 AuthenticationManager 认证管理者进行验证。
packagecom. crazymaker. SpringCloud. base. security. filter;
//省略 import
publicclassJwtAuthenticationFilterextendsOncePerRequestFilter
{
...
@Override
protectedvoiddoFilterInternal (HttpServletRequestrequest, HttpServletResponse
response, FilterChainfilterChain) throwsServletException, IOException
{
...
AuthenticationpassedToken=null;
AuthenticationExceptionfailed=null;
//从 HTTP 请求获取 JWT 令牌的头部字段
Stringtoken=null;
//用户端存放的 JWT 的 HTTP 头部字段为 token
StringsessionIDStore=SessionHolder.getSessionIDStore ();
if (sessionIDStore.equals (SessionConstants. SESSION_STORE))
{
token=request.getHeader (SessionConstants. AUTHORIZATION_HEAD);
}
//管理端存放的 JWT 的 HTTP 头部字段为 Authorization
elseif (sessionIDStore.equals (SessionConstants. ADMIN_SESSION_STORE))
{
token=request.getHeader (SessionConstants. ADMIN_AUTHORIZATION_HEAD);
}
//没有拿到头部，抛出异常
else
{
failed=newInsufficientAuthenticationException ("请求头认证消息为空");
unsuccessfulAuthentication (request, response, failed);
return;
}
token=StringUtils.removeStart (token,"Bearer");
try
{
if (StringUtils.isNotBlank (token))
{
//组装令牌
JwtAuthenticationTokenauthToken=newJwtAuthenticationToken
(JWT.decode (token));
//提交给 AuthenticationManager 认证管理者进行令牌验证，获取认证后的令牌
passedToken=this.getAuthenticationManager (). authenticate (authToken);
//获取认证后的用户信息，主要是用户 id
UserDetailsdetails=(UserDetails) passedToken.getDetails ();
//通过 details.getUsername () 获取用户 id，并作为请求属性进行缓存
request.setAttribute (SessionConstants. USER_IDENTIFIER,
details.getUsername ());
}else
{
failed=newInsufficientAuthenticationException ("请求头认证消息为空");
}
}catch (JWTDecodeExceptione)
{


...
}
...
filterChain.doFilter (request, response);
}
...
}
AuthenticationManager 认证管理者将调用注册在内部的 JwtAuthenticationProvider 认证提供者，
对 JwtAuthenticationToken 进行验证。
为了使得过滤器能够生效，必须将过滤器加入 HTTP 请求的过滤处理责任链，这一步可以通过
实现一个 AbstractHttpConfigurer 配置类来完成。

第四步：定制一个 HTTP 的安全认证配置类（AbstractHttpConfigurer 子类），将上一步的定制
的过滤器加入请求的过滤处理责任链。
packagecom. crazymaker. SpringCloud. base. security. configurer;
...
publicclassJwtAuthConfigurer<TextendsJwtAuthConfigurer<T,B>, Bextends
HttpSecurityBuilder<B>>extendsAbstractHttpConfigurer<T,B>
{
privateJwtAuthenticationFilterjwtAuthenticationFilter;
publicJwtAuthConfigurer ()
{
//创建认证过滤器
this. jwtAuthenticationFilter=newJwtAuthenticationFilter ();
}
//将过滤器加入 http 过滤处理责任链
@Override
publicvoidconfigure (Bhttp) throwsException
{
//获取 SpringSecurity 共享的 AuthenticationManager 认证提供者实例
//将其设置到 jwtAuthenticationFilter 认证过滤器
jwtAuthenticationFilter.setAuthenticationManager (http. getSharedObject
(AuthenticationManager. class));
jwtAuthenticationFilter.setAuthenticationFailureHandler (new
AuthFailureHandler ());
JwtAuthenticationFilterfilter=postProcess (jwtAuthenticationFilter);
//将过滤器加入 http 过滤处理责任链
http.addFilterBefore (filter, LogoutFilter. class);
}
...
}
第五步：定义一个 SpringSecurity 安全配置类（WebSecurityConfigurerAdapter 子类），对 HTTP
的安全认证机制进行配置。这是最后一步，有两项工作：一是在 HTTP 安全策略上应用
JwtAuthConfigurer 配置实例，二是构造 AuthenticationManagerBuilder 认证管理者实例。这一步可以
通过继承 WebSecurityConfigurerAdapter 适配器来完成。
packagecom. crazymaker. SpringCloud. cloud. center. zuul. config;
...
@ConditionalOnWebApplication
@EnableWebSecurity ()
publicclassZuulWebSecurityConfigextendsWebSecurityConfigurerAdapter
{


//注入会话存储实例，用于查找会话（根据 sessionid）
@Resource
RedisOperationsSessionRepositorysessionRepository;
//配置 HTTP 请求的安全策略，应用上 DemoAuthConfigurer 配置类实例
@Override
protectedvoidconfigure (HttpSecurityhttp) throwsException
{
http.csrf (). disable ()
...
.authorizeRequests ()
.and ()
.authorizeRequests (). anyRequest (). authenticated ()
.and ()
.formLogin (). disable ()
.sessionManagement (). disable ()
.cors ()
.and ()
//在 HTTP 安全策略上应用 JwtAuthConfigurer 配置类实例
.apply (newJwtAuthConfigurer<>())
.tokenValidSuccessHandler (jwtRefreshSuccessHandler ()). permissiveReques
tUrls ("/logout")
.and ()
.logout (). disable ()
.sessionManagement (). disable ();
}
//配置认证 Builder，由其负责建造 AuthenticationManager 实例
//Builder 所建造的 AuthenticationManager 实例将作为 HTTP 请求的共享对象
//可以通过 http.getSharedObject (AuthenticationManager. class) 来获取
@Override
protectedvoidconfigure (AuthenticationManagerBuilderauth) throwsException
{
//在 Builder 实例中加入自定义的认证提供者实例
auth.authenticationProvider (jwtAuthenticationProvider ());
}
//创建一个 JwtAuthenticationProvider 提供者实例
@DependsOn ({"sessionRepository"})
@Bean ("jwtAuthenticationProvider")
protectedAuthenticationProviderjwtAuthenticationProvider ()
{
returnnewJwtAuthenticationProvider (sessionRepository);
}
...
}
至此，一个基于 JWT+SpringSecurity 的用户认证处理流程就已经定义完了。但是，此流程仅仅
涉及 JWT 令牌的认证，没有涉及 JWT 令牌的生成。一般来说，JWT 令牌的生成需要由系统的 UAA
（用户账号与认证）服务（或者模块）负责完成。

#### 7. 5. 3 Zuul 网关与 UAA 微服务的配合

crazy-SpringCloud 脚手架是通过 Zuul 网关和 UAA（用户账号与认证）微服务相互结合来完成
整个用户的登录与认证闭环流程的。二者的关系大致为：

1 ）登录时，UAA 微服务负责用户名称和密码的验证，并且将用户信息（包括令牌加密盐）放
在分布式会话中，然后返回 JWT 令牌（含 sessionid）给前台。


2 ）认证时，前台请求带上 JWT 令牌，Zuul 网关能根据令牌中的 sessionid 取出分布式会话中的
加密盐，对 JWT 令牌进行验证。在 crazy-SpringCloud 脚手架的会话架构中，Zuul 网关必须能和 UAA
微服务进行会话的共享，如图 7 - 7 所示。

图 7 - 7 Zuul 网关和 UAA 微服务进行会话的共享
在 crazy-SpringCloud 的 UAA 微服务提供者 crazymaker-uaa 实现模块中，controller（控制层）的
REST 登录接口的定义如下：
@Api (value="用户端登录与退出", tags={"用户信息、基础学习 DEMO"})
@RestController
@RequestMapping ("/api/session")
publicclassSessionController
{
//用户端会话服务
@Resource
privateFrontUserEndSessionServiceImpluserService;
//用户端的登录 REST 接口
@PostMapping ("/login/v 1 ")
@ApiOperation (value="用户端登录")
publicRestOut<LoginOutDTO>login (@RequestBodyLoginInfoDTOloginInfoDTO,
HttpServletRequestrequest,
HttpServletResponseresponse)
{
//调用服务层登录方法获取令牌
LoginOutDTOdto=userService.login (loginInfoDTO);
response.setHeader ("Content-Type","text/html; charset=utf- 8 ");
response.setHeader (SessionConstants. AUTHORIZATION_HEAD,dto.getToken ());
returnRestOut.success (dto);
}
//...
}
用户登录时，在服务层，客户端会话服务 FrontUserEndSessionServiceImpl 负责从用户数据库中
获取用户，然后进行密码验证。


packagecom. crazymaker. SpringCloud. user. info. service. impl;
//省略 import

@Slf 4 j
@Service
publicclassFrontUserEndSessionServiceImpl
{
//DaoBean，用于查询数据库用户
@Resource
UserDaouserDao;
//加密器
@Resource
privatePasswordEncoderpasswordEncoder;
//缓存操作服务
@Resource
RedisRepositoryredisRepository;
//Redis 会话存储服务
@Resource
privateRedisOperationsSessionRepositorysessionRepository;
/**
*登录处理
*@paramdto 用户名、密码
*@return 登录成功的 dto
*/
publicLoginOutDTOlogin (LoginInfoDTOdto)
{
Stringusername=dto.getUsername ();
//从数据库获取用户
List<UserPO>list=userDao.findAllByUsername (username);
if (null==list||list.size ()<= 0 )
{
throwBusinessException.builder (). errMsg ("用户名或者密码错误");
}
UserPOuserPO=list.get ( 0 );
//进行密码的验证
//Stringencode=passwordEncoder.encode (dto.getPassword ());
Stringencoded=userPO.getPassword ();
Stringraw=dto.getPassword ();
booleanmatched=passwordEncoder.matches (raw, encoded);
if (! matched)
{
throwBusinessException.builder (). errMsg ("用户名或者密码错误");
}
//设置会话，方便 SpringSecurity 进行权限验证
returnsetSession (userPO);
}
/**
* 1 ：将 userid->sessionid 作为“键－值对”缓存起来，防止频繁创建会话
* 2 ：将用户信息保存到分布式会话
* 3 ：创建 JWT 令牌，提供给 SpringSecurity 进行权限验证
*@paramuserPO 用户信息
*@return 登录的输出信息
*/
privateLoginOutDTOsetSession (UserPOuserPO)
{
if (null==userPO)


{
throwBusinessException.builder (). errMsg ("用户不存在或者密码错误"). build ();
}
/**
* 根据用户 id 查询之前保持的 sessionid
* 防止频繁登录的时候会话被大量创建
*/
Stringuid=String.valueOf (userPO.getUserId ());
Stringsid=redisRepository.getSessionId (uid);
Sessionsession=null;
try
{
/**
*查找现有的会话
*/
session=sessionRepository.findById (sid);
}catch (Exceptione)
{
//e.printStackTrace ();
log.info ("查找现有的会话失败，将创建一个新的会话");
}
if (null==session)
{
session=sessionRepository.createSession ();
//新的 sessionid 和用户 ID 一起作为“键－值对”进行保存
//用户访问的时候可以根据用户 ID 查找 sessionid
sid=session.getId ();
redisRepository.setSessionId (uid, sid);
}
Stringsalt=userPO.getPassword ();
//构建 JWT 令牌
Stringtoken=AuthUtils.buildToken (sid, salt);
/**
*将用户信息缓存到分布式会话
*/
UserDTOcacheDto=newUserDTO ();
BeanUtils.copyProperties (userPO, cacheDto);
cacheDto.setToken (token);
session.setAttribute (G_USER,JsonUtil.pojoToJson (cacheDto));
LoginOutDTOoutDTO=newLoginOutDTO ();
BeanUtils.copyProperties (cacheDto, outDTO);
returnoutDTO;
}
}
如果用户验证通过，前端会话服务 FrontUserEndSessionServiceImpl 在 setSession 方法中创建
Redis 分布式会话（如果不存在旧会话），然后将用户信息（密码为令牌的 salt）缓存起来。如果用
户存在旧的会话，旧的会话的 id 将通过用户的 uid 查找到，然后通过 sessionRepository 找到旧的会话，
做到在频繁登录的场景下不会导致会话被大量创建。
最终，uaa-provider 微服务将返回 JWT 令牌（subject 设置为 sessionid）给前台。由于 Zuul 网关和
uaa-provider 微服务共享分布式会话，在进行请求认证时，Zuul 网关能通过 JWT 令牌中的 sessionid
取出分布式会话中的用户信息和加密盐，对 JWT 令牌进行验证。


#### 7. 5. 4 使用 Zuul 过滤器添加代理请求的用户标识

完成用户认证后，Zuul 网关的代理请求将转发给上游的微服务提供者实例。此时，代理请求
仍然需要带上用户的身份标识，而此时身份标识不一定是 sessionid，而是和上游的微服务提供者强
相关：

1 ）如果微服务提供者是将 JWT 令牌作为用户身份标识（和 Zuul 一样），则 Zuul 网关将 JWT 令
牌传给微服务提供者即可。
2 ）如果微服务提供者是将 sessionid 作为用户身份标识，则 Zuul 需要将 JWT 令牌的 subject 中的
sessionid 解析出来，然后传给 r 微服务提供者。
3 ）如果微服务提供者是将 userid 作为用户身份标识，则 Zuul 既不能将 JWT 令牌传给微服务提
供者，也不能将 sessionid 传给微服务提供者，而是要将会话中缓存的 userid 传递给微服务提供者。

前两种用户身份标识的传递方案，都要求 Provider 微服务和网关共享会话，而实际场景中，这
种可能性不是 100 %的。另外，负责安全认证的网关可能不是 Zuul，而是性能更高的 OpenResty（甚
至是 Kong），如果这样，共享会话技术难度就会更大。总之，为了使程序的可扩展性和可移植性
更好，建议使用第三种用户身份标识的代理传递方案。
crazy-SpringCloud 脚手架采用的是第三种用户标识传递方案。JWT 令牌被验证成功后，网关的
代理请求被加上"USER-ID"头，将用户 id 作为用户身份标识添加到请求头部，传递给上游微服务提
供者。这个功能使用一个 Zuul 过滤器实现，代码如下：
packagecom. crazymaker. SpringCloud. cloud. center. zuul. filter;
//省略 import
@Component
@Slf 4 j
publicclassModifyRequestHeaderFilterextendsZuulFilter
{
/**
*根据条件去判断是否需要路由，是否需要执行该过滤器
*/
@Override
publicbooleanshouldFilter ()
{
RequestContextctx=RequestContext.getCurrentContext ();
HttpServletRequestrequest=ctx.getRequest ();
/**
*存在用户端认证令牌
*/
Stringtoken=request.getHeader (SessionConstants. AUTHORIZATION_HEAD);
if (! StringUtils.isEmpty (token))
{
returntrue;
}
/**
*存在管理端认证令牌
*/
token=request.getHeader (SessionConstants. ADMIN_AUTHORIZATION_HEAD);
if (! StringUtils.isEmpty (token))
{
returntrue;


```
}
returnfalse;
}
/**
*调用上游微服务之前，修改请求头，加上“USER-ID”头
*
*@return
*@throwsZuulException
*/
@Override
publicObjectrun () throwsZuulException
{
RequestContextctx=RequestContext.getCurrentContext ();
HttpServletRequestrequest=ctx.getRequest ();
//认证成功，请求的 "USER-ID"（USER_IDENTIFIER）属性被设置
Stringidentifier=(String)
request.getAttribute (SessionConstants. USER_IDENTIFIER);
//代理请求加上"USER-ID"头
if (StringUtils.isNotBlank (identifier))
{
ctx.addZuulRequestHeader (SessionConstants. USER_IDENTIFIER, identifier);
}
returnnull;
}
@Override
publicStringfilterType ()
{
returnFilterConstants. PRE_TYPE;
}
@Override
publicintfilterOrder ()
{
return 1 ;
}
}
```
### 7. 6 微服务提供者之间的会话共享关系

一套分布式微服务集群可能会运行几个或者几十个网关（Gateway），以及几十个甚至几百个
微服务提供者（Provider）。如果集群的节点规模较小，会话共享关系上，同一个用户在所有的网
关和微服务提供者之间共享同一个分布式会话是可行的，如图 7 - 8 所示。
如果集群的节点规模较大，分布式会话在 IO 上会存在性能瓶颈。除此之外，还存在一个架构
设计上的问题：在网关（如 Zuul）和微服务提供者之间传递 sessionid，并且双方都依赖了相同的会
话信息（如用户详细信息），将导致网关和微服务提供者、微服务提供者之间的耦合度很高，一定
程度上降低了微服务提供者的移植性和复用性，导致违背了系统架构的高内聚和低耦合原则。
架构的调整方案：缩小分布式会话的共享规模，网关（如 Zuul）和微服务提供者之间按需共
享分布式会话。在网关和微服务提供者之间不再直接传递 sessionid 作为用户身份标识，而是改成为
传递用户 id，如图 7 - 9 所示。


```
图 7 - 8 共享分布式会话
```
图 7 - 9 会话共享的架构与实现方案
会话共享的架构与实现方案肯定不止以上两种，而且以上第二种方案也不一定是最优的。疯
狂创客圈的 Crazy-Cloud 脚手架对上面的第二种分布式会话架构方案提供了实现代码，供大家参考
和学习。

#### 7. 6. 1 分布式会话的起源和实现方案

###### HTTP 协议本身是一种无状态的协议，这就意味着每一次请求都需要进行用户的身份信息查询，并

```
JWTs 令 es 牌 si 请 on 求 i 头 d：
```
```
JWTs 令 es 牌 si 请 on 求 i 头 d：
```

###### 且需要用户提供用户名和密码来进行用户认证。为什么呢？因为服务端并不知道是哪个用户发出的请

###### 求。所以，为了能识别出是哪个用户发出的请求，需要在服务端存储一份用户的身份信息，并且在登

###### 录成功后将用户身份信息的标识传递给客户端，告诉客户端保存好用户身份标识，在下次请求时带上

###### 该身份标识。然后，在服务端维护一个用户的会话，用户的身份信息保存在会话中。通常，对于传统

###### 的单体架构服务器，会话都是保存在内存中，而随着认证用户的增多，服务端的开销会明显增大。

###### 大家都知道，单体架构模式最大的问题是没有分布式架构，无法支持横向扩展。在分布式微

###### 服务架构下，需要在服务节点之间进行会话的共享。解决方案是使用一个统一的会话数据库来保存

###### 会话数据并实现共享。当然，这种会话数据库的选型一定不能是重量级的关系数据库，而应该是轻

量级的基于内存的高速数据库（如 Redis）。
在生产场景中，可以使用成熟稳定的 SpringSession 开源组件作为分布式会话的解决方案，不
过 SpringSession 开源组件比较重，在简单的会话共享场景中可以自己实现一套相对简单的 Redis
Session 组件。具体的实现方案可以参考疯狂创客圈的社群博客“RedisSession 自定义”一文。从学
习角度来说，自制一套 RedisSession 方案可以帮助大家深入了解 Web 请求的处理流程，使得大家更
容易学习 SpringSession 的核心原理。
SpringSession 作为独立的组件将会话从 Web 容器中剥离，存储在独立的数据库中，目前支持多
种形式的数据库：内存数据库（如 Redis）、关系数据库（如 MQSQL）、文档型数据库（如 MogonDB）
等。通过合理的配置，当请求进入 Web 容器时，Web 容器将会话的管理责任委托给 SpringSession 承担，
由 SpringSession 负责从数据库中存取会话，如果存在则返回，如果不存在则创建持久化至数据库中。

#### 7. 6. 2 SpringSession 的核心组件和存储细节

这里先介绍 SpringSession 中的三个核心组件：Session 接口、RedisSession 会话类、
SessionRepository 存储接口。

1 .Session 接口
SpringSession 单独抽象出 Session 接口，该接口是 SpringSession 对会话的抽象，主要是为了鉴
定用户，为 HTTP 请求和响应提供上下文容器。Session 接口的主要方法如下：

1 ）getId：获取 sessionid。
2 ）setAttribute：设置会话属性。
3 ）getAttribte：获取会话属性。
4 ）setLastAccessedTime：设置最近会话过程中最近的访问时间。
5 ）getLastAccessedTime：获取最近的访问时间。
6 ）setMaxInactiveIntervalInSeconds：设置会话的最大闲置时间。
7 ）getMaxInactiveIntervalInSeconds：获取最大闲置时间。
8 ）isExpired：判断会话是否过期。
SpringSession 和 Tomcat 的会话在实现模式上有很大不同，Tomcat 中直接对 Servlet 规范的
HttpSession 接口进行实现，而 SpringSession 中则抽象出单独的 Session 接口。问题是 SpringSession
如何处理自定义的 Session 接口与 Servlet 规范的 HttpSession 接口的关系呢？答案是 SpringSession 定
义了一个适配器类，可以将 Session 实例适配成 Servlet 规范中的 HttpSession 实例。


之所以 SpringSession 要单独抽象出 Session 接口，主要是为了应对多种传输与存储场景下的会
话管理，比如 HTTP 会话场景（HttpSession）、WebSocket 会话场景（WebSocketSession）、非 Web
会话场景（如 Netty 传输会话）、Redis 存储场景（RedisSession）等。

2 .RedisSession 会话类
RedisSession 用于使用 Redis 进行会话属性存储的场景。在 RedisSession 中有两个非常重要的成
员属性：

1 ）cached：实际上是一个 MapSession 实例用于本地缓存，每次进行 getAttribute 操作时优先从
cached 中获取，没有取到就再从 Redis 中获取，以提升性能。而 MapSession 是由 SpringSecurityCore
定义的一个通过内部的 HashMap 缓存“键－值对”（Key-ValuePair）的本地缓存类。
2 ）delta：用于跟踪变化数据，目的是保持变化的 Session 的属性。
RedisSession 提供了一个非常重要的 saveDelta 方法，用于持久化 Session 至 Redis 中：当调用
RedisSession 中的 saveDelta 方法后，变化的属性将被持久化到 Redis 中。

3 .SessionRepository 存储接口
SessionRepository 为管理 SpringSession 的存储接口，它的主要方法如下：
1 ）createSession：创建 Session 实例。
2 ）findById（Stringid）：根据 id 查找 Session 实例。
3 ）voiddelete（Stringid）：根据 id 删除 Session 实例。
4 ）save（Ssession）：存储 Session 实例。
根据 Session 的实现类不同，Session 存储实现类分为很多种。RedisSession 会话的存储类为
RedisOperationsSessionRepository，负责 Session 会话数据到 Redis 数据库的读写。
接下来，简单看一下 Redis 中的 Session 数据存储细节。RedisSession 在 Redis 缓存中的存储细节
大致有三种 Key（根据版本不同可能不完全一致），分别如下：
spring:session:SESSION_KEY:sessions: 0 cefe 354 - 3 c 24 - 40 d 8 - a 859 - fe 7 d 9 d 3 c 0 dba
spring:session:SESSION_KEY:expires: 33 fdd 1 b 6 - b 496 - 4 b 33 - 9 f 7 d-df 96679 d 32 fe
spring:session:SESSION_KEY:expirations: 1581695640000
 第一种键（Key）用来存储会话的详细信息，键的最后部分为 sessionid，这是一个 UUID。
这个键在 Redis 中是一个哈希类型，包括会话的过期时间间隔、最近的访问时间、attributes
等。键的过期时间为会话的最大过期时间+ 5 分钟，如果设置的会话过期时间为 30 分钟，则
这个键的过期时间为 35 分钟。
 第二种键用来表示会话在 Redis 中已经过期，这个“键－值对”不存储任何有用数据，只是
为了表示会话过期而设置。
 第三种键存储过去的一段时间内过期的 sessionid 集合。这个键的最后部分是一个时间戳
（timestamp），代表一个计时的起始时间。键对应的值（Value）所使用的 Redis 数据结构是集
合（set），集合中的元素是时间戳滚动至下一分钟计算得出的过期 SessionKey（第二种键）。

#### 7. 6. 3 SpringSession 的使用和定制

```
结合 Redis 使用 SpringSession 需要导入以下两个 Maven 依赖包：
```

<dependency>
<groupId>org. springframework. session</groupId>
<artifactId>spring-session-data-redis</artifactId>
</dependency>
<dependency>
<groupId>org. springframework. session</groupId>
<artifactId>spring-session-core</artifactId>
</dependency>
按照 SpringSession 官方文档的说明，在添加所需的依赖项后，可以通过以下配置启用基于 Redis
的分布式会话：
@EnableRedisHttpSession
publicclassConfig{
//创建一个连接到默认 Redis（localhost： 6379 ）的 RedisConnectionFactory
@Bean
publicLettuceConnectionFactoryconnectionFactory (){
returnnewLettuceConnectionFactory ();
}
}
@EnableRedisHttpSession 注解创建一个名为 springSessionRepositoryFilter 的过滤器，负责将原
始的 HttpSession 替换为 RedisSession。为了使用 Redis 数据库，这里还创建一个连接 SpringSession 到
Redis 服务器的 RedisConnectionFactory 实例，该连接工厂实例所连接的为默认 Redis 数据库，主机和
端口分别为 localhost 和 6379 。有关 SpringSession 的具体配置可参阅官方文档。
在 crazy-SpringCloud 脚手架的共享会话架构中，网关（Gateway）和微服务提供者（Provider）、
微服务提供者之间所传递的不是 sessionid 而是用户 id，所以目标微服务提供者收到请求之后，需要
通过用户 id 找到 sessionid，然后找到 RedisSession，最后从 Session 中加载缓存数据。整个流程需要
定制三个过滤器，如图 7 - 10 所示。

```
图 7 - 10 crazy-SpringCloud 脚手架共享会话架构中的过滤器
 第一个过滤器叫作 SessionIdFilter，其作用是根据请求头中的用户身份标识（UserID）定位
到分布式会话的 sessionid。
 第二个过滤器叫作 CustomedSessionRepositoryFilter，这个类的源码来自 SpringSession，其
主要的逻辑是将请求和响应进行包装，将 HttpSession 替换成 RedisSession。
```

 第三个过滤器叫作 SessionDataLoadFilter，判断 RedisSession 中的用户数据是否存在，如果
是首先次创建的会话，则从数据库中将常用的用户数据加载到会话，以便控制层的业务逻
辑代码能够高速访问。
在 crazy-SpringCloud 脚手架中，按照高度复用的原则，所有的和会话有关的代码都封装在
base-session 基础模块中。如果某个微服务提供者模块需要使用分布式会话，只需要在 Maven 中引入
base-session 模块依赖即可。

#### 7. 6. 4 通过用户身份标识查找 sessionid

通过用户身份标识（UserID）查找 sessionid 的工作是由 SessionIdFilter 过滤器完成的。在前面
介绍的 UAA 提供者服务（crazymaker-uaa）中，用户的 ID 和 sessionid 之间的绑定关系位于缓存 Redis
中。base-session 也借鉴了同样的思路。当带着用户 id 的请求进来时，SessionIdFilter 会根据用户 id 去
Redis 查找绑定的 sessionid，如果查找成功，则过滤器的任务完成；如果查找不成功，则后面的两
个过滤器会创建新的 RedisSession，并在 Redis 中缓存用户 id 和 sessionid 之间的绑定关系。
SessionIdFilter 的代码如下：
packagecom. crazymaker. SpringCloud. base. filter;
//省略 import
@Slf 4 j
publicclassSessionIdFilterextendsOncePerRequestFilter
{
publicSessionIdFilter (RedisRepositoryredisRepository,
RedisOperationsSessionRepositorysessionRepository)
{
this. redisRepository=redisRepository;
this. sessionRepository=sessionRepository;
}
/**
* RedisSessionDAO
*/
privateRedisOperationsSessionRepositorysessionRepository;
/**
*RedisDAO
*/
RedisRepositoryredisRepository;
/**
*返回 true 代表不执行过滤器，false 代表执行
*/
@Override
protectedbooleanshouldNotFilter (HttpServletRequestrequest)
{
StringuserIdentifier=request.getHeader (SessionConstants. USER_IDENTIFIER);
if (StringUtils.isNotEmpty (userIdentifier))
{
returnfalse;
}
returntrue;
}


/**
*将 sessionuserIdentifier（用户 ID）转成 sessionid
*
*@paramrequest 请求
*@paramresponse 响应
*@paramchain 过滤器链
*/
@Override
protectedvoiddoFilterInternal (HttpServletRequestrequest,
HttpServletResponseresponse, FilterChainchain) throwsIOException,
ServletException
{
/**
*从请求头中获取 sessionuserIdentifier（用户 ID）
*/
StringuserIdentifier=request.getHeader (SessionConstants. USER_IDENTIFIER);
SessionHolder.setUserIdentifer (userIdentifier);
/**
*在 Redis 中根据用户 ID 获取缓存的 sessionid
*/
Stringsid=redisRepository.getSessionId (userIdentifier);
if (StringUtils.isNotEmpty (sid))
{
/**
*判断分布式会话是否存在
*/
Sessionsession=sessionRepository.findById (sid);
if (null!=session)
{
//保存 sessionid 在线程中的局部变量，供后面的过滤器使用
SessionHolder.setSid (sid);
}
}
chain.doFilter (request, response);
}
}
SessionIdFilter 过滤器中含有两个 DAO 层的成员：一个 RedisRepository 类型的 DAO 成员负责根
据用户 id 去 Redis 查找绑定的 sessionid；而另一个 DAO 成员的类型为 SpringSession 专用的
RedisOperationsSessionRepository，负责根据 sessionid 去查找 RedisSession 实例，用于验证会话是否
真正存在。

#### 7. 6. 5 查找或创建分布式会话

SessionIdFilter 过滤处理完成后，请求将进入下一个过滤器 CustomedSessionRepositoryFilter。这
个类的源码来自 SpringSession，其主要的逻辑是将请求和响应进行包装，并将原始请求的
HttpSession 替换成 RedisSession。定制之后的过滤器稍微做了一点过滤条件的修改：如果请求头中
携带了用户身份标识，则开启分布式会话，否则不会进入分布式会话的处理流程。
CustomedSessionRepositoryFilter 的部分代码如下：
packagecom. crazymaker. SpringCloud. base. filter;
//省略 import
publicclassCustomedSessionRepositoryFilter<SextendsSession>extends
OncePerRequestFilter
{


//执行过滤
@Override
protectedvoiddoFilterInternal (HttpServletRequestrequest,
HttpServletResponseresponse, FilterChainfilterChain)
throwsServletException, IOException
{
...
//包装上一个过滤器的 HttpServletRequest 请求至 SessionRepositoryRequestWrapper
SessionRepositoryRequestWrapperwrappedRequest=
newSessionRepositoryRequestWrapper (request, response,
this. servletContext);
//包装上一个过滤器的 HttpServletResponse 响应至 SessionRepositoryResponseWrapper
SessionRepositoryResponseWrapperwrappedResponse=
newSessionRepositoryResponseWrapper (wrappedRequest, response);
try
{
filterChain.doFilter (wrappedRequest, wrappedResponse);
}finally
{
//会话持久化到数据库
wrappedRequest.commitSession ();
}
}
/**
*返回 true 代表不执行过滤器，false 代表执行
*/
@Override
protectedbooleanshouldNotFilter (HttpServletRequestrequest)
{
//如果请求中携带了用户身份标识
if (null==SessionHolder.getUserIdentifer ())
{
returntrue;
}
returnfalse;
}
...
}
SessionRepositoryFilter 首先会根据一个 sessionIds 清单查找会话，查找失败才创建新的
RedisSession。它会调用 CustomedSessionIdResolver 实例的 resolveSessionIds 方法去获取 sessionIds 清单。
作为 sessionid 的解析器，CustomedSessionIdResolver 的部分代码如下：
packagecom. crazymaker. SpringCloud. base. core;
...
@Data
publicclassCustomedSessionIdResolverimplementsHttpSessionIdResolver
{
...
/**
*解析 sessionid，用于在 Redis 中查找会话
*@paramrequest 请求
*@returnsessionid 列表
*/
@Override
publicList<String>resolveSessionIds (HttpServletRequestrequest)
{
//获取第一个过滤器保存的 sessionid


Stringsid=SessionHolder.getSid ();
return (sid!=null)? Collections.singletonList (sid): Collections.emptyList ();
}
...
}
CustomedSessionRepositoryFilter 会对 sessionIds 清单进行判断，然后根据结果进行分布式会话的
查找或创建：

1 ）如果清单中的某个 sessionid 对应的会话存在于 Redis，则过滤器会将分布式 RedisSession 查
找出来，作为当前会话。
2 ）如果清单为空，或者所有 sessionid 对应的 RedisSession 都不存在于 Redis，则过滤器会创建
一个新的 RedisSession。

#### 7. 6. 6 加载高速访问数据到分布式会话

CustomedSessionRepositoryFilter 处理完成后，请求将进入下一个过滤器 SessionDataLoadFilter。
这个类的主要逻辑是加载高速访问数据到分布式会话，具体如下：

1 ）获取前面的 SessionIdFilter 过滤器加载的 sessionid，用于判断 sessionid 是否变化。如果变化
了，则表明旧的会话不存在或者旧的 sessionid 已经过期，需要更新 sessionid，并且在 Redis 中进行
缓存。
2 ）获取前面的 CustomedSessionRepositoryFilter 创建的会话，如果是新创建的会话，则加载必
要的需要高速访问的数据，以提高后续操作的性能。需要高速访问的数据比较常见的有用户的基础
信息、角色、权限等，还有可能是一些基础的业务信息。

```
CustomedSessionRepositoryFilter 的部分代码如下：
packagecom. crazymaker. SpringCloud. base. filter;
...
@Slf 4 j
publicclassSessionDataLoadFilterextendsOncePerRequestFilter
{
UserLoadServiceuserLoadService;
RedisRepositoryredisRepository;
publicSessionDataLoadFilter (UserLoadServiceuserLoadService, RedisRepository
redisRepository)
{
this. userLoadService=userLoadService;
this. redisRepository=redisRepository;
}
...
@Override
protectedvoiddoFilterInternal (HttpServletRequestrequest,
HttpServletResponseresponse,
FilterChainfilterChain)
throwsServletException, IOException
{
//获取前面的 SessionIdFilter 过滤器加载的 sessionid
Stringsid=SessionHolder.getSid ();
//获取前面的 CustomedSessionRepositoryFilter 创建的会话，加载必要的数据到会话
```

HttpSessionsession=request.getSession ();
/**
*之前的会话不存在
*/
if (StringUtils.isEmpty (sid)||! sid.equals (request.getSession (). getId ()))
{
//获取得当前的 sessionid
sid=session.getId ();
//userID 和 sessionid 作为“键—值对”保存到 Redis
redisRepository.setSessionId (SessionHolder.getUserIdentifier (), sid);
SessionHolder.setSid (sid);
}
/**
*获取会话中的用户信息
*为空表示用户第一次发起请求，加载用户信息到会话中
*/
if (null==session.getAttribute (G_USER))
{
Stringuid=SessionHolder.getUserIdentifier ();
UserDTOuserDTO=null;
if (SessionHolder.getSessionIDStore (). equals (SessionConstants. SESSION_STORE))
{
//用户端：装载用户端的用户信息
userDTO=userLoadService.loadFrontEndUser (Long.valueOf (uid));
}else
{
//管理控制台：装载管理控制台的用户信息
userDTO=userLoadService.loadBackEndUser (Long.valueOf (uid));
}
/**
*将用户信息缓存起来
*/
session.setAttribute (G_USER,JsonUtil.pojoToJson (userDTO));
}
/**
*将会话请求保存到 SessionHolder 的 ThreadLocal 本地变量中，方便统一存取
*/
SessionHolder.setSession (session);
SessionHolder.setRequest (request);
filterChain.doFilter (request, response);
}
/**
*返回 true 代表不执行过滤器，false 代表执行
*/
@Override
protectedbooleanshouldNotFilter (HttpServletRequestrequest)
{
if (null==SessionHolder.getUserIdentifier ())
{
returntrue;
}
returnfalse;
}
}


### 本章的知识扩展

##### 1 .阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna
（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2 .本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html

spring security（史上最全）
https://www.cnblogs.com/crazymakercircle/p/ 14488160 .html

spring security 原理+实战
https://www.cnblogs.com/crazymakercircle/p/ 12040402 .html

Zuul 详解，带视频
https://www.cnblogs.com/crazymakercircle/p/ 12046484 .html
Zuul 修改请求头、响应头（死磕）
https://www.cnblogs.com/crazymakercircle/p/ 12037587 .html

##### 3 .相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 4 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 8 章 Nginx/OpenResty 详解

Nginx（或 OpenResty）在生产场景中使用的广泛程度已经到了令人咂舌的地步。不管其实际
的市场占用率如何，以笔者这些年所经历的项目来看，其使用率在 100 %。
然而，笔者周围的大量的开发人员对 Nginx（或 OpenResty）的了解程度都停留在基本配置的
程度，对其核心原理和高性能配置了解不多。
本书不仅为大家解读 Nginx 的核心原理和高性能配置，还将介绍 Nginx+Lua 实战编程，帮助大
家掌握一个解决高并发问题的新利器。

## 8. 1 Nginx 简介

Nginx 是一个高性能的 HTTP 和反向代理 Web 服务器，是由伊戈尔·赛索耶夫为俄罗斯访问量
第二的 Rambler. ru 站点开发的 Web 服务器。Nginx 源码以类 BSD 许可证的形式发布，它的第一个公开
版本 0. 1. 0 发布于 2004 年 10 月 4 日， 2011 年 6 月 1 日发布了 1. 0. 4 版本。Nginx 因高稳定性、丰富的功能集、
内存消耗少、并发能力强而闻名全球，目前得到非常广泛的使用，比如百度、京东、新浪、网易、
腾讯、淘宝等都是它的用户。
Nginx 有以下 3 个主要社区分支：
（ 1 ）Nginx 官方版本
更新迭代比较快，并且提供免费版本和商业版本。
（ 2 ）Tengine
Tengine 是由淘宝网发起的 Web 服务器项目。它在 Nginx 的基础上针对大访问量网站的需求添加
了很多高级功能和特性。Tengine 的性能和稳定性已经在大型的网站如淘宝网、天猫商城等得到了
很好的检验。它的最终目标是打造一个高效、稳定、安全和易用的 Web 平台。


（ 3 ）OpenResty
2011 年，中国人章亦春老师把 LuaJITVM 嵌入 Nginx 中，实现了 OpenResty 这个高性能服务端的
解决方案。OpenResty 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、
第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、
Web 服务和动态网关。

OpenResty 的目标是让 Web 服务直接运行在 Nginx 服务内部，充分利用 Nginx 的非阻塞 I/O 模型，
不仅对 HTTP 客户端请求，甚至于对远程后端（诸如 MySQL、PostgreSQL、Memcached 以及 Redis
等）都进行一致的高性能响应。
OpenResty 通过汇聚各种设计精良的 Nginx 模块（主要由 OpenResty 团队自主开发）将 Nginx 有
效地变成一个强大的通用 Web 应用平台，使得 Web 开发人员和系统工程师可以使用 Lua 脚本语言调
动 Nginx 支持的各种 C 以及 Lua 模块，快速构造出足以胜任 10 KB 乃至 1000 KB 以上单机并发连接的高
性能 Web 应用系统。
通过 OpenResty 官网的链接地址可以查看 OpenResty 支持的组件。

#### 8. 1. 1 正向代理与反向代理

###### 这里先简明扼要地介绍什么是正向代理和反向代理。正向代理和反向代理的用途都是代理服

###### 务进行客户端请求的转发，但是区别还是很大的。

###### 正向代理最大的特点是客户端非常明确要访问的服务器地址，如图 8 - 1 所示。

图 8 - 1 正向代理的特点
在正向代理服务器中，客户端需要配置目标服务器信息，比如 IP 和端口。一般来说，正向代
理服务器是一台和客户端网络连通的局域网内部的机器或者是可以打通两个隔离网络的双网卡机
器。通过正向代理的方式，客户端的 HTTP 请求可以转发到之前与客户端网络不通的其他不同的目
标服务器。
反向代理与正向代理相反，客户端不知道目标服务器的信息，代理服务器就像是原始的目标
服务器，客户端不需要进行任何特别的设置。反向代理最大的特点是客户端不知道目标服务器地址，
如图 8 - 2 所示。
客户端向反向代理服务器直接发送请求，接着反向代理服务器将请求转发给目标服务器，并
将目标服务器的响应结果按原路返回给客户端。


图 8 - 2 反向代理的特点
正向代理和反向代理的使用场景说明如下：
1 ）正向代理的主要场景是客户端。由于网络不通等物理原因，需要通过正向代理服务器这种
中间转发环节顺利访问目标服务器。当然，也可以通过正向代理服务器对客户端的某些详细信息进
行一些伪装和改变。
2 ）反向代理的主要场景服务端。服务提供方可以通过反向代理服务器轻松实现目标服务器的
动态切换，实现多目标服务器的负载均衡等。

通俗来说，正向代理（如 squid、proxy）是对客户端的伪装，隐藏了客户端的 IP、头部或者其
他信息，服务器得到的是伪装过的客户端信息；反向代理（如 Nginx）是对目标服务器的伪装，隐
藏了目标服务器的 IP、头部或者其他信息，客户端得到的是伪装过的目标服务器信息。

#### 8. 1. 2 Nginx 的启动与停止

Nginx 及其扩展（如 Openresty）是目前主流的反向代理服务器。本书使用 Openresty 作为演示服
务器，它的下载、安装和使用的教程可以参考疯狂创客圈社群的博客文章。

1 ）文章一：Windows 平台 Openresty 安装和启动（图文死磕）。
2 ）文章二：Linux 平台 Openresty 安装（图文死磕）。
3 ）文章三：Openresty 服务器下的 Lua 开发调试（图文死磕）。
本书的案例主要在 Windows 系统上演示，所使用的是 32 位的 Openresty 1. 13. 6. 2 版本（用 64 位的
版本进行 Lua 调试时会发生断点不能命中的情况）。

#### 8. 1. 3 Nginx 的启动命令和参数详解

在 Windows 平台安装好 Openresty 并且设置好 path 环境变量之后，就可以启动 Openresty 了。
Openresty 的原始启动命令为 Nginx，其参数大致有-v、-t、-p、-c、-s 等，大致的使用说明如下：

```
1 ）-v：表示查看 Nginx 的版本。
C:\dev\refer\LuaDemoProject\src>nginx-v
nginxversion: openresty/ 1. 13. 6. 2
```

2 ）-c：指定一个新的 Nginx 配置文件来替换默认的 Nginx 配置文件。
//启动时，在 cmd 窗口切换到 src 目录，然后执行以下命令：
C:\dev\refer\LuaDemoProject\src>nginx-p ./ -c nginx-debug. conf
3 ）-t：表示测试 Nginx 的配置文件。如果不能确定 Nginx 配置文件的语法是否正确，就可以通
过 Nginx 命令的-t 参数来测试。此参数代表不运行配置文件，而仅测试配置文件。
C:\dev\refer\LuaDemoProject\src>nginx-t -cnginx-debug. conf
nginx: theconfigurationfile./nginx-debug. confsyntaxisok
nginx: configurationfile./nginx-debug. conftestissuccessful
4 ）-p：表示设置前缀路径。
C:\dev\refer\LuaDemoProject\src>nginx-p./-cnginx-debug. conf
上面的命令中，“-p./”表示将当前目录“C:\dev\refer\LuaDemoProject\src”作为前缀路径，
也就是说，nginx-debug. conf 配置文件中所用到的相对路径都要加上这个前缀。
5 ）-s：表示给 Nginx 进程发送信号，包含 stop（停止）和 reload（重写加载）。
//重启 Nginx 进程，发送 reload 信号
C:\dev\refer\LuaDemoProject\src> nginx-p./-cnginx-debug. conf-sreload
//停止 nginx 进程，发送 stop 信号
C:\dev\refer\LuaDemoProject\src> nginx-p./-cnginx-debug. conf-sstop

#### 8. 1. 4 Linux 下 Openresty 的启动、停止脚本

为什么要专门介绍 Linux 系统下 Openresty 的启动和停止脚本呢？
1 ）Nginx/Openresty 发布包中，并没有提供好用的启动、停止脚本。
2 ）掌握一些基础的脚本指令并能编写基础的运行脚本是 Java 工程师的必备基础能力。很多的
面试场景都会出现“你使用过哪些 Linux 操作指令”的面试题。

作为参考，这里提供一份笔者常用的 Linux 下的 Openresty/Nginx 启动脚本，它公布在疯狂创客
圈的网盘上，具体如下：
#!/bin/bash
#设置Openresty的安装目录
OPENRESTRY_PATH="/usr/local/openresty"
#设置Nginx项目的工作目录
PROJECT_PATH="/work/develop/LuaDemoProject/src/"
#设置项目的配置文件
#PROJECT_CONF ="nginx-location-demo. conf"
PROJECT_CONF="nginx. conf"
echo"OPENRESTRY_PATH:$OPENRESTRY_PATH"
echo"PROJECT_PATH:$PROJECT_PATH"
#查找Nginx所有的进程id
pid=$(ps-ef|grep-v'grep'|egrepnginx|awk'{printf$ 2 ""}')
#echo "$pid"
if["$pid"!=""]; then


```
#如果已经在执行 ，则提示
echo"openrestry/nginxisstartedalready, andpidis$pid, operatingfailed!"
else
#如果没有执行 ，则启动
$OPENRESTRY_PATH/nginx/sbin/nginx -p${PROJECT_PATH} \
```
- c${PROJECT_PATH}/conf/${PROJECT_CONF}
pid=$(ps-ef|grep-v'grep'|egrepnginx|awk'{printf$ 2 ""}')
echo"openrestry/nginxstartingsucceeded!"
echo"pidis$pid"
fi
使用以上脚本之前，需要在脚本中配置好 Openresty/Nginx 的安装目录、项目的工作目录、项
目的配置文件 3 个选项。配置完成后，在 Linux 的命令窗口执行 openresty-start. sh 启动脚本，可以启动
Openresty。
[ root@localhostlinux ] #/work/develop/LuaDemoProject/sh/linux/openresty-start .sh
OPENRESTRY_PATH:/usr/local/openresty
PROJECT_PATH:/work/develop/LuaDemoProject/src/
openrestry/nginxstartingsucceeded!
pidis 3140331409
下面简单介绍上面的 openresty-start. sh 脚本中主要用到的指令：
1 ）echo 显示命令：用于显示信息到终端屏幕。
2 ）ps 进程列表：用于显示本地机器上当前运行的进程列表。
3 ）grep 查找命令：用于查找文件里符合条件的字符串。
以上 3 个命令是经常用到的、非常基础的 Linux 命令。在疯狂创客圈社群网盘中除了提供上面
的 openresty-start. sh 脚本之外，还提供了另外 3 个有用的 Openresty 操作脚本，具体如下：

```
1 ）openresty-stop. sh，用于停止 Openresty/Nginx。
2 ）openresty-status. sh，用于输出 Openresty/Nginx 的运行状态和进程信息。
3 ）openresty-restart. sh，用于重启 Openresty/Nginx。
```
#### 8. 1. 5 Windows 下 Openresty 的启动、停止脚本

除了提供 Linux 下的 Shell 脚本外，这里还为大家提供了 Windows 脚本文件。Windows 下的脚本
通常叫作批处理脚本（batchfile），批处理脚本扩展名为“. bat”，包含一系列 DOS 命令。
作为参考，这里提供一份 Windows 下的 Openresty/Nginx 的启动、停止、重启、查看状态的脚本，
大家可以在疯狂创客圈社群网盘下载，其中启动脚本 openresty-start. bat 的具体内容如下：
@echooff
rem 启动标志，flag= 0 表示之前已经启动，flag= 1 表示现在立即启动
setflag= 0
rem 设置 Openresty/Nginx 的安装目录
setinstallPath=E:/tool/openresty- 1. 13. 6. 2 - win 32
rem 设置 Nginx 项目的工作目录
setprojectPath=C:/dev/refer/LuaDemoProject/src
rem 设置项目的配置文件
setPROJECT_CONF=nginx-location-demo. conf
remsetPROJECT_CONF=nginx. conf


echoinstallPath:%installPath%
echoprojectprefixpath:%projectPath%
echoconfigfile:%projectPath%/conf/%PROJECT_CONF%
echoopenrestystarting...
rem 查找 Openresty/Nginx 进程信息，然后设置 flag 标志位
tasklist|find/i"nginx. exe">nul
if%errorlevel%== 0 (
echo"Openresty/Nginxalreadyrunning!"
remexit/b
) elsesetflag= 1
rem 如果需要，则启动 Openresty/Nginx
cd/d%installPath%
if%flag%== 1 (
startnginx. exe-p"%projectPath%"-c"%projectPath%/conf/%PROJECT_CONF%"
pinglocalhost-n 2 >nul
)
rem 输出 Openresty/Nginx 的进程信息
tasklist/fi"imagenameeqnginx. exe"
tasklist|find/i"nginx. exe">nul
if%errorlevel%== 0 (
echo"Openresty/Nginx starting succeeded!"
)
使用之前，需要在启动脚本 openresty-start. bat 中配置好 Openresty/Nginx 的安装目录、项目的工
作目录、项目的配置文件，然后在 WindowsCMD 命令窗口中执行 openresty-start. bat 启动脚本，即可
启动 Openresty。
PSC:\dev\refer\LuaDemoProject\sh\windows>.\openresty-start. bat
installPath:E:/tool/openresty- 1. 13. 6. 2 - win 32
projectprefixpath:C:/dev/refer/LuaDemoProject/src
configfile:C:/dev/refer/LuaDemoProject/src/conf/nginx-location-demo. conf
openrestystarting...
"Openresty/Nginxalreadyrunning!"
映像名称 PID 会话名会话# 内存使用
========================================================================
nginx. exe 34264 Console 2 9 , 084 K
nginx. exe 25912 Console 2 8 , 992 K
"Openresty/Nginx starting succeeded!"
上面的. bat 批处理文件主要用到的指令如下：
1 ）rem 注释命令：一般用来给程序加上注释，该命令后的内容不被执行。
2 ）echo 显示命令：用于显示信息到终端屏幕。
3 ）cd 目录切换：用于切换当前的目录。
4 ）tasklist 进程列表：用于显示在本地或远程机器上当前运行的进程列表。
除了上面的 openresty-start. sh 脚本外，本书的配套源码中还提供了 3 个有用的 Openresty 操作批处
理脚本，具体如下：

```
1 ）openresty-stop. bat：用于停止 Openresty/Nginx。
2 ）openresty-status. bat：用于输出 Openresty/Nginx 的运行状态和进程信息。
```

3 ）openresty-restart. bat：用于重启 Openresty/Nginx。
从提高效率的维度来说，这些脚本还是非常有用的。大家可从疯狂创客圈社群网盘自行下载、
研究学习和定制使用。

### 8. 2 Nginx 的核心原理

本节为大家介绍 Nginx 的核心原理，包含 Reactor 反应器模型、Nginx 的模块化设计、Nginx 的请
求处理阶段。
虽然本节的知识有一定的理论深度，但是与另一个著名的 Java 底层通信框架 Netty 在原理上有
很多相似的地方。如果大家了解 Netty 原理和 Reactor 模式，阅读本节将会更加轻松和愉快。

#### 8. 2. 1 Reactor 反应器模型

Nginx 对高并发 IO 的处理使用了 Reactor 事件驱动模型。Reactor 反应器模型的基本组件包含了事
件收集器、事件发送器和事件处理器 3 个基本单元，其核心思想是将所有要处理的 I/O 事件注册到一
个中心的 I/O 多路复用器上，同时主线程/进程阻塞在多路复用器上；一旦有 I/O 事件到来或是准备就
绪（文件描述符或 socket 可读、写），多路复用器返回并将事先注册的相应 I/O 事件分发到对应的处
理器中。
Reactor 模式中，事件收集器、事件发送器、事件处理器这 3 个基本单元的职责分别如下：
1 ）事件收集器：负责收集 Worker 进程的各种 I/O 请求。
2 ）事件发送器：负责将 I/O 事件发送到事件处理器。
3 ）事件处理器：负责各种事件的响应工作。
Nginx 的 Reactor 反应器模型的设计大致如图 8 - 3 所示。

```
图 8 - 3 Nginx 的 Reactor 反应器模型的设计
```

###### 事件收集器将各个连接通道的 IO 事件放入一个待处理事件列，通过事件发送器发送给对应的

###### 事件处理器来处理。而事件收集器之所以能够同时管理上百万连接通道事件，是基于操作系统提供

的“多路 IO 复用”技术，常见的包括 select 和 epoll 两种模型。
正是由于 Nginx 使用了高性能的 Reactor 模式，因此它是目前并发能力最高的 Web 服务器之一，
成为迄今为止使用最为广泛的工业级 Web 服务器。当然，Nginx 也解决了著名的网络读写的 C 10 K 问
题。什么是 C 10 K 问题呢？网络服务在处理数以万计的客户端连接时，往往效率低下甚至完全瘫痪，
这被称为 C 10 K 问题。
Reactor 模式的知识对于 Java 工程师来说非常重要，如果对 Reactor 模式或者其实现了解不够，请
参阅本书的姊妹篇《Java 高并发核心编程卷 1 （加强版）：NIO、Netty、Redis、ZooKeeper》一书。

#### 8. 2. 2 Nginx 的两类进程

一般来说，Nginx 在启动后会以 daemon 方式在后台运行，其后台进程有两类：一类称为 Master 进程
（相当于管理进程），另一类称为 Worker 进程（工作进程）。Nginx 的进程结构图大致如图 8 - 4 所示。

图 8 - 4 Nginx 的进程结构图
Nginx 启动方式有两种：
1 ）单进程启动：此时系统中仅有一个进程，该进程既充当 Master（管理）进程的角色，也充
当 Worker（工作）进程的角色。
2 ）多进程启动：此时系统有且仅有一个 Master 进程，至少有一个 Worker 进程。
一般来说，单进程模式是用来调试的。在生产环境下一般会配置成多进程模式，并且 Worker
进程的数量和机器 CPU 核数配置不是一样多的。
了解 Worker 进程之前，首先了解一下 Master 进程的主要工作，主要有以下两点：
1 ）Master 进程主要负责调度 Worker 进程，比如加载配置、启动工作进程、接收来自外界的信
号、向各 Worker 进程发送信号、监控 Worker 进程的运行状态等。所以 Nginx 启动后，我们就能看到
至少有两个 Nginx 进程。
2 ）Master 负责创建监听套接字（Socket），交给 Worker 进程进行连接监听。
接下来介绍 Nginx 的 Worker 进程。


Worker 进程主要用来处理网络事件，当一个 Worker 进程在接收一条连接通道之后，就开始读
取请求、解析请求、处理请求，处理完成产生数据后，再返回给客户端，最后才断开连接通道。
各个 Worker 进程之间是对等且相互独立的，它们同等竞争来自客户端的请求，一个请求只可
能在一个 Worker 进程中处理。这都是典型的 Reactor 模型中 Worker 进程（或者线程）的职能。
如果启动了多个 Worker 进程，则每个 Worker 子进程独自尝试 accept（接受）已连接的套接字监听
通道，accept 操作默认会上锁，优先使用操作系统的共享内存原子锁，如果操作系统不支持则使用文
件上锁。
经过配置，Worker 进程的 accept 操作也可以不使用锁，在多个进程同时 accept 时，当一个连接
进来的时候多个工作进程同时被唤起，会导致惊群问题。而在上锁的场景下，只会有一个 Worker
阻塞在 accept 上，其他的进程则因不能获取锁而阻塞，所以上锁的场景不存在惊群问题。

#### 8. 2. 3 Nginx 的模块化设计

Nginx 服务器被分解为多个模块，模块之间严格遵循“高内聚，低耦合”的原则，每个模块都
聚焦于一个功能。高度模块化的设计是 Nginx 的架构基础。
什么是 Nginx 模块呢？在 Nginx 的实现中，一个模块包含一系列的命令（cmd）和与这些命令相
对应的处理函数（cmd→handler）。Nginx 的 Worker 进程在执行过程中会通过配置文件的配置指令
定位到对应的功能模块的某个命令（cmd），然后调用命令对应的处理函数来完成相应的处理。
Nginx 的 Worker 进程首先会调用 Nginx 的 Core 核心模块。让我们所知，在 Reactor 模型中会维护
一个运行循环（run-loop），主要包括事件收集、事件分发、事件处理，这个工作在 Nginx 中由 Core
核心模块负责。Core 模块负责执行网络请求处理的基础操作，比如网络读写、存储读写、内容传输、
外出过滤以及将请求发往上游服务器等。
Nginx 的 Core 模块是启动时一定会加载的，其他的模块只有在解析配置时遇到了这个模块的命令
才会加载对应的模块。Core 模块为其他模块构建了基本的运行时环境，并成为其他模块的协作基础。
除了 Core 模块外，Nginx 还有 Event、Conf、Http、Mail 等一系列的模块，并且还可以在编译时
加入第三方模块。
Nginx 的模块结构具体如图 8 - 5 所示。

```
图 8 - 5 Nginx 的模块结构图
```

Nginx 的主要模块的说明如下：
（ 1 ）Core 核心模块
Core 核心模块是 Nginx 服务器正常运行必不可少的模块，它提供错误日志记录、配置文件解析、
Reactor 事件驱动机制、进程管理等核心功能。

（ 2 ）标准 HTTP 模块
标准 HTTP 模块提供 HTTP 协议解析相关的功能，比如端口配置、网页编码设置、HTTP 响应头
设置等。

（ 3 ）可选 HTTP 模块
可选 HTTP 模块主要用于扩展标准的 HTTP 功能，让 Nginx 能处理一些特殊的服务，比如 Flash
多媒体传输、网络传输压缩、安全协议 SSL 支持等。

（ 4 ）邮件服务模块
邮件服务模块主要用于支持 Nginx 的邮件服务，包括对 POP 3 协议、IMAP 协议和 SMTP 协议的
支持。

（ 5 ）第三方模块
第三方模块是为了扩展 Nginx 服务器的功能而定制的开发者自定义功能，比如 JSON 支持、Lua
支持等。
Nginx 的非核心模块可以在编译时按需加入，Nginx 的安装编译过程可以参考疯狂创客圈社群
博文“Linux 平台 Openresty 安装（图文死磕）”，这里不再赘述。

总之，Nginx 通过模块化设计使得大家可以根据需要对功能模块进行适当的选择和修改，编译
成具有特定功能的服务器。

#### 8. 2. 4 Nginx 配置文件上下文结构

前面介绍到，Nginx 的功能模块包含一系列的命令（cmd），以及与命令对应的处理函数（cmd
→handler）。而 Nginx 根据配置文件中的配置指令就知道对应到哪个模块的哪个命令，然后调用命
令对应的处理函数来处理。
一个 Nginx 配置文件包含若干配置项，每个配置项由配置指令和指令参数两部分组成，可以简
单认为配置项是一个键—值对（Key-ValuePair）。图 8 - 6 中有三个简单的 Nginx 配置项。
Nginx 配置文件中的配置指令如果包含空格，则需要用单引号或双引号引起来。指令参数如果
是由简单字符串构成，则简单配置项需要以分号结束；指令参数如果是复杂的多行字符串，则配置
项需要用花括号（{}）括起来。
Nginx 配置项的具体功能与其所处的作用域 (上下文、配置块) 是强相关的。Nginx 指令的作用
域配置块大致有 5 种，它们之间的层次关系如图 8 - 7 所示。


图 8 - 6 三个简单的 Nginx 配置项图 8 - 7 5 种 Nginx 指令的作用和它们之间的层次关系
一个标准的 Nginx 配置文件大致的上下文结构如下：
... #main全局配置块 ，例如工作进程数
events{ #events事件处理模式配置块 ，例如 IO 读写模式、连接数等
...
}
http #HTTP协议配置块
{
... #HTTP协议的全局配置块
server #server虚拟服务器配置块一
{
... #server全局块
location[PATTERN] #location路由规则配置块一
{
...
}
location[PATTERN] #location路由规则配置块二
{
...
}
}
server #server虚拟服务器配置块二
{
...
}
... #其他HTTP协议的全局配置块
}
mail #mail服务配置块
{
... #email相关协议如SMTP/IMAP/POP 3 的处理配置
}
对以上 6 种作用域（上下文、配置块），大致介绍如下。
1 .main 全局配置块
配置影响 Nginx 全局的指令。一般有运行 Nginx 服务器的用户组、Nginx 进程 pid 存放路径、日志
存放路径、配置文件引入、允许生成的 Worker 进程数等。


2 .events 事件处理模式参数配置块
配置 Nginx 服务器的 IO 多路复用模型、客户端的最大连接数限制等。Nginx 支持多种 IO 多路复
用模型，可以使用 use 指令在配置文件中设置 IO 读写模型。

3 .HTTP 协议配置块
可以配置与 HTTP 协议处理相关的参数，比如 keepalive 长连接参数、gzip 压缩参数、日志输出
参数、mime-type 参数、连接超时参数等。

4 .server 虚拟服务器配置块
配置虚拟主机的相关参数，如主机名称、端口等。一个 HTTP 协议配置块中可以有多个 server
虚拟服务器配置块。

5 .location 路由规则块
配置客户端请求的路由匹配规则，以及请求过程中的处理流程。一个 server 虚拟服务器配置块
中一般都会有多个 location 路由规则块。

6 .mail 服务配置块
Nginx 为 email 相关协议（如 SMTP/IMAP/POP 3 ）提供反向代理时，mail 服务配置块负责配置一
些相关的配置项。

```
以上所介绍的 Nginx 的配置块主要针对的是 Nginx 基本应用程序配置文件。包括
基本配置文件在内，Nginx 的常用配置文件大致有下面这些：
1 ）nginx. conf：应用程序基本配置文件。
2 ）mime. types：与 MIME 类型关联的扩展配置文件。
3 ）fastcgi. conf：与 fastcgi 相关的配置文件。
4 ）proxy. conf：与 proxy 相关的配置文件。
5 ）sites. conf：单独配置 Nginx 提供的虚拟主机。
```
#### 8. 2. 5 Nginx 的请求处理流程

```
Nginx 中 HTTP 请求的处理流程可以分为 4 步：
读取解析请求行。
读取解析请求头。
多阶段处理，也就是执行 handler 处理器列表。
将结果返回给客户端。
Nginx 中 HTTP 请求的处理流程具体如图 8 - 8 所示。
```
```
图 8 - 8 Nginx 中 HTTP 请求的处理流程
```

多阶段处理是 Nginx 的 HTTP 处理流程中最为重要的一步。Nginx 把请求处理划分成了 11 个阶段，
在完成了第一步读取请求行和第二步读取请求头之后，Nginx 将整个请求封装到了一个请求结构体
ngx_http_request_t 实例中（相当于 Java 中的一个请求对象），然后进入第三步多阶段处理，也就是
执行 handler 处理器列表。列表中的每个 handler 处理器都会对请求对象进行处理，例如重写 URI、权
限控制、路径查找、生成内容以及记录日志等。
在《Java 高并发核心编程卷 1 （加强版）：NIO、Netty、Redis、ZooKeeper》一书中，笔者深
入剖析了 Netty 的业务处理器流水线。Netty 将所有的业务处理器装配成一条处理器的流水线
（pipeline）。Nginx 也将 HTTP 请求处理流程分成了 11 个阶段，每个阶段都涉及一些 handler 处理器。
HTTP 请求到来时，这些组装在一个列表的 handler 处理器会按组装的先后次序执行。这一点和 Netty
的流水线在原理上是类同的。
在 Nginx 进行多阶段处理时， 11 个阶段中所涉及的 handler 处理器的执行次序除了和配置文件中
的对应指令的配置顺序次序有关外，还和指令所处的阶段的先后次序相关。
Nginx 请求处理的 11 个阶段以及阶段与阶段之间的执行次序如图 8 - 9 所示。

图 8 - 9 Nginx 请求处理的 11 个阶段
对 HTTP 请求进行多阶段处理是 Nginx 模块化非常关键和非常重要的功能，第三方模块的处理
器都在多阶段处理注册，例如：

```
1 ）用 MemCache 进行页面缓存的第三方模块。
2 ）用 Redis 集群进行页面缓存的第三方模块。
3 ）执行 Lua 脚本的第三方模块。
```
#### 8. 2. 6 HTTP 请求处理的 11 个阶段

Nginx 请求处理的 11 个阶段的具体介绍如下：
1 .post-read 阶段
在完成了第一步读取请求行和第二步读取请求头之后就进入多处理阶段，首当其冲的就是
post-read 阶段。注册在 post-read 阶段的处理器不多，标准模块的 ngx_realip 处理器就注册在这个阶段。
ngx_realip 处理器模块的用途是改写请求的来源地址。


###### 为何要改写请求的来源地址呢？

当 Nginx 处理的请求经过了某个正向代理服务器（Nginx、CDN）的转发后，请求中的 IP 地址
（$remote_addr）可能就不是客户端的真实 IP 了，变成了下游代理服务器的 IP。如何获取用户请求
的真实 IP 地址呢？解决办法之一：在下游的正向代理服务器中把请求的原始来源地址编码成某个特
殊的 HTTP 请求头，在 Nginx 中把这个请求头中编码的地址恢复出来，然后传给 Nginx 自己后头的上
游服务器。ngx_realip 模块正是用来处理这个需求的。
下面有一个简单的例子，假定前头的正向代理服务器能将客户端 IP 编码成某个特殊的 HTTP 请
求头（如 X-My-IP），Nginx 就可以通过 ngx_realip 模块的 real_ip_header 指令取出 X-My-IP 请求头的 IP，
作为请求中的 IP 地址（$remote_addr）。
server{
listen 8080 ;
set_real_ip_from 192. 168. 0. 100 ;
real_ip_header X-My-IP;
location/test{
echo"from:$remote_addr";
}
}
这里的配置是让 Nginx 把来自正向代理服务器 192. 168. 0. 100 的所有请求的 IP 来源地址都改写为
请求头 X-My-IP 所指定的值，放在$remote_addr 内置标准变量中。

2 .server-rewrite 阶段
server-rewrite 阶段，简单的翻译就是 server 块中的请求地址重写阶段。在进行请求 URI 与 location
路由规则匹配之前，可以修改请求的 URI 地址。
大部分直接配置在 server 配置块中的配置项都运行在 server-rewrite 阶段。
server{
listen 8080 ;
set$ahello; #server-rewrite阶段运行
location/test{
set$b"$a, world";
echo$b;
}
set$bhello; #server-rewrite阶段运行
}
其中，两个变量赋值的配置项 set$ahello 和 set$ahello 直接写在 server 配置块中，因此它们就运
行在 server-rewrite 阶段。

3 .find-config
紧接在 server-rewrite 阶段后面的是 find-config 阶段，也叫配置查找阶段，主要功能是根据请求
URL 地址去匹配 location 路由表达式。
find-config 阶段由 NginxHTTPCore（ngx_http_core_module）模块全部负责，完成当前请求 URL
与 location 配置块之间的配对工作。这个阶段不支持 Nginx 模块注册处理程序。
在 find-config 阶段之前，客户端请求并没有与任何 location 配置块相关联。因此，对于运行在此
之前的 post-read 和 server-rewrite 阶段来说，只有 server 配置块以及更外层作用域中的配置项才会起作
用，location 配置块中的配置项不起作用。


4 .rewrite
由于 Nginx 已经在 find-config 阶段完成了当前请求与 location 的匹配，所以从 rewrite 阶段开始，
location 配置块中的指令就起作用了。
rewrite 阶段也叫请求地址重写阶段，可以注册在 rewrite 阶段的指令首先是 ngx_rewrite 模块的指
令，比如 break、if、return、rewrite、set 等。其次，第三方 ngx_lua 模块中的 set_by_lua 指令和 rewrite_by_lua
指令也可以在此阶段注册。

5 .post-rewrite
请求地址 URI 重写提交（post）阶段，防止递归修改 URI 造成死循环（一个请求执行 10 次就会
被 Nginx 认定为死循环），该阶段只能由 NginxHTTPCore（ngx_http_core_module）模块实现。

6 .preaccess
访问权限检查准备阶段，控制访问频率的 ngx_limit_req 模块和限制并发度的 ngx_limit_zone 模
块的相关指令就注册在此阶段。

7 .access
访问权限检查阶段，配置指令多用于执行访问控制类型的任务，比如检查用户的访问权限、检
查用户的来源 IP 地址是否合法等。在此阶段能注册的指令有：HTTP 标准模块 ngx_http_access_module
的指令、第三方 ngx_auth_request 模块的指令、第三方 ngx_lua 模块的 access_by_lua 指令等。
比如，deny 和 allow 指令属于 ngx_http_access_module 模块，它的使用示例如下：
server{
#...
#拒绝全部
location=/denyall{
denyall;
}
#允许来源IP属于 192. 168. 0. 0 / 24 网段或 127. 0. 0. 1 的请求
#其他来源IP全部拒绝
location=/allowsome{
allow 192. 168. 0. 0 / 24 ;
allow 127. 0. 0. 1 ;
deny all;
echo"youareok";
}
#...
}
如果同一个 location 块配置了多个 allow/deny 配置项，由于 access 阶段的配置项之间是按顺序匹
配的，则匹配成功一个便跳出。上面的例子中，如果客户端源 IP 是 127. 0. 0. 1 ，则能匹配到“allow
127. 0. 0. 1 ;”配置项后便跳出不再继续匹配后面的，也就是说该请求不会被拒绝。
特别提醒：echo 指令用于返回内容，在 location 上下文中该指令注册在 content 生产阶段。由于
echo 指令不是注册在 access 阶段，因此在 access 阶段该指令的配置项不执行。

```
8 .post-access
访问权限检查提交阶段。如果请求不被允许访问 Nginx 服务器，该阶段负责向用户返回错误响
```

应。在 access 阶段可能存在多个访问控制模块的指令注册，post-access 阶段的 satisfy 配置指令可以用
于控制它们彼此之间的协作方式。下面是一个简单的实例：
#satisfy指令进行协调
location=/satisfy-demo{
satisfyany;
access_by_lua"ngx.exit (ngx. OK)";
deny all;
echo"youareok";
}
在上面的例子中，deny 指令属于 HTTP 标准模块 ngx_http_access_module 访问控制模块，而
access_by_lua 指令属于第三方 ngx_lua 模块，两个模块都有自己的计算结果，需要经过最终的结果统一。
不同的访问控制模块的计算结果的统一工作，这里由 satisfy 指令负责，有两种统一的方式：
1 ）“逻辑或”操作：具体的配置项为“satisfyany;”，表示访问控制模块 A、B、C 或更多，
只要其中任意一个通过验证就算通过。
2 ）“逻辑与”操作：具体的配置项为“satisfyall;”，表示访问控制模块 A、B、C 或更多，全
部模块都通过验证才能最终通过。

9 .try-files
如果 HTTP 请求访问静态文件资源，try_files 配置项可以使这个请求顺序地访问多个静态文件资
源，直到某个静态文件资源符合选取条件。这个阶段只有一个标准配置指令 try_files，并不支持 Nginx
模块注册处理程序。
try_files 指令接收两个以上任意数量的参数，每个参数都指定了一个 URI，则 Nginx 会在 try-files
阶段依次把前 N－ 1 个参数映射为文件系统上的对象（文件或者目录），然后检查这些对象是否存
在。一旦 Nginx 发现某个文件系统对象存在，则查找成功，就会在 try-files 阶段把当前请求的 URI 改
写为该对象所对应的参数 URI（但不会包含末尾的斜杠字符，也不会发生“内部跳转”）。如果
前 N－ 1 个参数所对应的文件系统对象都不存在，则 try-files 阶段就会立即发起“内部跳转”，跳转
到最后一个参数（第 N 个参数）所指定的 URI。
下面是一个简单的实例：
root/var/www/; # root 指令把“查找文件的根目录”配置为/var/www/
location=/try_files-demo{
try_files /foo /bar /last;
}
#对应到前面try_files的最后一个URI
location/last{
echo"uri:$uri";
}
}
这里 try_files 会在文件系统中查找前两个参数对应的文件/var/www/foo和/var/www/bar所对应
的文件是否存在。如果不存在，则 Nginx 会在 try-files 阶段发起到最后一个参数所指定的 URI（即/last）
的“内部跳转”，如图 8 - 10 所示。


图^8 -^10 内部跳转
10 .content
大部分 HTTP 模块会介入 content（内容产生）阶段，该阶段是所有请求处理阶段中的重要阶段。
Nginx 的 echo 指令、第三方 ngx_lua 模块的 content_by_lua 指令都注册在此阶段。
这里要注意的是，每一个 location 只能有一个“内容处理程序”，因此，当在 location 中同时使
用多个模块的 content 阶段指令时，只有其中一个模块能成功注册成为“内容处理器”。例如 echo
和 content_by_lua 同时注册，最终只会有一个生效，但具体哪一个生效，结果是不稳定的。

11 .log
日志模块处理阶段记录日志。
最后，总结如下：
1 ）Nginx 将一个 HTTP 请求分为 11 个处理阶段，这样做让每个 HTTP 模块可以只专注于完成一
个独立、简单的功能。而一个请求的完整处理过程由多个 HTTP 模块共同合作完成，可以极大地提
高多个模块合作的协同性、可测试性和可扩展性。
2 ）Nginx 请求处理的 11 个阶段中，有些阶段是必备的，有些阶段是可选的，各个阶段可以允
许多个模块的指令同时注册。但是，find-config、post_rewrite、post_access、try_files 这四个阶段是
不允许其他模块的处理指令注册的，它们仅注册了 HTTP 框架自身实现的那几个固定的方法。
3 ）同一个阶段内的指令，Nginx 会按照各个指令的上下文顺序执行对应的 handler 处理器方法。

### 8. 3 Nginx 的基础配置

本节介绍一下 Nginx 的基础配置，包括事件模型配置、虚拟主机配置、错误页面配置、长连接
配置、访问日志配置等。然后，本节还会介绍在配置过程中可能会用到的 Nginx 内置变量。

#### 8. 3. 1 events 事件驱动配置

```
一个典型的 events 事件模型配置块的示例如下：
events{
useepoll; #使用epoll类型IO多路复用模型
worker_connections 204800 ; #最大连接数限制为 20 万
accept_mutexon; #各个Worker通过锁来获取新连接
}
（ 1 ）worker_connections 指令
worker_connections 指令用于配置每个 Worker 进程能够打开的最大并发连接数量。指令参数为
```

###### 连接数的上限。

###### 顺便说一下，配置文件中的符号“#”是注释符号，后边的字符串起到注释说明的作用。

（ 2 ）use 指令
use 指令用于配置 IO 多路复用模型，有多种模型可配，常用的有 select 和 epoll 两种。
Linux 系统下 select 类型 IO 多路复用模型有两个较大的缺陷。缺陷之一：单服务进程并发数不够，
默认最大的客户端连接数为 1024 / 2048 。因为 Linux 系统一个进程所打开的 FD 文件描述符是有限制的，
由 FD_SETSIZE 设置，默认值是 1024 / 2048 ，因此 select 模型的最大并发数就被相应限制了。缺陷之
二：性能问题，每次 IO 事件查询都会线性扫描全部的 FD 集合，连接数越大，性能就会线性下降。
总之，select 类型 IO 多路复用模型，性能是不高的。
使用 Nginx 的目标之一是为了高性能和高并发。所以，在 Linux 系统下建议使用 epoll 类型的 IO
多路复用模型。epoll 模型在 Linux 2. 6 内核中实现，是 select 系统调用的增强版本。epoll 模型中有专
门的 IO 就绪队列，不再像 select 模型一样进行全体连接扫描，时间复杂度从 select 模型的 O (n) 下降到
了 O ( 1 )。在 IO 事件的查询效率上，无论上百万连接还是数十个连接，对于 epoll 模型而言差距是不大
的；而对 select 模型而言，效率的差距就非常巨大了。
select、epoll 都是常见的 IO 多路复用模型。本质上都是查询多个 FD 描述符，一旦某个描述符的
IO 事件就绪（一般是读就绪或者写就绪），就进行相应的读写操作，而且都是在读写事件就绪后
应用程序自己负责进行读写。所以，select、epoll 本质上都是同步 I/O，因为它们的读写过程是阻塞
的。虽然不是异步 I/O，但是通过合理的设计，epoll 类型的 IO 多路复用模型的性能还是非常高，足
以应对目前的高并发处理要求。
关于 IO 多路复用模型以及高性能 IO 处理的原理的深入介绍、详细的使用分析，可参见本书姊
妹篇《Java 高并发核心编程卷 1 （加强版）：NIO、Netty、Redis、ZooKeeper》一书。
如果没有配置 IO 多路复用模型，在 Windows 平台下，Nginx 默认的 IO 多路复用模型为 select。这
一点可以通过设置 errors_log 的日志级别为 debug，打开日志文件可以看出来，具体如下：
...[notice] 3928 # 18648 : usingthe"select"eventmethod
...[notice] 3928 # 18648 : openresty/ 1. 13. 6. 2
至于 Nginx 在 Linux 平台上的默认事件驱动模型，大家可按照统一的方法自行实验。
（ 3 ）accept_mutex 指令
accept_mutex 指令用于配置各个 Worker 进程是否通过互斥锁有序接收新的连接请求。on 参数表
示各个 Worker 通过互斥锁有序接收新请求；off 参数表示每个新请求到达时通知（唤醒）所有的
Worker 进程参与争抢，但只有一个进程可获得连接。
配置 off 参数会造成“惊群”问题影响性能。accept_mutex 指令的参数默认为 on。

#### 8. 3. 2 虚拟主机配置

```
配置虚拟主机可使用 server 指令。虚拟主机的基础配置包含套接字配置、虚拟主机名称配置等。
1 .虚拟主机的监听套接字配置
虚拟主机的监听套接字配置使用 listen 指令，具体的配置有多种形式，分别说明如下：
```

（ 1 ）使用 listen 指令，直接配置监听端口
server{
listen 80 ;
...
}
（ 2 ）使用 listen 指令，配置监听的 IP 和端口
server{
listen 127. 0. 0. 1 : 80 ;
...
}
2 .虚拟主机名称配置
虚拟主机的名称配置可使用 server_name 指令。基于微服务架构的分布式平台会有很多类型的
服务，比如文件服务、后台服务、基础服务等。很多情况下，可以通过域名前缀的方式进行 URL
路径区分，演示实例如下：
#后台管理服务虚拟主机demo
server{
listen 80 ;
server_name admin. crazydemo. com; #后台管理服务的域名前缀为admin
location/{
default_type'text/html';
charsetutf- 8 ;
echo"thisisadminserver";
}
}
#文件服务虚拟主机demo
server{
listen 80 ;
server_name file. crazydemo. com; #文件服务的域名前缀为admin
location/{
default_type'text/html';
charsetutf- 8 ;
echo"thisisfileserver";
}
}
#默认服务虚拟主机demo
server{
listen 80 default;
server_name crazydemo. com *. crazydemo. com; #如果没有前缀 ，则是默认访问的虚拟主机
location/{
default_type'text/html';
charsetutf- 8 ;
echo"thisisdefalutserver";
}
...
}
当然，客户端需要通过域名服务器或者本地的 hosts 文件解析出域名所对应的服务器 IP，HTTP
请求才能最终到达 Nginx 服务器。故为了访问上面配置的三个虚拟主机，修改一下 Windows 系统本
地的 hosts 文件，加上以下几条映射规则：
127. 0. 0. 1 crazydemo. com #基础服务域名
128. 0. 0. 1 file. crazydemo. com #文件服务域名


```
127. 0. 0. 1 admin. crazydemo. com #后台管理服务域名
128. 0. 0. 1 xxx. crazydemo. com #...
重启 Nginx，在浏览器中访问http://admin.crazydemo.com/，返回的内容如图 8 - 11 所示。
```
图 8 - 11 多个虚拟主机配置之后的访问结果
多个虚拟主机之间，匹配优先级从高到低大致如下：
（ 1 ）字符串精确匹配
如果请求的域名为 admin. crazydemo. com，那么首先会匹配到名称为 admin. crazydemo. com 的虚
拟管理主机。

（ 2 ）左侧*通配符匹配
如果浏览器请求的域名为 xxx. crazydemo. com，则会匹配到*. crazydemo. com 虚拟主机。为啥呢？
因为配置文件中并没有 server_name 为 xxx. crazydemo. com 的主机，所以名称为*. crazydemo. com 的虚
拟主机按照通配符规则匹配成功。

（ 3 ）右侧*通配符匹配
右侧*通配符和左侧*通配符匹配类似，只不过其优先级低于左侧*通配符匹配。
（ 4 ）正则表达式匹配
和通配符匹配类似，不过优先级更低。
（ 5 ）default_server
在 listen 指令后面如果带有 default 的指令参数，则代表这是默认的、最后兜底的虚拟主机，如
果前面的匹配规则都没有命中，则只能命中 default_server 默认主机。

#### 8. 3. 3 错误页面配置

```
错误页面的配置指令为 error_page，格式如下：
error_pagecode...[=[response]]uri;
例如，下面例子分别为 404 、 500 等错误码——设置错误页面，具体如下：
#后台管理服务器demo
server{
listen 80 ;
server_name admin. crazydemo. com;
root/var/www/;
location/{
default_type'text/html';
charsetutf- 8 ;
echo"thisisadminserver";
}
```

#设置错误页面
error_page 404 / 404 .html;
#设置错误页面
error_page 500502503504 / 50 x.html;
}
为了防止 404 页面被劫持，也就是被前面的代理服务器换掉，可以修改一下响应状态码，参考
如下：
error_page 404 = 200 / 404 .html #防止 404 页面被劫持
error_page 指令除了可用于 server 上下文外，还可用于 http、server、location、ifinlocation 等上
下文。

#### 8. 3. 4 长连接相关配置

配置长连接的有效时长可使用 keepalive_timeout 指令，格式为：
keepalive_timeouttimeout[header_timeout];
配置项中的 timeout 参数用于设置保持连接超时时长， 0 表示禁止长连接，默认为 75 秒。
如果要配置长连接的一条连接允许的最大请求数，那么可以使用 keepalive_requests 指令，格
式为：
keepalive_requests number;
配置项中的 number 参数用于设置在一条长连接上允许被请求的资源的最大数量，默认为 100 。
如果要配置向客户端发送响应报文的超时限制，那么可以使用下面的指令：
send_timeouttime;
配置项中的 time 参数用于设置向客户端发送响应报文的超时限制，此处时长是指两次向客户端
写操作之间的间隔时长，并非整个响应过程的传输时长。

#### 8. 3. 5 访问日志配置

Nginx 将客户端的访问日志信息记录到指定的日志文件中，供后期进行用户的浏览行为分析等，
此功能由 ngx_http_log_module 模块负责，其指令在 HTTP 处理流程的 log 阶段执行。
访问记录配置指令的完整格式为：
access_log path [format [buffer=size] [gzip[=level]] [flush=time] [if=conditio
n]];
其中，path 表示日志文件的本地路径，format 表示日志输出的格式名称。定义日志输出格式的
配置指令为 log_format，其完整格式为：
log_format name string ...;
其中，name 参数用于指定格式名称，string 参数用于设置格式字符串，可以有多个，字符串中
可以使用 Nginx 核心模块及其他模块的内置变量。
下面是一个比较完整的例子：


http{
#先定义日志格式 ，format_main 是日志格式的名字
log_format format_main '$remote_addr-$remote_user[$time_local]$request-'
'$status-$body_bytes_sent[$http_referer]'
'[$http_user_agent][$http_x_forwarded_for]';
#配置日志文件 、访问日志格式
access_log logs/access_main. log format_main;
...
}
修改配置后，需要重启 Nginx。然后在浏览器中访问http://crazydemo.com/demo/hello，在
access_main. log 文件中可以看到一条新增的日志记录：
127. 0. 0. 1 - -[ 12 /Jan/ 2020 : 18 : 32 : 28 + 0800 ]GET/demo/helloHTTP/ 1. 1 - 200 - 32 [-]
[Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 ;rv: 72. 0 ) Gecko/ 20100101 Firefox/ 72. 0 ][-]
接下来，对以上实例中所有用到的 Nginx 的内置变量进行简单说明，具体如下：
1 ）$request：记录用户的 HTTP 请求的起始行信息。
2 ）$status：记录 HTTP 状态码，即请求返回的状态，例如 200 、 404 、 502 等。
3 ）$remote_addr：记录访问网站的客户端地址。
4 ）$remote_user：记录远程客户端用户名称。
5 ）$time_local：记录访问时间与时区。
6 ）$body_bytes_sent：记录服务器发送给客户端的响应体的字节数。
7 ）$http_referer：记录此次请求是从哪个链接访问过来的，可以据此进行盗链的监测。
8 ）$http_user_agent：记录客户端访问信息，如浏览器、手机客户端等。
9 ）$http_x_forwarded_for：当前端有正向代理服务器时，此参数保持了客户端的真实 IP 地址。
该参数生效的前提是前端的代理服务器也进行了相关的 x_forwarded_for 设置。

#### 8. 3. 6 Nginx 核心模块内置变量

Nginx 核心模块 ngx_http_core_module 中定义了一系列存储 HTTP 请求信息的变量，例如
$http_user_agent,$http_cookie 等。这些内置变量在 Nginx 配置过程中使用较多，故对它们做一些介
绍，具体如下：

1 ）$arg_PARAMETER：请求 URL 中以 PARAMETER 为名称的参数值。请求参数即 URL 的“?”
后面的 name=value 形式的参数对，$arg_name 得到的值为 value。
另外，$arg_PARAMETER 中的参数名称不区分字母大小写，$arg_name 不仅可以匹配 name 参
数，也可以匹配 NAME、Name 请求参数，Nginx 会在匹配参数名之前自动把原始请求中的参数名调
整为全部小写的形式。
2 ）$args：请求 URL 中的整个参数串，其作用与$query_string 相同。
3 ）$binary_remote_addr：二进制形式的客户端地址。
4 ）$body_bytes_sent：传输给客户端的字节数，响应头不计算在内。
5 ）$bytes_sent：传输给客户端的字节数，包括响应头和响应体。
6 ）$content_length：等同于$http_content_length，用于获取请求体的大小。指的是 Nginx 从客
户端收到的请求头中 Content-Length 字段的值，不是发送给客户端响应中的 Content-Length 字段的值，


如果需要获取响应中的 Content-Length 字段的值，可使用$sent_http_content_length 变量。
7 ）$request_length：请求的字节数（包括请求行、请求头和请求体）。注意，由于$request_length
是在请求解析过程中不断累加的，如果解析请求时出现异常，则$request_length 只是已经累加部分
的长度，并不是 Nginx 从客户端收到的完整请求的总字节数（包括请求行、请求头、请求体）。
8 ）$connection：TCP 连接的序列号。
9 ）$connection_requests：TCP 连接当前的请求数量。
10 ）$content_type：请求中的 Content-Type 请求头的字段值。
11 ）$cookie_name：请求中名称 name 的 Cookie 值。
12 ）$document_root：当前请求的文档根目录或别名。
13 ）$uri：当前请求中的 URI（不带请求参数，参数位于$args 变量）。$uri 变量值不包含主机
名，例如“/foo/bar. html”。此参数可以修改，也可以通过内部重定向。
14 ）$request_uri：包含客户端请求参数的原始 URI，不包含主机名，此参数不可以修改，例如
“/foo/bar. html? name=value”。
15 ）$host：请求的主机名。优先级为 HTTP 请求行的主机名>HOST 请求头字段>符合请求的
服务器名。
16 ）$http_name：名称为 name 的请求头的值。如果实际请求头 name 中包含中画线（-），那么
需要将中画线（-）替换为下画线（_）；如果实际请求头 name 中包含大写字母，则可以替换为小写
字母。如获取 Accept-Language 请求头的值，变量名称为$http_accept_language。
17 ）$msec：当前的 Unix 时间戳。Unix 时间戳是从 1970 年 1 月 1 日（UTC/GMT 的午夜）开始所
经过的秒数，不考虑闰秒。
18 ）$nginx_version：获取 Nginx 版本。
19 ）$pid：获取 Worker 进程的 PID。
20 ）$proxy_protocol_addr：代理访问服务器的客户端地址，如果是直接访问，则该值为空字
符串。
21 ）$realpath_root：当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实
路径。
22 ）$remote_addr：客户端请求地址。
23 ）$remote_port：客户端请求端口。
24 ）$request_body：客户端的请求主体。此变量可在 location 中使用，将请求主体通过 proxy_pass、
fastcgi_pass、uwsgi_pass 和 scgi_pass 传递给下一级的代理服务器。
25 ）$request_completion：如果请求成功，则值为“OK”；如果请求未完成或者请求不是一个
范围请求的最后一部分，则为空。
26 ）$request_filename：当前请求的文件路径，由 root 或 alias 指令与 URI 请求结合生成。
27 ）$request_length：请求的长度，包括请求的地址、HTTP 请求头和请求主体。
28 ）$request_method：HTTP 请求方法，比如 GET 或 POST 等。
29 ）$request_time：处理客户端请求使用的时间，从读取客户端的第一个字节开始计时。
30 ）$scheme：请求使用的 Web 协议，如 http 或 https。
31 ）$sent_http_name：设置任意名称为 name 的 HTTP 响应头字段。如需要设置响应头
Content-length，那么将“-”替换为下画线，大写字母替换为小写，变量为$sent_http_content_length。


32 ）$server_addr：服务器端地址为了避免访问操作系统内核，应将 IP 地址提前设置在配置文
件中。
33 ）$server_name：虚拟主机的服务器名，如 crazydemo. com。
34 ）$server_port：虚拟主机的服务器端口。
35 ）$server_protocol：服务器的 HTTP 版本，通常为“HTTP/ 1. 0 ”或“HTTP/ 1. 1 ”。
36 ）$status：HTTP 响应代码。

### 8. 4 location 路由规则配置详解

location 路由匹配发生在 HTTP 请求处理的 find-config 配置查找阶段，主要功能是根据请求的
URI 地址去匹配 location 路由表达式，如果匹配成功，则执行 location 后面的上下文配置块。
本节实战案例说明：本节的配置实例处于源码工程的 nginx-location-demo. conf 配置文件中。运
行本节实例前，需要修改 openresty-start. bat（或 openresty-start. sh）脚本中的 PROJECT_CONF 配置文
件变量的值，将其改为 nginx-location-demo. conf，然后重启 OpenRestry/Nginx。

#### 8. 4. 1 location 语法详解

Nginx 配置文件中，location 配置项的语法格式为：
location [=|~|~*|^~] 模式字符串 {
...
}
按照匹配的符号的不同，主要分成精准匹配、普通匹配、正则匹配、默认根路径匹配。下面
逐一进行介绍。

1 .精准匹配
精准匹配的符号标记为“=”，下面是一个简单的精准匹配 location 的例子。
#精准匹配
location=/lua{
echo "hitlocation:=/Lua";
}
如果请求 URI 和精准匹配的模式字符串/lua 完全相同，则精准匹配通过。在所有的匹配类型中，
精准匹配的优先级最高。
运行本书的配套案例，在同时存在多个/lua 匹配模式的 location 的情况下，在浏览器中给 Nginx
发送http://localhost/lua的请求地址，输出的是精准匹配的结果，如图 8 - 12 所示。

```
图 8 - 12 输出精准匹配
```

###### 2 .普通匹配

普通匹配的类型标记为“^~”，下面是一个简单的普通匹配 location 的例子。
location^~ /lua{
echo "hitlocation:^~/lua";
}
普通匹配属于字符串前缀匹配，详细来说，如果请求路径 URI 头部匹配到 location 的模式字符
串，即为匹配成功。如果匹配到多个的前缀，则最长前缀模式匹配优先。
在本书配套实例中配置了以下两个普通匹配类型的 location，具体如下：
#普通匹配一
location^~ /lua{
echo "普通匹配: ^~ /lua";
}
#普通匹配二 ，长一点
location ^~ /lua/long {
echo "普通匹配: ^~ /lua/long";
}
在浏览器中给 Nginx 发送http://localhost/lua/long/path的请求地址，输出了普通匹配 location 的结
果，如图 8 - 13 所示。

```
图 8 - 13 输出普通匹配
```
```
普通匹配是前缀匹配，也是 Nginx 默认的匹配类型。也就是说，类型符号^~可以
省略，如果 location 没有任何的匹配类型，则为普通的前缀匹配。
```
为了对以上结论进行论证，在这里举个例子，在配置文件中配置两个同样字符串模式的 location，
一个不带类型符号，一个带“^~”符号，具体如下：
#不带类型符号 ，默认为普通匹配
location /demo{
echo "hitlocation:/demo";
}
#带 ^~符号，普通匹配
location ^~ /demo{
echo "hitlocation:^~/demo";
}
重启 Nginx 的脚本 openresty-restart. bat，发现 Nginx 不能启动，查看 error. log 错误日志，报错信息
如下：
... 17 : 33 : 39 [emerg] 18760 # 25944 : duplicatelocation"/demo"
in.../nginx-location-demo. conf: 115
从错误信息可以看出，配置文件中有两个重复的 location 配置。


###### 3 .正则匹配

###### 正则匹配的类型按照类型符号的不同可以细分为以下四种：

###### 1 ）~：标准正则匹配，区分字母大小写进行正则表达式测试，如果测试成功，则匹配成功。

###### 2 ）~*：不区分字母大小写，进行正则表达式匹配，测试成功，则匹配成功。

###### 3 ）!~：区分字母大小写，进行正则表达式测试，如果测试不成功，则匹配成功。

###### 4 ）!~*：不区分字母大小写，进行正则表达式测试，如果测试不成功，则匹配成功。

下面是一个正则匹配的例子，可以匹配以 hello. php 或 hello. asp 结尾的 URL 请求。
#正则匹配
location ~* hello\. (asp|php)$ {
echo "正则匹配: hello. (asp|php)$";
}
在浏览器中给 Nginx 发送 http：//localhost/ 1 / 2 /hellp. php 的请求地址，输出的请求结果如图 8 - 14
所示。

图 8 - 14 输出请求结果
如果配置文件中存在多个正则匹配 location，则它们之间的规则是顺序优先，只要匹配到第一
个正则类型的 location，就停止后面的正则类型的 location 测试。
例如，这里有两个正则匹配的 location 规则：\. (do|jsp)$和 hello\. (do|jsp)$，具体如下：
#正则匹配类型
location ~* \. (do|jsp)${
echo "正则匹配:. (do|jsp)$";
}
#正则匹配类型
location ~* hello\. (do|jsp)${
echo "正则匹配: hello. (do|jsp)$";
}
在浏览器中给 Nginx 发送http://localhost/ 1 / 2 /hellp. do 的请求地址，输出的结果是由配置在前面
的 location 输出的，如图 8 - 15 所示。

```
图 8 - 15 输出结果
4 .默认根路径匹配
根路径的路径规则就是单个“/”符号，示例如下：
```

location / {
echo"默认根路径匹配:/";
}
通过浏览器随便访问一个地址，如http://localhost/foo，使之不能匹配到其他的 location，只能匹
配到“/”根路径，返回的结果如图 8 - 16 所示。

图 8 - 16 返回的结果
表面看上去，location/{...}根路径匹配非常类似普通匹配，但实际上该规则确实是自成一类，
虽然只有唯一的一个路径，此类规则优先级也是最低的。

最后总结一下 4 种 location 之间的匹配次序，大致如下：
1 ）类型之间的优先级：精准匹配>普通匹配>正则匹配>“/”默认根路径匹配。
2 ）普通匹配同类型 location 之间的优先级：最长前缀优先。普通匹配的优先级与 location 在配
置文件中所处的先后顺序无关，而与匹配到的前缀长度有关。
3 ）正则匹配同类型 location 之间的优先级为顺序优先。只要匹配到第一个正则规则的 location，
就停止后面的正则规则的测试。正则匹配与 location 规则定义在配置文件中的先后顺序强相关。

#### 8. 4. 2 常用的 location 路由配置

###### 第一个应该配置的是“/”根路由规则。“/”根路由规则可以路由到一个静态首页：

location /{
root html;
index index. htmlindex. htm;
}
表示在请求 URI 匹配到“/”根规则时，首先 Nginx 会在 html 目录下查找 index. html 文件，如果没
有找到，就查找 index. htm 文件，将找到的文件内容返回给客户端。
“/”根路由规则也可以路由到一个访问很频繁的上游服务器，比方说 SpringCloud 微服务架构
中的服务网关：
location /{
proxy_passhttp:// 127. 0. 0. 1 : 7799 /;
}
这里的 127. 0. 0. 1 : 7799 假定为 Zuul 网关的 IP 和端口，当请求匹配到“/”根路由规则时，将直接
转发给上游 Zuul 应用网关服务器。
第二个应该配置的是静态文件路由规则。对静态文件请求进行响应，这是 Nginx 作为 HTTP 服
务器的强项。静态文件匹配规则有两种配置方式：目录匹配（前缀匹配）或后缀匹配（正则匹配），
可以任选其一，也可以搭配使用。
目录匹配（前缀匹配）配置实例如下：
root /www/resources/static/;
#前缀匹配


location^~ /static/{
root /www/resources/;
}
所有的匹配/static/... 规则的静态资源请求（如/static/img/ 1 .png）都将路由到 root 指令所配置的
文件目录/www/resources/static/下对应的某个文件（如/www/resources/static/img/ 1 .png）。
后缀匹配（正则匹配）配置实例如下：
location~*\. (gif|jpg|jpeg|png|css|js|ico)${
root/www/resources/;
}
所有的匹配以上正则规则的静态资源请求（如/static/img/ 2 .png）都将路由到 root 指令所配置的
文件目录/www/resources/static/下对应的某个文件（如/www/resources/static/img/ 2 .png）。

### 8. 5 Nginx 的 rewrite 模块指令

Nginx 的 rewrite 模块即 ngx_http_rewrite_module 标准模块，主要功能是重写请求 URI，也是 Nginx
默认安装的模块。rewrite 模块会根据 PCRE 正则匹配重写 URI，然后根据指令参数或者发起内部跳
转再一次进行 location 匹配，或者直接做 30 x 重定向返回客户端。
rewrite 模块的指令就是一门微型的编程语言，包含 set、rewrite、break、if、return 等一系列指令。

#### 8. 5. 1 set 指令

set 指令由 ngx_http_rewrite_module 标准模块提供，用于向变量存放值。在 Nginx 配置文件中，
变量只能存放一种类型的值，因此也只存在一种类型的值，那就是字符串。
set 指令的配置项格式如下：
set $variable value;

Nginx 配置文件中，变量的定义和使用都要以$开头。Nginx 变量名前面有一个$符
号，这是记法上的要求。所有的 Nginx 变量在引用时也必须带上$前缀。另外，Nginx 变量不
能与 Nginx 服务器预设的全局变量同名。
比如，我们的 nginx. conf 文件中有下面这一行配置：
set $a "helloworld";
在上面的语句中，set 配置指令对变量$a 进行了赋值操作，把字符串"helloworld"赋给了它。也
可以直接把变量嵌入字符串常量中以构造出新的字符串：
set $a "foo";
set $b "$a,$a";
这个例子通过前面定义的变量$a 的值来构造变量$b 的值，于是这两条指令顺序执行完之后，
$a 的值是"foo"，而$b 的值则是"foo, foo"。把变量嵌入字符串常量中以构造出新的字符串，这种技
术在 LinuxShell 脚本中也常被用到，并且被称为“变量插值”（variableinterpolation）。
set 指令不仅有赋值的功能，还有创建 Nginx 变量的功能，即当作为赋值对象的变量尚不存在时，


它会自动创建该变量。比如在上面例子中，如果$a 变量尚未创建，则 set 指令会自动创建$a 这个用户
变量。
Nginx 变量一旦创建，其变量名的可见范围就是整个 Nginx 配置，甚至可以跨越不同虚拟主机
的 server 配置块。但是，对于每个请求，所有变量都有一份独立的副本，或者说都有各变量用来存
放值的容器的独立副本，彼此互不干扰。Nginx 变量的生命期是不可能跨越请求边界的。

#### 8. 5. 2 rewrite 指令

rewrite 指令也由 ngx_http_rewrite_module 标准模块提供，主要功能是改写请求 URI。rewrite 指令
的格式如下：
rewrite regrex replacement [flag];
如果 regrex 匹配 URI，那么 URI 就会被替换成 replacement 的计算结果，replacement 一般是一个“变
量插值”表达式，其计算之后的字符串就是新的 URI。
下面的例子有两个重新配置项，具体如下：
location /download/ {
rewrite ^/download/(.*)/video/(.*)$ /view/$ 1 /mp 3 /$ 2 .mp 3 last;
rewrite ^/download/(.*)/audio/(.*)*$ /view/$ 1 /mp 3 /$ 2 .rmvb last;
return 404 ;
}
location/view{
echo"uri:$uri";
}
在浏览器中请求http://crazydemo.com/download/ 1 /video/ 10 ，地址发生了重写，并且发生了
location 的跳转，结果如图 8 - 17 所示。

图 8 - 17 输出的结果
在演示例子中，replacement 中的占位变量$ 1 、$ 2 的值是指令参数 regrex 正则表达式从原始 URI
中匹配出来的子字符串，也叫正则捕获组，编号从 1 开始。
rewrite 指令可以使用的上下文为 server、location、ifinlocation。
如果 rewrite 的同一个上下文中有多个这样的 rewrite 重新指令，则匹配会依照 rewrite 指令出现的
先后顺序依次进行下去，匹配成功之后并不会终止，而是继续往下匹配，直到返回最后一个匹配成
功为止。如果想要中途中止，不再继续往下匹配，可以使用第三个指令参数 flag。flag 参数的值有：
last、break、redirect、permanent。
如果 flag 参数使用 last 值并且匹配成功，那将停止处理任何与 rewrite 相关的指令，立即用计算后
的新 URI 开始下一轮的 location 匹配和跳转。前面的例子使用的就是 last 参数值。
如果 flag 参数使用 break 值，就如同 break 指令本身的字面意思一样，也停止处理任何与 rewrite
的相关指令，但是不进行 location 跳转。
将上面的 rewrite 例子中的 last 参数值改成 break 值，代码如下：


location/view{
echo"view:$uri";
}
location/download_break/{
rewrite ^/download_break/(.*)/video/(.*)$ /view/$ 1 /mp 3 /$ 2 .mp 3 break;
rewrite ^/download_break/(.*)/audio/(.*)*$ /view/$ 1 /mp 3 /$ 2 .rmvb break;
echo "download_breaknewuri:$uri";
}
在浏览器中请求http://crazydemo.com/download_break/ 1 /video/ 10 ，地址发生了重写，但是
location 并没有跳转，而是直接结束了，结果如图 8 - 18 所示。

图 8 - 18 显示的结果
在 location 上下文中 last 和 break 是有区别的：last 其实就相当于一个新的 URL，Nginx 进行了一次
新的 location 匹配，通过 last 获得一个可以跳转到其他 location 的配置中进行处理的机会（内部的重定
向）；而 break 在一个 location 中将原来的 URL（包括 URI 和 args）改写之后，再继续进行后面的处理，
这个重写之后的请求始终都是在同一个 location 上下文中并没有发生内部跳转。
这里要注意 last 和 break 的区别仅发生在 location 上下文；如果是在 server 上下文，那么 last 和 break
的作用是一样的。
还要注意的是：在 location 上下文的 rewrite 指令使用 last 指令参数，会再次以新的 URI 重新发起
内部重定向，再一次进行 location 匹配，而新的 URI 极有可能和旧的 URI 一样再次匹配到相同的目标
location 中，这样就发生了死循环。当循环到第 10 次时，Nginx 会终止这样无意义的循环并返回 500
错误响应码。
如果 rewrite 指令使用的 flag 参数的值是 permanent，则表示进行永久外部重定向，也就是在客户
端进行重定向。此时，服务器将新 URI 地址返回给客户端浏览器，并且返回 301 （永久重定向的响
应码）给客户端。客户端使用新的重定向地址，再发起一次远程请求。
永久重定向 permanent 的使用示例如下：
#rewrite指令permanent参数演示
location/download_permanent/{
rewrite ^/download_permanent/(.*)/video/(.*)$ /view/$ 1 /mp 3 /$ 2 .mp 3 permanent;
rewrite ^/download_permanent/(.*)/audio/(.*)*$ /view/$ 1 /mp 3 /$ 2 .rmvb
permanent;
return 404 ;
}
在浏览器中请求http://crazydemo.com/download_permanent/ 1 /video/ 10 ，输出的结果如图 8 - 19 所示。

```
图 8 - 19 输出的结果
```

###### 从以上结果可以看出，永久重定向有两个比较大的特点：

1 ）浏览器的地址栏中的地址变成了重定向地址，为http://crazydemo.com/view/ 1 /mp 3 / 10 .mp 3 。
2 ）从 Fiddler 抓包工具可以看到，第一个请求地
址的响应状态码为 301 ，如图 8 - 20 所示。

外部重定向与内部重定向是有本质区别的。从
数量上说，外部重定向有两次请求，内部重定向则
只有一次请求。通过上面的几个示例，大家应该体
会得相当深刻了。
如果 rewrite 指令使用的 flag 参数的值是 redirect，
也表示进行外部重定向，表现的行为与 permanent 参
数值完全一样，不同的是返回 302 （临时重定向的响
应码）给客户端。
有关 redirect 参数值的实例这里不做演示，大家可自行下载和运行本书源码并细细体会。
rewrite 能够利用正则捕获组设置变量，所以我们可以在 Nginx 的配置文件中加入这么一条
location 规则：
location/capture_demo{
rewrite^/capture_demo/(.*)/video/(.*)$ /view/$ 1 /mp 3 /$ 2 .mp 3 break;
rewrite^/capture_demo/(.*)/audio/(.*)*$ /view/$ 1 /mp 3 /$ 2 .rmvb break;
echo"捕获组 1 :$ 1 ;捕获组 2 :$ 2 ";
}
在浏览器中请求http://crazydemo.com/capture_demo/group 1 /video/group 2 ，输出的结果如图 8 - 21
所示。

```
图 8 - 21 输出的结果
```
#### 8. 5. 3 if 条件指令

if 条件指令配置项的格式如下：
if (condition){...}
当满足 if 条件时，执行配置块中的配置指令；if 的配置块相当于引入了一个新的上下文作用域。
if 条件指令适用于 server 和 location 这两个上下文。
condition 条件表达式用到的一系列的比较操作符大致如下：
1 ）==：相等。
2 ）!=：不相等。
3 ）~：区分字母大小写模式匹配。

```
图 8 - 20 永久重定向的响应码示意图
```

###### 4 ）~*：不区分字母大小写模式匹配。

###### 5 ）还有几个其他的专用比较符号，比如判断文件及目录是否存在的符号，这里可以省略。

下面是一个简单的演示程序，根据内置变量$http_user_agent 的值判断客户端的类型，代码
如下：
#if指令的演示程序
location/if_demo{
if ($http_user_agent~*"Firefox"){ #$匹配 firefox 火狐浏览器
return 403 ;
}
if ($http_user_agent~*"Chrome"){ #匹配chrome谷歌浏览器
return 301 ;
}
if ($http_user_agent~*"iphone"){ #匹配iphone手机
return 302 ;
}
if ($http_user_agent~*"android"){ #匹配安卓手机
return 404 ;
}
return 405 ; #其他浏览器默认访问规则
}
在火狐浏览器中访问http://crazydemo.com/if_demo，结果如图 8 - 22 所示。

图 8 - 22 火狐浏览器的访问结果
在谷歌浏览器中访问http://crazydemo.com/if_demo，结果如图 8 - 23 所示。
在演示代码中用到了 return 指令，该指令用于返回 HTTP 的状态码。return 指令会停止同一个作
用域的剩余指令处理，并返回给客户端指定的响应码。

```
图 8 - 23 谷歌浏览器的访问结果
return 指令可用于 server、location、if 上下文中，执行阶段是 rewrite 阶段。其指令的格式如下：
```

```
#格式一 ：返回响应的状态码和提示文字，提示文字可选
return code [text];
#格式二 ：返回响应的重定向状态码 (如 301 ) 和重定向 URL
return code URL;
#格式三 ：返回响应的重定向 URL，默认的返回状态码是临时重定向 302
return URL;
```
#### 8. 5. 4 add_header 指令

responseheader 一般都是 key：value 的形式，例如 Content-Encoding: gzip、Cache-Control: no-store，
设置的命令如下：
add_headerCache-Controlno-store
add_headerContent-Encodinggzip
但是有一个十分常用的 responseheader 比较特性就是 Content-Type，它在设置类型的同时还会
指定 charset，例如“text/html; charset=utf- 8 ”，由于存在分号，而分号在配置文件中作为结束符，
所以在配置时需要用引号引起来，配置如下：
add_header Content-Type'text/html; charset=utf- 8 ';
另外，由于没有单独设置 charset 的键（Key），因此要设置响应的 charset 就需要使用 Content-Type
来指定 charset。
使用 AJAX 进行跨域请求时，浏览器会向跨域资源的服务端发送一个 OPTIONS 请求，用于判断
实际请求是否安全或者用于判断服务端是否允许跨域访问，这种请求也叫作预检请求。跨域访问的
预检请求是浏览器自动发出的，用户程序往往不知情，如果不进行特别的配置，那么客户端发出一
次请求在服务端往往会收到两个请求：一个是预检请求，另一个是正式的请求。后端的服务器（PHP
或者 Tomcat）如果不经过特殊过滤，很容易将 OPTIONS 预检请求当成正式的数据请求。
对于客户端而言，只有预检请求返回成功客户端才开始正式请求。在实际的使用场景中，预
检请求比较影响性能，用户往往会有两倍请求的感觉，所以一般会在 Nginx 代理服务端对预检请求
进行提前拦截，同时对预检请求设置比较长时间的有效期。
upstreamzuul{
#server 192. 168. 233. 1 : 7799 ;
server" 192. 168. 233. 128 : 7799 ";
keepalive 1000 ;
}
server{
listen 80 ;
server_name nginx. server *. nginx. server;
default_type'text/html';
charsetutf- 8 ;
#转发到上游服务器 ，但是'OPTIONS'请求直接返回空
location /{
if ($request_method='OPTIONS'){
add_header Access-Control-Max-Age 1728000 ;
add_header Access-Control-Allow-Origin *;
add_header Access-Control-Allow-Credentials true;
add_header Access-Control-Allow-Methods 'GET, POST, OPTIONS';
add_header Access-Control-Allow-Headers 'Keep-Alive, User-Agent,
X-Requested-With,\If-Modified-Since, Cache-Control, Content-Type, token';


return 204 ;
}
proxy_passhttp://zuul/;
}
}
配置 Nginx，加入“Access-Control-Max-Age”请求头，用来指定本次预检请求的有效期，单
位为秒。上面结果中有效期是 1728000 秒（ 20 天），即允许缓存该条回应 1728000 秒，在此期间不用
发出另一条预检请求。

#### 8. 5. 5 指令的执行顺序

大多数 Nginx 新手都会有这样一个困惑，那就是当同一个 location 配置块使用了多个 Nginx 模块
的配置指令时，这些指令的执行顺序很可能会跟它们的书写顺序大相径庭。现在就来看这样一个令
人困惑的例子：
location /sequence_demo_ 1 {
set $a foo;
echo $a;
set $a bar;
echo $a;
}
上面的代码先给变量$a 赋值 foo，然后输出，随后给变量$a 赋值 bar，随后输出。如果这里是
一段 Java 代码，毫无疑问，最终的输出结果一定为“foobar”。然而事实并非如此，在浏览器中访
问http://crazydemo.com/sequence_demo_ 1 ，结果如图 8 - 24 所示。

图 8 - 24 输出的结果
为什么出现了这种不合常理的现象呢？
前面讲到，Nginx 的请求处理阶段共有 11 个之多，分别是 post-read、server-rewrite、find-config、
rewrite、post-rewrite、preaccess、access、post-access、try-files、content 以及 log。其中有 3 个阶段按
照执行时的先后顺序依次是 rewrite 阶段、access 阶段以及 content 阶段。
Nginx 的配置指令一般只会注册并运行在其中的某一个处理阶段。比如 set 指令就是在 rewrite 阶
段运行的，而 echo 指令就只会在 content 阶段运行。在一次请求处理流程中，rewrite 阶段总是在 content
阶段之前执行。因此属于 rewrite 阶段的配置指令（示例中的 set）也总是会无条件地在 content 阶段的
配置指令（示例中的 echo）之前执行，即便是 echo 配置项出现在 set 配置项的前面。
上面例子中的指令按照请求处理阶段的先后次序排序，实际的执行次序为：
location /sequence_demo_ 1 {
#rewrite阶段的配置指令 ，执行在前面
set $a foo;
set $a bar;
#content阶段的配置指令 ，执行在后面


```
echo $a;
echo $a;
}
所以，输出的结果就是 barbar 了。
```
8. (^6) 反向代理与负载均衡配置
接下来介绍 Nginx 的重要功能：反向代理+负载均衡。单体 Nginx 的性能虽然不错，但也是有瓶
颈的。打个比方：用户发起一个请求，网站显示的图片量又比较大，如果说这个时候有大量用户同
时访问，那么全部的工作量都放在了一台服务器上，服务器不负重压，可能就崩溃了。高并发场景
下，自然需要多台服务器做集群，这样既能够防止单个节点崩溃导致平台不能使用，也能提高效率。
一般来说，Nginx 完成 10 万以上的用户同时访问，程序就容易崩溃。
要做到高并发和高可用，肯定需要做 Nginx 集群的负载均衡，而 Nginx 负载均衡的基础之一就
是反向代理。

#### 8. 6. 1 演示环境说明

###### 为了较好地演示反向代理的效果，本小节调整一下演示的环境：不再通过浏览器发出 HTTP 请

求，而是使用 curl 指令从笔者的 Centos 虚拟机 192. 168. 233. 128 向 Windows 宿主机器 192. 168. 233. 1 上的
Nginx 发起请求。
为了完成演示，在宿主机 Nginx 的配置文件 nginx-proxy-demo. conf 中配置两个 server 虚拟主机，
一个端口为 80 ，另一个端口为 8080 。具体如下：
#模拟目标主机
server{
listen 8080 ;
server_name localhost;
default_type'text/html';
charsetutf- 8 ;
location/{
echo "-uri=$uri"
"-host=$host"
"-remote_addr=$remote_addr"
"-proxy_add_x_forwarded_for=$proxy_add_x_forwarded_for"
"-http_x_forwarded_for= $http_x_forwarded_for";
}
}
#模拟代理主机
server{
listen 80 default;
server_name localhost;
default_type'text/html';
charsetutf- 8 ;
location/{
echo"默认根路径匹配:/";
}
...


}
本节要用到的配置文件为源码工程 nginx-proxy-demo. conf 文件。运行本小节实例前，需要修改
openresty-start. bat（或 openresty-start. sh）脚本中的 PROJECT_CONF 变量的值，改为
nginx-proxy-demo. conf，然后重启 OpenRestry/Nginx。

#### 8. 6. 2 proxy_pass 反向代理指令

这里介绍的 proxy_pass 反向代理指令处于 ngx_http_proxy_module 模块，并且注册在 HTTP 请求
11 个阶段中的 content 阶段。
proxy_pass 反向代理指令的格式如下：
proxy_pass 目标 URL 前缀；
如果 proxy_pass 后面的目标 URL 格式为"协议"+"IP[: port]"+"/"根路径的格式，末尾有“/根路
径”，则 Nginx 不会把 location 的 URI 前缀加到结果路径中，这里称之为不带前缀代理。如果目标 URL
格式为"协议"+"IP[: port]"，末尾没有“/根路径”，则 Nginx 会把 location 指令的 URI 前缀加到结果路
径中，这里称之为带前缀代理。

```
1 .不带 location 前缀代理
配置的时候，在目标 URL 后加“/根路径”，转发时会去掉 location 块的部分路径，实例如下：
#不带location前缀的代理类型
location/foo_no_prefix{
proxy_passhttp:// 127. 0. 0. 1 : 8080 /;
}
通过 CentOS 的 curl 指令发出请求 http:// 192. 168. 233. 1 /foo_no_prefix//bar. html，结果如下：
[ root@localhost ~] #curl http:// 192. 168. 233. 1 /foo_no_prefix/. html
```
- uri=/bar. html -host= 127. 0. 0. 1 - remote_addr= 127 bar. 0. 0. 1
- proxy_add_x_forwarded_for= 127. 0. 0. 1 - http_x_forwarded_for=
可以看到，$uri 变量所输出的代理 URI 为/bar. html，并没有在结果 URL 中看到 location 配置指令
的前缀/foo_no_prefix。

```
2 .带 location 前缀代理
配置的时候，在目标 URL 后不加“/根路径”，转发时会留着 location 块的部分路径，实例如下：
#带location前缀代理
location/foo_prefix{
proxy_passhttp:// 127. 0. 0. 1 : 8080 ;
}
通过 CentOS 的 curl 指令发出请求 http:// 192. 168. 233. 1 /foo_prefix/bar. html，结果如下：
[ root@localhost ~] #curlhttp :// 192. 168. 233. 1 /foo_prefix/bar. html
```
- uri=/foo_prefix/bar. html -host= 127. 0. 0. 1 - remote_addr= 127. 0. 0. 1
- proxy_add_x_forwarded_for= 127. 0. 0. 1 - http_x_forwarded_for=
可以看到，$uri 变量所输出的代理 URI 为/foo_prefix/bar. html，也就是说，在结果 URL 中看到了
location 配置指令的前缀/foo_prefix。
除了以上两种代理（带 location 前缀代理和不带 location 前缀代理），之外，还有一种带目标 URL


###### 前缀的代理。


###### 3 .带部分 URI 路径代理

如果 proxy_pass 的路径参数中不止有 IP 和端口，还有部分目标 URI 的路径，则最终的代理 URL
由两部分组成：第一部分为配置项中的目标 URI 前缀，第二部分为将请求 URI 去掉 location 中的前缀
的剩余部分。
下面是两个实例：
#带部分URI路径代理 ，实例 1
location/foo_uri_ 1 {
proxy_passhttp:// 127. 0. 0. 1 : 8080 /contextA/;
}
#带部分URI路径代理 ，实例 2
location/foo_uri_ 2 {
proxy_passhttp:// 127. 0. 0. 1 : 8080 /contextA-;
}
通过 CentOS 的 curl 指令发出两个请求分别匹配到这两个 location 配置，结果如下：
[ root@localhost ~] #curl [http://](http://) 192. 168. 233. 1 /foo_uri_ 1 /bar. html

- uri=/contextA/bar. html -host= 127. 0. 0. 1 - remote_addr= 127. 0. 0. 1
- proxy_add_x_forwarded_for= 127. 0. 0. 1 - http_x_forwarded_for=
[ root@localhost ~] #curl [http://](http://) 192. 168. 233. 1 /foo_uri_ 2 /bar. html
- uri=/contextA-bar. html -host= 127. 0. 0. 1 - remote_addr= 127. 0. 0. 1
- proxy_add_x_forwarded_for= 127. 0. 0. 1 - http_x_forwarded_for=
从输出结果可以看出，无论是例子中的目标 URI 前缀/contextA/，还是例子中的目标 URI 前缀
/contextA-都加在了最终的代理路径上。只是在代理路径中去掉了 location 指令的匹配前缀。
新的问题来了：仅使用 proxy_pass 指令进行请求转发，发现很多原始请求信息都丢了。明显的是客
户端 IP 地址，前面的例子中请求都是从 192. 168. 233. 128 CentOS 机器发出去的，经过代理服务器之后，
服务端返回的 remote_addr 客户端 IP 地址并不是 192. 168. 233. 128 ，而是变成了代理服务器的 IP 127. 0. 0. 1 。
如何解决原始信息的丢失问题呢？答案是使用 proxy_set_header（请求头设置）指令。

#### 8. 6. 3 proxy_set_header 指令

在反向代理之前，proxy_set_header 指令能重新定义/添加字段传递给代理服务器的请求头。请
求头的值可以包含文本、变量和它们的组合。其格式如下：
#head_field表示请求头 ，field_value 表示值
proxy_pass_header head_field field_value;
前面说到，经过反向代理后，对于目标服务器来说，客户端在本质上已经发生了变化，因此，
后端的目标 Web 服务器无法直接拿到客户端的 IP，假设后端的服务器是 Tomcat，则在 Java 中
request.getRemoteAddr () 取得的是 Nginx 的地址，而不是客户端的真实 IP。
如果需要取得真实 IP，可以通过 proxy_set_header 指令在发生反向代理调用之前，将保持在内
置变量$remote_addr 中的真实的客户端地址保存到请求头中（一般为 X-real-ip），如下所示：
#不带location前缀代理
location/foo_no_prefix/{
proxy_pass [http://](http://) 127. 0. 0. 1 : 8080 /;
proxy_set_header X-real-ip $remote_addr;
}


在 Java 端使用 request.getHeader ("X-real-ip") 获取 X-real-ip 请求头的值，就可以获得真正的客户端
IP 了。
在整个请求处理的链条上可能不止一个反向代理，可能会经过多次反向代理。为了获取整个
的代理转发记录，也可以使用 proxy_set_header 指令在配置文件中进行如下配置：
#带location前缀代理
location/foo_prefix{
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_pass [http://](http://) 127. 0. 0. 1 : 8080 ;
}
这里使用了$proxy_add_x_forwarded_for 内置变量，它的作用就是记录转发历史，其值的第一
个地址就是真实地址$remote_addr，然后每经过一个代理服务器就在后面累加一次代理服务器地址。
上面的演示程序中，如果在 Java 服务器程序中通过如下的代码获取代理转发记录：
request.getHeader ("X-Forwarded-For")
则 Java 程序获得的返回值为“ 192. 168. 233. 128 , 127. 0. 0. 1 ”，表示最初的请求客户端的 IP 为
192. 168. 233. 128 ，经过了 127. 0. 0. 1 代理服务器。每经过一次代理服务器，都会在后面追加上它的 IP，
并且使用逗号分隔开。
为了不丢失信息，反向代理的设置如下：
location/hello{
proxy_pass [http://](http://) 127. 0. 0. 1 : 8080 ;
proxy_set_header Host $host;
proxy_set_header X-real-ip $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_redirect off;
}
设置了请求头 Host、X-real-ip、X-Forwarded-For，分别将当前的目标主机、客户端 IP、转发记
录保存在请求头中。
proxy_redirect 指令的作用是修改从上游被代理服务器传来的应答头中的 Location 和 Refresh 字段，尤
其是当上游服务器返回的响应码是重定向或刷新请求（如 HTTP 响应码是 301 或者 302 ）时，proxy_redirect
可以重设 HTTP 头部的 location 或 refresh 字段值。off 参数表示禁止所有的 proxy_redirect 指令。

#### 8. 6. 4 upstream 上游服务器组

假设 Nginx 仅有反向代理没有负载均衡的话，其价值会大打折扣。Nginx 在配置反向代理时，
可以通过负载均衡机制配置一个上游服务器组（多台上游服务器）。当组内的某台服务器宕机时仍
能保持系统可用，从而实现高可用。
Nginx 的负载均衡配置主要用到 upstream（上游服务器组）指令，其格式如下：
语法：upstream name{...}
上下文：http 配置块
其中，upstream 指令后面的 name 参数是上游服务器组的名称；在 upstream 块中将使用 server 指
令定义组内的上游候选服务器。


upstream 指令的作用与 server 有点类似，其功能是加入一个特殊的虚拟主机节点。特殊之处在
于这是上游服务器的服务组，可以包含一个或者多个上游服务器。
一个 upstream 负载均衡主机节点的配置实例大致如下：
#upstream负载均衡虚拟节点
upstreambalanceNode{
server" 192. 168. 1. 2 : 8080 "; #上游候选服务 1
server" 192. 168. 1. 3 : 8080 "; #上游候选服务 2
server" 192. 168. 1. 4 : 8080 "; #上游候选服务 3
server" 192. 168. 1. 5 : 8080 "; #上游候选服务 4
}
实例中配置的 balanceNode 相当于一个主机节点，不过，这是一负载均衡类型的特定功能虚拟
主机。当请求过来时，balanceNode 主机节点的作用是按照默认负载均衡算法（带权重的轮询算法），
在 4 个上游候选服务中选取一个进行请求转发。
实战案例：在随书源码的 nginx-proxy-demo. conf 配置文件中配置了三个 server 主机和一个
upstream 负载均衡主机组。此处配置了一个 location 块，将目标端口为 80 的请求反向代理到 upstream
主机组，以便于负载均衡主机的操作。
实战案例的配置代码节选如下：
#负载均衡主机组 ，给虚拟主机 1 与虚拟主机 2 做负载均衡
upstreambalance{
server " 127. 0. 0. 1 : 8080 "; #虚拟主机 1
server " 127. 0. 0. 1 : 8081 "; #虚拟主机 2
}
#虚拟主机 1
server{
listen 8080 ;
server_name localhost;
location/{
echo "serverport: 8080 ";
}
}
#虚拟主机 2
server{
listen 8081 ;
server_name localhost;
location/{
echo "serverport: 8081 ";
}
}
#虚拟主机 3 ：默认虚拟主机
server{
listen 80 default;
#...
#负载均衡测试连接
location/balance{
proxy_passhttp://balance; #反向代理到负载均衡节点
}
}
在运行本小节实例前，需要修改启动脚本 openresty-start. bat（或 openresty-start. sh）中的
PROJECT_CONF 变量的值，将其改为 nginx-proxy-demo. conf，然后重启 OpenRestry/Nginx。


在 CentOS 服务器中使用 curl 命令请求 http:// 192. 168. 233. 1 /balance 链接地址（IP 根据 Nginx 情况而
定），并且多次发起请求，就会发现虚拟主机 1 和虚拟主机 2 被轮流访问到，具体的输出如下：
[ root@localhost ~] #curlhttp :// 192. 168. 233. 1 /balance
serverport: 8080
[ root@localhost ~] #curlhttp :// 192. 168. 233. 1 /balance
serverport: 8081
[ root@localhost ~] #curlhttp :// 192. 168. 233. 1 /balance
serverport: 8080
[ root@localhost ~] #curlhttp :// 192. 168. 233. 1 /balance
serverport: 8081
[ root@localhost ~] #curlhttp :// 192. 168. 233. 1 /balance
serverport: 8080
从运行结果可以看到，upstream 负载均衡指令起到了负载均衡的效果。默认情况下，upstream
会依照轮询方式进行负载分配，每个请求按请求顺序逐一分配给不同的上游候选服务器。

#### 8. 6. 5 upstream 的上游服务器配置

在 upstream 块中将使用 server 指令定义组内的上游候选服务器。内部 server 指令的语法如下：
语法：server address [parameters];
上下文：upstream 配置块
此内嵌的 server 指令用于定义上游服务器的地址和其他可选参数。其地址可以指定为域名或 IP
地址带有可选端口，如果未指定端口，则使用端口 80 。
内嵌的 server 指令的可选参数大致如下：
（ 1 ）weight=number：设置上游服务器的权重
默认情况下，upstream 使用加权轮询（WeightedRoundRobin）负载均衡方法在上游服务器之
间分发请求。weight 值默认为 1 ，并且各上游服务器的 weight 值相同，表示每个请求按先后顺序逐一
分配给不同的上游服务器，如果某个上游服务器宕机，则自动剔除。
如果希望改变某个上游节点的权重，则可以使用 weight 显式进行配置，参考实例如下：
#负载均衡主机组
upstreambalance{
server " 127. 0. 0. 1 : 8080 " weight= 2 ; #上游虚拟主机 1 ，权重为 2
server " 127. 0. 0. 1 : 8081 " weight= 1 ; #上游虚拟主机 2 ，权重为 1
}
权重越大的节点将被分发到更多的请求。
（ 2 ）max_conns=number：设置上游服务器的最大连接数
max_conns 参数限制到上游节点的最大同时活动连接数。默认值为 0 ，表示没有限制。如果
upstream 服务器组没有通过 zone 指令设置共享内存，则在单个 Worker 进程范围内对上游服务的最大
连接数进行限制；如果 upstream 服务器组通过 zone 指令设置了共享内存，则在全体的 Worker 进程范
围内对上游服务进行统一的最大连接数限制。


（ 3 ）backup（可选参数）
backup 参数标识该 server 是备份的上游节点，当普通的上游服务（非 backup）不可用时，请求
将被转发到备份的上游节点；当普通的上游服务（非 backup）可用时，备份的上游节点不接收处理
请求。

（ 4 ）down（可选参数）
backup 参数标识上游 server 节点是不可用或者为永久下线的状态。
（ 5 ）max_fails=number：最大错误次数
如果上游服务不可访问了，如何判断呢？使用 max_fails 参数，该参数表示请求转发最多失败
number 次就判定该 server 为不可用。max_fails 参数的默认次数为 1 ，表示转发失败 1 次该 server 即为不
可用。如果此参数设置为 0 ，则会禁用不可用的判断，会一直不断地尝试连接后端 server。

（ 6 ）fail_timeout=time：失败测试的时间长度
这是一个失效监测参数，一般与上面的参数 max_fails 协同使用。fail_timeout 意思是失败测试的
时间长度，指的是在 fail_timeout 时间范围内最多尝试 max_fails 次，就判定该 server 为不可用了。
fail_timeout 参数的默认值为 10 秒。

server 指令在进行 max_conns 连接数配置时，Nginx 内部会涉及共享内存区域的使用，配置共享
内存区域的指令如下：

语法：zone name [size];
上下文：upstream 配置块
其中，zone 的 name 参数设置共享内存区的名称，size 可选参数设置共享内存区域的大小。如果
配置了 upstream 的共享内存区域，则其运行时状态（包括最大连接数）在所有的 Worker 进程之间是
共享的。在 name 相同的情况下，不同的 upstream 组将共享同一个区，这种情况下，size 参数的大小
值只需设置一次。
下面是一个 server 指令和 zone 指令的综合使用实例：
upstreamzuul{
zone upstream_zuul 64 k; //名称为 upstream_zuul，大小为 64 K 的共享内存区域
server" 192. 168. 233. 128 : 7799 " weight= 5 max_conns= 500 ;
server" 192. 168. 233. 129 : 7799 " fail_timeout= 20 s max_fails= 2 ;//默认权重为 1
server" 192. 168. 233. 130 : 7799 " backup; //后备服务
}

#### 8. 6. 6 upstream 的负载分配方式

upstream 负载分配方式大致有以下 3 种：
1 .加权轮询
默认情况下，upstream 使用加权轮询负载均衡方法在上游服务器之间分发请求。默认的 weight
值为 1 ，并且各上游服务器的 weight 值相同，表示每个请求按到达的先后顺序逐一分配给不同的上
游服务器，如果某个上游服务器宕机，则自动剔除。


指定 weight 值，weight 和分配比率成正比，用于后端服务器性能不均的情况。下面是一个简单
的例子：
upstreambackend{
server 192. 168. 1. 101 weight= 1 ;
server 192. 168. 1. 102 weight= 2 ;
server 192. 168. 1. 103 weight= 3 ;
}
2 .hash 指令
基于 hash 函数值进行负载平衡，hash 函数的 key 可以包含文本、变量或二者的组合。hash 函数
负载均衡是一个独立的指令，指令的格式如下：

语法：hash key [consistent];
上下文：upstream 配置块
注意，如果 upstream 组中去除掉一个 server，会导致 hash 值重新计算，也即原来的大多数的 key
可能会寻址到不同的 server 上。若配置有 consistent 参数，则 hash 一致性将选择 Ketama 算法。这个算
法的优势是，如果有 server 从 upstream 组里去除掉之后，只有少数的 key 会重新映射到其他的 server
上去，即大多数的 key 不受 server 去除的影响，还映射原来的 server。这对提高缓存 server 命中率有很
大帮助。下面是一个简单的通过请求的$request_uri 的 hash 值进行负载均衡的例子：
upstreambackend{
hash $request_uri consistent;
server 192. 168. 1. 101 ;
server 192. 168. 1. 102 ;
server 192. 168. 1. 103 ;
}
3 .ip_hash 指令
基于客户端 IP 的 hash 值进行负载平衡，这样每个客户端固定访问同一个后端服务器，可以解决
类似 session 不能跨服务器的问题。如果上游 server 不可用，需要手工去除或者配置 down 参数。ip_hash
是一条独立的指令，使用示例如下：
upstreambackend{
ip_hash;
server 192. 168. 1. 101 : 7777 ;
server 192. 168. 1. 102 : 8888 ;
server 192. 168. 1. 103 : 9999 ;
}


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
linux openresty 安装（图文死磕）
https://www.cnblogs.com/crazymakercircle/p/ 12115651 .html
windowsopenresty 死磕：安装和启动脚本
https://www.cnblogs.com/crazymakercircle/p/ 12111283 .html

##### 3. 相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题 ：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 9 章 Nginx Lua 编程

经过合理配置，Nginx 毫无疑问是高性能 Web 服务器的最佳选择。除此之外，Nginx 还具备可
编程能力，理论上可以使用 Nginx 的扩展组件 ngx_lua 开发各种复杂的动态应用。不过，由于 Lua 是
一种脚本/动态语言，不太适合进行复杂业务逻辑的程序开发。但是，在高并发场景下，NginxLua
编程是解决性能问题的利器。

## 9. 1 NginxLua 编程主要的应用场景

NginxLua 编程主要的应用场景如下：
1 ）API 网关：实现如数据校验前置、请求过滤、API 请求聚合、AB 测试、灰度发布、降级、
监控等功能。著名的开源网关 Kong 就是基于 NginxLua 开发的。
2 ）高速缓存：可以对响应内容进行缓存，减少到后端的请求，从而提升性能。比如，NginxLua
可以和 Java 容器（如 Tomcat、Redis）整合，由 Java 容器进行业务处理和数据缓存，而 Nginx 负责读
缓存并进行响应，从而解决 Java 容器的性能瓶颈。
3 ）简单的动态 Web 应用：可以完成一些业务逻辑处理较少但是耗费 CPU 的简单应用，比如模
板页面的渲染。一般的 NginxLua 页面渲染处理流程为：从 Redis 获取业务处理结果数据，从本地加
载 XML/HTML 页面模板，然后进行页面渲染。
4 ）网关限流：缓存、降级、限流是解决高并发的三大利器，Nginx 内置了令牌限流的算法，
但是对于分布式的限流场景，可以通过 NginxLua 编程定制自己的限流机制。

## 9. 2 NginxLua 编程简介

```
本节对 NginxLua 编程的基础知识、NginxLua 项目结构和启动做一个简单的介绍。
```

#### 9. 2. 1 ngx_lua 简介

Lua 是一种轻量级、可嵌入式的脚本语言，可以非常容易地嵌入其他语言中使用。因为 Lua 的小
巧轻量级，可以在 Nginx 中嵌入 LuaVM（Lua 虚拟机），请求时创建一个 VM，请求结束时回收 VM。
ngx_lua 是 Nginx 的一个扩展模块，将 LuaVM 嵌入 Nginx 中，从而可以在 Nginx 内部运行 Lua 脚本，
使得 Nginx 变成了一个 Web 容器，这样开发人员就可以使用 Lua 语言开发高性能 Web 应用了。ngx_lua
提供了很多与 Nginx 交互的 API，对于开发人员来说只需要掌握这些 API 就可以进行功能开发；而对
于开发 Web 应用来说，如果接触过 Servlet 的话就可以知道，ngx_lua 的开发和 Servlet 类似，无外乎就
是接收请求、参数解析、功能处理、返回响应这些内容。
使用 ngx_lua 开发 Web 应用时，有很多源码的 Lua 基础性模块可供使用，比如 OpenResty 就提供
了一些常用的 ngx_lua 开发模块：

1 ）lua-resty-memcached：通过 Lua 操作 Memcached 缓存。
2 ）lua-resty-mysql：通过 Lua 操作 MySQL 数据库。
3 ）lua-resty-redis：通过 Lua 操作 Redis 缓存。
4 ）lua-resty-dns：通过 Lua 操作 DNS 域名服务器。
5 ）lua-resty-limit-traffic：通过 Lua 进行限流。
6 ）lua-resty-template：通过 Lua 进行模板的渲染。
除了上述的 MySQL 数据库操作、Redis 操作、限流、模板渲染等常用功能组件外，还有很多第
三方的 ngx_lua 组件，如 lua-resty-jwt、lua-resty-kafka 等。对于大部分应用场景来说，现在 ngx_lua 生
态环境中的组件已经足够多了。如果仍然不能满足自己的需求，也可以开发自己的 Lua 模块。

#### 9. 2. 2 NginxLua 项目的创建

在开始 NginxLua 项目开发之前，首先需要搭建 Lua 的开发环境，具体的开发工具选择和环境搭
建的教程可参考疯狂创客圈社群的视频“NginxLua 开发环境搭建，带视频”。
在 IDEA 创建 Lua 脚本的工程。在选择工程类型时选择 Lua 项目类型，如图 9 - 1 所示。剩余的操作
只要选择默认值，直到创建完成即可。

```
图 9 - 1 在 IDEA 创建 Lua 脚本的工程
```

#### 9. 2. 3 Lua 项目的工程结构

创建完成 Lua 工程之后，这里规划一下工程目录，Lua 项目
的工程结构如图 9 - 2 所示。
图 9 - 2 所示的工程结构都处于工程的 src 目录下，包含了两
大部分的内容：第一部分为 Nginx 的配置，第二部分为 Lua 脚本
的目录结构。

第一部分 Nginx 的配置可以进一步细分，包含了两块内容：
1 ）Nginx 的调试配置文件 nginx-debug. conf。
2 ）Nginx 的调试日志目录。
第二部分 Lua 脚本统一放在了 src/luaScript（名称自己定）
目录下，luaScript 的目录结构可以进一步细分，包含了 3 块内容：

1 ）src/luaScript/initial 目录，用于存放 Lua 程序初始化时需要加载的其他 Lua 脚本，比如
mobdebug. lua 调试脚本。
2 ）src/luaScript/module 目录，用于存放业务模块的 Lua 脚本，比如 helloworld. lua。
3 ）src/luaScript/redis 用于存放操作 Redis 的一些公有方法的代码，比如分布式锁 Lock. lua。这里
仅以 Redis 为例说明：如果是一些耦合度较高的 Lua 模块，可以在 src/luaScript 目录下单独建一个子目录。

```
Nginx 调试时的配置文件 nginx-debug. conf 需要在 src 目录下，与 Lua 脚本的目录平
级。为什么呢？因为在 nginx-debug. conf 会应用到 Lua 脚本，使用的是相对路径，如果目录的
相对位置不对，就会找不到 Lua 脚本。
```
nginx-debug. conf 的部分配置如下：
location/test{
default_type'text/html';
charsetutf- 8 ;
content_by_lua_fileluaScript/test. lua;
}
location/helloworld{
default_type'text/html';
charsetutf- 8 ;
content_by_lua_fileluaScript/module/demo/helloworld. lua;
}
在启动 Nginx 开始调试时，会将 src 目录作为启动的根目录。在这种场景下，如果 nginx-debug. conf
配置文件和 luaScript 不在同一个目录下的话，上面所配置的 luaScript/test. lua 和 luaScript/module/demo/
helloworld. lua 这两个脚本都会找不到。

#### 9. 2. 4 Lua 项目的启动

开始调试 Lua 项目的脚本之前，需要通过启动 Nginx 来执行 Lua 项目。但是，这里不使用默认的
Nginx 参数启动，而使用-p 参数和-c 参数。启动和重启 Lua 项目的 Nginx 命令如下：

```
图 9 - 2 Lua 项目的工程结构
```

//启动 Lua 项目的命令
C:\dev\refer\LuaDemoProject\src> nginx-p./-cnginx-debug. conf
//开发过程中，可能还会用到重启 Lua 项目的命令
C:\dev\refer\LuaDemoProject\src> nginx-p./-cnginx-debug. conf-s reload
//停止 Lua 项目的命令
C:\dev\refer\LuaDemoProject\src> nginx-p./-cnginx-debug. conf -sstop
可以通过第 8 章介绍的 openresty-start. bat（在 Linux 下使用 openresty-start. sh）脚本来启动 Nginx，
不过在启动之前需要调整其中的变量，具体如下：
@echooff
rem 启动标志，flag= 0 ，表示之前已经启动，flag= 1 ，表示现在立即启动
setflag= 0
rem 设置 openresty/Nginx 的安装目录
setinstallPath=E:/tool/openresty- 1. 13. 6. 2 - win 32
rem 设置 Nginx 项目的工作目录
setprojectPath=C:/dev/refer/LuaDemoProject/src
rem 设置项目的配置文件
setPROJECT_CONF=nginx-debug-demo. conf
echoinstallPath:%installPath%
echoprojectprefixpath:%projectPath%
echoconfigfile:%projectPath%/conf/%PROJECT_CONF%
echoopenrestystarting...
rem 查找 openresty/Nginx 进程信息，然后设置 flag 标志位
tasklist|find/i"nginx. exe">nul
if%errorlevel%== 0 (
echo"Openresty/Nginxalreadyrunning!"
remexit/b
) elsesetflag= 1
rem 如果需要，就启动 openresty/Nginx
cd/d%installPath%
if%flag%== 1 (
startnginx. exe-p"%projectPath%"-c"%projectPath%/conf/%PROJECT_CONF%"
pinglocalhost-n 2 >nul
)
rem 输出 openresty/Nginx 的进程信息
tasklist/fi"imagenameeqnginx. exe"
tasklist|find/i"nginx. exe">nul
if%errorlevel%== 0 (
echo"openresty/Nginx starting succeeded!"
)
需要修改的变量为 projectPath 和 PROJECT_CONF，分别为项目的根目录和配置文件的名称。
在 NginxLua 项目开发过程中会涉及 Lua 脚本的调试，具体的调试工具和调试方法可参考疯狂创
客圈社群的博文“NginxLua 开发的调试工具和调试方法”。

### 9. 3 Lua 开发基础

Lua 是一个可扩展的轻量级脚本语言，Lua 的设计目的是为了嵌入应用程序中，从而为应用程序
提供灵活的扩展和定制功能。Lua 代码简洁优美，几乎在所有操作系统和平台上都可以编译、运行。


Lua 脚本需要通过 Lua 解释器来解释执行，除了 Lua 官方的默认解释器外，目前使用广泛的 Lua
解释器叫作 LuaJIT。
LuaJIT 是采用 C 语言编写的 Lua 脚本解释器。LuaJIT 被设计成全兼容标准 Lua 5. 1 ，因此 LuaJIT
代码的语法和标准 Lua 的语法没多大区别。LuaJIT 和 Lua 的一个区别是，LuaJIT 的运行速度比标准 Lua
快数十倍，可以说是 Lua 的高效率版本。

#### 9. 3. 1 Lua 模块的定义和使用

与 Java 类似，实际开发时 Lua 代码需要进行分模块开发。Lua 中的一个模块对应于一个 Lua 脚本
文件。使用 require 指令导入 Lua 模块，第一次导入模块后，所有 Nginx 进程全局共享模块的数据和代
码，当 Worker 进程需要时会得到此模块的一个副本，不需要重复导入，从而提高了 Lua 应用的性能。
接下来，演示开发一个简单的 Lua 模块，用来存放公共的基础对象和基础函数。
//代码清单：src/luaScript/module/common/basic. lua

- -定义一个应用程序公有的 Lua 对象 app_info
localapp_info={version=" 0. 10 "}
- -增加一个 path 属性，保存 Nginx 进程所保存的 Lua 模块路径，包括 conf 文件配置的部分路径
app_info. path=package. path;
- -局部函数, 取得最大值
localfunctionmax (num 1 ,num 2 )
    if (num 1 >num 2 ) then
       result=num 1 ;
    else
       result=num 2 ;
    end
    returnresult;
end
- -统一的模块对象
local_Module={
    app_info=app_info;
    max=max;
}
return_Module
模块内的所有对象、数据、函数都定义成局部变量或者局部函数。对于需要暴露给外部的对
象或者函数，作为成员属性保存到一个统一的 Lua 局部对象（_Module）中，通过返回这个统一的
局部对象将内部的成员对象或者方法暴露出去，从而实现 Lua 的模块化封装。

#### 9. 3. 2 Lua 模块的使用

接下来，创建一个 Lua 脚本 src/luaScript/module/demo/helloworld. lua 来调用刚才定义的这个基础
模块 src/luaScript/module/common/basic. lua 文件。
helloworld. lua 的代码如下：
//代码清单：src/luaScript/module/demo/helloworld. lua

- --启动调试
localmobdebug=require ("luaScript. initial. mobdebug");
mobdebug.start ();
- -导入自定义的模块
localbasic=require ("luaScript. module. common. basic");


- -使用模块的成员属性
ngx.say ("Luapathis: ".. basic. app_info. path);
ngx.say ("<br>");
- -使用模块的成员方法
ngx.say ("max 1 and 11 is: "..basic.max ( 1 , 11 ));
在使用 require 内置函数导入 Lua 模块时，对于多级目录下的模块，使用 require ("目录 1 .目录 2 .模
块名") 的形式进行加载，源目录之间的斜杠分割符号（/）改成点号分隔符（.）。这一点和 Java 的
包名分隔符很类似。
- -导入自定义的模块
localbasic=require ("luaScript. module. common. basic");
查找 Lua 文件时，首先会在 Nginx 的当前工作目录下查找，如果没有找到，则会在 Nginx 的 Lua
包路径 lua_package_path 和 lua_package_cpath 声明的位置查找。整个 Lua 文件的查找过程和 Java
的. class 文件的查找过程也很类似。需要注意的是，Lua 包路径需要在 nginx. conf 配置文件中进行配置：
lua_package_path"E:/tool/ZeroBraneStudio- 1. 80 /lualibs/?/?. lua;;";
lua_package_cpath"E:/tool/ZeroBraneStudio- 1. 80 /bin/clibs/?. dll;;";
这里有两个包路径配置项：lua_package_path 用于配置 Lua 文件的包路径，lua_package_cpath 用
于配置 C 语言模块文件的包路径。在 Linux 系统上，C 语言模块文件的类型是“. so”类型；在 Windows
平台上，C 语言模块文件的类型是“. dll”类型。
如果 Lua 包路径需要配置多个路径，则路径之间使用分号（;）分隔。末尾的两个双分号（;;）
表示加上 Nginx 默认的 Lua 包搜索路径，其中包含了 Nginx 的安装目录下的 lua 目录。
//路径清单：一个默认的 Lua 文件搜索路径输出案例
./site/lualib/?. ljbc;
./site/lualib/?/init. ljbc;
./lualib/?. ljbc;
./lualib/?/init. ljbc;
./site/lualib/?. lua;
./site/lualib/?/init. lua;
./lualib/?. lua;
./lualib/?/init. lua;
.\?. lua;
E:\tool\openresty- 1. 13. 6. 2 - win 32 \lualib\?. lua;
E:\tool\openresty- 1. 13. 6. 2 - win 32 \lua\?. lua;
E:\tool\openresty- 1. 13. 6. 2 - win 32 \lua\?\init. lua;;
在 openResty 的 lualib 下已经提供了大量第三方开发库，如 CJSON、Redis 客户端、MySQL 客户
端等，并且这些 Lua 模块已经包含到默认的搜索路径中。openResty 的 lualib 下的模块可以直接在 Lua
文件中通过 require 方式进行导入：
- -导入 Redis 操作模块
localredis = require ("resty. redis")
- -导入 CJSON 操作模块
localcjson = require ("cjson")

#### 9. 3. 3 Lua 的数据类型

```
Lua 中大致有以下 8 种数据类型，具体如表 9 - 1 所示。
```

```
表 9 - 1 8 种数据类型
类型名称说明
number 实数可以是整数、浮点数
string 字符串字符串类型，值是不可改变的
boolean 布尔类型 false 和 nil 为假，其他都为真
table 数组、容器 table 类型实现了一种抽象的“关联数组”，相当于 Java 中的 Map
userdata 类其值他有语可言能中就的是对 us 象 erd 类 at 型 a 类，转型换，过判来断就空变值成的时 us 候 erd 要 at 小 a 类心型。比如说 Redis 返回的空
```
```
thread 线程和和命 Ja 令 va 指中针的线程差不多，代表一条执行序列，拥有自己的独立的栈、局部变量
```
nil 空类型变变量量没赋被值赋成值 ni，l 也类表型示默删认除为变 n 量 il。nil 类型就 nil 一个值，表示变量是否被赋值，
function 函数由 C 和 Lua 编写的函数，属于一种数据类型
Lua 是弱类型语言，和 JavaScript 等脚本语言类似，变量没有固定的数据类型，每个变量可以包
含任意类型的值。使用内置的 type（变量名称）方法可以获取该变量的数据类型。下面是一段简单
的类型输出演示程序。

- -输出数据类型
localfunctionshowDataType ()
    locali;
    basic.log ("字符串的类型",type ("helloworld"))
    basic.log ("方法的类型",type (showDataType))
    basic.log ("true 的类型",type (true))
    basic.log ("整数数字的类型",type ( 360 ))
    basic.log ("浮点数字的类型",type ( 360. 0 ))
    basic.log ("nil 值的类型",type (nil))
    basic.log ("未赋值变量 i 的类型",type (i))
end
上面的方法定义在 luaScript. module. demo. dataType 模块中。然后定义一个专门的调试模块
runDemo，调用上面定义的 showDataType 方法。runDemo. lua 的代码清单如下：
- --启动调试
localmobdebug=require ("luaScript. initial. mobdebug");
mobdebug.start ();
- -导入自定义的基础模块
localbasic=require ("luaScript. module. common. basic");
- -导入自定义的 dataType 模块
localdataType=
require ("luaScript. module. demo. dataType");
ngx.say ("下面是数据类型演示的结果输出：<br>");
dataType.showDataType ();
在 nginx-debug. conf 配置好 runDemo. lua 之后就可以
通过浏览器执行了，浏览器输出的结果如图 9 - 3 所示。
关于 Lua 的数据类型有以下几点需要注意：
1 ）nil 是一种类型，Lua 中 nil 表示“无效值”。nil
也是一个值，表示变量是否被赋值，如果变量没有被
赋值，则值为 nil，类型也为 nil。图^9 -^3 Lua 数据类型输出


与 Nginx 略微有一点不同，OpenResty 还提供了一种特殊的空值，即 ngx. null，用来表示空值，
但是不同于 nil。
2 ）boolean（布尔类型）的可选值为 true 和 false。Lua 中，仅 nil 与 false 为“假”，其他所有值均
为“真”，比如数字 0 和空字符串都是“真”。这一点和 Java 语言的 boolean 类型还是有一点区别的。
3 ）number 类型用于表示实数，与 Java 中的 double 类型很类似，但是又有区别：Lua 的整数类型
也是 number。一般来说，Lua 中 number 类型就用双精度浮点数来实现，可以使用数学函数 math. lua
来操作 number 类型的变量。在 math. lua 模块中定义了大量的数字操作方法，比如定义了 floor（向下
取整）和 ceil（向上取整）等操作。下面是一个演示方法。

- -演示取整操作
localfunctionintPart (number)
    basic.log ("演示的整数", number)
    basic.log ("向下取整是",math.floor (number));
    basic.log ("向上取整是",math.ceil (number))
end
上面的方法定义在 luaScript. module. demo. dataType 模块中，然后在 runDemo. lua 模块中调用上面
定义的 intPart 方法。调用的代码清单如下：
- --启动调试
localmobdebug=require ("luaScript. initial. mobdebug");
mobdebug.start ();
- -导入自定义的 dataType 模块
localdataType=require ("luaScript. module. demo. dataType");
ngx.say ("<hr>下面是数字取整的输出：<br>");
dataType.intPart ( 0. 01 );
dataType.intPart ( 3. 14 );
运行之后，输出的结果如图 9 - 4 所示。
4 ）table 类型实现了一种抽象的“关联数组”，相当于 Java 中的 Map。“关联数组”是一种具
有特殊索引方式的数组，索引（也就是 Map 中的 key）通常是 number 类型或者字符串（string），但
也可以是除 nil 以外的任意类型的值。默认情况下，table 中的 key 是 number 类型，并且 key 的值为递增
的数字。

图 9 - 4 floor（向下取整）和 ceil（向上取整）操作的结果
5 ）和 JavaScript 脚本语言类型，在 Lua 中的函数也是一种数据类型，类型为 function。函数可以
存储在变量中，也可以作为参数传递给其他函数，还可以作为其他函数的返回值。定义一个有名字
的函数本质上是定义一个函数对象，然后赋值给变量名称（函数名）。

```
例如，前面定义在 basic. lua 中的 max 函数，可以变成如下的形式：
```

- -局部函数, 取得最大值
localmax=function (num 1 ,num 2 )
    localresult=nil;
    if (num 1 >num 2 ) then
       result=num 1 ;
    else
       result=num 2 ;
    end
    returnresult;
end
在上面的代码中，首先定义一个变量 max，然后使用 function 定义一个匿名函数，并且将函数
赋给 max 变量。这种定义方法等价于直接定义一个带名字的函数的方式：
- -局部函数, 取得最大值
localmax=function (num 1 ,num 2 )
//... 省略函数体
end

#### 9. 3. 4 Lua 的字符串

Lua 中有三种方式表示字符串：
1 ）使用一对匹配的半角英文单引号，例如'hello'。
2 ）使用一对匹配的半角英文双引号，例如"hello"。
3 ）还可以用一种长括号（即[[]]）括起来的方式定义，例如 [["add\name",'hello']]。
需要说明的是，长括号内的任何转义符不被处理，比如长括号内的"\n"就不会被转义。
Lua 的字符串的值是不可改变的，和 Java 一样，string 类型是不可变类型。如果需要改变，则需要
根据修改要求来创建一个新的字符串并且返回。另外，Lua 也不支持通过下标来访问字符串中的某个
字符。
Lua 定义了一个负责字符串操作的 string 模块，包含很多强大的字符操作函数。主要的字符串操
作介绍如下：

```
（ 1 ）两点符号 (..)：字符串拼接符号
Lua 中，如果需要进行字符串的连接，可使用两点符号（..）。例如：
```
- -演示字符串操作
localfunctionstringOperator (s)
    localhere="这里是：".."高性能研习社群".."疯狂创客圈";
    print (here);
    basic.log ("字符串拼接演示", here);
end
（ 2 ）string.len (s)：获取字符串的长度
此函数接收一个字符串作为参数，返回它的长度。此函数的功能和“#”运算符类似，后者也
是取字符串的长度。实际开发过程中，建议尽量使用“#”运算符来获取 Lua 字符串的长度。
- -演示字符串长度的获取
localfunctionstringOperator (s)
localhere="这里是：".."高性能研习社群".."疯狂创客圈";
basic.log ("获取字符串的长度",string.len (here));
basic.log ("获取字符串的长度方式二", #here ); end


```
（ 3 ）string.format (formatString,...)：格式化字符串
第一个参数 formatString 表示进行格式化字符串的规则，通常由常规文本和格式指令组成，比如：
```
- -简单的圆周率格式化规则
string.format ("保留两位小数的圆周率%. 4 f", 3. 1415926 );
- -格式化日期
string.format ("%s% 02 d-% 02 d-% 02 d","今天 is: ", 2020 , 1 , 1 ))；
formatString 参数中，除了常规文本之外还有格式指令。格式指令由%符号加上一个类型字母组
成，比如%s（字符串格式化）、%d（整数格式化）、%f（浮点数格式化）等，在%符号和类型符号
的中间，可以选择性地加上一些格式控制数据，比如% 02 d 表示进行两位的整数格式输出。总体来说，
formatString 参数中的格式化指令的规则与标准 C 语言中 printf 函数的格式化规则基本相同。
format 函数后面的参数是一个可变长参数，表示一系列的需要进行格式化的值。一般来说，前
面的 formatString 参数中有多少格式化指令，后面就可以放置对应数量的参数值，并且后面的参数
类型需要与 formatString 参数中对应位置的格式化指令中的类型符号相匹配。

（ 4 ）string.find (s, pattern[, init[, plain]])：字符串匹配
在 s 字符串中查找第一个匹配正则表达式 pattern 的子字符串，返回第一次在 s 中出现的满足条件
的子串的开始位置和结束位置；若匹配失败，则返回 nil。第三个参数 init 默认为 1 ，表示从起始位置
1 开始找起。第四个参数的值默认为 false，表示第二个参数 pattern 为正则表达式，默认进行表达式
匹配，当第四个参数为 true 时，只会把 pattern 看成一个普通字符串。

- -演示字符串查找操作
localfunctionstringOperator (s)
    localhere="这里是：".."高性能研习社群".."疯狂创客圈";
    localfind=string. find;
    basic.log ("字符串查找",find (here,"疯狂创客圈"));
end
（ 5 ）string.upper (s)：字符串转成大写
接收一个字符串 s，返回一个把所有小写字母变成大写字母的字符串。
- -演示字符串操作
localfunctionstringOperator (s)
localsrc="Helloworld!";
basic.log ("字符串转成大写",string.upper (src));
basic.log ("字符串转成小写",string.lower (src));
end
与 string.upper (s) 方法类似，string.lower (s) 方法的作用是接收一个字符串 s，返回一个全部字母
变成小写的字符串。

#### 9. 3. 5 Lua 中的数组容器

Lua 数组的类型定义的关键词为 table，通过名字进行翻译的话可以直接翻译为二维表。和 Java
数组对比起来，Lua 数组有以下几个要点：

要点一：Lua 数组内部实际采用哈希表保存“键－值对”，这一点和 Java 的容器 HashMap 类似。
不同的是，Lua 在初始化一个普通数组时，如果不显式地指定元素的键，则默认用数字索引作为键。
要点二：使用花括号定义一个数组，中间加上初始化的元素序列，元素之间以逗号隔开即可。


- -定义一个数组
    localarray 1 ={"这里是：","高性能研习社群","疯狂创客圈"}
- -定义一个元素类型为“键－值对”的数组，相当于 Java 的 HashMap
    localarray 2 ={k 1 ="这里是：", k 2 ="高性能研习社群", k 3 ="疯狂创客圈"}
要点三：普通 Lua 数组的数字索引对应 Java 的元素下标，是从 1 开始计数的。
要点四：普通 Lua 数组的长度的计算方式和 C 语言有点儿类似。从第一个元素开始，计算到最
后一个非 nil 的元素为止，中间的元素数量即是长度。
要点五：使用[]符号取得数组元素值，形式为 array[key]，其中，array 代表数组变量名称，key
代表元素的索引，这一点和 Java 语言是类似的。对于普通的数组，key 为元素的索引值；对于“键
－值对”类型的数组容器，key 就是“键－值对”中的键。
- -迭代上面定义的普通数组
fori= 1 , 3 do
ngx.say (i.."=".. array 1 [i]..",");
end
ngx.say ("<br>");
- -迭代上面定义“键－值对”的数组容器
fork,vinpairs (array 2 ) do
ngx.say (k.."=".. array 2 [k]..",");
end
ngx.say ("<br><br>");
Lua 定义了一个负责数组和容器操作的 table 模块，主要的字符串操作大致如下：
（ 1 ）table.getn (t)：获取长度
对于普通的数组，键从 1 到 n 放着一些非空值时，它的长度就精确为 n。如果数组有一个元素为
“空值”（nil 值被夹在中间，相当于有一个空洞），那么数组的长度为“空值”前面部分的数组
长度，“空值”后面的数组元素不会计算在内。
要获取数组长度，Lua 中还有一个更为简单的操作符，即一元操作符#。并且，在 Lua 5. 1 之后的
版本去掉了 table.getn (t) 方法，直接使用 #获取长度 。
- -定义一个数组
localarray 1 ={"这里是：","高性能研习社群","疯狂创客圈"}
- -定义一个 K-V 元素类型的数组
localarray 2 ={k 1 ="这里是：", k 2 ="高性能研习社群", k 3 ="疯狂创客圈"}
- -取得数组长度
basic.log ("使用 table.getn (t) 获取长度",table.getn (array 1 ));
basic.log ("使用一元操作符 #获取长度 ", #array 1 );
（ 2 ）table.concat (array,[, sep,[, i,[, j]]])：连接数组元素
按照 array[i].. sep.. array[i+ 1 ].. sep.. array[j]的方式，将普通数组中所有的元素连接成一个字符串
并返回。分隔字符串 sep 默认为空白字符串。起始位置 i 默认为 1 ，结束位置 j 默认是 array 的长度。如
果 i 大于 j，则返回一个空字符串。
localtestTab={ 1 , 2 , 3 , 4 , 5 , 6 , 7 }
basic.log ("连接元素",table.concat (testTab)) --输出: 1234567
basic.log ("带分隔符连接元素",table.concat (testTab,"*", 1 , 3 )) --输出: 1 * 2 * 3
（ 3 ）table.insert (array,[pos,], value)：插入元素
在 array 的位置 pos 处插入元素 value，后边的元素向后面顺移。pos 的默认值为 #list + 1 ，因此调
用 table.insert (array, x) 会将 x 插在普通数组 array 的末尾。
localtestTab={ 1 , 2 , 3 , 4 }


- -插入一个元素到末尾
table.insert (testTab, 5 )
basic.printTable (testTab) --输出: 1 2 3 4 5
- -插入一个元素到位置索引 2
table.insert (testTab, 2 , 10 )
basic.printTable (testTab) --输出: 1 10 2 3 4 5
上面用了一个新的成员 basic.printTable (testTab)，是为了输出数组元素，在 basic 模块中专门定
义了一个新输出方法_printTable (tab)，然后暴露为 printTable。代码如下：
- -在屏幕上输出 table 元素
function_printTable (tab)
localoutput=""
fori,vinipairs (tab) do
ngx.say (v.." ");
end
ngx.say ("<br>");
end
（ 4 ）table.remove (array[, pos])：删除元素
删除 array 中 pos 位置上的元素，并返回这个被删除的值。当 pos 是 1 到 #list之间的整数时 ，将后
面的所有元素前移一位，并删除最后一个元素。
testTab={ 1 , 2 , 3 , 4 , 5 , 6 , 7 }
- -删除最后一个元素
table.remove (testTab)
basic.printTable (testTab) --输出: 1 2 3 4 5 6
- -删除第二个元素
table.remove (testTab, 2 ) --输出: 1 3 4 5 6
basic.printTable (testTab)

#### 9. 3. 6 Lua 的控制结构

1 .分支控制结构 if-else
if-else 是 Java 工程师熟知的一种控制结构，有 3 种类型：单分支结构、两分支结构和多分支结构。
（ 1 ）单分支结构：if
以关键词 if 开头，以关键词 end 结束。这一点和 Java 不同，Java 中使用右花括号作为分支结构体
的结束符号。

- -单分支结构
    Local x='疯狂创客圈'
    if x=='疯狂创客圈' then
       basic.log ("单分支演示：","这个是一个高性能研习社群")
    end
输出的结果是：
单分支演示：这个是一个高性能研习社群
（ 2 ）两分支结构：if-else
与 Java 类似，两分支结构的控制语句是在单分支的基础上加入 else 子句。
- -两分支
Local x='疯狂创客圈'
if x== '这个是一个高性能研习社群' then


basic.log ("两分支演示：","这儿是疯狂创客圈")
else
basic.log ("两分支演示：","这儿还是疯狂创客圈")
end
输出的结果是：
两分支演示：这儿还是疯狂创客圈
（ 3 ）多个分支结构：if-elseif-else
多分支结构就是添加 elseif 条件子句，可以添加多个 elseif 条件子句。与 Java 语言的不同之处是
else 与 if 不是分开的，是连在一起的。

- -多分支
    if x=='这个是一个高性能研习社群' then
       basic.log ("多分支演示：","这儿是疯狂创客圈")
    elseif x=='疯狂创客圈' then
       basic.log ("多分支演示：","这个是一个高性能研习社群")
    else
       basic.log ("多分支演示：","这儿不是疯狂创客圈")
    end
输出的结果是：
多分支演示：这个是一个高性能研习社群
2 .for 循环控制结构
for 循环控制结构分成两类：基础 for 循环和增强版的 foreach 循环。
（ 1 ）基础的 for 循环
语法如下：
forvar=begin, finish, stepdo
- -body
end
基础的 for 循环的语法中，var 表示迭代变量，begin、finish、step 表示控制的变量。迭代变量 var
从 begin 开始，一直变化到 finish 循环结束，每次变化都以 step 作为步长递增。begin、finish、step 可
以是表达式，但是这 3 个表达式只会在循环开始时执行一次。其中，步长表达式 step 是可选的，如
果没有设置，默认值就为 1 。迭代变量 var 的作用域仅在 for 循环内，并且循环过程中不能改变迭代变
量 var 的值，否则会带来不可预知的影响。
- -for 循环，步长为 2
fori= 1 , 5 , 2 do
ngx.say (i.."")
end
- -for 循环，步长为 1
ngx.say ("<br>");
fori= 1 , 5 do
ngx.say (i.."")
end
输出的结果分别为：
135
12345


```
（ 2 ）增强版的 foreach 循环
语法如下：
forkey,valueinpairs (table) do
```
- -body
end
前面讲到，在 Lua 的 table 内部保存的有一个“键－值对”的列表，foreach 循环就是对这个列表
中的“键－值对”进行迭代，pairs（table）函数的作用就是取得 table 内部的“键－值对”列表。
- -foreach 循环，打印 tablet 中所有的键和值
localdays={
"Sunday","Monday","Tuesday","Wednesday",
"Thursday","Friday","Saturday"
}
forkey,valueinpairs (days) do
ngx.say (key..": ".. value..";")
end
localdays 2 ={
Sunday= 1 ,Monday= 2 ,Tuesday= 3 ,Wednesday= 4 ,
Thursday= 5 ,Friday= 6 ,Saturday= 7
}
forkey,valueinpairs (days 2 ) do
ngx.say (key..": ".. value..";")
end
输出的结果如下：
1 : Sunday; 2 : Monday; 3 : Tuesday; 4 : Wednesday; 5 : Thursday; 6 : Friday; 7 : Saturday;
Tuesday: 3 ;Monday: 2 ;Sunday: 1 ;Thursday: 5 ;Friday: 6 ;Wednesday: 4 ;Saturday: 7 ;

#### 9. 3. 7 Lua 的函数定义

Lua 函数使用关键词 function 来定义，使用函数的好处如下：
1 ）降低程序的复杂性：模块化编程的好处是将复杂问题变成一个一个小问题，然后分而治之。
把函数作为一个独立的模块或者当作一个黑盒，而不需要考虑函数里面的细节。
2 ）增强代码的复用度：当程序中有相同的代码部分时，可以把这部分写成一个函数，通过调
用函数来实现这部分代码的功能，节约空间、减少代码长度。
3 ）隐含局部变量：在函数中使用局部变量，变量的作用域（即作用范围）不会超出函数，这
样它就不会给外界带来干扰。

首先来看下 Lua 的函数定义，格式如下：
optional_function_scopefunctionfunction_name (argument 1 ,argument 2 ,argument 3 ...,
argumentn)
function_body
returnresult_params_comma_separated
end
参数说明如下：
1 ）optional_function_scope：该参数表示所定义的函数是全局函数还是局部函数，该参数是可
选参数，默认为全局函数，如果定义为局部函数，则设置为关键字 local。


2 ）function_name：该参数指定函数名称。
3 ）argument 1 ，argument 2 ，argument 3 ，...，argumentn：函数参数，多个参数以逗号隔开，函
数也可以不带参数。
4 ）function_body：函数体，函数中需要执行的代码语句块。
5 ）result_params_comma_separated：函数返回值，Lua 语言中函数可以返回多个值，每个值以
逗号分隔开。

```
下面定义了一个局部函数 max ()，参数为 num 1 和 num 2 ，用于比较两值的大小，并返回最大值：
```
- -局部函数, 取得最大值
localfunctionmax (num 1 ,num 2 )
    localresult=nil;
    if (num 1 >num 2 ) then
       result=num 1 ;
    else
       result=num 2 ;
    end
    returnresult;
end
怎么使用 Lua 的可变参数呢？和 Java 语言类似，Lua 函数可以接收可变数目的参数，在函数参
数列表中使用三点（...）表示函数有可变的参数。在函数的内部可以通过一个数组访问可变参数的
实参列表，简称可变实参数组。只不过访问可变实参数组前需要将其赋值给一个变量。
- -在屏幕上打印日志，可以输入多个打印的数据
localfunctionlog (...)
localargs={...} --这里的... 和{}符号中间需要有空格号，否则会出错
fori,vinpairs (args) do
print ("index: ", i,"value: ", v)
ngx.say (v..",");
end
ngx.say ("<br>");
end
这里不得不提一下函数参数值的传递方式。大家知道，主要有两种传递方式：一种是值传递，
另一种是引用传递。Lua 函数的参数大部分是按值传递的。值传递就是调用函数时，实参把实参的
值通过赋值传递给形参，然后形参的改变和实参就没有关系了。在这个过程中，实参和形参是通过
在参数表中的位置匹配起来。但是有一种数据类型除外，这就是 table 数组类型，table 类型的传递方
式是引用传递。当函数参数是 table 类型时，传递进来的是实际参数的引用（内存地址），此时在函
数内部对该 table 所做的修改会直接对实际参数生效，而无须自己返回结果和让调用者进行赋值。
怎么使得函数可以有多个返回值呢？这是 Lua 具有一项与众不同的特性，允许函数返回多个值。
比如，Lua 的内置函数 string. find，在源字符串中查找目标字符串，若查找成功，则返回两个值：一
个起始位置和一个结束位置。
locals, e=string.find ("helloworld","lo") -->返回值为： 4 5
print (s, e) -->输出： 4 5
如果一个函数需要在 return 后面返回多个值时，值与值之间用“,”分隔开。
最后来总结一下定义一个 Lua 函数要注意的几点：
1 ）利用名字来解释函数和变量是为了使人通过名字就能看出来函数和变量的作用。让代码自
己说话，不需要注释最好。


###### 2 ）由于全局变量一般会占用全局名字空间，同时也有性能损耗（即查询全局环境表的开销），

因此我们应当尽量使用“局部函数”，在开头加上 local 修饰符。
3 ）由于函数定义本质上就是变量赋值，而变量的定义总是应放置在变量使用之前，因此函数
的定义也需要放置在函数调用之前。

#### 9. 3. 8 Lua 的面向对象编程

大家知道，在 Lua 中使用表（table）实现面向对象，一个表就是一个对象。由于 Lua 的函数
（function）也是一种数据类型，因此表可以拥有前面介绍的 8 大数据类型的成员属性。
下面在 DataType. lua 模块中定义带有一个成员的_Square 类，代码如下：

- -正方形类
_Square={side= 0 }
_Square.__index=_Square
- -类的方法 getArea
function_Square.getArea (self)
    returnself. side*self. side;
end
- -类的方法 new
function_Square.new (self, side)
    localcls={}
    setmetatable (cls, self)
    cls. side=sideor 0
    returncls
end
- -一个统一的模块对象
local_Module={
    ...//省略其他的
    Square=_Square;
}
在调用的 Square 类的方法时，建议将点号改为冒号。使用冒号进行成员方法调用时，Lua 会隐
性传递一个 self 参数，它将调用者对象本身作为第一个参数传递进来。
ngx.say ("<br><hr>下面是面向对象操作的演示：<br>");
localSquare=dataType. Square;
localsquare=Square: new ( 20 );
ngx.say ("正方形的面积为",square: getArea ());
输出的结果如下：
下面是面向对象操作的演示：
正方形的面积为 400
Lua 的面向对象用到了两个重要的概念：
1 ）metatable 元表：简单来说，如果一个表（也叫对象）的属性找不到，就去它的元表中查找。
通过 setmetatable（table，metatable）方法设置一个表的元表。
2 ）上面这一点不完全对。为什么呢？准确来说，不是直接查找元表的属性，而是去原表中的
一个特定的属性，名称叫作__index 的表（对象）中查找属性。__index 也是一个 table 类型，Lua 会在
__index 中查找相应的属性。

```
所以，在上面的代码中，_Square 表设置了__index 属性的值为自身，当为新创建的 new 对象查找
```

getArea 方法时，需要在原表_Square 表的__index 的属性中查找，找到的就是 getArea 方法的定义。这个调
用的链条如果断了，新创建的 new 对象的 getArea 方法就会导航失败。

### 9. 4 NginxLua 编程基础

OpenResty 通过汇聚各种设计精良的 Nginx 模块（主要由 OpenResty 团队自主开发）将 Nginx 变
成一个强大的通用 Web 应用平台。这样，Web 开发人员和系统工程师可以使用 Lua 脚本语言调动
Nginx 支持的各种 C 以及 Lua 模块，快速构造出足以胜任 10 KB 乃至 1000 KB 以上单机并发连接的高性
能 Web 应用系统。
OpenResty 的目标是让 Web 服务直接运行在 Nginx 服务内部，充分利用 Nginx 的非阻塞 I/O 模型，
不仅对 HTTP 客户端请求，甚至对远程后端诸如 MySQL、PostgreSQL、Memcached 以及 Redis 等都进
行一致的高性能响应。
实战案例说明：本节用到的配置文件为源码工程 nginx-lua-demo. conf。运行本小节实例前，需
要修改 openresty-start. bat（或 openresty-start. sh）脚本中的 PROJECT_CONF 变量的值，将它改为
nginx-lua-demo. conf，然后重启 OpenRestry。

#### 9. 4. 1 NginxLua 的执行原理

在 OpenResty 中，每个 Worker 进程使用一个 LuaVM，当请求被分配到 Worker 时，将在这个 Lua
VM 中创建一个协程，协程之间数据隔离，每个协程都具有独立的全局变量。
ngx_lua 是将 Lua 嵌入 Nginx，让 Nginx 执行 Lua 脚本，并且高并发、非阻塞地处理各种请求。Lua
内置协程，可以很好地将异步回调转换成顺序调用的形式。ngx_lua 在 Lua 中进行的 IO 操作都会委托
给 Nginx 的事件模型，从而实现非阻塞调用。开发者可以采用串行的方式编写程序，ngx_lua 会在进
行阻塞的 IO 操作时自动中断，保存上下文，然后将 IO 操作委托给 Nginx 事件处理机制，在 IO 操作完
成后，ngx_lua 会恢复上下文，程序继续执行，这些操作对用户程序都是透明的。
每个 Nginx 的 Worker 进程持有一个 Lua 解释器或 LuaJIT 实例，被 Worker 处理的所有请求共享这
个实例。每个请求的 Context 上下文会被 Lua 轻量级的协程分隔，从而保证各个请求是独立的，如图
9 - 5 所示。

```
图 9 - 5 工作进程相互独立
```
```
Master 进程
```
```
Worker 进程 Worker 进程
```

1 ）每个 Worker（工作）进程创建一个 LuaVM，Worker 内所有协程共享 VM。
2 ）将 NginxI/O 原语封装后注入 LuaVM，允许 Lua 代码直接访问。
3 ）每个外部请求都由一个 Lua 协程处理，协程之间数据隔离。
4 ）Lua 代码调用 I/O 操作等异步接口时会挂起当前协程（并保护上下文数据），而不阻塞 Worker
进程。
5 ）I/O 等异步操作完成时还原协程相关的上下文数据，并继续运行。
ngx_lua 采用“one-coroutine-per-request”的处理模型，对于每个用户请求，ngx_lua 会唤醒一个
协程用于执行用户代码处理请求，当请求处理完成后，这个协程会被销毁。每个协程都有一个独立
的全局环境（变量空间），继承于全局共享的、只读的公共数据。所以，被用户代码注入全局空间
的任何变量都不会影响其他请求的处理，并且这些变量在请求处理完成后会被释放，这样就保证所
有的用户代码都运行在一个“sandbox”（沙箱）中，这个沙箱与请求具有相同的生命周期。得益于
Lua 协程的支持，ngx_lua 在处理 10000 个并发请求时只需要很少的内存。根据测试，ngx_lua 处理每个
请求只需要 2 KB 的内存，如果使用 LuaJIT 则会更少。所以 ngx_lua 非常适合用于实现可扩展的、高并
发的服务。

#### 9. 4. 2 NginxLua 的配置指令

ngx_lua 定义了一系列的 Nginx 配置指令，用于配置何时运行用户 Lua 脚本以及如何返回 Lua 脚本
的执行结果。
ngx_lua 定义的 Nginx 配置指令大致如表 9 - 2 所示。
表 9 - 2 ngx_lua 定义的 Nginx 配置指令
NginxLua 配置指令名称指令说明
lua_package_path 配置 Lua 外部库的搜索路径，搜索的文件类型为. lua 文件
lua_package_cpath
配置 Lua 外部库的搜索路径，搜索 C 语言编写的外部库文件。Linux 系统下搜
索类型为. so 文件；Windows 系统下为. dll 文件
init_by_lua Master 进程启动时挂载的 Lua 代码块，常用于导入公有模块
init_by_lua_file Master 进程启动时挂载的 Lua 脚本文件
init_worker_by_lua Worker 进程启动时挂载的 Lua 代码块，常用来执行一些定时器任务
init_worker_by_lua_file Worker 进程启动时挂载的 Lua 脚本文件，常用来执行一些定时器任务
set_by_lua 类似于 rewrite 模块的 set 指令，将 Lua 代码块的返回结果设置在 Nginx 的变量中
set_by_lua_file 类似于
rewrite 模块的 set 指令，将 Lua 脚本文件的返回结果设置在 Nginx 的
变量中
content_by_lua 执码行块在是在 CoNntgeinntx 阶字段符的串 L 中 ua 编代写码的块 L，ua 执脚行本结，果可将能作需为要请进求行响特应殊的字内符容转。义 Lua 代
content_by_lua_file 执行在 content 阶段的 lua 脚本文件，执行结果将作为请求响应的内容
content_by_lua_block
与 content_by_lua 指令类似，不同之处在于该指令是直接在一对花括号（{}）
内联 Lua 源码，而不是在 Nginx 字符串中（它需要特殊字符转义）
rewrite_by_lua 执行在 rewrite 阶段的 Lua 代码块，完成转发、重定向、缓存等功能
rewrite_by_lua_file 执行在 rewrite 阶段的 Lua 脚本文件，完成转发、重定向、缓存等功能


```
（续表）
NginxLua 配置指令名称指令说明
access_by_lua 执行在 access 阶段的 Lua 代码块，完成 IP 准入、接口权限等功能
access_by_lua_file 执行在 access 阶段的 Lua 脚本文件，完成 IP 准入、接口权限等功能
header_filter_by_lua 响应头部过滤处理的 Lua 代码块，比如可以用于添加响应头部信息
body_filter_by_lua 响应体的过滤处理的 Lua 代码块，比如可以用于加密响应体
log_by_lua 异到步 ET 完 L 成集日群志记录的 Lua 代码块，比如既可以在本地记录日志，也可以记录
```
```
ngx_lua 配置指令在 Nginx 的 HTTP 请求处理阶段中所处的位置大致如图 9 - 6 所示。
```
```
图 9 - 6 ngx_lua 配置指令在 Nginx 的 HTTP 请求处理阶段中所处的位置
```
#### 9. 4. 3 NginxLua 的常用配置指令

1 ）lua_package_path 指令，它的格式如下：
lua_package_path lua-style-path-str
lua_package_path 指令用于设置“. lua”外部库的搜索路径，此指令的上下文为 http 配置块。其
默认值为 LUA_PATH 环境变量内容或者 Lua 编译的默认值。lua-style-path-str 字符串是标准的 luapath
格式，“;;”常用于表示原始的搜索路径。下面是一个简单的例子：
#设置纯Lua扩展库的搜寻路径 (';;'是默认路径)
lua_package_path'/foo/bar/?. lua;/blah/?. lua;;';
Openresty 可以在搜索路径中使用插值变量。例如，可以使用插值变量$prefix 或${prefix}获取
虚拟服务器 server 的前缀路径，server 的前缀路径通常在 Nginx 服务器启动时通过-pPATH 命令行选
项来指定。

```
2 ）lua_package_cpath 指令，它的格式如下：
lua_apckage_cpath lua-style-cpath-str
```

lua_package_path 指令用于设置 Lua 的 C 语言模块外部库“. so”（Linux）或“. dll”（Windows）
外部库的搜索路径，此指令的上下文为 http 配置块。lua-style-cpath-str 字符串是标准的 luacpath 格式，
“;;”常用于表示原始的 cpath。下面是一个简单的例子：
#设置C编写的Lua扩展模块的搜寻路径 (也可以用';;')
lua_package_cpath '/bar/baz/?. so;/blah/blah/?. so;;';
同样，Openresty 可以在搜索路径 lua-style-cpath-str 中使用插值变量，比如通过$prefix 或${prefix}
获取服务器前缀的路径。

3 ）init_by_lua 指令，它的格式如下：
init_by_lua lua-script-str
init_by_lua 指令只能用于 http 上下文，运行在配置加载阶段。当 Nginx 的 Master 进程在加载 Nginx
配置文件时，在全局 LuaVM 级别上运行由参数 lua-script-str 指定的 Lua 脚本块。当 Nginx 接收到 HUP 信
号并开始重新加载配置文件时，LuaVM 将会被重新创建，并且 init_by_lua 也将在新的 VM 上再次运行。
如果 Lua 脚本的缓存是关闭的，那么每请求一次都运行一次 init_by_lua 处理程序。通过
lua_code_cache 指令可以关闭 Lua 脚本缓存，缓存默认是开启的。

```
生产场景下都会开启 Lua 脚本缓存，在 init_by_lua 调用 require 所加载的模块文件
会缓存在全局的 Lua 注册表 package. loaded 中，所以在这里定义的全局的变量和函数可能会污
染命名空间，当然也会影响性能。
```
4 ）lua_code_cache 指令，它的格式如下：
lua_code_cache on|off
lua_code_cache 用于启用或者禁用 Lua 脚本缓存，可以使用的上下文有 http、server、location 配
置块。当缓存关闭时，通过 ngx_lua 提供的每个请求都将在一个单独的 LuaVM 实例中运行。在缓存
关闭的场景下，由 set_by_lua_file、content_by_lua_file、access_by_lua_file 等指令中引用的 Lua 脚本
都将不会被缓存，所有的 Lua 脚本都将从头开始加载。
通过该指令，开发人员可以进行编辑刷新模型的快速开发，改动代码后不需要重启 Nginx。
在缓存关闭的情况下，在 nginx. conf 配置文件中编写的内联 Lua 脚本并不会被重新加载。如由
set_by_lua、content_by_lua、access_by_lua 和 rewrite_by_lua 指定的 Lua 脚本块将不会被反复更新，Lua
代码改动后，需要重启 Nginx。
关闭缓存会对整体性能产生负面的影响。例如，在禁用 Lua 脚本缓存后，一个简单的"hello
world"Lua 示例的性能可能会下降一个数量级。
强烈禁止在生产环境中关闭 Lua 脚本缓存，仅仅可以在开发期间关闭 Lua 脚本缓存。
5 ）set_by_lua 指令，它的格式如下：
set_by_lua $destVar lua-script-str params
set_by_lua 指令的功能类似于 rewrite 模块的 set 指令，具体来说，是将 Lua 脚本块的返回结果设置
在 Nginx 的变量中。set_by_lua 指令所处的上下文和执行阶段与 Nginx 的 set 指令基本相同。
下面是一个简单示例，将 Lua 脚本的相加结果设置给 Nginx 变量$sum，具体的代码如下：


location/set_by_lua_demo{
#set指令定义两个Nginx变量
set$foo 1 ;
set$bar 2 ;
#调用内联代码 ，将结果放入 Nginx 变量$sum
set_by_lua $sum 'returntonumber (ngx. arg[ 1 ])+tonumber (ngx. arg[ 2 ])' $foo
$bar;
echo$sum;
}
上面代码中的 set_by_lua 指令调用一段非常简单的 Lua 脚本，将两个输入参数$a、$b 累加起来，
然后将相加的结果设置到 Nginx 变量$sum 中。
启动 Nginx，访问http://crazydemo.com/set_by_lua_demo，得到的结果如图 9 - 7 所示。

图 9 - 7 set_by_lua 指令通过 Lua 脚本为 Nginx 变量设值
使用 set_by_lua 配置指令时，可以在 Lua 脚本的后面带上一个调用参数列表。在 Lua 脚本中可以
通过 NginxLua 模块内置的 ngx. arg 表容器读取实际参数。

6 ）access_by_lua 指令，它的格式如下：
access_by_lua$destVar lua-script-str
access_by_lua 执行在 HTTP 请求处理 11 个阶段中的 access 阶段，使用 Lua 脚本进行访问控制。
access_by_lua 指令运行于 access 阶段的末尾，因此总是在 allow 和 deny 这样的指令之后运行，虽然它
们同属 access 阶段。一般可以通过 access_by_lua 进行比较复杂的用户权限验证，因为能借助 Lua 脚本
执行一系列复杂的验证操作，比如实时查询数据库或者其他后端服务。
我们来看一个简单的例子，利用 access_by_lua 来实现 ngx_access 模块的 IP 地址过滤功能：
location/access_demo{
access_by_lua'
ngx.log (ngx. DEBUG,"remote_addr=".. ngx. var. remote_addr);
ifngx. var. remote_addr==" 192. 168. 233. 128 "then
return;
end
ngx.exit (ngx. HTTP_UNAUTHORIZED);
';
echo"helloworld";
}
以上代码中能放行的 IP 地址为 192. 168. 233. 128 ，此 IP 地址为笔者机器上的虚拟 CentOS 地址。重
启 Nginx，在 CentOS 上通过 curl 命令访问/access_demo，得到的结果如下：
[ root@localhost ~] #curlhttp :// 192. 168. 233. 1 /access_demo
helloworld
如果请求的来源 IP 地址不是 192. 168. 233. 128 ，则通过 ngx_lua 模块提供的 Lua 函数 ngx. exit 中断当前
的整个请求处理流程，直接返回 401 未授权错误给客户端。如果是 access_by_lua 指令没有中断 HTTP 请求
处理流程，则处于 access 阶段后面的 content 阶段就会顺利执行，echo 指令的结果就能输出给客户端。


7 ）content_by_lua 指令，它的格式如下：
content_by_lua lua-script-str
content_by_lua 指令用于设置执行在 content 阶段的 Lua 代码块，执行结果将作为请求响应的内容。
该指令可以用于 location 上下文，执行于 content 阶段。
需要注意的是，lua-script-str 代码块是在 Nginx 配置文件中编写的字符串形式的 Lua 脚本，可能
需要进行特殊字符转义，所以在 Openrestyv 0. 9. 17 发行版之后的版本不鼓励使用此指令，改为使用
content_by_lua_block 指令。content_by_lua_block 指令在 Lua 代码块中使用花括号（{}）定义，不再
使用字符串分隔符。
至此，主要的 NginxLua 配置指令介绍完了，但是，以上只介绍了 set_by_lua、access_by_lua、
content_by_lua，没有介绍 set_by_lua_file、access_by_lua_file、content_by_lua_file 等指令。后面的系
列指令和前面对应的指令功能是一样的，只是 Lua 脚本所在的位置不是内联在 Nginx 配置文件中，
而是写在了单独的脚本文件中。

#### 9. 4. 4 NginxLua 的内置常量和变量

```
NginxLua 常用的内置变量如表 9 - 3 所示。
表 9 - 3 NginxLua 常用的内置变量
NginxLua 内置变量内部变量说明
ngx. arg n 调 gx 用. a 参 rg 数的值类。型例为如 L，ua 可 ta 以 ble 用，此 ngtxa. balreg. 获 VA 取 R 跟 IA 在 BLseEt_用 by 于_l 获 ua 取指 n 令 gx 后_l 面 ua 的配调置用指参令数后值面的
```
```
ngx. var
```
```
ngx. arg 的类型为 Luatable，ngx. var. VARIABLE 引用某个 Nginx 变量。
如果需要对 Nginx 变量进行赋值，如 ngx. var. b= 2 ，则变量 b 必须提前声明。另外，
可以使用 ngx. var[捕获组序号]的格式，引用 location 配置块中被正则表达式捕获的
捕获组
ngx. ctx n 与 gx 当. c 前 tx 请的求类相型同为（Lu 类 a 似 tabNleg，in 可 x 以变用量来）访问当前请求的 Lua 上下文数据，其生存周期
```
```
ngx. header
```
```
ngx. header 的类型为 Lua table，用于访问 HTTP 响应头，可以通过
ngx. header. HEADER 形式引用某个头，比如通过 ngx. header. set_cookie 可以访问响
应头部的 Cookie 信息
ngx. status 用于设置当前请求的 HTTP 响应码
NginxLua 常用的内置常量如表 9 - 4 所示。
表 9 - 4 NginxLua 常用的内置常量
内置常量类型常量值说明
```
```
核心常量
```
```
ngx.OK ( 0 )
ngx.ERROR (- 1 )
ngx.AGAIN (- 2 )
ngx.DONE (- 4 )
ngx.DECLINED (- 5 )
ngx. null
```

（续表）
内置常量类型常量值说明

HTTP 方法常量

```
ngx. HTTP_GET
ngx. HTTP_HEAD
ngx. HTTP_PUT
ngx. HTTP_POST
ngx. HTTP_DELETE
ngx. HTTP_OPTIONS
ngx. HTTP_MKCOL
ngx. HTTP_COPY
ngx. HTTP_MOVE
ngx. HTTP_PROPFIND
ngx. HTTP_PROPPATCH
ngx. HTTP_LOCK
ngx. HTTP_UNLOCK
ngx. HTTP_PATCH
ngx. HTTP_TRACE
```
HTTP 状态码常量

```
ngx. HTTP_OK ( 200 )
ngx. HTTP_CREATED ( 201 )
ngx. HTTP_SPECIAL_RESPONSE ( 300 )
ngx. HTTP_MOVED_PERMANENTLY ( 301 )
ngx. HTTP_MOVED_TEMPORARILY ( 302 )
ngx. HTTP_SEE_OTHER ( 303 )
ngx. HTTP_NOT_MODIFIED ( 304 )
ngx. HTTP_BAD_REQUEST ( 400 )
ngx. HTTP_UNAUTHORIZED ( 401 )
ngx. HTTP_FORBIDDEN ( 403 )
ngx. HTTP_NOT_FOUND ( 404 )
ngx. HTTP_NOT_ALLOWED ( 405 )
ngx. HTTP_GONE ( 410 )
ngx. HTTP_INTERNAL_SERVER_ERROR ( 500 )
ngx. HTTP_METHOD_NOT_IMPLEMENTED ( 501 )
ngx. HTTP_SERVICE_UNAVAILABLE ( 503 )
ngx. HTTP_GATEWAY_TIMEOUT ( 504 )
```
日志类型常量

```
ngx. STDERR
ngx. EMERG
ngx. ALERT
ngx. CRIT
ngx. ERR
ngx. WARN
ngx. NOTICE
ngx. INFO
ngx. DEBUG
```

### 9. 5 NginxLua 编程实例

本节介绍几个使用 NginxLua 编程的简单实例。
实战案例运行准备：本节涉及的配置文件为源码工程 nginx-lua-demo. conf 文件。在运行本节实
例前，需要修改启动脚本 openresty-start. bat（或 openresty-start. sh）中的 PROJECT_CONF 变量的值，
将其改为 nginx-lua-demo. conf，然后重启 OpenRestry。

#### 9. 5. 1 Lua 脚本获取 URL 中的参数

下面的例子通过 Lua 脚本从 URL 中获取了两个参数，然后进行简单相加，代码如下：
location/add_params_demo{
set_by_lua $sum '
localargs=ngx. req. get_uri_args ();
locala=args["a"];
localb=args["b"];
returna+b;
';
echo"$arg_a+$arg_b=$sum";
}
以上 Lua 脚本通过 ngx_lua 模块的内置方法 ngx. req. get_uri_args () 获取了 URL 后面的请求参数并
保存在 Lua 变量 args 中，然后分别通过该变量获取 a、b 参数值并且相加，相加之后的结果返回之后
被 set_by_lua 指令设置在 Nginx 变量$sum 中。
以上的代码位于 nginx-lua-demo. conf 文件，修改后需重启 OpenRestry，然后可以通过浏览器访
问/add_params_demo，具体的访问结果如图 9 - 8 所示。

图 9 - 8 Lua 脚本从 URL 获取的 a 与 b 两个参数的和
除了通过 ngx. req. get_uri_args () 模块方法获取参数，还可以通过 Nginx 内置变量
$arg_PARAMETER 获取请求参数的值，然后传递给 set_by_lua 指令，具体的代码如下：
location/add_params_demo{
set_by_lua $sum "
locala=tonumber (ngx. arg[ 1 ]);
localb=tonumber (ngx. arg[ 2 ]);
returna+b;
"$arg_a$arg_b;
echo"$arg_a+$arg_b=$sum";
}
以上代码中使用了内置变量$arg_a 和$arg_b 获取请求参数的值，然后通过 set_by_lua 指令将值传
递给了 Lua 脚本。脚本中通过 ngx. arg[n]变量获取传入的指令参数，相加之后的结果返回后被
set_by_lua 指令设置在 Nginx 变量$sum 中。


以上的代码位于 nginx-lua-demo. conf 文件，改动后需重启 OpenRestry，然后可以通过浏览器访
问/add_params_demo_ 2 ，具体的访问结果如图 9 - 9 所示。

```
图 9 - 9 访问结果
```
#### 9. 5. 2 NginxLua 的内置方法

```
NginxLua 的内置方法及其说明如表 9 - 5 所示。
表 9 - 5 NginxLua 的内置方法及其说明
ngx_lua 内置方法方法说明
ngx.log (log_level,...) 按照 log_level 设定的等级输出到 error. log 日志文件
Print (...) 输出到 error. log 日志文件，等价于 ngx.log (ngx. NOTICE,...)
ngx.print (...) 输出响应内容到客户端
ngx.say (...) 输出响应内容到客户端，自动添加'\n'换行符
```
```
ngx.exit (status)
```
```
如果 status>= 200 ，此方法会结束当前请求处理，并且返回 status 状
态到客户端；如果 status== 0 ，此方法会结束当前请求处理的当前阶
段，进入到下一个请求处理阶段
```
```
ngx. send_headers ()
```
```
显式地发送响应头；当调用 ngx. say/ngx. print 时，ngx_lua 模块会自动
发送响应头；可以通过 ngx. headers_sent 内置变量的值判断是否发送
了响应头
ngx.exec (uri, args?) 内部跳转到 uri 地址
ngx.redirect (uri, status?) 外部跳转到 uri 地址
ngx.location.capture (uri, options?) 发起一个子请求
ngx. location. capture_multi (uris) 发{{起 ur 多 i, o 个 pt 子 ion 请 s? 求},。{u 参 ri, 数 oputiroinss 是?}一,. 个..}table，其格式为：
ngx. is_subrequest () 当前请求是否是子请求
ngx.sleep (seconds) 无阻塞的休眠（使用定时器实现）秒数
```
```
ngx. get_phase ()
```
```
获取当前的 Lua 脚本的执行阶段名称。为以下 Lua 脚本的执行阶段
之一：init、init_worker、ssl_cert、ssl_session_fetch、ssl_session_store、
set、rewrite、balancer、access、content、header_filter、body_filter、
log、timer
ngx. req. start_time () 请求的开始时间
ngx. req. http_version () 请求的 HTTP 版本号
ngx. req. raw_header () 获取原始的请求头（包括请求行）
ngx. req. get_method () 获取请求方法
ngx. req. set_method (method) 覆盖当前请求的方法
ngx. req. get_uri_args () 获取请求参数
ngx. req. get_post_args () 获方取法之 po 前 st，请必求须内调容用体，ng 其x.r 用 eq 法. re 和 ad_nbgoxd.rye (q). g 来 et_读 he 取 adbeorsd (y) 体类似，调用此
```

（续表）
ngx_lua 内置方法方法说明

ngx. req. get_headers ()
获取请求头，默认只获取前 100 个；如果当前请求有多个 header 头，则
返回的是 table；如果想要获取全部请求头，可以调用 ngx. req. get_headers ( 0 )
ngx. resp. get_headers 获取响应头，使用的方式类似于 ngx. req. get_headers ()
ngx. req. read_body () 读取当前请求的请求体

ngx. req. set_header (name, value) 为名当称前 na 请 me 求的设 h 置 ea 一 de 个 r 已请经求存头在 he，ad 则 er 进，n 行 am 覆 e 盖为名称，value 为值。如果

ngx. req. clear_header (name) 为当前请求删除名称为 name 的请求头 header
ngx. req. set_body_data（data） 设置当前请求的请求体为 data

ngx. req. init_body (buffer_size?)

为当前请求创建一个空的请求体。如果 buffer_size 参数不为空，则新
请求体的大小为 buffer_size。如果 buffer_size 参数为空，则新请求体
的大小为 client_body_buffer_size 指令设置的请求体大小。如果未做
特定设置，默认的请求体大小为 8 KB（ 32 位系统、x 86 - 64 ）或者 16 KB
（其他的 64 位系统）
ngx. escape_uri (str) 对 uri 字符串进行编码
ngx. unescape_uri (str) 对编码过的 uri 字符串进行解码
ngx. encode_args (table) 将 Luatable 编码为一个参数字符串
ngx. decode_args (str) 将参数字符串解码为一个 Luatable
ngx. encode_base 64 (str) 将字符串 str 编码成 Base^64 摘要
ngx. decode_base 64 (str) 将 Base^64 摘要解码成原始字符串

ngx. hmac_sha 1 (secret, str)

将字符串 str 编码成二进制格式的 hmac_sha 1 哈希摘要 digest，并使
用 secret 进行加密。该二进制摘要可使用 ngx. encode_base 64 (digest)
方法进一步编码成字符串
ngx. md 5 (str) 将字符串 str 编码成十六进制 MD^5 摘要
ngx. md 5 _bin (str) 将字符串 str 编码成二进制 MD 5 摘要
ngx. quote_sql_str (str) SQL 语句转义，按照 MySQL 的格式进行转义
ngx.today () 获取当前日期
ngx.time () 获取 UNIX 时间戳
ngx.now () 获取当前时间
ngx. update_time () 刷新时间后再返回
ngx.localtime () 获取 yyyy-mm-ddhh:mm: ss 格式的本地时间
ngx. cookie_time () 获取可用于 Cookie 值的时间
ngx. http_time () 获取可用于 HTTP 头的时间
ngx. parse_http_time () 解析 HTTP 头的时间
ngx. config. nginx_version () 获取 Nginx 版本号
ngx. config. ngx_lua_version () 获取 ngx_lua 模块版本号
ngx.worker.pid () 获取当前 Worker 进程的 pid


#### 9. 5. 3 通过 ngx. header 设置 HTTP 响应头

ngx_lua 模块可以通过内置变量 ngx. header 来访问和设置 HTTP 响应头字段，ngx. header 的类型为
table，可以通过 ngx. header. HEADER 形式引用某个头。下面是一个简单的使用 ngx. header 设置响应
头的例子，代码如下：
content_by_lua_block{
ngx. header["header 1 "]="value 1 ";
ngx. header. header 2 = 2 ;
ngx. header. set_cookie={'Foo=bar; test=ok; path=/','age= 18 ;path=/'}
ngx.say ("演示程序: ngx. header 的使用")
}
以上代码设置了响应头 header 1 的值为 value 1 、响应头 header 2 的值为 2 。前面介绍过，
ngx. header. set_cookie 变量用于设置响应头 set-cookie 的值，使用 table 类型的对象可以一次设置多个
set-cookie 值。
以上的代码位于 nginx-lua-demo. conf 文件，修改该文件后重启 OpenRestry，然后可以通过火狐
浏览器访问/header_demo? foo=bar 并查看响应头信息，具体的访问结果如图 9 - 10 所示。

图 9 - 10 通过火狐浏览器访问/header_demo? foo=bar 并查看响应头信息
作为案例，本小节将重点介绍如何通过 ngx. header. set_cookie 访问保存在响应头部的 Cookie 信息。
熟悉 HTTP 协议的应该都知道，Cookie 是通过请求的 set-cookie 响应头来保存的，HTTP 响应内容中可
以包含多个 set-cookie 响应头，一个 set-cookie 响应头的值通常为一个字符串，该字符串大致包含如
表 9 - 6 所示的 Cookie 信息或属性（不区分字母大小写）。


```
表 9 - 6 Cookie 信息或属性及其描述
Cookie 信息（或属性） 说明
```
```
Cookie 名称
```
```
Cookie 名称的组成字符，只能使用能用于 URL 中的字符，一般为字母及数字，
不能包含特殊字符，如有特殊字符则先要进行转码。Cookie 名称为 Cookie 字符
串的第一组“键－值对”中的键
Cookie 值 CCooookkiiee 值值的为字 Co 符 ok 组 ie 成字规符则串，的和第 C 一 oo 组 kie“名键称－相值同对，”如中有的特值殊字符则先要进行转码。
```
```
expires C 将 o 这 ok 个 ie 过 Co 期 ok 日 ie 期删，除这，是当一不个设 G 置 MeTxp 格 ire 式 s 属的性时值间时，，当 C 过 o 了 ok 这 ie 个在日浏期览之器后关，闭浏后览消器失就会
```
```
path C 路 o 径 ok 的 ie 值的一访般问设路为径“，/此”属，性以设表置示指同定一路个径站下点的的页所面有才页可面以都访可问以该访问 Co 这 ok 个 ie。Co 访 ok 问 ie
```
```
domain
```
```
Cookie 的访问域名，此属性设置指定域名下的页面才可以访问该 Cookie。例如
要让 Cookie 只能在a.test. com 域名下访问，不能在b.test. com 域名下访问，则可
将其 domain 属性设置成a.test. com
```
```
secure
```
```
Cookie 的安全属性，此属性设置该 Cookie 是否只能通过 HTTPS 协议访问。一
般的 Cookie 使用 HTTP 协议即可访问，如果设置了 Secure 属性（没有值），则
只有使用 HTTPS 协议时 Cookie 才可以被页面访问
HttpOnly 如法果读取 Co 到 okCieo 设 ok 置 ie 信了息 H。ttpHOtntplyO 属 nly 性属，性那和么 S 通 ec 过 ur 程 e 安序全（属 JS 性脚一本样、都 A 没 pp 有 let 值等）将无
```
前面的演示实例设置了两个 Cookie，可以通过 Chrome 浏览器访问/header_demo? foo=bar，通过
其“检查”面板查看所设置的 Foo 和 age 这两个 Cookie 的属性，具体如图 9 - 11 所示。

图 9 - 11 通过 Chrome 浏览器查看 Foo 和 age 这两个 Cookie 的属性
Cookie 为什么需要设置 HttpOnly 和 Secure 属性呢？当设置了 HttpOnly 属性时，通过脚本将无法
获取 Cookie 信息，主要用于防止 XSS 攻击。而一旦设置了 Secure 属性，前后端之间只能在 HTTPS 协
议通信情况下，浏览器才能访问 Cookie，使用 HTTP 协议时浏览器无法获取 Cookie 信息，这同样是
对 Cookie 信息的保护。
不是所有的场景都需要使用 HTTPS 通信协议。但是，为了通信安全某些场景下只能在前后端
之间使用 HTTPS 协议通信，如微信小程序的官网要求必须要使用 HTTPS 协议。这种场景下，在内
网环境可以继续使用 HTTP 通信协议（毕竟开发和测试都方便、性能也更好），然后通过 Nginx 网关
完成外部 HTTPS 协议到内部 HTTP 协议的转换。此时，Nginx 外部网关可以对 Cookie 属性进行修改，
增加 Secure 安全属性。
另外，大部分场景下确实不需要在前端脚本中访问 Cookie，Cookie 信息仅在后端 Java 容器中访
问（如 sessionid），此时，可以对 Cookie 属性进行修改，增加 HttpOnly 安全属性。修改完成后，客


户端通过程序（JS 脚本、Applet 等）将无法读取 Cookie 信息，这将有助于缓解跨站点脚本攻击，降
低 Cookie 信息泄露的风险。
为 Cookie 增加 HttpOnly 安全属性的操作，可以通过 Servlet 过滤器的形式在 Java 容器中完成，参
考代码如下：
//过滤器：修改响应中的 Cookie 头
publicclassCookieHttpOnlyFilterimplementsFilter
{
//过滤器的方法，迭代所有的 Cookie 添加 HttpOnly 安全后缀
publicvoiddoFilter (ServletRequestrequest, ServletResponseresponse,
FilterChainfilterChain) throwsIOException, ServletException
HttpServletRequestreq=(HttpServletRequest) request;
HttpServletResponseresp=(HttpServletResponse) response;
Cookie[]cookies=req.getCookies ();
if (cookies!=null){
for (Cookiecookie:cookies){
Stringname=cookie.getName ()
Stringvalue=cookie.getValue ();
StringBuilderbuilder=newStringBuilder ();
builder.append (name+"="+value+";");
builder.append ("; httpOnly");
resp.setHeader ("Set-Cookie",builder.toString ());
}
filterChain.doFilter (request, response);
}
}
为 Cookie 添加 HttpOnly（甚至 Secure）安全属性的操作，除了可以在 Java 容器中完成之外，更
好的方式是在反向代理外部网关 Nginx 中完成，参考代码如下：
#模拟上游服务
location/header_demo{
content_by_lua_block{
ngx. header["header 1 "]="value 1 ";
ngx. header. header 2 = 2 ;
ngx. header. set_cookie={'Foo=bar; test=ok; path=/','age= 18 ;path=/'}
ngx.say ("演示程序: ngx. header 的使用")
}
}
#模拟反向代理外部网关
location/header_filter_demo{
proxy_passhttp:// 127. 0. 0. 1 /header_demo;
header_filter_by_lua_block{
localcookies=ngx. header. set_cookie
ifcookiesthen
iftype (cookies)=="table"then
localcookie={}
fork,vinpairs (cookies) do
cookie[k]=v.."; Secure; httponly"
end
ngx. header. set_cookie=cookie
else
ngx. header. set_cookie=cookies.."; Secure; httponly"
end
end
}
}


以上代码位于 nginx-lua-demo. conf 文件，修改该文件后重启 OpenRestry，然后使用 Chrome 浏览
器访问/header_filter_demo? foo=bar，通过其“检查”面板查看所修改的 Foo 和 age 这两个 Cookie 的
HttpOnly 属性值，具体如图 9 - 12 所示。

图 9 - 12 通过 Chrome 浏览器查看 Foo 和 age 这两个 Cookie 的 HttpOnly 属性值
通过 Chrome 浏览器可以看到，Foo 和 age 这两个 Cookie 的 HttpOnly 属性列已经被勾选了，而这两
个 Cookie 的 Secure 属性列仍然没有被勾选，尽管已经通过 Nginx 为它们增加 Secure 属性，原因是以上
程序并没有配置 HTTPS 协议。

#### 9. 5. 4 Lua 访问 Nginx 变量

前面介绍过 Nginx 提供了很多内置变量，如：
1 ）$arg_PARAMETER 可以访问请求参数（查询字符串）中名称为 PARAMETER 的参数值。
2 ）$args 可以访问整个请求参数字符（查询字符串）。
3 ）$binary_remote_addr 可以获取二进制码形式的客户端地址。
4 ）$uri 可以获取当前请求的 URI（不带请求参数）。
5 ）$request_method 可以获取当前请求的 HTTP 协议方法，通常为 GET 或 POST。
6 ）$server_protocol 可以获取请求使用的协议，通常是 HTTP/ 1. 0 或 HTTP/ 1. 1 。
除了内置变量，还可以在配置文件中使用 set 指令定义一些 Nginx 变量，无论是内部变量还是自
定义变量都可以在 Lua 代码中通过 ngx. var 进行访问。下面是一个通过 ngx. var 访问 Nginx 变量的实例，
具体如下：
#演示通过Lua访问Nginx变量
location/lua_var_demo{
#set指令自定义一个Nginx变量
set $hello world;
content_by_lua_block{
localbasic=require ("luaScript. module. common. basic");

- -定义一个 Luatable，暂存需要输出的 Nginx 内置变量
localvars={};
vars. remote_addr= ngx. var. remote_addr;
vars. request_uri= ngx. var. request_uri;
vars. query_string= ngx. var. query_string;
vars. uri=ngx. var. uri;


```
vars. nginx_version= ngx. var. nginx_version;
vars. server_protocol= ngx. var. server_protocol;
vars. remote_user= ngx. var. remote_user;
vars. request_filename= ngx. var. request_filename;
vars. request_method= ngx. var. request_method;
vars. document_root= ngx. var. document_root;
vars. body_bytes_sent= ngx. var. body_bytes_sent;
vars. binary_remote_addr= ngx. var. binary_remote_addr;
vars. args = ngx. var. args;
```
- -通过内置变量访问请求参数
vars. foo = ngx. var. arg_foo;
ngx.say ("演示程序: 将内置变量返回给客户端<br>");
- -使用自定义函数，将 Luatable 转换成字符串，然后输出
localstr=basic.tableToStr (vars,",<br>");
ngx.say (str);
ngx.say ("<br>演示程序: 将普通变量返回给客户端<br>");
- -访问自定义 Nginx 变量 hello
localhello= ngx. var. hello;
ngx.say ("hello=".. hello);
}
}
以上代码位于 nginx-lua-demo. conf 文件，修改后需要重启 OpenRestry，然后可以使用浏览器访
问/lua_var_demo? foo=bar，具体的访问结果如图 9 - 13 所示。

```
图 9 - 13 通过 ngx. var 访问 Nginx 变量
```
#### 9. 5. 5 Lua 访问请求上下文变量

Nginx 执行 Lua 脚本会涉及很多的阶段，如 init、init_worker、ssl_cert、ssl_session_fetch、
ssl_session_store、set、rewrite、balancer、access、content、header_filter、body_filter、log、timer。每
一个阶段都可以嵌入不同的 Lua 脚本，不同阶段的 Lua 脚本可以通过 ngx. ctx 进行上下文变量的共享。
ngx. ctx 上下文实质是一个 Luatable，其生存周期与当前请求相同，当前请求不同阶段嵌入的 Lua
脚本都可以读写 ngx. ctx 表中的属性。一个简单的实例如下：
#Lua访问请求上下文变量
location/ctx_demo{
rewrite_by_lua_block{

- -在上下文设置属性 var 1
ngx. ctx. var 1 = 1 ;
}
access_by_lua_block{


- -在上下文设置属性 var 2
ngx. ctx. var 2 = 10 ;
}
content_by_lua_block{
localbasic=require ("luaScript. module. common. basic");
- -在上下文设置属性 var 3
ngx. ctx. var 3 = 100 ;
- -三个上下文属性值求和
localresult=ngx. ctx. var 1 +ngx. ctx. var 2 +ngx. ctx. var 3 ;
ngx.say (result);
ngx. ctx. sum=result;
- -使用自定义函数，将 Luatable 转换成字符串
localstr=basic.tableToStr (ngx. ctx,",<br>");
ngx.say ("<br>");
ngx.say (str);
}
}
以上代码位于 nginx-lua-demo. conf 文件，修改后需重
启 OpenRestry，然后可以使用浏览器访问/ctx_demo，具
体的访问结果如图 9 - 14 所示。
通过上面的例子可以看出，ngx. ctx 表中定义的属性
可以在请求处理的 rewrite（重写）、access（访问）、content
（内容）等各处理阶段进行共享。另外，在 ngx_lua 模块
中，每个请求（包括子请求）都有一份独立的 ngx. ctx 表。

9. (^6) 重定向与内部子请求
Nginx 的 rewrite 指令不仅可以在 Nginx 内部的 server、location 之间进行跳转，还可以进行外部链
接的重定向。通过 ngx_lua 模块的 Lua 函数，除了能实现 Nginx 的 rewrite 指令的功能之外，还能顺利
完成内部子请求、并发子请求等复杂功能。
本节实战案例运行准备：本节涉及的配置文件为源码工程 nginx-lua-demo. conf。在运行本节实
例前，需要修改启动脚本 openresty-start. bat（或 openresty-start. sh）中的 PROJECT_CONF 变量的值，
将其改为 nginx-lua-demo. conf，然后重启 OpenRestry。

#### 9. 6. 1 NginxLua 内部重定向

ngx_lua 模块可以实现 Nginx 的 rewrite 指令的类似功能，该模块提供了两个对应的 API 来实现重
定向的功能，主要有：

```
1 ）ngx.exec (uri, args?)：内部重定向。
2 ）ngx.redirect (uri, status?)：外部重定向。
首先看第一个 ngx.exec (uri, args?) 内部重定向方法，它等价于下面的 rewrite 指令：
rewrite regrex replacement last;
下面是三个使用 ngx. exec 进行重定向的例子，第一个例子是不带参数的重定向：
```
```
图 9 - 14 通过 ngx. ctx 上下文设置变量
```

#重定向到/internal/sum
ngx.exec ('/internal/sum');
第二个例子是使用字符串作为追加参数的重定向例子：
#重定向到/internal/sum ? a= 3 &b= 5 ，并且追加参数 c= 6
ngx.exec ('/internal/sum? a= 3 &b= 5 ','c= 6 ');
第三个例子是使用 Luatable 作为追加参数的重定向例子：
#重定向到/internal/sum ，并且追加参数? a= 3 &b= 5 &c= 6
ngx.exec ('/internal/sum',{a= 3 ,b= 5 ,c= 6 });
下面是一个完整的 ngx. exec 重定向演示例子，通过内部重定向完成 3 个参数的累加，具体代码
如下：
location/internal/sum{
internal; #只允许内部调用
content_by_lua_block{

- -通过 ngx. var 访问 Nginx 变量
localarg_a=tonumber (ngx. var. arg_a);
localarg_b=tonumber (ngx. var. arg_b);
localarg_c=tonumber (ngx. var. arg_c);
- - 3 个参数值求和
localsum=arg_a+arg_b+arg_c;
- -输出结果
ngx.say (arg_a,"+", arg_b,"+", arg_c,"=", sum);
}
}
location/sum{
content_by_lua_block{
- - localres=ngx.exec ("/internal/sum",'a= 100 &b= 10 &c= 1 ')；
- - 内部重定向到/internal/sum
returnngx.exec ("/internal/sum",{a= 100 ,b= 10 ,c= 1 });
}
}
以上代码位于 nginx-lua-demo. conf 文件，修改后需重启 OpenRestry，然后可以使用浏览器访问
/sum，具体的访问结果如图 9 - 15 所示。

```
图^9 -^15 NginxLua 内部重定向演示
ngx. exec 的使用有两点需要注意：
1 ）如果有 args 参数，参数可以是字符串的形式，也可是 Luatable 的形式，如下所示：
ngx.exec ("/internal/sum",'a= 100 &b= 5 ')；--参数是字符串的形式
ngx.exec ("/internal/sum",{a= 100 ,b= 5 }); --参数是 Luatable 的形式
2 ）该方法可能不会主动返回，因此建议在调用该方法时最好明确加上 return，如下所示：
returnngx.exec (...)
```

#### 9. 6. 2 NginxLua 外部重定向

ngx_lua 模块的外部重定向方法为 ngx. redirect，它的语法格式为：
ngx.redirect (uri, status?)
ngx. redirect 外部重定向方法与 ngx. exec 内部重定向方法不同，外部重定向将通过客户端进行二
次跳转，所以 ngx. redirect 方法会产生额外的网络流量，该方法的第二个参数为响应状态码，可以传
递 301 / 302 / 303 / 307 / 308 重定向状态码。其中， 301 、 302 是 HTTP 1. 0 协议定义的响应码， 303 、 307 、
308 是 HTTP 1. 1 协议定义的响应码。
如果不指定 status 值，该方法默认的响应状态为 302 （ngx. HTTP_MOVED_TEMPORARILY）临
时重定向。下面是一个 ngx. redirect 方法与 rewrite 指令达到完全一模一样的跳转效果的实例，代码如下：
location/sum 2 {
content_by_lua_block{

- - 外部重定向
returnngx.redirect ("/internal/sum? a= 100 &b= 10 &c= 1 ");
}
}
location/sum 3 {
rewrite ^/sum 3 "/internal/sum? a= 100 &b= 10 &c= 1 " redirect;
}
以上代码位于 nginx-lua-demo. conf 文件，修改后需重启 OpenRestry，然后可以使用浏览器访问
/sum 2 或者/sum 3 ，具体的访问结果如图 9 - 16 所示。

图 9 - 16 NginxLua 外部重定向演示
如果指定 status 值为 301 ，对应的常量为（ngx. HTTP_MOVED_PERMANENTLY）永久重定向，
对应的 rewrite 指令标志位为 permanent。下面的例子中，ngx. redirect 方法与 rewrite 指令达到了完全一
模一样的跳转效果，代码如下：
location/sum 4 {
content_by_lua_block{

- - 外部重定向
returnngx.redirect ("/internal/sum? a= 100 &b= 10 &c= 1 ",
ngx. HTTP_MOVED_PERMANENTLY);
}
}
location/sum 5 {
rewrite ^/sum 5 "/internal/sum? a= 100 &b= 10 &c= 1 " permanent;
}
由于通过浏览器访问时已经发生了二次跳转，因此其“检查”面板已经查看不到跳转前的链
接（如/sum 4 、/sum 5 ）的响应码，但是可以通过抓包工具查看。/sum 5 的响应码具体如图 9 - 17 所示。


图 9 - 17 通过抓包工具查看/sum 5 的响应码
下面是一个综合性的跳转演示实例，通过 ngx. redirect 方法与 rewrite 指令进行了三种方式的外部
跳转，跳转到www.cnblogs.com博客园网站。具体的代码如下：
#使用location指令后面的正则表达式进行URL后缀捕获
location~*/blog/(.*){
content_by_lua_block{

- -使用 ngx. redirect 方法进行外部重定向
- -博客 URI 为正则捕获组 1
returnngx.redirect ("https://www.cnblogs.com/"..ngx.var[ 1 ]);
}
}
location~*/blog 1 /*{
#使用rewrite指令后面的正则表达式进行URL后缀捕获
rewrite ^/blog 1 /(.*)$ 1 break;
content_by_lua_block{
- -使用 ngx. redirect 方法进行外部重定向
- -博客 URI 为正则捕获组 1
returnngx.redirect ("https://www.cnblogs.com/"..ngx.var[ 1 ]);
}
}
location~*/blog 2 /*{
#使用rewrite指令进行外部重定向 ，并捕获博客 URI
rewrite ^/blog 2 /(.*) https://www.cnblogs.com/$ 1 redirect;
}
}
以疯狂创客圈社群的博客首页为例，外部跳转演示需要用到的 4 个地址，分别如下：
#以下为疯狂创客圈社群的博客首页原地址
https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
#以下为/blog/ (.*) 配置块的二次跳转演示地址
[http://nginx.server/blog/crazymakercircle/p/](http://nginx.server/blog/crazymakercircle/p/) 9904544 .html
#以下为/blog 1 /*配置块的二次跳转演示地址
[http://nginx.server/blog](http://nginx.server/blog) 1 /crazymakercircle/p/ 9904544 .html
#以下为/blog 2 /*配置块的二次跳转演示地址
[http://nginx.server/blog](http://nginx.server/blog) 2 /crazymakercircle/p/ 9904544 .html
通过浏览器访问以上的二次跳转演示地址（主机名 nginx. server 需要指向 Nginx 的 IP），发现都
能正常跳转到原地址（疯狂创客圈社群的博客首页）。


以上代码中，通过 location 指令、rewrite 指令进行了正则捕获，并使用 ngx. var[捕获组编号]访问
捕获到的捕获组，也就是博客地址的 URI 部分。
通过浏览器访问以上 4 个地址，最终的结果都为疯狂创客圈社群的博客首页，只是后面的 3 个
经过了跳转而已。跳转的结果具体如图 9 - 18 所示。

```
图 9 - 18 综合性跳转演示实例的跳转结果
ngx. redirect 方法不会主动返回，因此建议在调用该方法时明确加上 return，具体如下：
returnngx.redirect ("https://www.cnblogs.com/"..ngx.var[ 1 ]);
```
#### 9. 6. 3 ngx. location. capture 子请求

Nginx 子请求并非 HTTP 协议标准的实现是 Nginx 所特有的设计，主要是为了提高内部对单个客
户端请求处理的并发能力。
如果某个客户端的请求（可以理解为主请求）访问了多个资源，为了提高效率，可以为每一
处资源访问建立单个子请求，并让所有子请求同时进行，以提升效率。
子请求并不是由客户端直接发起的，它是由 Nginx 服务器在处理客户端请求时根据自身逻辑需
要而内部建立的新请求。因此子请求只在 Nginx 服务器内部进行处理，不会与客户端进行交互。
通常情况下，为保护子请求所定义的内部接口，会把这些接口设置为 internal，防止外部直接
访问。这么做的主要好处是可以让这个内部接口相对独立，不受外界干扰。
发起单个子请求，可以使用的 LuaAPI 为 ngx. location. capture 方法，其格式如下：
ngx.location.capture (uri, options?)
capture 方法的第二个参数 options 是一个 table 容器，用于设置子请求相关的选项，大致有如下可
以设置的选项：

```
1 ）method：子请求的方法，默认为 ngx. HTTP_GET 常量。
2 ）body：传给子请求的请求体，仅限于 string 或 nil。
3 ）args：传给子请求的请求参数，支持 string 或 table。
4 ）vars：传给子请求的变量表，仅限于 table。
5 ）ctx：父子请求共享的变量表 table。
```

6 ）copy_all_vars：复制所有变量给子请求。
7 ）share_all_vars：父子请求共享所有变量。
8 ）always_forward_body：用于设置是否转发请求体。
下面是一个综合性实例，包含两个请求接口，具体如下：
外部访问接口：/goods/detail/ 100 ?foo=bar。
内部访问接口：/internal/detail/ 100 。
外部接口专供外部访问，在准备好必要的请求参数、上下文环境变量、请求体之后，调用内
部访问接口获取执行结果，然后返回给客户端。外部接口的演示代码具体如下：
#向外公开的请求
location~/goods/detail/([ 0 - 9 ]+){
set$goodsId$ 1 ; #将location的正则捕获组 1 ，赋值到变量$goodsId
set$var 1 '';
set$var 2 '';
content_by_lua_block{

- -解析 body 参数之前一定要先读取请求体 requestbody
ngx. req. read_body ();
- -组装 uri
localuri="/internal/detail/".. ngx. var. goodsId;
localrequest_method=ngx. var. request_method;
- -获取父请求的参数
localargs=ngx. req. get_uri_args ();
localshareCtx={c 1 ="v 1 ", other="othervalue"}
localres=ngx.location.capture (uri,{
    method=ngx. HTTP_GET,
    args=args, --转发父请求的参数给子请求
    body='customed requestbody',
    vars={var 1 ="value 1 ", var 2 ="value 2 "},--传递的变量
    always_forward_body=true,--转发父请求的请求体
    ctx=shareCtx, --共享给子请求的上下文 table
});
ngx.say ("childres. status: ", res. status);
ngx.say (res. body);
ngx.say ("<br>shareCtx. c 1 =", shareCtx. c 1 );
}
}
内部接口用于模拟上游的服务（如 Java 容器服务），外部客户端是不能直接访问内部接口的。
内部接口的演示代码具体如下：
#内部请求
location~/internal/detail/([ 0 - 9 ]+){
internal;
#将捕获组 1 的值放到自定义 Nginx 变量$goodsId
set$goodsId$ 1 ;
content_by_lua_block{
ngx. req. read_body ();
ngx.say ("<br><hr>childstart：");
- -访问父请求传递的参数
localargs=ngx. req. get_uri_args ()
ngx.say (",<br>foo=", args. foo);


- -访问父请求传递的请求体
localdata=ngx. req. get_body_data ()
ngx.say (",<br>data=", data);
- -访问 Nginx 定义的变量
ngx.say ("<br>goodsId=", ngx. var. goodsId);
- -访问父请求传递的变量
ngx.say (",<br>var. var 1 =", ngx. var. var 1 );
- -访问父请求传递的共享上下文，并修改其属性
ngx.say (",<br>ngx. ctx. c 1 =", ngx. ctx. c 1 );
ngx.say ("<br>childend<hr>");
ngx. ctx. c 1 ="changedvaluebychild";
}
}
以上代码位于 nginx-lua-demo. conf 文件，修改后需重启 OpenRestry，然后可以使用浏览器访问
/goods/detail/ 100 ?foo=bar，具体的访问结果如图 9 - 19 所示。

图 9 - 19 浏览器访问/goods/detail/ 100 ?foo=bar 的结果
capture 方法的第二个参数 options 是一个 table 容器，用于设置子请求的选项。options 的 method
属性用于指定子请求的 method 类型，具体示例如下：
localres=ngx.location.capture (uri,{
method=ngx. HTTP_PUT,--method 为 PUT 类型的请求
...
});
method 属性值只接收 NginxLua 内部定义的请求类型的常量，如 ngx. HTTP_POST 表示 POST 类
型的请求，ngx. HTTP_GET 表示 GET 类型的请求。
options 的 body 属性指定子请求的请求体（仅接收字符串值），其请求体的内容仅限于 string 或
nil，具体示例如下：
localres=ngx.location.capture (uri,{
body='customed requestbody', --转发给子请求的请求体
...
});
options 的 args 属性用于指定子请求的 URI 请求参数（可以是字符串或者 Lua 表容器），具体示例
如下：
localres=ngx.location.capture (uri,{


args =ngx. req. get_uri_args (),--将父请求的参数 table，转发给子请求
...
});
上面的例子假定了父请求的类型为 HTTPGET，使用 ngx. req. get_uri_args () 获取父请求的参数列
表，原样转发给子请求。
options 的 vars 属性是一个 Lua 表容器，用于设置传递给子请求的 Nginx 变量。具体示例如下：
...
set$var 1 ''; #提前定义好变量
set$var 2 ''; #提前定义好变量
content_by_lua_block{
...
localres=ngx.location.capture (uri,{
vars={var 1 ="value 1 ", var 2 ="value 2 "}, --传递的 Nginx 变量
...
});
}
在通过 vars 向子请求中传递 Nginx 变量时，变量需要提前定义，否则将报出变量未定义的错误。
options 的 ctx 上下文属性指定一个 Lua 表作为子请求的 ngx. ctx 表。当然，可以直接将 ctx 属性值
设置为当前请求的 ngx. ctx 上下文表。options 的 ctx 的使用示例如下：
localc={c 1 ="v 1 ", other="othervalue"}
localres=ngx.location.capture (uri,{
...
ctx=c, --设置子请求的 ngx. ctx 上下文表
});
父请求如果修改了 ctx 表中的成员，子请求可以通过 ngx. ctx 获取；反过来，子请求也可以修改
ngx. ctx 中的成员，父请求通过 ctx 表获取。通过 ctx 属性值，可以方便地让父请求和子请求进行上下
文变量共享。
options 的 always_forward_body 属性用于设置是否转发请求体。当设置为 true 时，父请求中的请
求体将转发到子请求。always_forward_body 属性的使用示例如下：
localres=ngx.location.capture (uri,{
method=ngx. HTTP_GET,
always_forward_body=true, --转发父请求的 requestbody
});
ngx. location. capture 只能发起到当前 Nginx 服务器的内部路径的子请求，假如需要发起外部
HTTP 路径的子请求，则需要与 location（或者 upstream）反向代理配置去配合实现。

#### 9. 6. 4 ngx. location. capture_multi 并发子请求

经过解耦之后，微服务架构将提供大量的细粒度的接口，一次客户端（例如 App、网页端）请
求往往要调用多个微服务接口才能获取完整的页面内容。这种场景下可以通过网关（如 Nginx）进
行上游接口合并。
在 OpenResty 中，ngx. location. capture_multi 可以用于上游接口合并的场景，该方法可以完成内
部多个子请求的并发访问。其格式如下：
ngx. location. capture_multi ({{uri, options?},{uri, options?},...})


capture_multi 可以一次发送多个内部子请求，每一个子请求的参数使用方式与 capture 方法相同。
调用 capture_multi 前，可以把所有的子请求加入一个 table 容器表中作为调用参数传入；capture_multi
返回后，可以将其结果再用花括号（{}）包装成一个 table，方便后面的迭代处理。
下面是一个综合性实例，通过 capture_multi 方法一次并发地请求两个内部接口，具体代码如下：
#发起两个子请求 ，一个 get，一个 post
location/capture_multi_demo{
content_by_lua_block {
localpostBody=ngx. encode_args ({post_k 1 = 32 ,post_k 2 ="post_v 2 "});
localreqs={};
table.insert (reqs,{"/print_get_param",{args="a= 3 &b= 4 " }});
table.insert (reqs,{"/print_post_param",{method=ngx. HTTP_POST, body=
postBody}});

- -统一发并发请求，然后等待结果
localresps={ngx. location. capture_multi (reqs)};
- -迭代结果列表
fori,resinipairs (resps) do
    ngx.say ("child res. status: ", res. status,"<br>");
    ngx.say ("child res. body: ", res. body,"<br><br>");
end
}
}
两个内部接口用于模拟上游的服务（如 Java 容器服务），客户端是不能直接访问内部接口的。
两个内部接口的代码具体如下：
#模拟上游接口一 ：输出 get 请求的参数
location/print_get_param{
internal;
content_by_lua_block{
ngx.say ("<br><hr>childstart：");
localarg=ngx. req. get_uri_args ()
fork,vinpairs (arg) do
ngx.say ("<br>[GET]key: ", k,"v: ", v)
end
ngx.say ("<br>childend<hr>");
}
}
#模拟上游接口二 ：输出 post 请求的参数
location/print_post_param{
internal;
content_by_lua_block{
ngx.say ("<br><hr>childstart：");
ngx. req. read_body ()--解析 body 参数之前一定要先读取 body
localarg=ngx. req. get_post_args ();
fork,vinpairs (arg) do
ngx.say ("<br>[POST]key: ", k,"v: ", v)
end
ngx.say ("<br>childend<hr>");
}
}
两个内部接口的功能很简单，主要为获取请求参数（或者请求体），然后输出到客户端。以
上代码位于 nginx-lua-demo. conf 文件，修改后需重启 OpenRestry，然后可以使用浏览器访问外部接
口/capture_multi_demo，具体的访问结果如图 9 - 20 所示。


图 9 - 20 外部接口/capture_multi_demo 的访问结果
在所有子请求终止之前，ngx. location. capture_multi (...) 函数不会返回。此函数的耗时是单个子
请求的最长延迟，而不是所有子请求的耗时总和，因为所有的子请求是并发执行的。
在上面的例子中，利用 ngx. location. capture_multi (...) 完成了两个子请求的并发执行。当两个请
求没有先后依赖时，这个方法可以极大提高请求效率。如果两个请求各自需要 500 毫秒，顺序执行
需要 1000 毫秒，但是通过并发子请求，完成两个请求只需要 500 毫秒。

### 9. 7 NginxLua 操作 Redis

本节介绍如何使用开源的 lua-resty-redis 模块在 Lua 脚本中连接和访问 Redis。
本节实战案例运行准备：本节涉及的配置文件为源码工程 nginx-redis-demo. conf。在运行本节
实例前，需要修改启动脚本 openresty-start. bat（或 openresty-start. sh）中的 PROJECT_CONF 变量的值，
将其改为 nginx-redis-demo. conf，然后重启 OpenRestry。

#### 9. 7. 1 Redis 的 CRUD 基本操作

使用 Lua 模块 lua-resty-redis 之前，需要在官方网址下载 resty/redis. lua 库文件，然后将该库文件
加入项目工程所在的 Lua 外部库路径。lua-resty-redis 官方已经声明，大部分的 Redis 操作命令都实现
了同名的 LuaAPI 方法。有关 Redis 的安装使用及其具体的操作命令和命令参数，可以参考笔者的
《Java 高并发核心编程卷 1 （加强版）：NIO、Netty、Redis、ZooKeeper》一书。
下面是一个简单的使用 Lua 模块 lua-resty-redis 操作 Redis 的实例，代码如下：
localredis=require"resty. redis"
localconfig=require ("luaScript. module. config. redis-config");

- --启动调试
localmobdebug=require ("luaScript. initial. mobdebug");
mobdebug.start ();
- -设置超时时长
localred=redis: new ()
- -设置超时时长，单位为毫秒（ms）
red: set_timeouts (config. timeout, config. timeout, config. timeout)


- -连接服务器
localok, err=red: connect (config. host_name, config. port)
ifnotokthen
    ngx.say ("failedtoconnect: ", err)
    return
end
- -设置值
ok, err=red: set ("dog","ananimal")
ifnotokthen
    ngx.say ("failedtosetdog: ", err,"<br>")
    return
else
    ngx.say ("setdog: ok","<br>")
end
- -取值
localres, err=red: get ("dog")
- -判空演示
ifnotresorres==ngx. nullthen
    ngx.say ("failedtogetdog: ", err,"<br>")
    return
else
    ngx.say ("getdog: ok","<br>", res,"<br>")
end
- -批量操作，减少网络 IO 次数
red: init_pipeline ()
red: set ("cat","cat 1 ")
red: set ("horse","horse 1 ")
red: get ("cat")
red: get ("horse")
red: get ("dog")
localresults, err=red: commit_pipeline ()
ifnotresultsthen
    ngx.say ("failedtocommitthepipelinedrequests: ", err)
    return
end
fori,resinipairs (results) do
    iftype (res)=="table"then
       ifres[ 1 ]==falsethen
          ngx.say ("failedtoruncommand", i,": ", res[ 2 ],"<br>")
       else
          - -处理表容器
          ngx.say ("succeedtoruncommand", i,": ", res[i],"<br>")
       end
    else
       - -处理变量
       ngx.say ("succeedtoruncommand", i,": ", res,"<br>")
    end
end
- -简单的关闭连接
localok, err=red: close ()
ifnotokthen
    ngx.say ("failedtoclose: ", err)
    return
else
    ngx.say ("succeedtocloseredis")
end


以上 Lua 脚本位于工程目录下的 luaScript/redis/RedisDemo. lua 文件，大致完成了如下 Redis 的操作：
1 ）连接 Redis 服务器。
2 ）根据键（Key）设置缓存值。
3 ）根据键（Key）获取缓存值。
4 ）批量 Redis 操作。
5 ）简单地关闭 Redis 连接。
在 nginx-redis-demo. conf 配置文件中编写一个 location 配置块来使用该脚本，具体代码如下：
#redisCRUD简单操作演示
location/redis_demo{
content_by_lua_file luaScript/redis/RedisDemo. lua;
}
修改了 nginx-redis-demo. conf 文件后需要重启
OpenRestry，然后可以使用浏览器访问其地址
/redis_demo，具体的访问结果如图 9 - 21 所示。
RedisDemo. lua 通过 require 导入了 redis-config. lua
配置文件，该文件定义了整个项目都需要使用的全局
Redis 配置信息，其代码如下：

- -定义一个统一 Redis 配置模块
- -统一的模块对象
local_Module={
    - -Redis 服务器的地址
    host_name=" 192. 168. 233. 128 ";
    - -redis 服务器的端口
    port=" 6379 ";
    - -Redis 服务器的数据库
    db=" 0 ";
    - -Redis 服务器的密码
    password=' 123456 ';
    - -连接超时时长
    timeout= 20000 ;
    - -线程池的连接数量
    pool_size= 100 ;
    - -最大的空闲时间，单位为毫秒
    pool_max_idle_time= 10000 ;
}
return_Module

#### 9. 7. 2 实战：封装一个操作 Redis 的基础类

通过 Lua 操作 Redis 会涉及获取连接、操作数据、连接回收等基础性工作，这里建议将这些基础
性工作封装到一个 Redis 操作的基础类，主要的代码如下：
localredis=require"resty. redis"
localbasic=require ("luaScript. module. common. basic");
localconfig=require ("luaScript. module. config. redis-config");

- -连接池大小
localpool_size=config. pool_size;
- -最大的空闲时间，单位为毫秒

```
图 9 - 21 RedisCRUD 简单操作演示的访问结果
```

localpool_max_idle_time=config. pool_max_idle_time;

- -一个统一的模块对象
local_Module={}
_Module.__index=_Module
- -类的方法 new
function_Module.new (self)
    localobject={red=nil}
    setmetatable (object, self)
    returnobject
end
- -获取 Redis 连接
function_Module.open (self)
    localred=redis: new ()
    - -设置超时的时间为 2 sec, connect_timeout, send_timeout, read_timeout
    red: set_timeout (config. timeout, config. timeout, config. timeout);
    localok, err=red: connect (config. host_name, config. port)
    ifnotokthen
       basic.error ("连接 Redis 服务器失败: ", err)
       returnfalse;
    end
    if config. password then
       red: auth (config. password)
    end
    if config. dbthen
       red: select (config. db)
    end
    basic.log ("连接 Redis 服务器成功")
    self. red=red;
    returntrue;
end
- -缓存值
function_Module.setValue (self, key, value)
    ok, err=self.red: set (key, value)
    ifnotokthen
       basic.error ("Redis 缓存设置失败")
       returnfalse;
    end
    basic.log ("setresultok")
    returntrue;
end
- -值递增
function_Module.incrValue (self, key)
    ok, err=self.red: incr (key)
    ifnotokthen
       basic.error ("Redis 缓存递增失败")
       returnfalse;
    end
    basic.log ("incrok")
    returntrue;
end
- -过期
function_Module.expire (self, key, seconds)
    ok, err=self.red: expire (key, seconds)
    ifnotokthen


```
basic.error ("Redis 设置过期失败")
returnfalse;
end
returntrue;
end
```
- -获取值
function_Module.getValue (self, key)
    localresp, err=self.red: get (key)
    ifnotrespthen
       basic.error ("Redis 缓存读取失败")
       returnnil;
    end
    returnresp;
end
... 省略封装的其他 Redis 操作方法
- -将连接还给连接池
function_Module.close (self)
    ifnotself. redthen
       return
    end
    localok, err=self. red: set_keepalive (pool_max_idle_time, pool_size)
    ifnotokthen
       basic.error ("Redisset_keepalive 执行失败")
    end
    basic.log ("Redis 连接释放成功")
end
return_Module
此基础操作类的名称为 RedisOperator，需要使用时通过 require ("luaScript. redis. RedisOperator")
导入即可。

#### 9. 7. 3 在 Lua 中使用 Redis 连接池

在示例代码 RedisDemo. lua 脚本中，客户端每请求一次，lua-resty-redis 模块都会创建一个新的
Redis 连接。如果在生产环境，每请求一次都开启一个服务器新连接会导致以下问题：

1 ）连接资源被快速消耗。
2 ）网络一旦抖动，会有大量 TIME_WAIT 连接产生，需要定期重启服务程序或机器。
3 ）服务器工作不稳定，QPS 忽高忽低。
4 ）性能普遍上不去。
为什么会出现这些性能问题呢？因为每一次传输数据，需要完成创建连接、收发数据、拆除
连接三个基本步骤，在低并发场景下每次请求都完整走完这三步基本上不会有什么问题，然而一旦
挪到高并发应用场景，性能问题就出现了。
性能优化的第一件事情就是把短连接改成长连接，以减少大量创建连接、拆除连接的时间。
从性能上来说长连接肯定要比短连接好很多，但还是有比较大的浪费。
性能优化的第二件事情就是使用连接池。通过一个连接池将所有长连接缓存管理起来，谁需
要使用，就从这里取走，干完活立马放回来。


###### 实际上，在开发过程中所用到的连接池是非常多的，比如 HTTP 连接池、数据库连接、消息推

###### 送连接池等，几乎所有的点到点之间的连接资源复用都需要通过连接池完成。

在 OpenResty 中，lua-resty-redis 模块也管理了一个连接池，并且定义了 set_keepalive 方法完成连
接的回收和复用。set_keepalive 方法的语法如下：
ok, err=red: set_keepalive (max_idle_timeout, pool_size)
该方法将当前的 Redis 连接立即放入连接池。其中，max_idle_timeout 参数指定连接在池中的最
大空闲超时时长（以毫秒为单位）；pool_size 参数指定每个 Nginx 工作进程的连接池的最大连接数。
如果入池成功，则返回 1 ；如果入池出现错误，则返回 nil，并返回错误描述字符串。
下面是一个连接回收的示例，具体代码如下：
location/pool_demo{
content_by_lua_block{
localredis=require"resty. redis"
localconfig=require ("luaScript. module. config. redis-config");

- -连接池大小
localpool_size=config. pool_size;
- -最大的空闲时间，单位为毫秒
localpool_max_idle_time=config. pool_max_idle_time;
localred=redis: new ()
localok, err=red: connect (config. host_name, config. port)
ifnotokthen
    ngx.say ("failedtoconnect: ", err)
    return
else
    ngx.say ("succeedtoconnectRedis","<br>")
end
red: auth (config. password)
- -red: set_keepalive (pool_max_idle_time, pool_size) --①坑
ok, err=red: set ("dog","ananimal")
ifnotokthen
    - -red: set_keepalive (pool_max_idle_time, pool_size) --②坑
    return
end
- -③正确回收
red: set_keepalive (pool_max_idle_time, pool_size)
ngx.say ("succeedtocollectRedisconnection","<br>")
}
}
以上代码中，标记①②③的地方需要注意，具体介绍如下:
①坑：只有数据传输完毕、Redis 连接使用完成之后，才能调用 set_keepalive 方法将连接放到
连接池里，否则 set_keepalive 方法会立即将 red 连接对象转换为 closed 状态，后面的 Redis 调用将出错。
②坑：如果设值错误，则 red 连接对象不一定可用，不能把可用性存疑的连接放回连接池里，
如果另一个请求从连接池里获取一个不能用的连接，会直接报错。
③正确回收：此处的 set_keepalive 方法调用是正确的。
以上代码位于 nginx-redis-demo. conf 文件，修改后需重启 OpenRestry，然后可以使用浏览器访
问其地址/pool_demo，具体的访问结果如图 9 - 22 所示。


图 9 - 22 Redis 连接池演示实例的执行结果
set_keepalive 方法完成连接回收之后，下一次通过 red: connect (...) 获取连接时，connect 方法在
创建新连接前会先在连接池中查找空闲连接，只有查找失败时才会真正创建新连接。
总之，作为一个专业的服务端开发工程师，必须要对连接池有较深理解，其实不论是 Redis 连
接池，HTTP 连接池，还是数据库连接池，甚至是线程池，其原理都是差不多的。

### 9. 8 NginxLua 编程实战案例

```
作为实战练习，本节介绍如下 3 个 NginxLua 编程实战案例：
1 ）基于 Nginx+Redis 分布式架构的访问统计实战案例。
2 ）基于 Nginx+Redis+Java 容器架构的高并发访问实战案例。
3 ）基于 Nginx+Redis 架构的黑名单拦截实战案例。
```
#### 9. 8. 1 Nginx+Redis 进行分布式访问统计

###### 接口（或者页面）的访问统计数据是网站运营和优化的一个重要参考数据，对于分布式接口，

可以通过 Nginx+Redis 的架构来简单实现分布式受访统计。
得益于 Nginx 的高并发性能和 Redis 的高速缓存，基于 Nginx+Redis 的受访统计的架构设计比纯
Java 实现的受访统计的架构设计在性能上会高出很多。
作为参考案例，这里使用前面定义的 RedisOperator 基础操作类编写了一个简单的受访统计类，
具体的代码如下：

- --启动调试，正式环境请注释
localmobdebug=require ("luaScript. initial. mobdebug");
mobdebug.start ();
- -导入自定义的 RedisOperator 模块
localredisOp=require ("luaScript. redis. RedisOperator");
- -创建自定义的 redis 操作对象
localred=redisOp: new ();
- -打开连接
red: open ();
- -获取访问次数
localvisitCount=red: incrValue ("demo: visitCount");
ifvisitCount== 1 then
    - - 10 s 内过期
    red: expire ("demo: visitCount", 10 );
end
- -将访问次数设置到 Nginx 变量


```
ngx. var. count=visitCount;
```
- -归还连接到连接池
red: close ();
在 nginx-redis-demo. conf 配置文件中编写一个 location 配置块来使用该脚本，建议将该脚本执行
于 access 阶段而不是 content 阶段，具体代码如下：
#点击次数统计的演示
location/visitcount{
#定义一个Nginx变量 ，用于在 Lua 脚本中保存访问次数
set$count 0 ;
access_by_lua_file luaScript/redis/RedisVisitCount. lua;
echo" 10 s 内总的访问次数为: " $count;
}
修改了 nginx-redis-demo. conf 文件后需重启 OpenRestry，然后使用浏览器访问其地址/visitcount，
并且在浏览器中不断地刷新，会发现每刷新一次，页面的统计次数会加一，其结果具体如图 9 - 23
所示。

```
图 9 - 23 访问统计效果图
```
#### 9. 8. 2 Nginx+Redis+Java 容器实现高并发访问

在不需要高速访问的场景下，运行在 Java 后端的容器（如 Tomcat）会直接从数据库（如 MySQL）
查询数据，然后返回给客户端。
由于数据库的连接数限制、网络传输延迟、数据库的 IO 频繁等多方面原因，Java 后端容器直接
查询数据库的性能会很低，这时候会进行架构的调整，采用“Java 容器+Redis+数据库”的查询架
构。针对那些数据一致性要求不是特别高、但是访问频繁的 API 接口（实际上大部分都是），可以
将数据库数据放入 Redis 缓存，JavaAPI 可以优先查询 Redis，如果缓存未命中，则回源到数据库查
询，从数据库查询成功后，再将数据更新到 Redis 缓存。
“Java 容器+Redis+数据库”的查询架构，既起到了 Redis 分流大量查询请求的作用，又大大提
升了 API 接口的处理性能，可谓一举两得。该架构的请求处理流程如图 9 - 24 所示。

```
图 9 - 24 “Java 容器+Redis+DB”查询架构的请求处理流程
```

常用的后端 Java 容器（如 Tomcat、Jetty 等）的性能其实也不是太高，QPS 性能指标一般会在 1000
以内。从笔者经历过的多次的性能攻关的数据来看，Nginx 的性能会是 Java 容器的 10 倍左右（甚至
更大），并且稳定性更强，还不存在 FullGC 卡顿。
为了应对高并发，可以将“Java 容器+Redis+数据库”查询架构优化为“Nginx+Redis+Java 容器”
查询架构。新架构将后端 Java 容器的缓存判断、缓存查询前移到反向代理 Nginx，通过 Nginx 直接进
行 Redis 缓存判断、缓存查询。
“Nginx+Redis+Java 容器”的查询架构不仅为 Java 容器减少了很多请求，而且还能充分发挥
Nginx 的高并发优势和稳定性优势。该架构的请求处理流程如图 9 - 25 所示。

图 9 - 25 “Nginx+Redis+Java 容器”查询架构的请求处理流程
这里以秒杀系统的商品数据查询为例，提供一个“Nginx+Redis+Java 容器”的查询架构的参考
实现。首先定义两个接口，一个模拟 Java 容器的商品查询接口，另一个模拟供外部调用的商品查询
接口，具体如下：

```
模拟 Java 容器的商品查询接口：/java/good/detail。
模拟供外部调用的商品查询接口：/good/detail。
然后提供一个 Lua 操作缓存的类 RedisCacheDemo，主要定义如下 3 个方法：
1 ）getCache (self, goodId)：根据商品 id 取得 Redis 商品缓存。
2 ）goUpstream (self)：通过 capture 内部请求访问上游接口获取商品数据。
3 ）setCache (self, goodId, goodString)：设置商品缓存，此方法用于模拟后台 Java 代码。
缓存操作类 RedisCacheDemo 的核心代码具体如下：
```
- --启动调试，正式环境请注释
localmobdebug=require ("luaScript. initial. mobdebug");
mobdebug.start ();
- -导入自定义的基础模块
localbasic=require ("luaScript. module. common. basic");
- -导入自定义的 RedisOperator 模块
localredisOp=require ("luaScript. redis. RedisOperator");
localPREFIX="GOOD_CACHE: "
- -RedisCacheDemo 类
local_RedisCacheDemo={}
_RedisCacheDemo.__index=_RedisCacheDemo
- -类的方法 new
function_RedisCacheDemo.new (self)


localobject={}
setmetatable (object, self)
returnobject;
end

- -根据商品 id 获取缓存数据
function_RedisCacheDemo.getCache (self, goodId)
    - -创建自定义的 Redis 操作对象
    localred=redisOp: new ();
    - -打开连接
    ifnot red: open () then
       basic: error ("Redis 连接失败");
       returnnil;
    end
    - -获取缓存数据
    localjson=red: getValue (PREFIX.. goodId);
    red: close ();
    ifnotjsonorjson==ngx. nullthen
       basic: log (goodId.."的缓存没有命中");
       returnnil;
    end
    basic: log (goodId.."缓存成功命中");
    returnjson;
end
- -通过 capture 方法回源上游接口
function_RedisCacheDemo.goUpstream (self)
    localrequest_method=ngx. var. request_method
    localargs=nil
    - -获取参数的值
    if"GET"==request_methodthen
       args=ngx. req. get_uri_args ()
    elseif"POST"==request_methodthen
       ngx. req. read_body ()
       args=ngx. req. get_post_args ()
    end
    - -回源上游接口, 比如 Java 后端 rest 接口
    localres=ngx.location.capture ("/java/good/detail",{
       method=ngx. HTTP_GET,
       args=args --重要：将请求参数原样向上游传递
    })
    basic: log ("上游数据获取成功");
    - -返回上游接口的响应体
    returnres. body;
end
- -设置缓存，此方法主要用于模拟 Java 后台代码
function_RedisCacheDemo.setCache (self, goodId, goodString)
    - -创建自定义的 Redis 操作对象
    localred=redisOp: new ();
    - -打开连接
    ifnot red: open () then
       basic: error ("Redis 连接失败");
       returnnil;
    end
    - -set 缓存数据
    red: setValue (PREFIX.. goodId, goodString);


- - 60 s 内过期
red: expire (PREFIX.. goodId, 60 );
basic: log (goodId.."缓存设置成功");
- -归还连接到连接池
red: close ();
returnjson;
end
return_RedisCacheDemo;
在 nginx-redis-demo. conf 配置文件中，编写一个 location 配置块来使用该脚本，该配置块为提供
给外部调用的商品查询接口/good/detail，具体代码如下：
#首先从缓存查询商品 ，未命中再回源到 Java 后台
location=/good/detail{
content_by_lua_block{
localgoodId=ngx. var. arg_goodid;
- -判断 goodId 参数是否存在
ifnotgoodIdthen
ngx.say ("请输入 goodId");
return;
end
- -首先从缓存根据 ID 查询商品
localRedisCacheDemo=require"luaScript. redis. RedisCacheDemo";
localredisCacheDemo=RedisCacheDemo: new ();
localjson=redisCacheDemo: getCache (goodId);
- -判断缓存是否被命中
ifnotjsonthen
ngx.say ("缓存是否被命中，回源到上游接口<br>");
- -没有命中缓存，则回源到上游接口
json=redisCacheDemo: goUpstream ();
else
ngx.say ("缓存已经被命中<br>");
end
ngx.say ("商品信息：", json);
}
}
为了调试方便，在 nginx-redis-demo. conf 配置文件中再编写一个 location 配置块来模拟 Java 容器
的后台商品查询接口/java/good/detail。
理论上，后台接口的业务逻辑是从数据库查询商品信息并缓存到 Redis，然后返回商品信息。
这里为了方便演示对其进行了简化，具体的代码如下：
#模拟Java后台接口查询商品 ，然后设置缓存
location=/java/good/detail{
#指定规则为internal内部规则 ，防止外部请求命中此规则
internal;
content_by_lua_block{
localRedisCacheDemo=require"luaScript. redis. RedisCacheDemo";
- -Java 后台将从数据库查找商品, 这里简化...
localjson='{goodId: 商品 id, goodName: 商品名称}';
- -将商品缓存到 Redis
localredisCacheDemo=RedisCacheDemo: new ();
redisCacheDemo: setCache (ngx. var. arg_goodid, json);


- -返回商品到下游网关
ngx.say (json);
}
}
}
修改了 nginx-redis-demo. conf 文件后重启 OpenRestry，然后使用浏览器访问商品查询外部接口
/good/detail，并且多次刷新，发现从第二次请求开始就能成功命中缓存，结果如图 9 - 26 所示。

```
图 9 - 26 浏览器访问商品查询外部接口/good/detail 的结果
```
#### 9. 8. 3 Nginx+Redis 实现黑名单拦截

###### 在日常维护网站时经常会有这样一个需求：对于黑名单之内的 IP 拒绝提供服务。实现 IP 黑名单

###### 拦截有很多途径，比如以下的方式：

1 ）在操作系统层面，配置 iptables 防火墙规则，拒绝黑名单中 IP 的网络请求。
2 ）使用 Nginx 网关的 deny 配置指令，拒绝黑名单中 IP 的网络请求。
3 ）在 Nginx 网关的 access 处理阶段，通过 Lua 脚本检查客户端 IP 是否在黑名单中。
4 ）在 SpringCloud 内部网关（如 Zuul）的过滤器中，检查客户端 IP 是否在黑名单中。
以上的检查方式都是基于一个静态的、提前备好的黑名单进行的。在实际运行过程中，黑名
单往往需要动态计算，系统需要动态识别出大量发起请求的恶意爬虫或者恶意用户，并且将这些恶
意请求的 IP 放入一个动态的 IP 黑名单中。
Nginx 网关可以依据动态黑名单之内的 IP 进行请求拦截并拒绝提供服务。这里结合 Nginx 和
Redis，提供一个基于动态 IP 黑名单进行请求拦截的参考实现。
首先是黑名单的组成，黑名单应该包括静态部分和动态部分。静态部分为系统管理员通过控
制台设置的黑名单。动态部分主要通过流计算框架完成，具体的方法为：将 Nginx 的访问日志通过
Kafka 消息中间件发送到流计算框架，然后通过滑动窗口机制计算出窗口内的相同 IP 的访问计数，
将超出阈值的 IP 动态加入黑名单中，流计算框架可以选用 ApacheFlink 或者 ApacheStorm。当然，
除了使用流计算框架外，也可以使用 RxJava 滑动窗口进行访问计数的统计。
这里对黑名单的计算和生成不做研究，并假定 IP 黑名单已经生成并且定期更新在 Redis 中。
Nginx 网关可以直接从 Redis 获取计算好的 IP 黑名单，但是为了提升黑名单的读取速度，并不是每一
次请求过滤都从 Redis 读取 IP 黑名单，而是从本地的共享内存 black_ip_list 中获取，同时定期更新到
本地的共享内存中的 IP 黑名单。
Nginx+Redis 实现黑名单拦截的系统架构具体如图 9 - 27 所示。


```
图 9 - 27 Nginx+Redis 实现黑名单拦截的系统架构
```
这里提供一个“Nginx+Redis”黑名单拦截的参考实现，具体的 Lua 脚本如下：

- --启动调试，正式环境请注释
localmobdebug=require ("luaScript. initial. mobdebug");
mobdebug.start ();
- -导入自定义的基础模块
localbasic=require ("luaScript. module. common. basic");
- -导入自定义的 RedisOperator 模块
localredisOp=require ("luaScript. redis. RedisOperator");
localip=basic.getClientIP ();
basic.log ("ClientIP: ".. ip);
- -lua_shared_dictblack_ip_list 1 m; #配置文件定义的ip_blacklist共享内存变量
localblack_ip_list=ngx. shared. black_ip_list
- -获得本地缓存的刷新时间，如果没有过期，则直接使用
locallast_update_time=black_ip_list: get ("last_update_time");
iflast_update_time~=nilthen
    localdif_time=ngx.now ()-last_update_time
    ifdif_time< 60 then--缓存 1 分钟，没有过期
       ifblack_ip_list: get (ip) then
          returnngx.exit (ngx. HTTP_FORBIDDEN)--直接返回 403
       end
       return
    end
end
localKEY="limit:ip: blacklist";
- -创建自定义的 Redis 操作对象
localred=redisOp: new ();
- -打开连接
red: open ();
- -获取缓存的黑名单
localip_blacklist=red: getSmembers (KEY);
- -归还连接到连接池
red: close ();
ifnotip_blacklistthen
    basic.log ("blackipset isnull");
    return;
else
    - -刷新本地缓存
    black_ip_list: flush_all ();
    - -同步 Redis 黑名单到本地缓存
    fori,ipinipairs (ip_blacklist) do


- -本地缓存 Redis 中的黑名单
black_ip_list: set (ip, true);
end
- -设置本地缓存的最新更新时间
black_ip_list: set ("last_update_time",ngx.now ());
end
ifblack_ip_list: get (ip) then
returnngx.exit (ngx. HTTP_FORBIDDEN)--直接返回 403
end
该脚本名称为 black_ip_filter. lua，作为测试，在 nginx-redis-demo. conf 配置文件中编写一个
location 配置块来执行该脚本，建议将该脚本执行在 access 阶段而不是 content 阶段，具体代码如下：
location/black_ip_demo{
access_by_lua_fileluaScript/redis/black_ip_filter. lua;
echo"恭喜，没有被拦截";
}
另外，black_ip_filter. lua 使用了名称为 black_ip_list 的共享内存区进行黑名单本地缓存，所以需
要在配置文件中进行共享内存空间的定义，具体如下：
#定义存储IP黑名单的共享内存变量
lua_shared_dict black_ip_list 1 m;
这里使用 lua_shared_dict 指令定义了一块 1 MB 大小的共享内存空间，有关该指令的使用方法，
下一小节再详细展开。修改了 nginx-redis-demo. conf 文件后重启 OpenRestry，然后使用浏览器访问
/black_ip_demo 的完整链接地址，第一次访问时客户端 IP 没有加入黑名单，所以请求没有被拦截，
结果如图 9 - 28 所示。

图 9 - 28 第一次访问时客户端 IP 没有加入黑名单
在 Redis 服务器上新建 Set 类型的键 limit:ip: blacklist，并加入当前的客户端 IP。然后再一次访问
/black_ip_demo，发现请求已经被拦截，结果如图 9 - 29 所示。

```
图 9 - 29 客户端 IP 加入黑名单后请求被拦截
```
#### 9. 8. 4 使用 NginxLua 共享内存

NginxLua 共享内存就是在内存块中分配出一个共享内存空间，该共享内存是一种字典结构，
类似于 JavaMap 的“键－值对”映射结构。同一个 Nginx 下的 Worker 进程都能访问存储在这里面的
变量数据。在 Lua 定义共享内存非常简单，具体的指令如下：


语法：lua_shared_dict <DICT> <size>
上下文：http 配置块
例子：lua_shared_dict black_ip_list 1 m; #定义存储IP黑名单的共享内存变量
lua_shared_dict 指令用于定义一块名为 DICT 的共享内存空间，其内存大小为 size。通过该命令
定义的共享内存对于 Nginx 中的所有 Worker 进程都是可见的。对于共享内存的引用，可以使用以下
两种形式完成：

```
方式一：ngx. shared. DICT
方式二：ngx. shared["DICT"]
ngx_lua 提供了一系列 API 来操作共享内存，大致如表 9 - 7 所示。
表 9 - 7 ngx_lua 字典 API 及其方法
字典 API 方法说明
取值语根法据：kevyal 从 ue 字, f 典 lag 中 s=取 n 得 gx 值.shared.DICT: get (key)
```
```
设值
```
```
语法：success, err, forcible=ngx.shared.DICT: set (key, value, exptime?, flags?)
根据 key 在字典中设置值
可选参数 exptime 过期时间的单位为秒，如果不进行设置，则默认为永不过期；可
选参数 flags 用于设置额外的缓存内容，如果已经设置，则可以通过 get 方法取出
删除数据项语删法除数 ng 据x.s 项 hared.DICT: delete (key)
```
```
设置过期时间语设法置：kenyg 在x.s 字 ha 典 red 中. D 的 IC 生 T: 存 ex 时 pir 间 e (，keye, xepxtipmtiem 单 e) 位为秒
```
```
查询过期时间语查法询：ketytl, 在 er 字 r=典 n 中 gx 的. sh 剩 are 余d.生 DI 存 CT 时: tt 间 l (k，ey 单) 位为秒
```
```
全部过期语将法字：典 n 中 gx 所. s 有 har 的 ed 数. D 据 IC 项 T: 设 flu 置 sh 为_a 过 ll () 期，此操作并没有真正清除数据项
```
```
清除过期数据项
```
语法：flushed=ngx. shared. DICT: flush_expired (max_count?)
清除字典中的过期数据项，可选参数 max_count 表示清除数量，若没有设置，则表
示清空所有的过期数据项
如果熟悉 Redis 字符串的操作命令和参数，就会发现以上共享内存的 API 方法和 Redis 字符串的
操作命令及参数有惊人的相似之处。
共享内存的 API 方法都是原子操作，也就是说，lua_shared_dict 定义的同一个共享内存区自带
锁的功能，能够避免来自多个 Worker 进程的并发访问。
有关数据项的过期时间可以在新增数据项的时候进行设置。在新增数据项时，如果字典的内
存区域不够，ngx. shared. DICT. set 方法会根据 LRU 算法淘汰一部分内容。当 Nginx 退出时，共享内存
中的数据项都会丢失。


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
openresty lua 调试 （图文死磕）
https://www.cnblogs.com/crazymakercircle/p/ 12112568 .html

##### 3. 相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题 ：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 10 章限流原理与实战

在通信领域中，限流技术（TimeLimiting）被用来控制网络接口收发通信数据的速率，实现通
信时的优化性能、较少延迟和提高带宽等。
互联网领域中也借鉴了通信领域的限流概念，用来控制在高并发、大流量的场景中对服务接
口请求的速率，比如双十一秒杀、抢购、抢票、抢单等场景。
举一个具体的例子，假设某个接口能够扛住的 QPS 为 10 000 ，这时有 20 000 个请求进来，经过
限流模块，会先放 10000 个请求，其余的请求会阻塞一段时间。不简单粗暴地返回 404 让客户端重试，
同时又能起到流量削峰的作用。
每个 API 接口都是有访问上限的，当访问频率或者并发量超过其承受范围时，就必须通过限流
来保证接口的可用性或者降级可用性，给接口安装上保险丝，以防止非预期的请求对系统压力过大
而引起的系统瘫痪。
接口限流的算法主要有 3 种，分别是计数器限流、漏桶算法和令牌桶算法，接下来为大家一一
介绍。

## 10. 1 使用 Redis 实现简单限流策略

###### 首先来看一个常见的简单的限流策略。系统要限定某种请求在指定的时间里只能发生 N 次，如

何使用 Redis 的数据结构来实现这个限流的功能呢？
我们先来定义这个接口，通过这个接口读者就应该能明白期望达到的功能。
背景：通常在高并发访问的情况下会通过限流的手段来控制流量问题，以保证服务器处于正
常压力下，一般对超过的部分不做处理，即丢弃。

#### 10. 1. 1 3 种限流策略：计数器、漏桶、令牌桶

###### 限流的手段通常有计数器、漏桶、令牌桶。注意限流和限速（所有请求都会处理）的差别，

###### 视业务场景而定。

###### 1 ）计数器：在一段时间间隔内（时间窗）处理请求的最大数量固定，超过部分不做处理。

###### 2 ）漏桶：漏桶大小固定，处理速度固定，但请求进入速度不固定（在突发情况请求过多时，

###### 会丢弃过多的请求）。

###### 3 ）令牌桶：令牌桶的大小固定，令牌的产生速度固定，但是消耗令牌（即请求）的速度不固

###### 定（可以应对某些时间请求过多的情况）。每个请求都会从令牌桶中取出令牌，如果没有令牌则丢

###### 弃该次请求。

#### 10. 1. 2 计数器限流原理和 Java 参考实现

###### 计数器限流的原理非常简单：在一个时间窗口（间隔）内所处理的请求的最大数量是有限制


###### 的，对超过限制部分的请求不做处理。

###### 下面的代码是计数器限流算法的一个简单的演示实现和测试用例。

```
packagecom. crazymaker. SpringCloud. ratelimit;
...
//计速器，限速
@Slf 4 j
publicclassCounterLimiter
{
//起始时间
privatestaticlongstartTime=System.currentTimeMillis ();
//时间区间的时间间隔，单位为毫秒
privatestaticlonginterval= 1000 ;
//每秒限制数量
privatestaticlongmaxCount= 2 ;
//累加器
privatestaticAtomicLongaccumulator=newAtomicLong ();
//计数判断，是否超出限制
privatestaticlongtryAcquire (longtaskId, intturn)
{
longnowTime=System.currentTimeMillis ();
//在时间区间之内
if (nowTime<startTime+interval)
{
longcount=accumulator.incrementAndGet ();
if (count<=maxCount)
{
returncount;
}else
{
return-count;
}
}else
{
//在时间区间之外
synchronized (CounterLimiter. class)
{
log.info ("新时间区到了, taskId{}, turn{}..", taskId, turn);
//再一次判断，防止重复初始化
if (nowTime>startTime+interval)
{
accumulator.set ( 0 );
startTime=nowTime;
```

}
}
return 0 ;
}
}
//线程池，用于多线程模拟测试
privateExecutorServicepool=Executors.newFixedThreadPool ( 10 );
@Test
publicvoidtestLimit ()
{
//被限制的次数
AtomicIntegerlimited=newAtomicInteger ( 0 );
//线程数
finalintthreads= 2 ;
//每条线程的执行轮数
finalintturns= 20 ;
//同步器
CountDownLatchcountDownLatch=newCountDownLatch (threads);
longstart=System.currentTimeMillis ();
for (inti= 0 ;i<threads; i++)
{
pool.submit (()->
{
try
{
for (intj= 0 ;j<turns; j++)
{
longtaskId=Thread.currentThread (). getId ();
longindex=tryAcquire (taskId, j);
if (index<= 0 )
{
//被限制的次数累积
limited.getAndIncrement ();
}
Thread.sleep ( 200 );
}
}catch (Exceptione)
{
e.printStackTrace ();
}
//等待所有线程结束
countDownLatch.countDown ();
});
}
try
{
countDownLatch.await ();
}catch (InterruptedExceptione)
{
e.printStackTrace ();
}
floattime=(System.currentTimeMillis ()-start)/ 1000 F;
//输出统计结果
log.info ("限制的次数为："+limited.get ()+
", 通过的次数为："+(threads*turns-limited.get ()));
log.info ("限制的比例为："+(float) limited.get ()/(float)(threads*turns));


log.info ("运行的时长为："+time);
}
}
以上代码使用两条线程，每条线程各运行 20 次，每一次运行后休眠 200 毫秒，总计耗时 4 秒运
行 40 次，限流的输出结果具体如下：
[pool- 2 - thread- 2 ]INFO c.c.s.ratelimit. CounterLimiter-新时间区到了, taskId 16 ,turn 5 ..
[pool- 2 - thread- 1 ]INFO c.c.s.ratelimit. CounterLimiter-新时间区到了, taskId 15 ,turn 5 ..
[pool- 2 - thread- 2 ]INFO c.c.s.ratelimit. CounterLimiter-新时间区到了, taskId 16 ,turn
10 ..
[pool- 2 - thread- 2 ]INFO c.c.s.ratelimit. CounterLimiter-新时间区到了, taskId 16 ,turn
15 ..
[main]INFO c.c.s.ratelimit. CounterLimiter-限制的次数为： 32 ,通过的次数为： 8
[main]INFO c.c.s.ratelimit. CounterLimiter-限制的比例为： 0. 8
[main]INFO c.c.s.ratelimit. CounterLimiter-运行的时长为： 4. 104
大家可以自行调整参数运行以上自验证程序并观察实验结果，体验一下计数器限流的效果。
计数器限流算法虽然简单，但是有一个十分致命的问题，
那就是临界问题。如图 10 - 1 所示，假设现在按照每分钟 110
次请求的速率进行限流，有一个恶意用户在 0 : 59 瞬间发送了
100 个请求，并且在 1 : 00 又瞬间发送了 100 个请求，那么这个
用户在 1 分钟里的一个瞬间其实发送了 200 个请求，虽然计数
器限流策略的速率为每分钟 110 ，但是对此却无能为力了。
所以在使用计数器限流策略时，用户通过在时间窗口的
连接处突发大量请求，可以瞬间超过速率限制，从而瞬间压垮应用，这就是计数器限流算法的临界
问题。
如何解决临界问题呢？答案是漏桶算法或者令牌桶算法。

#### 10. 1. 3 漏桶算法限流原理和 Java 参考实现

###### 漏桶算法限流的基本原理为：水（对应请求）从进水口

###### 进入到漏桶里，漏桶以一定的速度出水（请求放行），当水

###### 流入速度过大时，桶内的总水量大于桶容量会直接溢出，请

###### 求被拒绝，如图 10 - 2 所示。

###### 大致的漏桶限流规则如下：

###### 1 ）水通过进水口（对应客户端请求）以任意速率流入漏

###### 桶。

###### 2 ）漏桶的容量是固定的，出水（放行）速率也是固定的。

###### 3 ）漏桶容量是不变的，如果处理速度太慢，桶内水量会

###### 超出桶的容量，则后面流入的水滴会溢出，表示请求被拒绝。

```
漏桶的 Java 参考实现代码如下：
packagecom. crazymaker. SpringCloud. ratelimit;
//省略 import
//漏桶限流
```
```
图 10 - 1 计数器限流算法的临界问题
```
```
图 10 - 2 漏桶
```

@Slf 4 j
publicclassLeakBucketLimiter{
//计算的起始时间
privatestaticlonglastOutTime=System.currentTimeMillis ();
//时间区间的时间间隔毫秒
privatestaticlonginterval= 1000 ;
//流出速率每秒 2 次
privatestaticintleakRate= 2 ;
//桶的容量
privatestaticintcapacity= 20 ;
//剩余的水量
privatestaticAtomicIntegerwaterInBucket=newAtomicInteger ( 0 );
//返回值说明：
//false：没有被限流
//true：被限流
publicstaticsynchronizedbooleanisLimit (longtaskId, intturn){
//如果是空桶，则当前时间作为漏出的时间
if (waterInBucket.get ()== 0 ){
lastOutTime=System.currentTimeMillis ();
waterInBucket.addAndGet ( 1 );
returnfalse;
}
//补充遗漏的场景，讲课的意义就是不断地优化代码
//场景一：当前请求和上次请求在同一个时间区间
longnowTime=System.currentTimeMillis ();
//当前时间在时间区间之内
//计算漏水，以时间区间为计算维度，同一个区间没有必要重复去计算漏水
if (nowTime<lastOutTime+interval){
//尝试加水，并且水还未满，放行
if ((waterInBucket.get ())<capacity){
waterInBucket.addAndGet ( 1 );
returnfalse;
}else{
//水满，拒绝加水，限流
returntrue;
}
}
//场景二：桶里边有水
//当前时间在时间区间之外
//计算漏水，以时间的区间为计算维度
intwaterLeaked=((int)((System.currentTimeMillis ()-lastOutTime)/ 1000 ))
*leakRate;
//计算剩余水量
intwaterLeft=waterInBucket.get ()-waterLeaked;
//校正数据
waterLeft=Math.max ( 0 ,waterLeft);
waterInBucket.set (waterLeft);
//重新更新 leakTimeStamp
lastOutTime=System.currentTimeMillis ();
//尝试加水，并且水还未满，放行
if ((waterInBucket.get ())<capacity){
waterInBucket.addAndGet ( 1 );
returnfalse;
}else{
//水满，拒绝加水，限流


returntrue;
}
}
...
}
以上代码使用两条线程，每条线程各运行 20 次，每一次运行后休眠 200 毫秒，总计耗时 4 秒运
行 40 次，部分输出结果如下：
[pool- 2 - thread- 1 ]INFO c.c.s.r.LeakBucketLimiter-water 0 pastTime 75 outWater 0
...
[pool- 2 - thread- 1 ]INFO c.c.s.r.LeakBucketLimiter-water 1 pastTime 601 outWater 1
...
[pool- 2 - thread- 1 ]INFO c.c.s.r.LeakBucketLimiter-water 2 pastTime 416 outWater 0
[pool- 2 - thread- 2 ]INFO c.c.s.r.LeakBucketLimiter-water 1 pastTime 601 outWater 1
[pool- 2 - thread- 1 ]INFO c.c.s.r.LeakBucketLimiter-water 2 pastTime 15 outWater 0
[pool- 2 - thread- 2 ]INFO c.c.s.r.LeakBucketLimiter-water 2 pastTime 201 outWater 0
[pool- 2 - thread- 1 ]INFO c.c.s.r.LeakBucketLimiter-water 2 pastTime 216 outWater 0
[main]INFO c.c.s.r.LeakBucketLimiter-限制的次数为： 32 ,通过的次数为： 8
[main]INFO c.c.s.r.LeakBucketLimiter-限制的比例为： 0. 8
[main]INFO c.c.s.r.LeakBucketLimiter-运行的时长为： 4. 107
漏桶的出水速度是固定的，也就是请求放行速度是固定的，故漏桶不能有效应对突发流量，
但是能起到平滑突发流量（整流）的作用。

#### 10. 1. 4 令牌桶限流原理和 Java 参考实现

###### 令牌桶算法以一个设定的速率产生令牌并放入令

###### 牌桶，每次用户请求都得申请令牌，如果令牌不足，

###### 则拒绝请求。

###### 在令牌桶算法中，新请求到来时会从桶里拿走一个

###### 令牌，如果桶内没有令牌可拿，就拒绝服务。当然，令

###### 牌的数量也是有上限的。令牌的数量与时间和发放速率

###### 强相关，流逝的时间越长，往桶里加入令牌的越多，如

###### 果令牌发放速度比申请速度快，则令牌会放入令牌桶，

###### 直到占满整个令牌桶，如图 10 - 3 所示。

###### 另外，可以设置令牌的发送速率，从而对突发流

###### 量进行有效应对。

###### 令牌桶限流大致的规则如下：

###### 1 ）进水口按照某个速度向桶中放入令牌。

###### 2 ）令牌桶的容量是固定的，但是放行的速度不是固定的，只要桶中还有剩余令牌，一旦请求

###### 过来就能申请成功，然后放行。

###### 3 ）如果令牌的发放速度慢于请求的到来速度，则桶内就无牌可领，请求就会被拒绝。

```
令牌桶的 Java 参考实现代码如下：
packagecom. crazymaker. SpringCloud. ratelimit;
...
//令牌桶，限速
```
```
图 10 - 3 令牌桶
```

@Slf 4 j
publicclassTokenBucketLimiter{
//上一次令牌发放时间
publiclonglastTime= 0 ;
//桶的容量
publicintcapacity= 2 ;
//令牌生成速度个/秒
publicintrate= 2 ;
//当前令牌数量
publicAtomicIntegertokens=newAtomicInteger ( 0 );
//返回值说明：
//false：没有被限流
//true：被限流
publicsynchronizedbooleanisLimited (longtaskId, intapplyCount){
longnow=System.currentTimeMillis ();
//时间间隔, 单位为 ms
longgap=now-lastTime;
//补充遗漏的场景，讲课的意义就是不断地优化代码
//场景一：当前请求和上次请求在同一个时间区间
//当前时间在时间区间之内
//以时间区间为计算维度，同一个区间，没有必要重复去计算令牌数量
if (lastTime!= 0 &&gap< 1000 /*interval*/){
if (tokens.get ()<applyCount){
//若拿不到令牌，则拒绝
//log.info ("被限流了.."+taskId+", applyCount: "+applyCount);
returntrue;
}else{
//还有令牌，领取令牌
tokens.getAndAdd (-applyCount);
//log.info ("剩余令牌.."+tokens);
returnfalse;
}
}
System.out.println ("时区之外 gap="+gap);
if (lastTime== 0 ){
gap= 1000 /*interval*/;
}
//计算时间段内的令牌数
intreverse_permits=(int)(gap*rate/ 1000 );
intall_permits=tokens.get ()+reverse_permits;
//当前令牌数
tokens.set (Math.min (capacity, all_permits));
log.info ("tokens{}capacity{}gap{}", tokens, capacity, gap);
lastTime=now;
if (tokens.get ()<applyCount){
//若拿不到令牌，则拒绝
//log.info ("被限流了.."+taskId+", applyCount: "+applyCount);
returntrue;
}else{
//还有令牌，领取令牌
tokens.getAndAdd (-applyCount);
//log.info ("剩余令牌.."+tokens);
returnfalse;
}
}
...
}


###### 运行这个示例程序，部分结果如下：

[pool- 2 - thread- 2 ]INFO c.c.s.r.TokenBucketLimiter-tokens 0 capacity 2 gap 104
[pool- 2 - thread- 1 ]INFO c.c.s.r.TokenBucketLimiter-tokens 0 capacity 2 gap 114
[pool- 2 - thread- 2 ]INFO c.c.s.r.TokenBucketLimiter-tokens 0 capacity 2 gap 314
[pool- 2 - thread- 1 ]INFO c.c.s.r.TokenBucketLimiter-tokens 0 capacity 2 gap 314
[pool- 2 - thread- 2 ]INFO c.c.s.r.TokenBucketLimiter-tokens 1 capacity 2 gap 515
[pool- 2 - thread- 1 ]INFO c.c.s.r.TokenBucketLimiter-tokens 0 capacity 2 gap 0
...
[pool- 2 - thread- 2 ]INFO c.c.s.r.TokenBucketLimiter-tokens 0 capacity 2 gap 401
[pool- 2 - thread- 1 ]INFO c.c.s.r.TokenBucketLimiter-tokens 0 capacity 2 gap 402
[main]INFO c.c.s.r.TokenBucketLimiter-限制的次数为： 34 ,通过的次数为： 6
[main]INFO c.c.s.r.TokenBucketLimiter-限制的比例为： 0. 85
[main]INFO c.c.s.r.TokenBucketLimiter-运行的时长为： 4. 119
令牌桶的好处之一就是可以方便地应对突发流量。比如，可以改变令牌的发放速度，算法能
按照新的发送速率调大令牌的发放数量，使得突发流量能被处理。

### 10. 2 分布式计数器限流

分布式计数器限流使用 Redis 存储限流键（Key）的统计计数。这里介绍两种限流的实现方案：
NginxLua 分布式计数器限流和 RedisLua 分布式计数器限流。

#### 10. 2. 1 NginxLua 分布式计数器限流

###### 本小节以对用户 IP 计数器限流为例，实现单 IP 在一定时间周期（如 10 秒）内只能访问一定次数

（如 10 次）的限流功能。由于使用到 Redis 存储分布式访问计数，因此这里将这类型的限流称为 Nginx
Lua 分布式计数器限流。
本小节的 NginxLua 分布式计数器限流案例的具体架构如图 10 - 4 所示。

```
图^10 -^4 NginxLua 分布式计数器限流架构
```

首先看一下限流计数器脚本 RedisKeyRateLimiter. lua，该脚本负责完成访问计数和限流的结果
判断，其中涉及 Redis 的存储访问，具体的代码如下：
localredisExecutor=require ("luaScript. redis. RedisOperator");

- -一个统一的模块对象
local_Module={}
_Module.__index=_Module
- -方法：创建一个新的实例
function_Module.new (self, key)
    localobject={red=nil}
    setmetatable (object, self)
    - -创建自定义的 redis 操作对象
    localred=redisExecutor: new ();
    red: open ();
    object. red=red;
    object. key="count_rate_limit: ".. key;
    returnobject
end
- -方法：判断是否能通过流量控制
- -返回值 true 表示通过流量控制，false 表示被限制
function_Module.acquire (self)
    localredis=self. red;
    localcurrent=redis: getValue (self. key);
    - -判断是否大于限制次数
    locallimited=currentandcurrent~=ngx.nullandtonumber (current)> 10 ; --限
流的次数
- -被限流
iflimitedthen
redis: incrValue (self. key);
returnfalse;
end
ifnotcurrentorcurrent==ngx. nullthen
redis: setValue (self. key, 1 );
redis: expire (self. key, 10 ); --限流的时间范围
else
redis: incrValue (self. key);
end
returntrue;
end
- -方法：获取访问次数，供演示使用
function_Module.getCount (self)
    localcurrent=self.red: getValue (self. key);
    ifcurrentandcurrent~=ngx. nullthen
       returntonumber (current);
    end
    return 0 ;
end
- -方法：归还 Redis 连接
function_Module.close (self)
    self.red: close ();
end
return_Module
以上代码位于练习工程 LuaDemoProject 的 src/luaScript/module/ratelimit/文件夹下，文件名为
RedisKeyRateLimiter. lua。


然后介绍 access_auth_nginx 限流脚本，该脚本使用上面定义的 RedisKeyRateLimiter. lua 通用访问
计数器脚本，完成针对同一个 IP 的限流操作，具体的代码如下：

- --此脚本的环境：Nginx 内部
- --启动调试
- -localmobdebug=require ("luaScript. initial. mobdebug");
- -mobdebug.start ();
- -导入自定义的计数器模块
localRedisKeyRateLimiter=
require ("luaScript. module. ratelimit. RedisKeyRateLimiter");
- -定义出错的 JSON 输出对象
localerrorOut={resp_code=- 1 ,resp_msg="限流出错", datas={}};
- -获取用户的 IP
localshortKey= ngx. var. remote_addr;
- -没有限流键（Key），提示错误
ifnotshortKeyorshortKey==ngx. nullthen
errorOut. resp_msg="shortKey 不能为空"
ngx.say (cjson.encode (errorOut));
return;
end
- -拼接计数的 Redis 键（Key）
localkey="ip: ".. shortKey;
locallimiter=RedisKeyRateLimiter: new (key);
localpassed=limiter: acquire ();
- -如果通过流量控制
ifpassedthen
ngx. var. count=limiter: getCount ();
- -注意，在这里直接输出会导致 content 阶段的指令被跳过
- -ngx.say ("目前的访问总数：",limiter: getCount (),"<br>");
end
- -回收 Redis 连接
limiter: close ();
- -如果没有流量控制，终止 Nginx 的处理流程
ifnotpassedthen
errorOut. resp_msg="抱歉，被限流了";
ngx.say (cjson.encode (errorOut));
ngx.exit (ngx. HTTP_UNAUTHORIZED);
end
return;
以上代码位于练习工程 LuaDemoProject 的 src/luaScript/module/ratelimit/文件夹下，文件名为
access_auth_nginx. lua。access_auth_nginx. lua 在拼接计数器的 key 时，使用了 Nginx 的内置变量
$remote_addr 获取客户端的 IP 地址，最终在 Redis 存储访问计数的 key 的格式如下：
count_rate_limit:ip: 192. 168. 233. 1
这里的 192. 168. 233. 1 为笔者的本地测试 IP，存储在 Redis 中的针对此 IP 的限流计数结果具体如
图 10 - 5 所示。


图 10 - 5 存储在 Redis 中的针对 IP 的限流计数
在 Nginx 的 access 请求处理阶段，使用 access_auth_nginx. lua 脚本进行请求限流的配置如下：
location=/access/demo/nginx/lua{
set$count 0 ;
access_by_lua_fileluaScript/module/ratelimit/access_auth_nginx. lua;
content_by_lua_block{
ngx.say ("目前的访问总数：", ngx. var. count,"<br>");
ngx.say ("helloworld!");
}
}
以上配置位于练习工程 LuaDemoProject 的 src/conf/nginx-ratelimit. conf 文件中，在使之生效之前，
需要在 openresty-start. sh 脚本中换上该配置文件，然后重启 Nginx。
接下来，开始限流自验证。
在上面的代码中，由于 RedisKeyRateLimiter 所设置的限流规则为单 IP 在 10 秒内限制访问 10 次，
因此，在验证的时候，在浏览器中刷新 10 次之后就会被限流。在浏览器输入如下测试地址：

[http://nginx.server/access/demo/nginx/lua?seckillGoodId=](http://nginx.server/access/demo/nginx/lua?seckillGoodId=) 1
10 秒内连续刷新，第 6 次的输出如图 10 - 6 所示。 10 秒之内连续刷新，发现第 10 次之后的请求被
限流了，说明 Lua 限流脚本工作是正常的，被限流后的输出如图 10 - 7 所示。

图 10 - 6 自验证时第 6 次刷新的输出图 10 - 7 自验证时刷新 10 次之后的输出
以上代码有两点缺陷：
1 ）数据一致性问题：计数器的读取和自增由两次 Redis 远程操作完成，如果多个网关同时进行
限流，就可能会出现数据一致性问题。
2 ）性能问题：同一次限流操作需要多次访问 Redis，存在多次网络传输，大大降低了限流的
性能。

#### 10. 2. 2 RedisLua 分布式计数器限流

Redis 允许将 Lua 脚本加载到 Redis 服务器中执行，可以调用大部分 Redis 命令，并且 Redis 保证了
脚本的原子性。由于既使用 Redis 存储分布式访问计数，又通过 Redis 执行限流计数器的 Lua 脚本，
因此这里将这类型的限流称为 RedisLua 分布式计数器限流。


```
本小节的 RedisLua 分布式计数器限流案例的具体架构如图 10 - 8 所示。
```
图 10 - 8 RedisLua 分布式计数器限流架构
首先来看限流的计数器脚本 redis_rate_limiter. lua，该脚本负责完成访问计数和限流结果的判断，
其中会涉及 Redis 计数的存储访问。需要注意的是，该脚本将在 Redis 中加载和执行。
计数器脚本 redis_rate_limiter. lua 的代码如下：

- --此脚本的环境：Redis 内部，不是运行在 Nginx 内部
- -返回 0 表示被限流，返回其他表示统计的次数
localcacheKey= KEYS[ 1 ]
localdata=redis.call ("incr", cacheKey)
localcount=tonumber (data)
- -首次访问，设置过期时间
ifcount== 1 then
    redis.call ("expire", cacheKey, 10 )--设置超时时间为 10 秒
end
ifcount> 10 then --设置超过的限制为 10 人
    return 0 ;-- 0 表示需要限流
end
- -redis.debug (redis.call ("get", cacheKey))
return count;
以上代码位于练习工程 LuaDemoProject 的 src/luaScript/module/ratelimit/文件夹下，文件名为
redis_rate_limiter. lua。在调用该脚本之前，首先要将其加载到 Redis，并且获取加载之后的 sha 1 编码，
以供 Nginx 上的限流脚本 access_auth_evalsha. lua 使用。
将 redis_rate_limiter. lua 加载到 Redis 的 LinuxShell 命令如下：
[ root@localhost ~]# cd /work/develop/LuaDemoProject/src/luaScript/module/ratelimit/
[ root@localhostratelimit ]# /usr/local/redis/bin/redis-cliscriptload"$(cat
redis_rate_limiter. lua)"
" 2 c 95 b 6 bc 3 be 1 aa 662 cfee 3 bdbd 6 f 00 e 8115 ac 657 "
然后来看 access_auth_evalsha. lua 限流脚本，该脚本使用 Redis 的 evalsha 操作指令远程访问加载
在 Redis 上的 redis_rate_limiter. lua 访问计数器脚本，完成针对同一个 IP 的限流操作。
access_auth_evalsha. lua 限流脚本的代码如下：


- --此脚本的环境：Nginx 内部
localRedisKeyRateLimiter=require ("luaScript. module. ratelimit. RedisKeyRateLimiter");
- -定义出错的 JSON 输出对象
localerrorOut={resp_code=- 1 ,resp_msg="限流出错", datas={}};
- -读取 get 参数
localargs=ngx. req. get_uri_args ()
- -获取用户的 IP
localshortKey= ngx. var. remote_addr;
- -没有限流键（Key），提示错误
ifnotshortKeyorshortKey==ngx. nullthen
    errorOut. resp_msg="shortKey 不能为空"
    ngx.say (cjson.encode (errorOut));
    return;
end
- -拼接计数的 Redis 键（Key）
localkey="count_rate_limit:ip: ".. shortKey;
locallimiter=RedisKeyRateLimiter: new (key);
localpassed=limiter: acquire ();
- -如果通过流量控制
ifpassedthen
    ngx. var. count=limiter: getCount ();
    - -注意，在这里直接输出会导致 content 阶段的指令被跳过
    - -ngx.say ("目前的访问总数：",limiter: getCount (),"<br>");
end
- -回收 Redis 连接
limiter: close ();
- -如果没有流量控制，终止 Nginx 的处理流程
ifnotpassedthen
    errorOut. resp_msg="抱歉，被限流了";
    ngx.say (cjson.encode (errorOut));
    ngx.exit (ngx. HTTP_UNAUTHORIZED);
end
return;
以上代码位于练习工程 LuaDemoProject 的 src/luaScript/module/ratelimit/文件夹下，文件名为
access_auth_evalsha. lua。在 Nginx 的 access 请求处理阶段，使用 access_auth_evalsha. lua 脚本进行请求
限流的配置如下：
location=/access/demo/evalsha/lua{
set$count 0 ;
access_by_lua_fileluaScript/module/ratelimit/access_auth_evalsha. lua;
content_by_lua_block{
ngx.say ("目前的访问总数：", ngx. var. count,"<br>");
ngx.say ("helloworld!");
}
}
以上配置位于练习工程 LuaDemoProject 的 src/conf/nginx-ratelimit. conf 文件中，在使之生效之前，
需要在 openresty-start. sh 脚本中换上该配置文件，然后重启 Nginx。
接下来开始限流自验证。在浏览器中访问以下地址：
[http://nginx.server/access/demo/evalsha/lua](http://nginx.server/access/demo/evalsha/lua)


10 秒之内连续刷新，发现第 10 次刷新之后请求被限流了，说明 Redis 内部的 Lua 限流脚本工作是
正常的，被限流后的输出如图 10 - 9 所示。

图 10 - 9 自验证时刷新 10 次之后的输出
通过将 Lua 脚本加载到 Redis 上执行，有以下优势：
1 ）减少网络开销：不使用 Lua 的代码需要向 Redis 发送多次请求，而脚本只需一次即可，减少
了网络传输。
2 ）原子操作：Redis 将整个脚本作为一个原子执行，无须担心并发，也就不需要事务。
3 ）复用：只要 Redis 不重启，脚本加载之后会一直缓存在 Redis 中，其他客户端就可以通过 sha 1
编码去执行。

### 10. 3 Nginx 漏桶限流详解

使用 Nginx 可从通过配置的方式完成接入层的限流，其 ngx_http_limit_req_module 模块所提供的
limit_req_zone 和 limit_req 两个指令使用漏桶算法进行限流。其中，limit_req_zone 指令用于定义一个
限流的具体规则（或者计数内存区），limit_req 指令则应用前者定义的规则完成限流动作。
假定要配置 Nginx 虚拟主机的限流规则为单 IP 限制为每秒 1 次请求，整个应用限制为每秒 10 次
请求，则具体的配置如下：
limit_req_zone $binary_remote_addr zone=perip: 10 m rate= 6 r/m;
limit_req_zone $server_name zone=perserver: 1 m rate= 10 r/s;
server{
listen 8081 ;
server_name localhost;
default_type'text/html';
charsetutf- 8 ;
limit_req zone=perip;
limit_req zone=perserver;
location/nginx/ratelimit/demo{
echo "-uri=$uri -remote_addr=$remote_addr"
"-server_name= $server_name";
}
}
上面的配置通过 limit_req_zone 指令定义了两条限流规则：第一条规则名为 perip，将来自每个
相同客户端 IP 的请求限速在 6 次/分钟（即 1 次/ 10 秒）；第二条限流规则名为 preserver，用于将同一
虚拟主机的请求限速在 10 次/秒。
以上配置位于练习工程 LuaDemoProject 的 src/conf/nginx-ratelimit. conf 文件中，在使之生效之前，
需要在 openresty-start. sh 脚本中换上该配置文件，然后重启 Nginx。
接下来开始验证上面的限流配置。在浏览器中输入如下测试地址：


```
http://nginx.server: 8081 /nginx/ratelimit/demo
10 秒内连续刷新，第 1 次的输出如图 10 - 10 所示。
```
```
图 10 - 10 Nginx 限流后 10 秒内连续刷新的第 1 次输出
10 秒内连续刷新，第 1 次之后的输出如图 10 - 11 所示。
```
图 10 - 11 Nginx 限流后 10 秒内连续刷新第 1 次之后的输出
接下来详细介绍一下 Nginx 的 limit_req_zone 和 limit_req 这两个指令。limit_req_zone 用于定义一
个限流的具体规则，limit_req 应用前者所定义的规则。limit_req_zone 指令的格式如下：

语法：limit_req_zonekeyzone=name:sizerate=rate[sync];
上下文：http 配置块
limit_req_zone 指令的 key 部分是一个表达式，它在运行时的值将作为流量计数的键（Key），
key 表达式可以是变量、文本和它们的组合。在上面的配置实例中，$binary_remote_addr 和
$server_name 为两个 Nginx 变量，$binary_remote_addr 为客户端 IP 地址二进制值，$server_name 为虚
拟主机名称。在限流规则应用之后，它们的值将作为限流的键，同一个 key 会在限流的共享内存区
域保存一份连接计数，而 limit_req_zone 限流指令所配置的速度限制只会对同一个键发生作用。
limit_req_zone 指令的 zone 属性用于定义存储相同 key 的连接计数的共享内存区，格式为
name: size，其中，name 表示共享内存区域的名称（或者说限流规则的名称），size 为内存区域的大
小。在上面的配置实例中，perip： 10 m 表示一个名为 perip、大小为 10 MB 的内存区域。 1 MB 大约能
存储 16000 个 IP 地址， 10 MB 大约可以存储 16 万个 IP 地址访问信息，也就是可以对 16 万客户端进行并
发限速。当共享内存区域耗尽时，Nginx 会使用 LRU 算法淘汰最长时间未使用的键。
limit_req_zone 指令的 rate 属性用于设置最大访问速率，rate= 10 r/s 表示一个键每秒最多能计数的
连接数为 10 个（ 10 个请求/秒），rate= 6 r/m 表示一个键每分钟最多能计数的连接数为 6 个（ 1 个请求/ 10
秒）。由于 Nginx 的漏桶限流的时间计算是基于毫秒的，当设置的速度为 6 r/m 时，转换一下就是 10
秒内单个 IP 只允许通过 1 个请求，在第 11 秒开始才允许通过第二个请求。
limit_req_zone 指令只是定义限流的规则和共享内存区，规则要生效的话，还得靠 limit_req 限流
指令完成。
limit_req 指令的格式如下：
语法：limit_req zone=name [burst=number] [nodelay|delay=number];
上下文：http 配置块，server 配置块，location 配置块


limit_req 指令的 zone 区域属性指定的限流共享内存区（或者说限流的规则）与上面限流规则指
令 limit_req_zone 中的 name 对应。limit_req 指令的 burst 突发属性表示可以处理的突发请求数。
limit_req 指令的第二个参数 burst 是爆发数量的意思，此参数设置一个大小为 number 的爆发缓冲
区，当有大量请求过来时，超过了限流频率的那些请求可以先放到爆发缓冲区内，直到爆发缓冲区
满后才拒绝。
limit_req 指令的 burst 参数的配置使得 Nginx 限流具备了一定的突发流量的缓冲能力（有点像令
牌桶）。但是 burst 的作用仅仅是让爆发的请求先放到队列里，然后慢慢处理，它的处理速度还是由
limit_req_zone 规则指令所配置的速度（比如 1 个请求/ 10 秒）决定，在速率低的情况下它的缓冲效果
其实并不是太理想。如果想迅速地处理爆发的请求，那么可以再加上 nodelay 参数，队列中的请求
会立即处理，而不再按照 rate 设置的速度（平均间隔）慢慢处理。

10. (^4) 实战：分布式令牌桶限流
本节的分布式令牌桶限流通过 Lua+Java 结合完成，首先在 Lua 脚本完成限流的计算，然后在 Java
代码中进行组织和调用。

#### 10. 4. 1 分布式令牌桶限流 Lua 脚本

分布式令牌桶限流 Lua 脚本的核心逻辑和 Java 令牌桶的执行逻辑是类似的，只是限流计算相关
的统计和时间数据存放于 Redis 中。
这里将限流的脚本命名为 rate_limiter. lua，该脚本既使用 Redis 存储令牌桶信息，自身又执行于
Redis 中，所以笔者将该脚本放置于 base-redis 基础模块中，它的代码如下：

- --此脚本的环境：Redis 内部，不是运行在 Nginx 内部
- --方法：申请令牌
- --- 1 failed
- -- 1 success
- --@paramkey 限流键
- --@paramapply 申请的令牌数量
localfunctionacquire (key, apply)
    localtimes=redis.call ('TIME');
    - -times[ 1 ]秒数 --times[ 2 ]微秒数
    localcurr_mill_second=times[ 1 ]* 1000000 +times[ 2 ];
    curr_mill_second=curr_mill_second/ 1000 ;
    localcacheInfo=redis.pcall ("HMGET", key,"last_mill_second","curr_permits",
"max_permits","rate")
- --局部变量：上次申请的时间
locallast_mill_second=cacheInfo[ 1 ];
- --局部变量：之前的令牌数
localcurr_permits=tonumber (cacheInfo[ 2 ]);
- --局部变量：桶的容量
localmax_permits=tonumber (cacheInfo[ 3 ]);
- --局部变量：令牌的发放速率
localrate=cacheInfo[ 4 ];
- --局部变量：本次的令牌数
locallocal_curr_permits=max_permits;
if (type (last_mill_second)~='boolean'andlast_mill_second~=nil) then
- -计算时间段内的令牌数


localreverse_permits=math.floor (((curr_mill_second-last_mill_second)/ 1000 )
*rate);

- -令牌总数
localexpect_curr_permits=reverse_permits+curr_permits;
- -可以申请的令牌总数
local_curr_permits=math.min (expect_curr_permits, max_permits);
else
- -第一次获取令牌
redis.pcall ("HSET", key,"last_mill_second", curr_mill_second)
end
localresult=- 1 ;
- -有足够的令牌可以申请
if (local_curr_permits-apply>= 0 ) then
- -保存剩余的令牌
redis.pcall ("HSET", key,"curr_permits", local_curr_permits-apply);
- -保存时间，下次获取令牌时使用
redis.pcall ("HSET", key,"last_mill_second", curr_mill_second)
- -返回令牌获取成功
result= 1 ;
else
- -保存令牌总数
redis.pcall ("HSET", key,"curr_permits", local_curr_permits);
- -返回令牌获取失败
result=- 1 ;
end
returnresult
end
- --方法：初始化限流器
- -- 1 success
- --@paramkeykey
- --@parammax_permits 桶的容量
- --@paramrate 令牌的发放速率
localfunctioninit (key, max_permits, rate)
localrate_limit_info=redis.pcall ("HMGET", key,"last_mill_second",
"curr_permits","max_permits","rate")
localorg_max_permits=tonumber (rate_limit_info[ 3 ])
localorg_rate=rate_limit_info[ 4 ]
if (org_max_permits==nil) or (rate~=org_rateormax_permits~=org_max_permits)
then
redis.pcall ("HMSET", key,"max_permits", max_permits,"rate", rate,
"curr_permits", max_permits)
end
return 1 ;
end
- --方法：删除限流的键
localfunctiondelete (key)
redis.pcall ("DEL", key)
return 1 ;
end
localkey=KEYS[ 1 ]
localmethod=ARGV[ 1 ]
ifmethod=='acquire'then
returnacquire (key, ARGV[ 2 ], ARGV[ 3 ])
elseifmethod=='init'then
returninit (key, ARGV[ 2 ], ARGV[ 3 ])
elseifmethod=='delete'then
returndelete (key)


```
else
```
- -ignore
end
该脚本有 3 个方法，其中 2 个方法比较重要：
1 ）限流器初始化方法 init (key, max_permits, rate)，此方法在限流开始时被调用。
2 ）限流检测方法 acquire (key, apply)，此方法在请求到来时被调用。

#### 10. 4. 2 Java 分布式令牌桶限流

rate_limiter. lua 脚本既可以在 Java 中调用，也可以在 Nginx 中调用。本小节先介绍它在 Java 中的
使用，下一章再介绍它在 Nginx 中的使用。
Java 分布式令牌桶限流器的实现就是通过 Java 代码向 Redis 加载 rate_limiter. lua 脚本，然后封装
其令牌桶初始化方法 init (...) 和限流监测方法 acquire (...)，以供外部调用，它的代码如下：
packagecom. crazymaker. SpringCloud. standard. ratelimit;
...
/**
*实现：令牌桶限流服务
*createby 尼恩@疯狂创客圈
**/
@Slf 4 j
publicclassRedisRateLimitImplimplementsRateLimitService, InitializingBean
{
/**
*限流器的 Rediskey 前缀
*/
privatestaticfinalStringRATE_LIMITER_KEY_PREFIX="rate_limiter: ";
//privateScheduledExecutorServiceexecutorService=
Executors.newScheduledThreadPool ( 1 );
privateRedisRateLimitPropertiesredisRateLimitProperties;
privateRedisTemplateredisTemplate;
//Lua 脚本的实例
privatestaticRedisScript<Long>rateLimiterScript=null;
//Lua 脚本的类路径
privatestaticStringrateLimitLua="script/rate_limiter. lua";
static
{
//从类路径文件中加载令牌桶 Lua 脚本
Stringscript=IOUtil.loadJarFile (RedisRateLimitImpl.class.getClassLoader (),
rateLimitLua);
if (StringUtils.isEmpty (script))
{
log.error ("luascriptloadfailed: "+rateLimitLua);
}else
{
//创建 Lua 脚本实例
rateLimiterScript=newDefaultRedisScript<>(script, Long. class);
}
}
publicRedisRateLimitImpl (
RedisRateLimitPropertiesredisRateLimitProperties,


RedisTemplateredisTemplate)
{
this. redisRateLimitProperties=redisRateLimitProperties;
this. redisTemplate=redisTemplate;
}
privateMap<String,LimiterInfo>limiterInfoMap=newHashMap<>();
/**
*限流器的信息
*/
@Builder
@Data
publicstaticclassLimiterInfo
{
/**
*限流器的键，如秒杀的 ID
*/
privateStringkey;
/**
*限流器的类型，如 seckill
*/
privateStringtype="default";
/**
*限流器的最大桶容量
*/
privateIntegermaxPermits;
/**
*限流器的速率
*/
privateIntegerrate;
/**
*限流器的 Redis 键（Key）
*/
publicStringfullKey ()
{
returnRATE_LIMITER_KEY_PREFIX+type+": "+key;
}
/**
*限流器的在 map 中的缓存键（Key）
*/
publicStringcashKey ()
{
returntype+": "+key;
}
}
/**
*限流检测：是否超过 Redis 令牌桶限速器的限制
*
*@paramcacheKey 计数器的键
*@returntrueorfalse
*/
@Override
publicBooleantryAcquire (StringcacheKey)
{
if (cacheKey==null)
{
returntrue;
}


if (cacheKey.indexOf (": ")<= 0 )
{
cacheKey="default: "+cacheKey;
}
LimiterInfolimiterInfo=limiterInfoMap.get (cacheKey);
if (limiterInfo==null)
{
returntrue;
}
Longacquire=(Long) redisTemplate.execute (rateLimiterScript,
ImmutableList.of (limiterInfo.fullKey ()),
"acquire",
" 1 ");
if (acquire== 1 )
{
returnfalse;
}
returntrue;
}
/**
*重载方法：限流器初始化
*
*@paramlimiterInfo 限流的类型
*/
publicvoidinitLimitKey (LimiterInfolimiterInfo)
{
if (null==rateLimiterScript)
{
return;
}
StringmaxPermits=limiterInfo.getMaxPermits (). toString ();
Stringrate=limiterInfo.getRate (). toString ();
//执行 redis 脚本
Longresult=(Long) redisTemplate.execute (rateLimiterScript,
ImmutableList.of (limiterInfo.fullKey ()),
"init",
maxPermits,
rate);
limiterInfoMap.put (limiterInfo.cashKey (), limiterInfo);
}

/**
*限流器初始化
*
*@paramtype 类型
*@paramkey ID
*@parammaxPermits 上限
*@paramrate 速度
*/
publicvoidinitLimitKey (Stringtype, Stringkey, IntegermaxPermits, Integerrate)
{
LimiterInfolimiterInfo=LimiterInfo.builder ()
.type (type)
.key (key)
.maxPermits (maxPermits)
.rate (rate)
.build ();
initLimitKey (limiterInfo);


```
}
/**
*获取 RedisLua 脚本的 SHA 1 编码，并缓存到 Redis
*/
publicStringcacheSha 1 ()
{
Stringsha 1 =rateLimiterScript. getSha 1 ();
redisTemplate.opsForValue (). set ("lua: sha 1 : rate_limiter", sha 1 );
returnsha 1 ;
}
}
```
#### 10. 4. 3 Java 分布式令牌桶限流的自验证

###### 自验证的工作：首先初始化的分布式令牌桶限流器，然后使用两条线程不断进行限流的检测。

###### 自验证的代码如下：

```
packagecom. crazymaker. SpringCloud. ratelimit;
...
@Slf 4 j
@RunWith (SpringRunner. class)
//指定启动类
@SpringBootTest (classes={DemoCloudApplication. class})
/**
*Redis 分布式令牌桶测试类
*/
publicclassRedisRateLimitTest
{
@Resource (name="redisRateLimitImpl")
RedisRateLimitImpllimitService;
//线程池，用于多线程模拟测试
privateExecutorServicepool=Executors.newFixedThreadPool ( 10 );
@Test
publicvoidtestRedisRateLimit ()
{
//初始化的分布式令牌桶限流器
limitService.initLimitKey (
"seckill", //Rediskey 中的类型
" 10000 ", //Rediskey 中的业务 key，比如商品 id
2 ,//桶容量
2 );//每秒令牌数
AtomicIntegercount=newAtomicInteger ();
longstart=System.currentTimeMillis ();
//线程数
finalintthreads= 2 ;
//每条线程的执行轮数
finalintturns= 20 ;
//同步器
CountDownLatchcountDownLatch=newCountDownLatch (threads);
for (inti= 0 ;i<threads; i++)
{
pool.submit (()->
{
try
{
//每个用户访问轮次（turns）
```

for (intj= 0 ;j<turns; j++)
{
booleanlimited=limitService.tryAcquire ("seckill: 10000 ");
if (limited)
{
count.getAndIncrement ();
}
Thread.sleep ( 200 );
}
}catch (Exceptione)
{
e.printStackTrace ();
}
countDownLatch.countDown ();
});
}
try
{
countDownLatch.await ();
}catch (InterruptedExceptione)
{
e.printStackTrace ();
}
floattime=(System.currentTimeMillis ()-start)/ 1000 F;
//输出统计结果
log.info ("限制的次数为："+count.get ()+"时长为："+time);
log.info ("限制的次数为："+count.get ()+
", 通过的次数为："+(threads*turns-count.get ()));
log.info ("限制的比例为："+(float) count.get ()/(float)(threads*turns));
log.info ("运行的时长为："+time);
try
{
Thread.sleep (Integer. MAX_VALUE);
}catch (InterruptedExceptione)
{
e.printStackTrace ();
}
}
}
两条线程各运行 20 次，每一次运行后休眠 200 毫秒，总计耗时 4 秒运行 40 次，部分输出结果
如下：
[main]INFO c.c.s.r.RedisRateLimitTest-限制的次数为： 32 时长为： 4. 015
[main]INFO c.c.s.r.RedisRateLimitTest-限制的次数为： 32 ,通过的次数为： 8
[main]INFO c.c.s.r.RedisRateLimitTest-限制的比例为： 0. 8
[main]INFO c.c.s.r.RedisRateLimitTest-运行的时长为： 4. 015
大家可以自行调整参数运行以上自验证程序并观察实验结果，体验一下分布式令牌桶限流的
效果。


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
限流：计数器、漏桶、令牌桶三大算法的原理与实战（史上最全）
https://www.cnblogs.com/crazymakercircle/p/ 15187184 .html

##### 3. 相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题 ：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4


# 第 11 章 Spring Cloud Nginx 秒杀实战

###### 在开发高并发系统时有三把利器——缓存、降级和限流来保护系统。缓存的目的是提升系统

###### 访问速度和增大系统能处理的容量，可谓是抗高并发流量的银弹；降级是当服务出问题或者影响到

###### 核心流程的性能时需要暂时屏蔽掉服务请求，待高峰或者问题解决后再打开；而有些场景并不能用

###### 缓存和降级来解决，比如稀缺资源（秒杀、抢购）、写服务（如评论、下单）、频繁的复杂查询（评

###### 论的最后几页），因此需有一种手段来限制这些场景的并发/请求量，即限流。

本章将通过一个综合性的实战——SpringCloudNginx 秒杀实战，介绍缓存、降级和限流的综
合应用。

11. (^1) 秒杀业务特定和技术难点

###### 秒杀和抢购类的案例在生活中随处可见，比如商品抢购、春运抢票和微信群抢红包。

###### 从业务的角度来说，秒杀业务非常简单：根据先后顺序下订单减库存。主要有以下特点：

###### 1 ）秒杀一般是访问请求数量远远大于库存数量，只有少部分用户能够秒杀成功，在这种场景

###### 下，需要借助分布式锁等保障数据一致性。

###### 2 ）秒杀时大量用户会在同一时间同时进行抢购，网站瞬时访问流量激增，这就需要进行削峰

###### 和限流。

#### 11. 1. 1 秒杀系统的业务功能

###### 从系统角度来说，秒杀系统的业务功能分成两大维度：

###### 1 ）商户维度的业务功能。

###### 2 ）用户维度的业务功能。

###### 秒杀系统的业务功能具体如图 11 - 1 所示。

```
图 11 - 1 秒杀系统的业务功能
```

###### 1 .商户维度的业务功能

###### 商户维度的业务功能主要涉及两个操作。

###### （ 1 ）增加秒杀

###### 通过后台的管理控制台界面增加特定商品、特定数量、特定时段的秒杀。

###### （ 2 ）暴露秒杀

###### 将符合条件的秒杀暴露给用户，以便互联网用户能参与商品的秒杀。这个操作可以由商户手

###### 动完成，在生产场景下，更合理的方式是系统自动维护。

###### 2 .用户维度的业务功能

###### 用户维度的业务功能主要涉及两个操作。

###### （ 1 ）减库存

###### 减库存简单来说就是减少被秒杀到的商品的库存数量，这也是秒杀系统中的一个处理难点。

###### 为什么呢？这不仅仅需要考虑如何避免同一用户重复秒杀的行为，而且在多个微服务并发情况下需

###### 要保障库存数据的一致性，避免超卖的情况发生。

###### （ 2 ）下订单

###### 减库存后需要下订单，也就是在订单表中添加订单记录，记录购买用户的姓名、手机号、购

买的商品 id 等。与减库存相比，下订单相对比较简单。

```
这里为了聚焦高并发技术知识体系的介绍，对秒杀业务功能进行了瘦身，去掉
了一些其他的但是也非常重要的功能，比如支付功能、提醒功能等。同时，由于商户维度的
业务功能比较简单，更多的是模型对象的 CRUD 操作逻辑，因此这里也对其进行了简化。
```
#### 11. 1. 2 秒杀系统面临的技术难题

###### 总体来说，秒杀系统面临的技术难题大致有如下几点。

###### （ 1 ）限流

###### 鉴于只有少部分用户能够秒杀成功，所以要限制大部分流量，只允许少部分流量进入服务后端。

###### （ 2 ）分布式缓存

###### 秒杀系统最大的瓶颈一般都是数据库读写，由于数据库读写属于磁盘 IO，性能很低，如果能

###### 够把部分数据或业务逻辑转移到分布式缓存中，那么效率会有极大的提升。

###### （ 3 ）可拓展

###### 秒杀系统的服务节点一定是可以弹性拓展的。如果流量来了，可以按照流量预估进行服务节

###### 点的动态增加和删除。比如淘宝、京东等双十一活动时，会增加大量机器应对交易高峰。

###### （ 4 ）超卖或者少卖问题

###### 比如 10 万次请求同时发起秒杀请求，正常需要进行 10 万次库存扣减，但是由于某种原因造成

###### 了多减库存或者少减库存，就会出现超卖或少卖问题。


###### （ 5 ）削峰

###### 秒杀系统是一个高并发系统，采用异步处理模式可以极大地提高系统并发量，实际上，削峰

###### 的典型实现方式就是通过消息队列实现异步处理。限流完成之后，对于后端系统而言，秒杀系统仍

###### 然会瞬时涌入大量请求，所以在抢购一开始会有很高的瞬间峰值。高峰值流量是压垮后端服务和数

###### 据库的很重要的原因，秒杀后端需要将瞬间的高流量变成一段时间内的平稳流量，常用的解决方法

###### 是利用消息中间件进行请求的异步处理。

11. (^2) 秒杀系统的系统架构
本节分多个维度介绍 crazy-SpringCloud 开发脚手架的架构，包括分层架构、限流架构、分布式
锁架构、削峰的架构。

#### 11. 2. 1 秒杀的分层架构

###### 从分层的角度来说，秒杀系统架构可以分成 3 层，大致如下：

###### 1 ）客户端：负责内容提速和交互控制。

###### 2 ）接入层：负责认证、负载均衡、限流。

###### 3 ）业务层：负责保障秒杀的数据一致性。

###### 1 .客户端负责内容提速和交互控制

###### 客户端需要完成秒杀商品的静态化展示。无论是在桌面浏览器还是移动端 APP 上展示秒杀商品，

###### 秒杀商品的图片和文字元素都需要尽可能静态化，尽量减少动态元素，这样就可以通过 CDN 来提

###### 速和抗峰值。

###### 另外，在客户端这一层的用户交互上需要具备一定的控制用户行为和禁止重复秒杀的能力。

###### 比如，当用户提交秒杀请求之后，可以将秒杀按钮置灰，禁止重复提交。

###### 2 .接入层负责认证、负载均衡、限流

###### 秒杀系统的特点是并发量极大，但实际的优惠商品有限，秒杀成功的请求数量很少，所以如

###### 果不在接入层进行拦截，则大量请求会造成数据库连接耗尽、服务端线程耗尽，导致整体雪崩。因

###### 此，必须在接入层进行用户认证、负载均衡、接口限流。

对于总流量较小的系统，可以在内部网关（如 Zuul）完成认证、负载均衡、接口限流的功能，
具体的分层架构如图 11 - 2 所示。


图 11 - 2 内部网关（如 Zuul）完成认证、负载均衡、接口限流
对于总流量较大的系统，会有一层甚至多层外部网关，因此，限流的职责会从内部网关剥离到外
部网关，内部网关（如 Zuul）仍然具备权限认证、负载均衡的能力，具体的分层架构如图 11 - 3 所示。

图 11 - 3 外部网关与内部网关相结合完成认证、负载均衡、接口限流
3 .业务层负责保障数据一致性
秒杀的业务逻辑主要是下订单和减库存，都是数据库操作。大家都知道，数据库层只能承担“能
力范围内”的访问请求，是最脆弱的一层，也是需要进行事务保护的一层。在业务层，还需要防止超
出库存的秒杀（超卖和少买），为了安全起见，可以使用分布式锁对秒杀的数据库操作进行保护。


#### 11. 2. 2 秒杀的限流架构

###### 前面提到，秒杀系统中秒杀商品总是有限的。除此之外，服务节点的处理能力、数据库的处

###### 理能力也都是有限的，因此，需要根据系统的负载能力进行秒杀限流。

###### 总体来说，在接入层可以进行两个级别的限流策略：应用级别的限流和接口级别的限流。

###### 什么是应用级别的限流策略呢？对于整个应用系统来说，一定会有一个 QPS 的极限值，如果超

###### 过了极限值，则整个应用就会不响应或响应得非常慢，因此需要在整个应用的维度做好应用级别的

###### 限流配置。

###### 应用级别的限流应该配置在最顶层的反向代理，具体如图 11 - 4 所示。

图 11 - 4 接入层的限流架构
应用级别的流量可以通过 Nginx 的 limit_req_zone 和 limit_req 两个指令完成。假定要配置 Nginx 虚拟
主机的限流规则为单 IP 限制为每秒 1 次请求，整个应用限制为每秒 10 次请求，则具体的配置如下：
limit_req_zone $binary_remote_addr zone=perip: 10 m rate= 1 r/s;
limit_req_zone $server_name zone=perserver: 1 m rate= 10 r/s;
server{
...
limit_req zone=perip burst= 5 ;
limit_req zone=perserver burst= 10 ;
}
什么是接口级别的限流策略呢？如果单个接口有突发访问情况，可能会由于突发访问量太大
造成系统崩溃，典型的就是本章所介绍的秒杀类接口。接口级别的限流就是配置单个接口的请求速
率，是细粒度的限流。
接口级别的限流也可以通过 Nginx 的 limit_req_zone 和 limit_req 两个指令配合完成，对获取秒杀
令牌的接口，同时进行用户 id 和商品 id 限流的配置大致如下：
limit_req_zone $arg_goodId zone=pergood: 10 m rate= 100 r/s;
limit_req_zone $arg_userId zone=peruser: 1 m rate= 1 r/s;
server{


# lua：获取秒杀令牌
location =/seckill-provider/api/seckill/redis/token/v 2 {
limit_req zone=peruser burst= 5 ;
limit_req zone=pergood burst= 10 ;
#获取秒杀令牌lua脚本
content_by_lua_fileluaScript/module/seckill/getToken. lua;
}
}
以上定义了两个限流规则 pergood 和 peruser：pergood 规则根据请求参数的 goodId 值进行限流，
同一个 goodId 值的限速为每秒 100 次请求；peruser 规则根据请求参数的 userId 值进行限流，同一个
userId 值的限速为每秒 1 次请求。
Nginx 的限流指令只能在同一块内存区域有效，而在生产场景中秒杀的外部网关往往是多节点
部署，所以这就需要用到分布式限流组件。高性能的分布式限流组件可以使用 Redis+Lua 来开发，
京东的抢购就是使用 Redis+Lua 完成限流的，并且无论是 Nginx 外部网关还是 Zuul 内部网关，都可以
使用 Redis+Lua 限流组件。
理论上，接入层的限流有多个维度：
1 ）用户维度限流：在某一时间段内只允许用户提交一次请求，比如可以采取客户端 IP 或者用
户 id 作为限流的键（Key）。
2 ）商品维度的限流：对于同一个抢购商品，在某个时间段内只允许一定数量的请求进入，可
以采取秒杀商品 id 作为限流的键（Key）。

无论是哪个维度的限流，只要掌握其中的一个，其他维度的限流在技术实现上都是差不多的。
本书的秒杀练习，使用的是接口级别的限流策略，在获取秒杀令牌的 REST 接口时，针对每一个秒
杀商品的 ID 配置限流策略，限制每一个商品 id 每秒内允许通过的请求次数。
如果大家对用户维度限流感兴趣，可以自行修改配置进行尝试。

#### 11. 2. 3 秒杀的分布式锁架构

###### 前面提到了超卖或少卖问题：比如 10 万次请求同时发起秒杀请求，正常需要进行 10 万次库存

###### 扣减，但是由于某种原因造成了多减库存或者少减库存，这就会出现超卖或少卖问题。

###### 解决超卖或者少卖问题有效的办法之一就是利用分布式锁对同一个商品的并行数据库操作予

###### 以串行化。秒杀场景的分布式锁应该具备的条件如下：

###### 1 ）一个方法在同一时间只能被一个机器的一个线程执行。

###### 2 ）高可用地获取锁与释放锁。

###### 3 ）高性能地获取锁与释放锁。

###### 4 ）具备可重入特性。

###### 5 ）具备锁失效机制，防止死锁。

###### 6 ）具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败。

常用的分布式锁有两种：ZooKeeper 分布式锁和 Redis 分布式锁。使用 ZooKeeper 分布式锁来保
护秒杀的数据库操作的架构图大致如图 11 - 5 所示。


图 11 - 5 使用 ZooKeeper 分布式锁来保护秒杀数据库操作
实际上，除了提供分布式锁外，ZooKeeper 还具有提供高可靠的分布式计数器、高可靠的分布
式 ID 生成器的基础能力。ZooKeeper 分布式计数器、分布式锁、分布式 ID 生成器等基础知识，也是
大家必须系统学习和掌握的知识，但是不在这里介绍，如果对这一块不了解，请翻阅本书姊妹篇书
籍《Java 高并发核心编程卷 1 （加强版）：NIO、Netty、Redis、ZooKeeper》。
ZooKeeper 分布式锁虽然高可靠，但是性能不高，不能满足秒杀场景分布式锁的第三个条件（高
性能地获取锁与释放锁），所以在秒杀的场景建议使用 Redis 分布式锁来保护秒杀的数据库操作。

#### 11. 2. 4 秒杀的削峰架构

###### 通过接入网关的限流能够拦截无效的刷单请求和超出预期的那部分请求，但是，当秒杀的订

###### 单量很大时，比如有 100 万商品需要参与秒杀，这时后端服务层和数据库的并发请求压力至少为 100

###### 万，这种请求下需要使用消息队列进行削峰。

###### 削峰从本质上来说就是更多地延缓用户请求，以及层层过滤用户的访问需求，遵从“最后落

###### 地到数据库的请求数要尽量少”的原则。通过消息队列可以大大地缓冲瞬时流量，把同步的直接调

###### 用转换成异步的间接推送，中间通过一个队列在入口承接瞬时的流量洪峰，在出口平滑地将消息推

###### 送出去。消息队列就像“水库”一样，拦蓄上游的洪水，削减进入下游河道的洪峰流量，从而达到

###### 减免洪水灾害的目的。使用消息队列对秒杀进行削峰的具体架构如图 11 - 6 所示。

###### 对于秒杀消息的入队，可以直接在内部网关完成。内部网关在完成用户的权限验证、秒杀令

###### 牌的有效性验证之后，将秒杀消息发往消息队列即可。秒杀服务通过消息队列的订阅完成秒杀消息

###### 的消费。

常用消息队列系统：Kafka、RocketMQ、ActiveMQ、RabbitMQ、ZeroMQ、MetaMQ 等。本书
的内容主要聚焦在 SpringCloud 和 Nginx，对于消息队列在这里不做过多的介绍，使用消息队列进行
削峰的秒杀实现版本，可参见疯狂创客圈社群博客。


```
图 11 - 6 使用消息队列对秒杀进行削峰
```
### 11. 3 秒杀业务的参考实现

本节从功能入手重点介绍 SpringCloud 秒杀实战的业务处理的 3 层实现：dao 层、service 层、
controller 层。

#### 11. 3. 1 秒杀的功能模块和接口设计

###### 秒杀系统的实现会有多种多样的版本，本书从方便演示的角度出发，设计了一个相当简单的

###### 秒杀练习版本，具体分为 3 个主要的模块。

（ 1 ）seckill-web 模块
此模块是一个独立的 SpringBoot 程序，作为一个静态的 Web 服务器独立运行，主要运行秒杀的
前端页面、脚本。在生产场景中，为了提高性能，可以将这个模块的所有静态资源全部迁移到 Nginx
高性能 Web 服务器。

（ 2 ）seckill-provider 模块
秒杀的后端 SpringCloud 微服务提供者主要运行获取秒杀令牌、秒杀订单等后端相关接口。
（ 3 ）uaa-provider 模块
用户账号与认证（UAA）的后端 SpringCloud 微服务提供者主要运行用户认证、用户信息相关
的后端接口。

以上 3 个模块的关系为：seckill-web 模块作为静态资源程序会将秒杀的操作页面呈现给用户，
seckill-web 的页面会根据用户的操作将相应的 URL 接口通过 Nginx 外部网关跳过内部网关 Zuul 直接
发送给后端的 uaa-provider 和 seckill-provider 微服务提供者。为什么要跳过 Zuul 内部网关呢？内部网
关需要对请求的 URL 做用户权限验证，如果请求没有带 token 或者没有通过验证，则会被拦截并返
回未授权的错误。这里为了在练习时调试方便，建议直接跳过 Zuul 内部网关的权限验证功能，通过
Nginx 的反向代理将请求直接代理到后端的微服务提供者。
秒杀练习系统中， 3 个模块的关系如图 11 - 7 所示。


图 11 - 7 秒杀练习系统中 3 个模块的关系
本秒杀练习系统中的秒杀操作流程，大致有以下 4 步：
（ 1 ）前端设置秒杀用户
在用户点击后，seckill-web 的前端页面将通过请求 uaa-provider 服务的/api/user/detail/v 1 接口获
取用户信息。在实际的秒杀场景中这一步是不需要的，因为这一步所获取的用户信息就是当前登录
用户本人的信息。

（ 2 ）前端设置秒杀商品
seckill-web 的前端页面通过请求 seckill-provider 服务的/api/seckill/good/detail/v 1 接口获取所需
要的秒杀商品。而在 seckill-provider 服务后端，会将商品的库存信息缓存到 Redis 方便下一步的秒杀
令牌的获取。

（ 3 ）前端获取秒杀令牌
seckill-web 的前端页面通过请求 seckill-provider 服务的/api/seckill/redis/token/v 1 接口获取商品的
秒杀令牌，执行秒杀操作，减少商品的 Redis 库存。后端接口首先减 Redis 库存量，如果减库存成功，
则生成秒杀专用的令牌存入 Redis，在下一步用户下单时拿来进行验证。如果减 Redis 库存失败，则
返回对应的错误提示。这一步操作没有涉及数据库，对库存的减少操作直接在 Redis 中完成，所减
少的并不是真正的商品库存。

（ 4 ）前端用户下单
seckill-web 的前端页面通过请求 seckill-provider 服务的/api/seckill/redis/do/v 1 接口执行真正的下
单操作。后端接口会判断秒杀专用的令牌是否有效，如果有效，则执行真正的下单操作，在数据库
减库存和生成秒杀订单，然后返回给前端。
秒杀练习系统的秒杀业务流程大致如图 11 - 8 所示。
开发过程中，为了使得来自 seckill-web 前端页面的请求能够顺利地跳过内部网关 Zuul 直接发送
给后端的微服务提供者 uaa-provider 和 seckill-provider，这里特意配置了一份专门的 Nginx 配置文件
nginx-seckill. conf，对秒杀练习的 3 大模块进行了定制化的反向代理配置，在启动 Nginx 的脚本
openresty-start. sh 文件中使用这份配置文件即可。


```
图 11 - 8 秒杀练习中的秒杀执行流程
```
配置文件 nginx-seckill. conf 的核心配置如下：
server{
listen 80 default;
server_name nginx. server*. nginx. server;
default_type'text/html';
charsetutf- 8 ;
#默认的代理
location/{
proxy_set_headerX-Real-IP$remote_addr;
proxy_set_headerX-Forward-For$proxy_add_x_forwarded_for;
proxy_set_headerHost$http_host;
proxy_set_headerX-Nginx-Proxytrue;
#代理到配置的上游Zuul网关
proxy_passhttp://Zuul;
}
#用户服务 ：开发调试的反向代理配置
location ^~/uaa-provider/{
#代理到Windows开发环境
#proxy_passhttp :// 192. 168. 233. 1 : 7702 /;
#代理到自验证CentOS环境
proxy_passhttp:// 192. 168. 233. 128 : 7702 /uaa-provider/;
}
#秒杀服务 ：开发调试的反向代理配置
location ^~/seckill-provider/{
#代理到Windows开发环境
proxy_passhttp:// 192. 168. 233. 1 : 7701 /seckill-provider/;
}
#秒杀Web页面 ：开发调试的反向代理配置
location ^~/seckill-web/{
#代理到Windows开发环境


proxy_passhttp:// 192. 168. 233. 1 : 6601 /seckill-web/;
}
...
}
由于笔者在开发过程中将 seckill-web、seckill-provider 两个进程启动在 IDEA 中（Windows 开发
环境），而 uaa-provider 进程运行在自验证 CentOS 环境（虚拟机）中，因此才进行上面的反向代理
配置。更多有关环境和运行的内容使用视频方式介绍起来更加直接，读者可查看疯狂创客圈社群的
秒杀练习演示视频。
接下来，为大家介绍一下秒杀练习的操作流程的特点，有以下 3 点。
（ 1 ）增加了获取秒杀令牌的环节，将秒杀和下单操作分离
这样做的好处有两个方面：一方面，可以让秒杀操作和下单操作从执行上进行分离，使得秒
杀操作可以独立于订单相关业务；另一方面，秒杀接口可以阻挡大部分并发流程，从而避免让低效
率的下单操作耗费大量的计算资源。

（ 2 ）前端缺少了轮询环节
在生产场景中，用户获取令牌后，前端应该会自动发起下单操作，然后通过前端 Ajax 脚本轮
询是否下单成功。本练习实例为了清晰地展示秒杀操作过程，将自动的下单操作修改成了手动下单
操作，由于后端下单没有经过消息队列做异步处理，因此前端也不需要做结果的轮询。

（ 3 ）后端缺少失效令牌的库存恢复操作
在生产场景中，存在用户拿到令牌而不去完成下单导致令牌失效的情况。所以，后端需要有
定时任务对秒杀令牌进行有效性检查，如果令牌没有被使用或者生效，则需要恢复 Redis 中的秒杀
库存，方便后边的请求去秒杀。无效令牌检查的定时任务可以设置为每分钟一次或者每两分钟一次，
以保障被无效令牌消耗的库存能够得到及时恢复。

#### 11. 3. 2 数据表和 PO 实体类设计

###### 秒杀系统的表设计还是相对简单清晰的，主要涉及两张核心表：秒杀商品表和订单表。

###### 当然，实际秒杀场景肯定不止这两张表，还有比如与付款信息相关的其他配套表，出于学习

###### 目的，这里我们只考虑秒杀系统的核心表，不考虑实际系统所涉及的其他配套表。

###### 为了与两个核心表相对应，在系统中设计了两个 PO 实体类：秒杀商品 PO 类和秒杀订单 PO 类。

本书的命名规范：Java 实体类统一使用 PO 作为后缀，Java 传输类统一使用 DTO 作为后缀。
由于本案例使用 JPA 作为持久层框架，可以基于 PO 类逆向生成数据库的表，因此这里就不对
数据表的结构进行展开说明，而是对 PO 类进行说明。
秒杀商品 PO 类 SeckillGoodPO 的代码如下：
packagecom. crazymaker. SpringCloud. seckill. dao. po;
//省略 import
/**
*秒杀商品 PO
*说明：秒杀商品表（PO）和主商品表（PO）不同
*/
@Entity
@Table (name="SECKILL_GOOD")


@Data
@AllArgsConstructor
@NoArgsConstructor
@Builder
publicclassSeckillGoodPOimplementsSerializable
{
//商品 id
@Id
@GenericGenerator (
name="snowflakeIdGenerator",
strategy="com. crazymaker. SpringCloud. standard. hibernate.
CommonSnowflakeIdGenerator")
@GeneratedValue (strategy=GenerationType. IDENTITY, generator=
"snowflakeIdGenerator")
@Column (name="GOOD_ID", unique=true, nullable=false, length= 8 )
privateLongid;
//商品标题
@Column (name="GOOD_TITLE", length= 400 )
privateStringtitle;
//商品标题
@Column (name="GOOD_IMAGE", length= 400 )
privateStringimage;
//商品原价格
@Column (name="GOOD_PRICE")
privateBigDecimalprice;
//商品秒杀价格
@Column (name="COST_PRICE")
privateBigDecimalcostPrice;
//创建时间
@DateTimeFormat (pattern="yyyy-MM-ddHH:mm: ss")
@JsonFormat (pattern="yyyy-MM-ddHH:mm: ss", timezone="GMT+ 8 ")
@Column (name="CREATE_TIME")
privateDatecreateTime;
//秒杀开始时间
@DateTimeFormat (pattern="yyyy-MM-ddHH:mm: ss")
@JsonFormat (pattern="yyyy-MM-ddHH:mm: ss", timezone="GMT+ 8 ")
@Column (name="START_TIME")
privateDatestartTime;
//秒杀结束时间
@DateTimeFormat (pattern="yyyy-MM-ddHH:mm: ss")
@JsonFormat (pattern="yyyy-MM-ddHH:mm: ss", timezone="GMT+ 8 ")
@Column (name="END_TIME")
privateDateendTime;
//剩余库存数量
@Column (name="STOCK_COUNT")
privatelongstockCount;
//原始库存数量
@Column (name="raw_stock")
privatelongrawStockCount;
}
秒杀订单 PO 类 SeckillOrderPO 的代码如下：
packagecom. crazymaker. SpringCloud. seckill. dao. po;
//省略 import
/**
*秒杀订单 PO（对应于秒杀订单表）


*/
@Entity
@Table (name="SECKILL_ORDER")
@Data
@AllArgsConstructor
@NoArgsConstructor
@Builder
publicclassSeckillOrderPOimplementsSerializable
{
//订单 ID
@Id
@GenericGenerator (
name="snowflakeIdGenerator",
strategy="com. crazymaker. SpringCloud. standard. hibernate.
CommonSnowflakeIdGenerator")
@GeneratedValue (strategy=GenerationType. IDENTITY, generator=
"snowflakeIdGenerator")
@Column (name="ORDER_ID", unique=true, nullable=false, length= 8 )
privateLongid;
//支付金额
@Column (name="PAY_MONEY")
privateBigDecimalmoney;
//秒杀的用户 id
@Column (name="USER_ID")
privateLonguserId;
//创建时间
@DateTimeFormat (pattern="yyyy-MM-ddHH:mm: ss")
@JsonFormat (pattern="yyyy-MM-ddHH:mm: ss", timezone="GMT+ 8 ")
@Column (name="CREATE_TIME")
privateDatecreateTime;
//支付时间
@DateTimeFormat (pattern="yyyy-MM-ddHH:mm: ss")
@JsonFormat (pattern="yyyy-MM-ddHH:mm: ss", timezone="GMT+ 8 ")
@Column (name="PAY_TIME")
privateDatepayTime;
//秒杀商品 id
@Column (name="GOOD_ID")
privateLonggoodId;
//订单状态，- 1 表示无效， 0 表示成功， 1 表示已付款
@Column (name="STATUS")
privateShortstatus;
}
在秒杀系统中，SECKILL_GOOD 商品表的 GOOD_ID 字段和 SECKILL_ORDER 订单表中的
GOOD_ID 字段在业务逻辑上存在一对多的关系，但是不建议在数据库层面使用表与表之间的外键
关系。为什么呢？因为如果秒杀订单量巨大，则必须进行分库分表，这时 SECKILL_ORDER 表和
SECKILL_GOOD 表中 GOOD_ID 相同的数据可能分布在不同的数据库中，所以数据库表层面的关联
关系可能会导致维护起来非常困难。

#### 11. 3. 3 使用分布式 ID 生成器

在实际的开发中，很多的项目为了应付交付和追求速度，简单粗暴地使用了 Java 的 UUID 作为
数据的 ID。实际上，由于 UUID 非常长，除了占用大量存储空间外，主要的问题是在索引上，在建


###### 立索引和基于索引进行查询时都存在性能问题。有关 UUID 的不足和分布式 ID 生成器的原理，笔者

在《Java 高并发核心编程卷 1 （加强版）：NIO、Netty、Redis、ZooKeeper》一书中做了非常细致
的总结，这里不再赘述。
这里使用主流的 ZooKeeper+Snowflake 算法的方式实现了高性能的 Long 类型分布式 ID 生成器，
并且封装成了一个通用的 Hibernate 的 ID 生成器类 CommonSnowflakeIdGenerator，具体的代码如下：
packagecom. crazymaker. SpringCloud. standard. hibernate;
...
/**
*通用的分布式 HibernateID 生成器
*buildby 尼恩@疯狂创客圈
**/
publicclassCommonSnowflakeIdGeneratorextendsIncrementGenerator
{
/**
*生成器的 map 缓存
*key 为 PO 类名，value 为分布式 ID 生成器
*/
privateMap<String,SnowflakeIdGenerator>generatorMap=newLinkedHashMap<>();
/**
*从父类继承方法：生成分布式 ID
*/
@Override
publicSerializablegenerate (
SharedSessionContractImplementorsessionImplementor, Objectobject)
throwsHibernateException
{
/**
* 获取 PO 的类名
* 作为 ID 的类型
* 每一个 PO 类型对应一个分布式 ID 生成器
*/
Stringtype=object.getClass (). getSimpleName ();
Serializableid=null;
/**
*从 map 中获取分布式 ID 生成器
*/
IdGeneratoridGenerator=getFromMap (type);
/**
*调用生成器的 ZooKeeper+Snowflake 算法生成 ID
*/
id=idGenerator.nextId ();
if (null!=id)
{
returnid;
}
/**
*如果生成失败，则通过父类生成
*/
id=sessionImplementor.getEntityPersister (null, object)
.getClassMetadata (). getIdentifier (object, sessionImplementor);
returnid!=null?id: super.generate (sessionImplementor, object);
}
/**


*从 map 中获取缓存的分布式 ID 生成器，没有则创建一个
*
*@paramtype 生成器的绑定类型为 PO 类名
*@return 分布式 ID 生成器
*/
publicsynchronizedIdGeneratorgetFromMap (Stringtype)
{
if (generatorMap.containsKey (type))
{
returngeneratorMap.get (type);
}
/**
*创建分布式 ID 生成器，并且存入 map
*/
SnowflakeIdGeneratoridGenerator=newSnowflakeIdGenerator (type);
generatorMap.put (type, idGenerator);
returnidGenerator;
}
}
以上的 HibernateID 生成器仅仅是对 ZooKeeper+Snowflake 算法分布式 ID 生成器的简单封装，有
关 ZooKeeper+Snowflake 算法分布式 ID 生成器的原理，可参考《Java 高并发核心编程卷 1 （加强版）：
NIO、Netty、Redis、ZooKeeper》一书，这里不再赘述。

11. 3. (^4) 秒杀的 controller 层设计
本节首先介绍一下秒杀练习的 REST 接口设计，然
后介绍其 controller 层的大致实现逻辑。启动秒杀服务
seckill-provider，然后通过 SwaggerUI 界面访问其 REST
接口清单，如图 11 - 9 所示。
秒杀服务 seckill-provider 的 controller 层的 REST 接
口分为 4 个部分：
（ 1 ）秒杀练习 RedisLock 版本
此秒杀版本含有两个接口，一个获取令牌接口和
一个执行秒杀接口。此版本使用 RedisLock 分布式锁保
护秒杀数据库操作。
（ 2 ）秒杀练习 ZkLock 版本
此秒杀版本也包含两个接口，一个获取令牌接口和一个执行秒杀接口。此版本使用 ZkLock 分
布式锁保护秒杀数据库操作。此版本的意义是为大家学习和使用 ZooKeeper 分布式锁提供案例。
（ 3 ）秒杀练习商品管理
此部分 REST 接口主要对秒杀的商品进行 CRUD 操作。
（ 4 ）秒杀练习订单管理
此部分 REST 接口主要对秒杀的订单进行查询、清除操作。
由于各部分 REST 接口所涉及的知识体系大致相同，因此本书仅介绍秒杀练习 RedisLock 版本的
controller 层实现，其他的 controller 层接口大家可自行分析和研究。
图 11 - 9 秒杀练习的 REST 接口示意图


秒杀练习 RedisLock 版本的 controller 层类的代码如下：
packagecom. crazymaker. SpringCloud. seckill. controller;
//省略 import
@RestController
@RequestMapping ("/api/seckill/redis/")
@Api (tags="秒杀练习 RedisLock 版本")
publicclassSeckillByRedisLockController
{
/**
*秒杀服务实现 Bean
*/
@Resource
RedisSeckillServiceImplredisSeckillServiceImpl;
/**
*获取秒杀的令牌
*/
@ApiOperation (value="获取秒杀的令牌")
@PostMapping ("/token/v 1 ")
RestOut<String>getSeckillToken (
@RequestBodySeckillDTOdto)
{
Stringresult=redisSeckillServiceImpl.getSeckillToken (
dto.getSeckillGoodId (),
dto.getUserId ());
returnRestOut.success (result). setRespMsg ("这是获取的结果");
}
/**
*执行秒杀的操作
*
*@return
*/
@ApiOperation (value="秒杀")
@PostMapping ("/do/v 1 ")
RestOut<SeckillOrderDTO>executeSeckill (@RequestBodySeckillDTOdto)
{
SeckillOrderDTOorderDTO=redisSeckillServiceImpl.executeSeckill (dto);
returnRestOut.success (orderDTO). setRespMsg ("秒杀成功");
}
}
以上的 SeckillByRedisLockController 仅做了 REST 服务的发布，真正的秒杀逻辑在服务层的
RedisSeckillServiceImpl 类中实现。

#### 11. 3. 5 service 层逻辑：获取秒杀令牌

###### 本书的秒杀案例特意删除了服务层的接口类，只剩下了服务层的实现类，表面上违背了“面

###### 向接口编程”的原则，实际上这样做能使代码更加干净和简洁，也减少了代码维护的工作量。之所

###### 以可以这样简化，主要的原因是删除的那些接口类都是单实现类接口（一个接口只有一个实现类），

###### 那些接口在使用时也不会存在将多种实现对象赋值给同一个接口变量的多态情况。笔者从事开发多

###### 年，可谓经历项目无数，发现有很多实际项目出于“面向接口编程”的原则，写了无数个单实现类

###### 接口，将“面向接口编程”的编程原则僵化和教条化。


回到主题，下面给大家介绍 RedisSeckillServiceImpl
秒杀实现类，该类主要有两个功能：获取秒杀令牌和
完成秒杀下单。
本小节介绍第一个功能——获取秒杀令牌，该功能
由 getSeckillToken 方法实现，具体的流程图如图 11 - 10 所示。
获取秒杀令牌方法的输入为用户的 userId 和秒杀
商品的 seckillGoodId，其输出为一个代表秒杀令牌的
UUID 字符串，获取秒杀令牌方法的重点是进行以下 3
个判断：

1 ）判断秒杀的商品是否存在，如果不存在，则抛
出对应异常。
2 ）判断秒杀商品的库存是否足够，如果没有足够
库存，则抛出对应异常。
3 ）判断用户是否已经获取过商品的秒杀令牌，如
果获取过，则抛出对应异常。

只有秒杀商品存在、库存足够，而且之前没有被
userId 代表的用户秒杀过这三个条件都满足的情况下，
才能允许用户获取商品的秒杀令牌。
获取秒杀令牌的代码节选如下：
packagecom. crazymaker. SpringCloud. seckill. service. impl;
//省略 import
@Configuration
@Slf 4 j
@Service
publicclassRedisSeckillServiceImpl
{
/**
*秒杀商品的 DAO 数据操作类
*/
@Resource
SeckillGoodDaoseckillGoodDao;
/**
*秒杀订单的 DAO 数据操作类
*/
@Resource
SeckillOrderDaoseckillOrderDao;
/**
*Redis 分布式锁实现类
*/
@Autowired
RedisLockServiceredisLockService;
/**
*缓存数据操作类
*/
@Resource
RedisRepositoryredisRepository;

```
图 11 - 10 获取秒杀令牌流程图
```

/**
*秒杀令牌操作的脚本
*/
staticStringseckillLua="script/seckill. lua";
staticRedisScript<Long>seckillScript=null;
{
Stringscript=IOUtil.loadJarFile (RedisLockService.class.getClassLoader (),
seckillLua);
seckillScript=newDefaultRedisScript<>(script, Long. class);
}
/**
*获取秒杀令牌
*
*@paramseckillGoodId 秒杀 id
*@paramuserId 用户 id
*@return 令牌信息
*/
publicStringgetSeckillToken (LongseckillGoodId, LonguserId)
{
Stringtoken=UUID.randomUUID (). toString ();
Longres=redisRepository.executeScript (
seckillScript, //Lua 脚本对象
Collections.singletonList ("setToken"), //执行 Lua 脚本的 key
String.valueOf (seckillGoodId), //执行 Lua 脚本的 value 1
String.valueOf (userId), //执行 Lua 脚本的 value 2
token //执行 Lua 脚本的 value 3
);
if (res== 2 )
{
throwBusinessException.builder (). errMsg ("秒杀商品没有找到"). build ();
}
if (res== 4 )
{
throwBusinessException.builder (). errMsg ("库存不足, 稍后再来"). build ();
}
if (res== 5 )
{
throwBusinessException.builder (). errMsg ("已经排队过了"). build ();
}
if (res!= 1 )
{
throwBusinessException.builder (). errMsg ("排队失败, 未知错误"). build ();
}
returntoken;
}
//省略下单部分代码
}
通过上面的代码可以看出，getSeckillToken 方法并没有获取令牌的核心逻辑，仅调用缓存在
Redis 内部的 seckill. lua 脚本的 setToken 方法去判断和设置秒杀令牌，然后对 seckill. lua 脚本的返回值
进行判断，并根据不同的返回作出不同的响应。
设置令牌的核心逻辑存在于 seckill. lua 脚本中，为什么要用 Lua 脚本呢？


1 ）由于 Lua 脚本在 Redis 内部作为一个整体执行，中间不会被其他命令插入，天然具备分布式
锁的特点，不需要使用专门的分布式锁对设置令牌的逻辑进行并发控制。
2 ）秒杀令牌在 Redis 中进行缓存，在设置新令牌之前，需要查找旧的令牌并进行是否存在的判
断，如果这些逻辑都写在 Java 程序中，则完成查找旧令牌和设置新令牌需要多次的 Redis 往返操作，
也就是说需要进行多次网络传输。大家知道，网络的传输延迟是损耗性能的大户，所以使用 Lua 脚
本能减少网络传输次数，从而提高性能。

在 seckill. lua 脚本中，除了有 setToken 方法外，还有其他的方法如 checkToken 方法，该脚本稍后
再为大家统一介绍。

#### 11. 3. 6 service 层逻辑：执行秒杀下单

前面讲到 RedisSeckillServiceImpl 秒杀实现类
主要有两个功能：获取秒杀令牌和完成秒杀下单。
下面来看秒杀下单的业务逻辑。
秒杀下单很简单、很清晰，就是两点：减库存
和存储用户秒杀订单明细。但是其中会涉及两个问
题：

1 ）数据一致性问题：同一商品在秒杀商品表
中的库存数和在订单表中的订单数需要保持一致。
2 ）超卖问题：秒杀商品的剩余库存数不能为
负数。

以上两个问题主要借助 Redis 分布式锁解决。
另外，由于代码中存在减库存和存储秒杀订单两次
数据库操作，为了防止出现一次失败一次成功的情
况，需要通过数据库事务对这两次操作进行数据一
致性保护。
秒杀下单的执行流程具体如图 11 - 11 所示。
由于存在数据库事务，因此将秒杀下单的整体
流程分成两个方法来实现：

1 ）executeSeckill (SeckillDTO)：负责下单前的
分布式锁获取和库存的检查。
2 ）doSeckill (SeckillDTO)：负责真正的下单操作（ 1 ，减库存； 2 ，存储秒杀订单）。
秒杀下单流程的实现代码如下：
packagecom. crazymaker. SpringCloud. seckill. service. impl;
//省略 import
@Configuration
@Slf 4 j
@Service
publicclassRedisSeckillServiceImpl
{

```
图 11 - 11 秒杀下单的流程图
```

/**
*秒杀商品的 DAO 数据操作类
*/
@Resource
SeckillGoodDaoseckillGoodDao;
/**
*秒杀订单的 DAO 数据操作类
*/
@Resource
SeckillOrderDaoseckillOrderDao;

/**
*Redis 分布式锁实现类
*/
@Autowired
RedisLockServiceredisLockService;
/**
*执行秒杀下单
*
*@paraminDto
*@return
*/
publicSeckillOrderDTOexecuteSeckill (SeckillDTOinDto)
{
longgoodId=inDto.getSeckillGoodId ();
LonguserId=inDto.getUserId ();
//判断令牌是否有效
Longres=redisRepository.executeScript (
seckillScript,Collections.singletonList ("checkToken"),
String.valueOf (inDto.getSeckillGoodId ()),
String.valueOf (inDto.getUserId ()),
inDto.getSeckillToken ()
);
if (res!= 5 )
{
throwBusinessException.builder (). errMsg ("请提前排队"). build ();
}
/**
*创建订单对象
*/
SeckillOrderPOorder=
SeckillOrderPO.builder ()
.goodId (goodId). userId (userId). build ();
DatenowTime=newDate ();
order.setCreateTime (nowTime);
order.setStatus (SeckillConstants. ORDER_VALID);
StringlockValue=UUID.randomUUID (). toString ();
SeckillOrderDTOdto=null;
/**
*创建重复性检查的订单对象
*/
SeckillOrderPOcheckOrder=
SeckillOrderPO.builder (). goodId (
order.getGoodId ()). userId (order.getUserId ()). build ();
//记录秒杀订单信息


longinsertCount=seckillOrderDao.count (Example.of (checkOrder));
//唯一性判断：goodId, id 保证一个用户只能秒杀一件商品
if (insertCount>= 1 )
{
//重复秒杀
log.error ("重复秒杀");
throwBusinessException.builder (). errMsg ("重复秒杀"). build ();
}
/**
*获取分布式锁
*/
StringlockKey="seckill:lock: "+String.valueOf (goodId);
booleanlocked=redisLockService.acquire (lockKey, lockValue, 1 ,
TimeUnit. SECONDS);
/**
*执行秒杀，秒杀前先抢到分布式锁
*/
if (locked)
{
Optional<SeckillGoodPO>optional=seckillGoodDao. findById
(order.getGoodId ());
if (! optional.isPresent ())
{
//秒杀不存在
throwBusinessException.builder (). errMsg ("秒杀不存在"). build ();
}
//查询库存
SeckillGoodPOgood=optional.get ();
if (good.getStockCount ()<= 0 )
{
//重复秒杀
throwBusinessException.builder (). errMsg ("秒杀商品被抢光"). build ();
}
order.setMoney (good.getCostPrice ());
try
{
/**
*进入秒杀事务
*执行秒杀逻辑： 1 ，减库存； 2 ，存储秒杀订单
*/
doSeckill (order);
dto=newSeckillOrderDTO ();
BeanUtils.copyProperties (order, dto);
}finally
{
try
{
/**
*释放分布式锁
*/
redisLockService.release (lockKey, lockValue);
}catch (Exceptione)
{
e.printStackTrace ();
}
}
}else


{
throwBusinessException.builder (). errMsg ("获取分布式锁失败"). build ();
}
returndto;
}
/**
*下单操作，加上了数据库事务
*
*@paramorder 订单
*/
@Transactional
publicvoiddoSeckill (SeckillOrderPOorder)
{
/**
*插入秒杀订单
*/
seckillOrderDao.save (order);
//减库存
seckillGoodDao.updateStockCountById (order.getGoodId ());
}
}
executeSeckill 在执行秒杀前，调用 seckill. lua 脚本中的 checkToken 方法判断令牌是否有效。如果
Lua 脚本的 checkToken 方法的返回值不是 5 （令牌有效标识），则抛出运行时异常。

#### 11. 3. 7 秒杀的 Lua 脚本设计

前面讲到，在 seckill. lua 脚本中完成设置令牌和令牌检查的工作有两大优势：一是在 Redis 内部
执行 Lua 脚本天然具备分布式锁的特点，二是能减少网络传输次数提高性能。
seckill. lua 脚本中定义了两个方法：setToken（令牌设置）方法和 checkToken（令牌检查）方法。
其中，setToken 方法的执行流程如下：

1 ）检查秒杀令牌是否存在，如果存在，则返回标志 5 ，表明排队过了。
2 ）检查以 JSON 格式缓存的秒杀商品的库存是否足够，如果库存不够，则返回标志 4 ，表明库
存不足。
3 ）为秒杀商品减少一个库存，并编码成 JSON 格式，再一次缓存起来。
4 ）使用 hset 命令将用户的秒杀令牌保存在 Redis 哈希表结构中，它的哈希键为用户的 userId。
5 ）最终返回标志 1 ，表明排队成功。
checkToken 方法的执行流程如下：
1 ）使用 hget 命令从保存秒杀令牌的 Redis 哈希表结构中，以用户的 userId 作为哈希键，取出之
前缓存的秒杀令牌。
2 ）如果令牌获取成功，则返回标志 5 ，表明排队成功。
3 ）如果令牌不存在，则返回标志‒ 1 ，表明没有排队。
seckill. lua 脚本的代码如下：


- -返回值说明
- - 1 排队成功
- - 2 排队商品没有找到
- - 3 人数超过限制
- - 4 库存不足
- - 5 排队过了
- - 6 秒杀过了
- -- 2 Lua 方法不存在
localfunctionsetToken (goodId, userId, token)
    - -检查秒杀令牌是否存在
    localoldToken=redis.call ("hget","seckill:queue: ".. goodId, userId);
    ifoldTokenthen
       return 5 ;--返回 5 说明之前已经排队过了
    end
    - -获取商品缓存次数
    localgoodJson=redis.call ("get","seckill:goods: ".. goodId);
    ifnotgoodJsonthen
       - -redis.debug ("秒杀商品没有找到")
       return 2 ; --返回 2 说明秒杀商品没有找到
    end
    - -redis.log (redis. LOG_NOTICE, goodJson)
    localgoodDto=cjson.decode (goodJson);
    - -redis.log (redis. LOG_NOTICE,"goodtitle=".. goodDto. title)
    localstockCount=tonumber (goodDto. stockCount);
    - -redis.log (redis. LOG_NOTICE,"stockCount=".. stockCount)
    ifstockCount<= 0 then
       return 4 ; --返回 4 说明库存不足
    end
    stockCount=stockCount- 1 ;
    goodDto. stockCount=stockCount;
    redis.call ("set","seckill:goods: "..goodId,cjson.encode (goodDto));
    redis.call ("hset","seckill:queue: ".. goodId, userId, token);
    return 1 ;--返回 1 说明排队成功
end
- -返回值说明
- - 5 排队过了
- -- 1 没有排队
localfunctioncheckToken (goodId, userId, token)
    - -检查令牌是否存在
    localoldToken=redis.call ("hget","seckill:queue: ".. goodId, userId);
    ifoldTokenand (token==oldToken) then
       - -return 1 ;
       return 5 ;--返回 5 说明排队过了
    end
    return- 1 ;--返回- 1 说明没有排队
end

localmethod=KEYS[ 1 ] --执行 Lua 脚本时传入的 key 1
localgoodId=ARGV[ 1 ] --执行 Lua 脚本时传入的 value 1
localuserId=ARGV[ 2 ] --执行 Lua 脚本时传入的 value 2
localtoken=ARGV[ 3 ] --执行 Lua 脚本时传入的 value 3
ifmethod=='setToken'then
returnsetToken (goodId, userId, token)
elseifmethod=='checkToken'then
returncheckToken (goodId, userId, token)
else


return- 2 ;--Lua 方法不存在
end
以上的 seckill. lua 脚本在 Java 中可以通过 spring-data-redis 包的以下方法执行：
RedisTemplate. execute（RedisScript<T>script, List<K>keys, Object... args）
在开发脚本的过程中往往需要进行脚本调试，可以通过 Shell 指令 redis-cli--eval 直接执行
seckill. lua 脚本，具体的调试执行过程可查看疯狂创客圈社群的秒杀练习演示视频。

#### 11. 3. 8 BusinessException 业务异常定义

###### 减库存操作和插入购买明细操作都会产生很多业务异常，比如库存不足、重复秒杀等，这些

业务异常与 crazy-SpringCloud 脚手架中其他业务异常一样，全部被封装成 BusinessException 通用业
务异常实例抛出。
一般的项目都是怎么划分自定义异常的呢？大致有两种方式：
1 ）按异常来源所处的 controller、service、dao 的层次来划分业务异常，例如 DaoException、
ServiceException、ControllerException 等。
2 ）按异常来源所处的模块组件（如数据库、消息中间件、业务模块）来划分业务异常，例如
MysqlExceptioin、RedisException、ElasticSearchException、SeckillException 等。

无论按照哪个维度划分都出于同一个目的：一旦出现异常，就可以很容易定位到是哪个层或
组件出现了问题。
在实际开发过程中，定义太多的异常类型之后，需要不厌其烦地将异常一层层抛出，一层层
捕获，反而会提升代码的复杂度。所以，虽然 crazy-SpringCloud 脚手架和其他项目一样定义了一个
自己的全局的异常基类 BusinessException ，但是 crazy-SpringCloud 脚手没有定义太多的业务异常子
类。一般情况下，重新定义一个异常的子类其实没有太多必要，因为可以根据异常的编码和异常的
消息进行区分。
crazy-SpringCloud 脚手架的基础业务异常类 BusinessException 的代码如下：
packagecom. crazymaker. SpringCloud. common. exception;
//省略 ipmort
@Builder
@Data
@AllArgsConstructor
publicclassBusinessExceptionextendsRuntimeException
{
privatestaticfinallongserialVersionUID= 1 L;
/**
*默认的错误编码
*/
privatestaticfinalintDEFAULT_CODE=- 1 ;
/**
*默认的错误提示
*/
privatestaticfinalStringDEFAULT_MSG="业务异常";
/**
*业务错误编码
*/


@lombok. Builder. Default
privateinterrCode=DEFAULT_CODE;
/**
*错误的提示信息
*/
@lombok. Builder. Default
privateStringerrMsg=DEFAULT_MSG;
publicBusinessException ()
{
super (DEFAULT_MSG);
}
/**
*带格式设置异常消息
*@paramformat 格式
*@paramobjects 替换的对象
*/
publicBusinessExceptionsetDetail (Stringformat, Object... objects){
format=StringUtils.replace (format,"{}","%s");
this. errMsg=String.format (format, objects);
returnthis;
}
}
该类有 errCode 和 errMsg 两个属性，其中，errCode 属性用于存放异常的编码，errMsg 属性用于
存放一些错误附加信息。
注意 BusinessException 类继承了 RuntimeException 运行时异常类，而不是 Exception 受检异常基
类，表明该类其实是一个非受检的运行时异常类。
为什么要这样呢？有两个原因：
1 ）默认情况下，SpringBoot 事务只有检查到 RuntimeException 运行时异常才会回滚，如果检查
到的是普通的受检异常，SpringBoot 事务是不会回滚的，除非经过特殊配置。
2 ）简化编程的代码，若无必要，不需要在业务程序中对异常进行捕获，而是由项目中的全局
异常解析器统一负责处理。

```
crazy-SpringCloud 脚手架的全局异常解析器 ExceptionResolver 的代码如下：
packagecom. crazymaker. SpringCloud. standard. config;
//省略 import
/**
*ExceptionResolver
*/
@Slf 4 j
@RestControllerAdvice
publicclassExceptionResolver
{
/**
* 其他异常
*/
privatestaticfinalStringOTHER_EXCEPTION_MESSAGE="其他异常";
/**
* 业务异常
*/
privatestaticfinalStringBUSINESS_EXCEPTION_MESSAGE="业务异常";
/**
*业务异常处理
```

*
*@paramrequest 请求体
*@parame 异常实例
*@return RestOut
*/
@Order ( 1 )
@ExceptionHandler (BusinessException. class)
publicRestOut<String>businessException (HttpServletRequestrequest,
BusinessExceptione)
{
log.info (BUSINESS_EXCEPTION_MESSAGE+": "+e.getErrMsg ());
returnRestOut.error (e.getErrMsg ());
}
/**
*业务异常之外的其他异常处理
*
*@paramrequest 请求体
*@parame 异常实例
*@returnRestOut
*/
@Order ( 2 )
@ExceptionHandler (Exception. class)
publicRestOut<String>finalException (HttpServletRequestrequest, Exceptione)
{
e.printStackTrace ();
log.error (OTHER_EXCEPTION_MESSAGE+": "+e.getMessage ());
returnRestOut.error (e.getMessage ());
}
}
上面的 ExceptionResolver 全局异常解析器使用了 SpringBoot 的@RestControllerAdvice 注解，该
注解首先会对系统的异常进行拦截并且交给对应的异常处理方法进行处理，然后将异常处理结果返
回给客户端。
ExceptionResolver 的每个异常处理方法都使用@ExceptionHandler 注解配置自己希望处理的异
常类型，传入的参数为异常类型的 class 实例，如果要处理多个异常类型，其参数可以是一个异常类
型 class 实例数组。需要注意的是，不能在两个异常处理方法的@ExceptionHandler 注解中去配置同
一个异常类型，如果一种异常类型被处理多次，则在初始化全局异常解析器时会失败。

### 11. 4 Zuul 内部网关实现秒杀限流

秒杀限流操作既可以在内部网关 Zuul 中完成，又可以在外部网关 Nginx 中完成。内部网关 Zuul
可以以 ZuulFilter 过滤器的形式对获取秒杀令牌的请求进行拦截，然后通过 Redis 令牌桶限流服务实
现分布式限流。
从前面的内容可知，Redis 中存储限流令牌桶信息的是一个哈希表结构，其内部的“键－值对”
包括 max_permits、curr_permits、rate、last_mill_second 四个哈希键，而整个令牌桶哈希表结构的缓
存键的格式为“rate_limiter:seckill: 1 ”（ 1 为商品 id），其中重要的部分是秒杀商品 id，该 id 表示限
流统计的范围是针对一个秒杀商品的，而不是针对整个秒杀接口。
秒杀商品（假设 ID 为 1 ）的限流令牌桶的 Redis 哈希表结构具体如图 11 - 12 所示。


在秒杀没有开始之前，需要初始化限流令牌桶的 Redis 哈希表结构，虽然真正的初始化工作是
在 rate_limit. lua 脚本中完成的，但是需要通过 Java 程序进行调用，并传入相关的初始化参数。
什么时候进行限流令牌桶的初始化呢？在生产环境中的秒杀开始之前，应该有一个秒杀商品暴
露（或者启动）的动作，该动作可以手动或者自动完成，限流的初始化工作可以在秒杀暴露时完成。

```
图 11 - 12 存储令牌桶限流信息的 Redis 哈希表结构
下面是一个限流初始化的简单示例：
packagecom. crazymaker. SpringCloud. seckill. controller;
//省略 import
@RestController
@RequestMapping ("/api/seckill/good/")
@Api (tags="秒杀练习商品管理")
publicclassSeckillGoodController
{
/**
*开启商品秒杀
*
*@paramdto 商品 id
*@return 商品 goodDTO
*/
@PostMapping ("/expose/v 1 ")
@ApiOperation (value="开启商品秒杀")
RestOut<SeckillGoodDTO>expose (@RequestBodySeckillDTOdto)
{
LonggoodId=dto.getSeckillGoodId ();
SeckillGoodDTOgoodDTO=seckillService.findGoodByID (goodId);
if (null!=goodDTO)
{
//初始化秒杀的限流器
rateLimitService.initLimitKey (
"seckill",
String.valueOf (goodId),
SeckillConstants. MAX_ENTER,
SeckillConstants. PER_SECKOND_ENTER
);
/**
*缓存限流 Lua 脚本的 SHA 1 编码，方便在其他地方获取
*/
rateLimitService. cacheSha 1 ();
/**
*缓存秒杀 Lua 脚本的 SHA 1 编码，方便在其他地方获取
*/
redisSeckillServiceImpl. cacheSha 1 ();
returnRestOut.success (goodDTO). setRespMsg ("秒杀开启成功");
}
returnRestOut.error ("秒杀开启失败");
}
...
}
```

限流初始化之后，就可以在 Zuul 内部网关或者 Nginx 外部网关进行请求拦截，使用分布式限流
器进行限流。Zuul 内部网关的限流拦截过程具体如图 11 - 13 所示。

```
图 11 - 13 Zuul 内部网关限流拦截示意图
Zuul 网关限流过滤器类 SeckillRateLimitFilter 的代码如下：
packagecom. crazymaker. SpringCloud. cloud. center. zuul. filter;
//省略 import
@Slf 4 j
@ConditionalOnBean (RedisRateLimitImpl. class)
@Component
publicclassSeckillRateLimitFilterextendsZuulFilter
{
/**
*Redis 限流服务实例
*/
@Resource (name="redisRateLimitImpl")
RateLimitServiceredisRateLimitImpl;
@Override
publicStringfilterType ()
{
return"pre";//路由之前
}
/**
*过滤的顺序
*/
@Override
publicintfilterOrder ()
{
return 0 ;
}
/**
*这里可以写逻辑判断，是否要过滤，true 为永远过滤
*/
@Override
publicbooleanshouldFilter ()
{
RequestContextctx=RequestContext.getCurrentContext ();
HttpServletRequestrequest=ctx.getRequest ();
```

/**
*如果请求已经被其他的过滤器终止，则本过滤器也不做处理
**/
if (! ctx.sendZuulResponse ())
{
returnfalse;
}
/**
*对秒杀令牌进行限流
*/
if (request.getRequestURI (). startsWith ("/seckill-provider/api/seckill/redis/
token/v 1 "))
{
returntrue;
}
returnfalse;
}
/**
*过滤器的具体逻辑
*/
@Override
publicObjectrun ()
{
RequestContextctx=RequestContext.getCurrentContext ();
HttpServletRequestrequest=ctx.getRequest ();
StringgoodId=request.getParameter ("goodId");
if (goodId!=null)
{
StringcacheKey="seckill: "+goodId;
Booleanlimited=redisRateLimitImpl.tryAcquire (cacheKey);
if (limited)
{
/**
*被限流后的降级
*/
Stringmsg="参与抢购的人太多，请稍后再试一试";
fallback (ctx, msg);
returnnull;
}
returnnull;
}else
{
/**
*参数输入错误时的降级处理
*/
Stringmsg="必须输入抢购的商品";
fallback (ctx, msg);
returnnull;
}
}
/**
*被限流后的降级处理
*
*@paramctx
*@parammsg
*/
privatevoidfallback (RequestContextctx, Stringmsg)


```
{
ctx.setSendZuulResponse (false);
try
{
ctx.getResponse (). setContentType ("text/html; charset=utf- 8 ");
ctx.getResponse (). getWriter (). write (msg);
}catch (Exceptione)
{
e.printStackTrace ();
}
}
}
```
### 11. 5 Nginx 高性能秒杀和限流

从性能上来说，内部网关 Zuul 限流理论上会比外部网关 Nginx 限流的性能会差一些。和 Zuul 一
样，外部网关 Nginx 也可以通过 Lua 脚本的形式执行缓存在 Redis 内部的令牌桶限流脚本来实现分布
式限流。
Nginx 秒杀限流有两种架构：
（ 1 ）Nginx 限流+Zuul 认证和路由+seckill-provider 微服务秒杀
这种架构属于非常典型的 Nginx+SpringCloud 微服务架构，限流的逻辑处于外部网关 Nginx 中，
用户的权限认证处于内部网关 Zuul 中，而获取秒杀令牌的逻辑处于 seckill-provider 微服务中。
这种典型的 Nginx+SpringCloud 微服务架构的秒杀流程具体如图 11 - 14 所示。

```
图 11 - 14 Nginx+SpringCloud 微服务架构的秒杀流程
（ 2 ）Nginx 限流+Lua 脚本秒杀
这种架构属于高性能的秒杀架构，不仅限流逻辑处于外部网关 Nginx 中，就连获取秒杀令牌逻
```

辑也处于外部网关 Nginx 中。和上一种秒杀架构相比，这种纯 Nginx+Lua 架构绝对能提高不少的性
能。为什么呢？因为除了 Nginx 本身的高性能之外，纯 Nginx+Lua 的架构还减少了两次网络传输，
而网络传输都是耗时较高的操作。
Nginx+Lua 架构的秒杀流程具体如图 11 - 15 所示。

图 11 - 15 Nginx+Lua 的架构的秒杀流程
学会了第二种纯 Nginx+Lua 架构的实现，第一种架构的实现也就迎刃而解了，所以这里为大家
展开介绍第二种架构的具体实现。纯 Nginx+Lua 架构的实现涉及以下两个 Lua 脚本：

1 ）getToken. lua：此脚本完成秒杀令牌的设置和获取。
2 ）getToken_access_limit. lua：此脚本完成分布式限流。
以上两个脚本，getToken. lua 执行在 Nginx 请求处理的 content 阶段，getToken_access_limit. lua 执
行在 Nginx 请求处理的 access 阶段。这两个脚本在 nginx-seckill. conf 文件中的具体配置如下：
#Nginx +lua 秒杀：获取秒杀令牌
location =/seckill-provider/api/seckill/redis/token/v 2 {
default_type'application/json';
charsetutf- 8 ;
#限流的Lua脚本
access_by_lua_fileluaScript/module/seckill/getToken_access_limit. lua;
#获取秒杀令牌的Lua脚本
content_by_lua_fileluaScript/module/seckill/getToken. lua;
}

#### 11. 5. 1 Lua 脚本：获取秒杀令牌

获取秒杀令牌脚本 getToken. lua 的逻辑与 seckill-provider 微服务模块中的 getSeckillToken 方法基
本类似，该脚本并没有判断和设置秒杀令牌的核心逻辑，仅仅调用缓存在 Redis 内部的 seckill. lua 脚
本的 setToken 方法设置和获取秒杀令牌，然后对 seckill. lua 脚本的返回值进行判断，并根据不同的返
回值做出不同的响应。
getToken. lua 脚本和 seckill. lua 脚本都是 Lua 脚本，但是执行的地点不同：getToken. lua 脚本执行
在 Nginx 中，而 seckill. lua 脚本执行在 Redis 中；getToken. lua 通过 evalsha 方法去调用缓存在 Redis 中的


seckill. lua 脚本。getToken. lua 脚本和 seckill. lua 脚本的关系如图 11 - 16 所示。

图 11 - 16 getToken. lua 脚本和 seckill. lua 脚本的关系
什么时候在 Redis 中加载 seckill. lua 脚本呢？和限流脚本一样，该脚本是在 Java 程序启动商品秒
杀时完成其在 Redis 的加载和缓存的，并且 Java 程序会将 seckill. lua 脚本加载完成之后的 SHA 1 编码通
过自定义的 key（具体为 lua: sha 1 :seckill）缓存在 Redis 中，以方便 Nginx 中的 getToken. lua 脚本去获取，
并在调用 evalsha 方法时使用。
什么是 SHA 1 编码呢？Redis 在缓存完 Lua 脚本后，会返回该脚本的固定长度的 SHA 1 编码，作为
Lua 脚本的摘要提供给外部调用 Lua 脚本使用。SHA- 1 摘要是通过 SHA- 1 （SecureHashAlgorithm 1 ，
即安全散列算法 1 ，也被称为安全哈希算法 1 ）算法生成的。SHA- 1 算法是第一代“安全散列算法”
的缩写，它的本质就是一个哈希（Hash）算法，主要用于生成字符串摘要（摘要经私有密钥加密
后就可成为数字签名），该算法曾被认为是 MD 5 算法的后继者。SHA- 1 算法能将一个最大 264 比特
的字符串散列成一串 160 位（ 20 字节）的散列值（即哈希值），散列值通常的呈现形式为 40 个十六
进制数。SHA- 1 算法始终能保证任何两组不同的字符串产生的摘要是不同的。
getToken. lua 获取秒杀脚本的代码如下：

- --此脚本的环境：Nginx 内部，不是运行在 Redis 内部
- --启动调试
- -localmobdebug=require ("luaScript. initial. mobdebug");
- -mobdebug.start ();
- -导入自定义的基础模块
- -localbasic=require ("luaScript. module. common. basic");
- -导入自定义的 RedisOperator 模块
localredisExecutor=require ("luaScript. redis. RedisOperator");
- -导入自定义的 uuid 模块
localuuid=require'luaScript. module. common. uuid'
- -ngx.print ("======"..uuid.generate ())
- -读取 post 参数
ngx. req. read_body ();
localdata=ngx. req. get_body_data ();--获取消息体
- -字符串转成 JSON
localargs=cjson.decode (data);
localgoodId=args["seckillGoodId"];
localuserId=args["userId"];
- -生成令牌的 uuid
localtoken=uuid.generate ();
localrestOut={resp_code= 0 ,resp_msg="操作成功", datas={}};
localerrorOut={resp_code=- 1 ,resp_msg="操作失败", datas={}};

```
真正地获取秒杀令牌脚本
seckill. lua
```
```
获取秒杀令牌脚本
getToken. lua
```

```
localseckillSha=nil;
```
- -创建自定义的 Redis 操作对象
localred=redisExecutor: new ();
- -打开连接
red: open ();
- -获取 Lua 脚本的 SHA 1 编码
seckillSha=red: getValue ("lua: sha 1 : seckill");
- -redis 没有缓存秒杀脚本
ifnotseckillShaorseckillSha==ngx. nullthen
    errorOut. resp_msg="秒杀还未启动";
    ngx.say (cjson.encode (errorOut));
    - -归还连接到连接池
    red: close ();
    return;
end
- -执行秒杀脚本
localrawFlag=red: evalSeckillSha (seckillSha,"setToken", goodId, userId, token);
- -归还连接到连接池
red: close ();
ifnotrawFlagorrawFlag==ngx. nullthen
    ngx.say (cjson.encode (errorOut));
    return;
end
localflag=tonumber (rawFlag);
ifflag== 5 then
    errorOut. resp_msg="已经排队过了";
    ngx.say (cjson.encode (errorOut));
    return;
end
ifflag== 2 then
    errorOut. resp_msg="秒杀商品没有找到";
    ngx.say (cjson.encode (errorOut));
    return;
end
ifflag== 4 then
    errorOut. resp_msg="库存不足，稍后再来";
    ngx.say (cjson.encode (errorOut));
    return;
end
ifflag~= 1 then
    errorOut. resp_msg="排队失败，未知错误";
    ngx.say (cjson.encode (errorOut));
    return;
end
restOut. datas=token;
ngx.say (cjson.encode (restOut));

#### 11. 5. 2 Lua 脚本：执行令牌桶限流

Nginx 的令牌桶限流脚本 getToken_access_limit. lua 执行在请求的 access 阶段，但是，该脚本并没
有实现限流的核心逻辑，仅仅调用缓存在 Redis 内部的 rate_limiter. lua 脚本进行限流。
getToken_access_limit. lua 脚本和 rate_limiter. lua 脚本的关系具体如图 11 - 17 所示。
什么时候在 Redis 中加载 rate_limiter. lua 脚本呢？和秒杀脚本一样，该脚本是在 Java 程序启动商


品秒杀时在 Redis 加载和缓存的。还有一点非常重要，Java 程序会将脚本加载完成之后的 SHA 1 编码
通过自定义的键（具体为"lua: sha 1 : rate_limiter"）缓存在 Redis 中，以方便 Nginx 的
getToken_access_limit. lua 脚本去获取，并且在调用 evalsha 方法时使用。

```
图 11 - 17 getToken_access_limit. lua 脚本和 rate_limiter. lua 脚本的关系
getToken_access_limit. lua 脚本的源码如下：
```
- --此脚本的环境：Nginx 内部，不是运行在 Redis 内部
- --启动调试
- -localmobdebug=require ("luaScript. initial. mobdebug");
- -mobdebug.start ();
- -导入自定义的基础模块
- -localbasic=require ("luaScript. module. common. basic");
- -导入自定义的 RedisOperator 模块
localredisExecutor=require ("luaScript. redis. RedisOperator");
- -读取 post 参数
ngx. req. read_body ();
localdata=ngx. req. get_body_data ();--获取消息体
localargs=cjson.decode (data);
localgoodId=args["seckillGoodId"];
localuserId=args["userId"];
localerrorOut={resp_code=- 1 ,resp_msg="限流出错", datas={}};
localkey="rate_limiter:seckill: ".. goodId;
localrateLimiterSha=nil;
- -创建自定义的 Redis 操作对象
localred=redisExecutor: new ();
- -打开连接
red: open ();
- -获取限流 Lua 脚本的 SHA 1 编码
rateLimiterSha=red: getValue ("lua: sha 1 : rate_limiter");
- -Redis 没有缓存秒杀脚本
ifnotrateLimiterShaorrateLimiterSha==ngx. nullthen
    errorOut. resp_msg="秒杀还未启动，请先设置商品";
    ngx.say (cjson.encode (errorOut));
    - -归还连接到连接池
    red: close ();
    return;
end
localconnection=red: getConnection ();
- -执行令牌桶限流
localresp, err=connection: evalsha (rateLimiterSha, 1 ,key,"acquire"," 1 ");


- -归还连接到连接池
red: close ();
ifnotresporresp==ngx. nullthen
    errorOut. resp_msg=err;
    ngx.say (cjson.encode (errorOut));
    return;
end
localflag=tonumber (resp);
- -ngx.say ("flag=".. flag);
ifflag~= 1 then
    errorOut. resp_msg="抱歉，被限流了";
    ngx.say (cjson.encode (errorOut));
    ngx.exit (ngx. HTTP_UNAUTHORIZED);
end
return;
细心的读者可能会发现，本书的 Nginx+Lua 秒杀架构缺少了用户 JWT 认证环节，主要原因是作
为高性能学习教程的秒杀案例，用户的认证已经不是重点。目前已经有非常成熟的开源插件完成
Nginx 上的 JWT 认证，如果对此感兴趣，可自行在 OpenResty 上安装 jwt-lua 插件尝试一下用户的认证
过程。
有关秒杀系统中的分布式锁、高并发测试，可关注疯狂创客圈的社群博客。


### 本章的知识扩展

##### 1. 阅读本章内容，如果遇到问题，请去社群交流，入群的方式：

###### （ 1 ）语雀扫码：

https://www.yuque.com/crazymakercircle/gkkw 8 s/khigna

（ 2 ）码云扫码：
https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md

##### 2. 本章的扩展内容，请参考 40 岁老架构师尼恩的博客，具体如下：

###### 疯狂创客圈 JAVA 高并发总目录

https://www.cnblogs.com/crazymakercircle/p/ 9904544 .html
Springcloud 微服务高并发（实战 1 ）：第 1 版秒杀
https://www.cnblogs.com/crazymakercircle/p/ 11669113 .html
秒杀超卖解决方案（史上最全）
https://www.cnblogs.com/crazymakercircle/p/ 14846136 .html

##### 3. 相关的面试题，请参考 3000 页《尼恩 Java 面试宝典》的 35 个面试专题 ：

https://www.cnblogs.com/crazymakercircle/p/ 13917138 .html

##### 3 .架构师尼恩积累了 20 年，价值 10 W 的架构师知识图谱如下：

###### 价值 10 W 的架构师知识图谱

https://www.processon.com/view/link/ 60 fb 9421637689719 d 246739

架构师哲学图谱
https://www.processon.com/view/link/ 616 f 801963768961 e 9 d 9 aec 8

尼恩 3 高架构知识宇宙图谱
https://www.processon.com/view/link/ 635097 d 2 e 0 b 34 d 40 be 778 ab 4



```
疯狂创客圈^
```
# 硬核推荐：尼恩 Java 硬核架构班

## 又名疯狂创客圈社群 VIP

## 详情：

## https://www.cnblogs.com/crazymakercircle/p/9904544.html


```
疯狂创客圈^
```
### 架构班（社群 VIP）的起源：^

最初的视频，主要是给读者加餐。很多的读者，需要一些高质量的实操、理论视频，所以，我就围绕书，和
底层，做了几个实操、理论视频，然后效果还不错，后面就做成迭代模式了。

### 架构班（社群 VIP）的功能：^

提供高质量实操项目整刀真枪的架构指导、快速提升大家的:
 开发水平
 设计水平
 架构水平
弥补业务中 CRUD 开发短板，帮助大家尽早脱离具备 3 高能力，掌握：
 高性能
 高并发
 高可用
作为一个高质量的架构师成长、人脉社群，把所有的卷王聚焦起来，一起卷：
 卷高并发实操
 卷底层原理
 卷架构理论、架构哲学
 最终成为顶级架构师，实现人生理想，走向人生巅峰


```
疯狂创客圈^
```
### 架构班（社群 VIP）的目的：^

 高质量的实操，大大提升简历的含金量，吸引力，增强面试的召唤率
 为大家提供九阳真经、葵花宝典，快速提升水平
 进大厂、拿高薪
 一路陪伴，提供助学视频和指导，辅导大家成为架构师
 自学为主，和其他卷王一起，卷高并发实操，卷底层原理、卷大厂面试题，争取狠卷 3 月成高手，狠卷
3 年成为顶级架构师


```
疯狂创客圈^
```
### N 个超高并发实操项目：简历压轴、个顶个精彩


```
疯狂创客圈^
```
#### 【样章】第 17 章：横扫全网 Rocketmq 视频第 2 部曲: 工业级 rocketmq 高可用（HA）

#### 底层原理和实操

工业级 rocketmq 高可用底层原理，包含：消息消费、同步消息、异步消息、单向消息等不同消息的底层原理
和源码实现；消息队列非常底层的主从复制、高可用、同步刷盘、异步刷盘等底层原理。
工业级 rocketmq 高可用底层原理和搭建实操，包含：高可用集群的搭建。
解决以下难题：
1 、技术难题：RocketMQ 如何最大限度的保证消息不丢失的呢？RocketMQ 消息如何做到高可靠投递？
2 、技术难题：基于消息的分布式事务，核心原理不理解
3 、选型难题： kafka or rocketmq ，该娶谁？
下图链接：https://www.processon.com/view/6178e8ae0e3e7416bde9da19


```
疯狂创客圈^
```
### 成功案例：^2 年翻^3 倍，^35 岁卷王成功转型为架构师^

##### 详情：http://topcoder.cloud/forum.php?mod=forumdisplay&fid=43&page=1


疯狂创客圈^


疯狂创客圈^


疯狂创客圈^


```
疯狂创客圈^
```
## 简历优化后的成功涨薪案例（VIP 含免费简历优化）


疯狂创客圈^


疯狂创客圈^


疯狂创客圈^


疯狂创客圈^


```
疯狂创客圈^
```
# 修改简历找尼恩（资深简历优化专家）

#####  如果面试表达不好，尼恩会提供简历优化指导

#####  如果项目没有亮点，尼恩会提供项目亮点指导

#####  如果面试表达不好，尼恩会提供面试表达指导

##### 作为 40 岁老架构师，尼恩长期承担技术面试官的角色：

#####  从业以来，“阅历”无数，对简历有着点石成金、改头换面、脱胎换骨的指导能力。

#####  尼恩指导过刚刚就业的小白，也指导过 P 8 级的老专家，都指导他们上岸。

##### 如何联系尼恩。尼恩微信，请参考下面的地址：

##### 语雀：https://www.yuque.com/crazymakercircle/gkkw8s/khigna

##### 码云：https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md


