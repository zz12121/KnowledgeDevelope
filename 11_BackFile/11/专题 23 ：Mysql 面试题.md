```
技术自由圈
```
# 牛逼的职业发展之路

40 岁老架构尼恩用一张图揭秘: Java 工程师的高端职业发展路径，走向食物链顶端的之路

链接：https://www.processon.com/view/link/618a2b62e0b34d73f7eb3cd


```
技术自由圈^
```
# 史上最全：价值 10 W 的架构师知识图谱

此图梳理于尼恩的多个 3 高生产项目：多个亿级人民币的大型 SAAS 平台和智慧城市项目

链接：https://www.processon.com/view/link/60fb9421637689719d


```
技术自由圈
```
# 牛逼的架构师哲学

40 岁老架构师尼恩对自己的 20 年的开发、架构经验总结

链接：https://www.processon.com/view/link/616f801963768961e9d9aec


```
技术自由圈
```
# 牛逼的 3 高架构知识宇宙

尼恩 3 高架构知识宇宙，帮助大家穿透 3 高架构，走向技术自由，远离中年危机

链接：https://www.processon.com/view/link/635097d2e0b34d40be778ab


```
技术自由圈
```
# 尼恩 Java 面试宝典

40 个专题（卷王专供+ 史上最全 + 2023 面试必备）
详情：https://www.cnblogs.com/crazymakercircle/p/13917138.html


```
技术自由圈^
```
# 未来职业，如何突围：三栖架构师


## 专题 23 ：Mysql 面试题（史上最全、定期更

## 新）

#### 本文版本说明：V

```
此文的格式，由markdown 通过程序转成而来，由于很多表格，没有来的及调整，出现一个格式
问题，尼恩在此给大家道歉啦。
由于社群很多小伙伴，在面试，不断的交流最新的面试难题，所以，《尼恩Java面试宝典》， 后
面会不断升级，迭代。
```
```
本专题，作为 《尼恩Java面试宝典》专题之一， 《尼恩Java面试宝典》一共 41 个面试专题，后
续还会增加
```
###### 《尼恩 Java 面试宝典》升级的规划为：

后续基本上， **每一个月，都会发布一次** ，最新版本，可以扫描扫架构师尼恩微信，发送 “领取电子书”
获取。

尼恩的微信二维码在哪里呢 ？ 请参见文末

###### 升级说明：

**V 106 版本升级说明（2023-09-06）：**

滴滴一面，痛失 40 K：因 MVCC 没说明白

**V 98 版本升级说明（2023-08-21）：**

网易一面：25 Wqps 高吞吐写 Mysql，100 W 数据 4 秒写完，如何实现？

**V 82 版本升级说明（2023-07-05）：**

美团一面： 100 亿级分库分表，如何不停机迁移？

**V 73 版本升级说明：**

京东太狠：100 W 数据去重，该用 distinct 还是 group by，说说理由？

**V 70 版本升级说明：**

淘宝一面：mysql 和 es 的 5 个一致性方案，你知道吗？


**V 46 版本升级说明：**

网易一面：select 分页要调优 100 倍，说说你的思路？ （内含 Mysql 的 36 军规）
滴滴一面：order by 调优 10 倍，思路是啥？

**V 39 版本升级说明：**

美团索命一问：一个 SQL ，怎么分析加了哪些锁？

梳理了四大场景、八大规则

**V 38 版本升级说明：**

美团一面：知道 MySQL 的 WAL、LSN、Checkpoint 吗？

携程二面：聊聊 MySQL 中的 WAL 策略和 CheckPoint 技术

美团一面：InndoDB 单表最多 2000 W，为什么？

**V 34 版本升级说明：**

新增 :

```
京东、阿里一面：Mysql索引 15 个夺命连环炮
阿里二面：为什么MySQL默认的Repeatable Read隔离级别，被改成了RC？
```
在疯狂创客圈的社群里边，经常有小伙伴 **面试 JD、阿里、美团、滴滴、网易等大厂，面完之后做面试
题交流。**

**其中，Mysql 索引，是面试中的绝对重点和难点**

现围绕这个重点，梳理了 15 个夺命连环炮，

以后遇到这个主题，大家再也不用怕了。

**V 17 版本升级说明：**

聊聊：什么是索引？ Mysql 支持那些类型索引结构？

聊聊：Mysql InnoDB 引擎支持那些索引？

聊聊: 为什么 B+树比 B 树更适合实现数据库索引？

聊聊：索引的优缺点

聊聊：什么情况下需要建索引？

聊聊：聚集索引、非聚集索引的区别？


聊聊：什么是回表查询？

聊聊：什么是索引覆盖？

聊聊：如何实现索引覆盖？

聊聊：哪些场景可以利用索引覆盖来优化 SQL？

聊聊：什么是最左匹配原则？

聊聊：为什么要用最左匹配呢？

聊聊：为什么要使用联合索引 ？

聊聊：索引失效的主要场景有哪些？

聊聊：索引的设计原则

聊聊：如何给长字符串, 创建如何更好的创建索引?

聊聊：什么是索引下推

聊聊：MySQL 索引使用的注意事项？

聊聊：如何创建有效的索引？

聊聊：什么是 MySQL 的 MRR 优化？

聊聊：如何使用 EXPLAIN 关键字？

聊聊：bin log 和 redo log 有什么区别？

聊聊：Mysql 主从复制同步方式有哪些？

聊聊：Mysql 主从同步延时产生原因? 怎么优化？

聊聊：Mysql 集群的高可用方案？

**V 16 版本升级说明：**

聊聊：什么是数据库事务？ 聊聊事务的特性？

聊聊：MySQL 的事务 ACID 是如何实现的？

聊聊：什么是脏读、幻读、不可重复读？

聊聊：如何实现 Transaction 的隔离性？Mysql 事务、Oracle 事务的默认隔离级别？

聊聊：Mysql 如何的控制事务的隔离级别？

聊聊：如何保证 REPEATABLE READ 级别不产生幻读？

聊聊：什么是间隙锁？

聊聊：什么是 MVCC 多版本并发控制协议？

聊聊：MVCC 工作的事务隔离级别是啥？

聊聊：Mysql 中 Undo Log 机制与 MVCC 的关系？

聊聊：Mysql 中 MVCC 是如何实现的？

聊聊：什么是表级锁、行级锁、页级锁？

聊聊：什么是共享锁、排它锁？


聊聊：什么是记录锁（Record Locks）？

聊聊：什么是间隙锁（Gap Locks）？

聊聊：什么是临键锁（Next-Key Locks）？

聊聊：什么是意向锁？

聊聊：什么是插入意向锁？

**V 11 版本升级说明：**

63 、能说几个常见的影响 MYSQL 性能的案例吗？

64 、如何进行 MySQL OOM（内存溢出）的排查和优化？

65 、批量向 MySQL 导 1000 W 数据，如何优化？

66 、数据库中事务的隔离级别有哪些？各自有什么特点？

67 、mysql 如何实现无数据插入，有数据更新？

68 、说说有哪些分库分表的思路和技巧？

69 、如何以最效率从 MySQL 中随机查询一条记录？

###### 面试问题交流说明：

如果遇到面试难题，或者职业发展问题，或者中年危机问题，都可以来疯狂创客圈社群交流，

加入交流群，加尼恩微信即可，

**入交流群** ，加尼恩微信即可，发送 **“入群”**


## 社群交流的面试真题

**社群面试真题 1 ：MySQL 单个实例 buffer 数据和磁盘数据是如何保证强一致性的**

```
具体问题： 由于MySQL底层有buffer存在，MySQL单个实例buffer数据和磁盘数据是如何保证强
一致性的。
```
社群小伙伴说明：
我的答案是不能保证强一致性，然后如果宕机了只能通过 redolog 和 binlog 进行数据恢复。但是好像面
试官说这个是错的，然后底层有机制保证。请问下大家知道是什么机制吗？我查了下书本，好像
INNODB 有个 double write 机制，但是也保证不了强一致性

**参考答案：**


innodb 三大特性的之一，双写缓冲区（double write）。另外， redolog 解决不了部分页写入问题，因
为 mysql 的页大小是 16 k，操作系统是 4 k，写了一半就断电，redolog 没法恢复。

双写缓冲区是 InnoDB 的三大特性之一，还有两个是 Buffer Pool 简称 BP、自适应 Hash 索引。
doublewrite 缓冲区是一个存储区，在该存储区中，InnoDB 将页面写入 InnoDB 数据文件中的适当位置
之前，先从缓冲池中刷新页面。如果在页面写入过程中存在操作系统，存储子系统或意外的 **mysqld** 进
程退出，则 InnoDB 可以在崩溃恢复期间从 doublewrite 缓冲区中找到页面的良好副本。注意：系统恢复
后，MySQL 可以根据 redolog 进行恢复，而 mysql 在恢复的过程中是检查 page 的 checksum，
checksum 就是 pgae 的最后事务号，发生 partial page write 问题时，page 已经损坏，找不到该 page 中
的事务号，就无法恢复。

InnoDB 的页大小默认为 16 K，可以使用参数 innodb_page_size 设置，可设置的值有： 64 KB，
32 KB，16 KB（默认），8 KB 和 4 KB。并且在数据校验时也针对页进行计算，即他们是一个整个对待，
包括把数据持久化到磁盘的操作。而计算机的硬件和操作系统在极端情况下（比如断电、系统崩溃）
时，刚写入了 4 K 或 8 K 数据，那么就不能保证该操作的原子性，称为 **部分页面写问题（Partial Write
Page）** 。

此时就引入了双写缓存区的机制，当发生极端情况时，可以从系统表空间的 Double Write Buffer【磁盘
上】进行恢复，下面是 InnoDB 的架构图、双写和恢复流程图。为了方便对比，将组件放在了相同的位
置：

```
为什么需要双写？个人理解宏观上还是与InnoDB需要支持事务（ACID）特性有关，而底层的原因是为了解决
Partial Write Page问题。
```
```
MYsql 为了实现事务InnoDB引入了比较多的组件，设计的特别复杂，InnoDB级别包括：（行锁、临建锁、
间隙锁）锁和加锁规则、MVCC、redo log、undo log、视图（Read View）。而官方文档也在隔离型和
持久性上面明确指向了数据双写机制，如下图
```

这样在极端情况下也能解决 Partial Write page 问题了，但是如果我自己的系统本身数据要求没有那么
高（比如日志数据库），这样的话毕竟双写是有一定的性能开销的。可以通过参数
**innodb_doublewrite** = 0 进行关闭，设置为 1 表示开启。官方认为，尽管需要写入两次数据，但是写
缓冲区不需要两次的 io 开销或操作，因为只需要调用一次操作系统的 fsync () 就可以将批量数据顺序写
入磁盘 -> 系统表空间的 Double Write Buffer（如上图），这里是顺序写而不是随机写（性能可以保
证），当然前提是配置刷盘策略参数 **innodb_flush_method** 为默认的 O_DIRECT。其实还有一点就是
真正提交的时候会使用组提交，我们可以用参数控制： **binlog_group_commit_sync_delay** ：组提交


执行 fsync () 延迟的微妙数，延迟时间越长批量数据越多，磁盘 io 越少性能越高。
**binlog_group_commit_sync_no_delay_count** ：组提交执行 fsync 的批个数。

## 史上最全 Java 面试题：Mysql 篇

#### 1 、MySQL 中有哪几种锁？

（ 1 ）表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。

（ 2 ）行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最
高。

（ 3 ）页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并
发度一般。

#### 2 、MySQL 中有哪些不同的表？

共有 5 种类型的表：

（ 1 ）MyISAM

（ 2 ）Heap

（ 3 ）Merge

（ 4 ）INNODB

（ 5 ）ISAM

**3 、简述在 MySQL 数据库中 MyISAM 和 InnoDB 的区别**

**MyISAM：**

（ 1 ）不支持事务，但是每次查询都是原子的；

（ 2 ）支持表级锁，即每次操作是对整个表加锁；

（ 3 ）存储表的总行数；

（ 4 ）一个 MYISAM 表有三个文件：索引文件、表结构文件、数据文件；

（ 5 ）采用菲聚集索引，索引文件的数据域存储指向数据文件的指针。辅索引与主索引基本一致，但是
辅索引不用保证唯一性。

**InnoDb：**

（ 1 ）支持 ACID 的事务，支持事务的四种隔离级别；


（ 2 ）支持行级锁及外键约束：因此可以支持写并发；

（ 3 ）不存储总行数：

（ 4 ）一个 InnoDb 引擎存储在一个文件空间（共享表空间，表大小不受操作系统控制，一个表可能分
布在多个文件里），也有可能为多个（设置为独立表空，表大小受操作系统文件大小限制，一般为
2 G），受操作系统文件大小的限制；

（ 5 ）主键索引采用聚集索引（索引的数据域存储数据文件本身），辅索引的数据域存储主键的值；因
此从辅索引查找数据，需要先通过辅索引找到主键值，再访问辅索引；最好使用自增主键，防止插入数
据时，为维持 B+树结构，文件的大调整。

#### 4 、MySQL 中 InnoDB 支持的四种事务隔离级别名称，以

#### 及逐级之间的区别

SQL 标准定义的四个隔离级别为：

（ 1 ）read uncommited ：读到未提交数据

（ 2 ）read committed：脏读，不可重复读

（ 3 ）repeatable read：可重读

（ 4 ）serializable ：串行事物

**5 、CHAR 和 VARCHAR 的区别** ？

（ 1 ）CHAR 和 VARCHAR 类型在存储和检索方面有所不同

（ 2 ）CHAR 列长度固定为创建表时声明的长度，长度值范围是 1 到 255 当 CHAR 值被存储时，它们被
用空格填充到特定长度，检索 CHAR 值时需删除尾随空格。

**6 、主键和候选键有什么区别？**

表格的每一行都由主键唯一标识, 一个表只有一个主键。

主键也是候选键。按照惯例，候选键可以被指定为主键，并且可以用于任何外键引用。

#### 7 、myisamchk 是用来做什么的？

它用来压缩 MyISAM 表，这减少了磁盘或内存使用。

MyISAM Static 和 MyISAM Dynamic 有什么区别？

在 MyISAM Static 上的所有字段有固定宽度。动态 MyISAM 表将具有像 TEXT，BLOB 等字段，以适应
不同长度的数据类型。


MyISAM Static 在受损情况下更容易恢复。

#### 8 、如果一个表有一列定义为 TIMESTAMP，将发生什么？

每当行被更改时，时间戳字段将获取当前时间戳。

列设置为 AUTO INCREMENT 时，如果在表中达到最大值，会发生什么情况？

它会停止递增，任何进一步的插入都将产生错误，因为密钥已被使用。

怎样才能找出最后一次插入时分配了哪个自动增量？

LAST_INSERT_ID 将返回由 Auto_increment 分配的最后一个值，并且不需要指定表名称。

#### 9 、你怎么看到为表格定义的所有索引？

索引是通过以下方式为表格定义的：

SHOW INDEX FROM ;

#### 10 、LIKE 声明中的％和_是什么意思？

％对应于 0 个或更多字符，_只是 LIKE 语句中的一个字符。

如何在 Unix 和 MySQL 时间戳之间进行转换？

UNIX_TIMESTAMP 是从 MySQL 时间戳转换为 Unix 时间戳的命令

FROM_UNIXTIME 是从 Unix 时间戳转换为 MySQL 时间戳的命令

#### 11 、列对比运算符是什么？

在 SELECT 语句的列比较中使用=，<>，<=，<，> =，>，<<，>>，<=>，AND，OR 或 LIKE 运算符。

#### 12 、BLOB 和 TEXT 有什么区别？

BLOB 是一个二进制对象，可以容纳可变数量的数据。TEXT 是一个不区分大小写的 BLOB。

BLOB 和 TEXT 类型之间的唯一区别在于对 BLOB 值进行排序和比较时区分大小写，对 TEXT 值不区分
大小写。

#### 13 、MySQL_fetch_array 和 MySQL_fetch_object 的区

#### 别是什么？


以下是 MySQL_fetch_array 和 MySQL_fetch_object 的区别：

MySQL_fetch_array（） – 将结果行作为关联数组或来自数据库的常规数组返回。

MySQL_fetch_object – 从数据库返回结果行作为对象。

#### 14 、MyISAM 表格将在哪里存储，并且还提供其存储格

#### 式？

每个 MyISAM 表格以三种格式存储在磁盘上：

（ 1 ）·“. frm”文件存储表定义

（ 2 ）·数据文件具有“. MYD”（MYData）扩展名

（ 3 ）索引文件具有“. MYI”（MYIndex）扩展名

#### 15 、MySQL 如何优化 DISTINCT？

DISTINCT 在所有列上转换为 GROUP BY，并与 ORDER BY 子句结合使用。

SELECT DISTINCT t 1. a FROM t 1, t 2 where t 1. a=t 2. a;

#### 16 、如何显示前 50 行？

在 MySQL 中，使用以下代码查询显示前 50 行：

SELECT*FROM

LIMIT 0,50;

#### 17 、可以使用多少列创建索引？

任何标准表最多可以创建 16 个索引列。

#### 18 、NOW（）和 CURRENT_DATE（）有什么区别？

NOW（）命令用于显示当前年份，月份，日期，小时，分钟和秒。


CURRENT_DATE（）仅显示当前年份，月份和日期。

#### 19 、什么是非标准字符串类型？

（ 1 ）TINYTEXT

（ 2 ）TEXT

（ 3 ）MEDIUMTEXT

（ 4 ）LONGTEXT

#### 20 、什么是通用 SQL 函数？

（ 1 ）CONCAT (A, B) – 连接两个字符串值以创建单个字符串输出。通常用于将两个或多个字段合并为一
个字段。

（ 2 ）FORMAT (X, D)- 格式化数字 X 到 D 有效数字。

（ 3 ）CURRDATE (), CURRTIME ()- 返回当前日期或时间。

（ 4 ）NOW（） – 将当前日期和时间作为一个值返回。

（ 5 ）MONTH（），DAY（），YEAR（），WEEK（），WEEKDAY（） – 从日期值中提取给定数据。

（ 6 ）HOUR（），MINUTE（），SECOND（） – 从时间值中提取给定数据。

（ 7 ）DATEDIFF（A，B） – 确定两个日期之间的差异，通常用于计算年龄

（ 8 ）SUBTIMES（A，B） – 确定两次之间的差异。

（ 9 ）FROMDAYS（INT） – 将整数天数转换为日期值。

#### 21 、MySQL 支持事务吗？

在缺省模式下，MySQL 是 autocommit 模式的，所有的数据库更新操作都会即时提交，

所以，在缺省情况下，MySQL 是不支持事务的。

但是如果你的 MySQL 表类型是使用 InnoDB Tables 或 BDB tables 的话，

你的 MySQL 就可以使用事务处理, 使用 SETAUTOCOMMIT=0 就可以使 MySQL 允许在非 autocommit
模式，

在非 autocommit 模式下，你必须使用 COMMIT 来提交你的更改，或者用 ROLLBACK 来回滚你的更
改。

#### 22 、MySQL 里记录货币用什么字段类型好


NUMERIC 和 DECIMAL 类型被 MySQL 实现为同样的类型，这在 SQL 92 标准允许。他们被用于保存
值，该值的准确精度是极其重要的值，例如与金钱有关的数据。当声明一个类是这些类型之一时，精度
和规模的能被 (并且通常是) 指定。

例如：

salary DECIMAL (9,2)

在这个例子中，9 (precision) 代表将被用于存储值的总的小数位数，而 2 (scale) 代表将被用于存储小数
点后的位数。

因此，在这种情况下，能被存储在 salary 列中的值的范围是从-9999999.99 到 9999999.99。

#### 23 、MySQL 有关权限的表都有哪几个？

MySQL 服务器通过权限表来控制用户对数据库的访问，权限表存放在 MySQL 数据库里，由
MySQL_install_db 脚本初始化。这些权限表分别 user，db，table_priv，columns_priv 和 host。

#### 24 、列的字符串类型可以是什么？

字符串类型是：

（ 1 ）SET

（ 2 ）BLOB

（ 3 ）ENUM

（ 4 ）CHAR

（ 5 ）TEXT

#### 25 、MySQL 数据库作发布系统的存储，一天五万条以上的

#### 增量，预计运维三年, 怎么优化？

（ 1 ）设计良好的数据库结构，允许部分数据冗余，尽量避免 join 查询，提高效率。

（ 2 ）选择合适的表字段数据类型和存储引擎，适当的添加索引。

（ 3 ）MySQL 库主从读写分离。

（ 4 ）找规律分表，减少单表中的数据量提高查询速度。

（ 5 ）添加缓存机制，比如 memcached，apc 等。

（ 6 ）不经常改动的页面，生成静态页面。

（ 7 ）书写高效率的 SQL。比如 SELECT * FROM TABEL 改为 SELECT field_1, field_2, field_3 FROM
TABLE.


#### 26 、锁的优化策略

（ 1 ）读写分离

（ 2 ）分段加锁

（ 3 ）减少锁持有的时间

（ 4 ）多个线程尽量以相同的顺序去获取资源

不能将锁的粒度过于细化，不然可能会出现线程的加锁和释放次数过多，反而效率不如一次加一把大
锁。

#### 27 、索引的底层实现原理和优化

B+树，经过优化的 B 树

主要是在所有的叶子结点中增加了指向下一个叶子节点的指针，

因此 InnoDB 建议为大部分表使用默认自增的主键作为主索引。

#### 28 、什么情况下设置了索引但无法使用

（ 1 ）以“%”开头的 LIKE 语句，模糊匹配

（ 2 ）OR 语句前后没有同时使用索引

（ 3 ）数据类型出现隐式转化（如 varchar 不加单引号的话可能会自动转换为 int 型）

#### 29 、实践中如何优化 MySQL

最好是按照以下顺序优化：

（ 1 ）SQL 语句及索引的优化

（ 2 ）数据库表结构的优化

（ 3 ）系统配置的优化

（ 4 ）硬件的优化

#### 30 、优化数据库的方法


（ 1 ）选取最适用的字段属性，尽可能减少定义字段宽度，尽量把字段设置 NOTNULL，例如’省份’、’性
别’最好适用 ENUM

（ 2 ）使用连接 (JOIN) 来代替子查询

（ 3 ）适用联合 (UNION) 来代替手动创建的临时表

（ 4 ）事务处理

（ 5 ）锁定表、优化事务处理

（ 6 ）适用外键，优化锁定表

（ 7 ）建立索引

（ 8 ）优化查询语句

#### 31 、简单描述 MySQL 中，索引，主键，唯一索引，联合

#### 索引的区别，对数据库的性能有什么影响（从读写两方

#### 面）

索引是一种特殊的文件 (InnoDB 数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所
有记录的引用指针。

普通索引 (由关键字 KEY 或 INDEX 定义的索引) 的唯一任务是加快对数据的访问速度。

普通索引允许被索引的数据列包含重复的值。如果能确定某个数据列将只包含彼此各不相同的值，在为
这个数据列创建索引的时候就应该用关键字 UNIQUE 把它定义为一个唯一索引。也就是说，唯一索引
可以保证数据记录的唯一性。

主键，是一种特殊的唯一索引，在一张表中只能定义一个主键索引，主键用于唯一标识一条记录，使用
关键字 PRIMARY KEY 来创建。

索引可以覆盖多个数据列，如像 INDEX (columnA, columnB) 索引，这就是联合索引。

索引可以极大的提高数据的查询速度，但是会降低插入、删除、更新表的速度，因为在执行这些写操作
时，还要操作索引文件。

#### 32 、数据库中的事务是什么?

事务（transaction）是作为一个单元的一组有序的数据库操作。如果组中的所有操作都成功，则认为事
务成功，即使只有一个操作失败，事务也不成功。如果所有操作完成，事务则提交，其修改将作用于所
有其他数据库进程。如果一个操作失败，则事务将回滚，该事务所有操作的影响都将取消。

事务特性：

（ 1 ）原子性：即不可分割性，事务要么全部被执行，要么就全部不被执行。

（ 2 ）一致性或可串性。事务的执行使得数据库从一种正确状态转换成另一种正确状态。

（ 3 ）隔离性。在事务正确提交之前，不允许把该事务对数据的任何改变提供给任何其他事务。


（ 4 ）持久性。事务正确提交后，其结果将永久保存在数据库中，即使在事务提交后有了其他故障，事
务的处理结果也会得到保存。

或者这样理解：

事务就是被绑定在一起作为一个逻辑工作单元的 SQL 语句分组，如果任何一个语句操作失败那么整个
操作就被失败，以后操作就会回滚到操作前状态，或者是上有个节点。为了确保要么执行，要么不执
行，就可以使用事务。要将有组语句作为事务考虑，就需要通过 ACID 测试，即原子性，一致性，隔离
性和持久性。

#### 33 、SQL 注入漏洞产生的原因？如何防止？

SQL 注入产生的原因：程序开发过程中不注意规范书写 sql 语句和对特殊字符进行过滤，导致客户端可
以通过全局变量 POST 和 GET 提交一些 sql 语句正常执行。

防止 SQL 注入的方式：

开启配置文件中的 magic_quotes_gpc 和 magic_quotes_runtime 设置

执行 sql 语句时使用 addslashes 进行 sql 语句转换

Sql 语句书写尽量不要省略双引号和单引号。

过滤掉 sql 语句中的一些关键词：update、insert、delete、select、 * 。

提高数据库表和字段的命名技巧，对一些重要的字段根据程序的特点命名，取不易被猜到的。

#### 34 、为表中得字段选择合适得数据类型

字段类型优先级: 整形>date, time>enum, char>varchar>blob, text

优先考虑数字类型，其次是日期或者二进制类型，最后是字符串类型，同级别得数据类型，应该优先选
择占用空间小的数据类型

#### 35 、存储时期

Datatime: 以 YYYY-MM-DD HH:MM: SS 格式存储时期时间，精确到秒，占用 8 个字节得存储空间，
datatime 类型与时区无关 Timestamp: 以时间戳格式存储，占用 4 个字节，范围小 1970-1-1 到 2038-
1-19，显示依赖于所指定得时区，默认在第一个列行的数据修改时可以自动得修改 timestamp 列得值

Date:（生日）占用得字节数比使用字符串. datatime. int 储存要少，使用 date 只需要 3 个字节，存储
日期月份，还可以利用日期时间函数进行日期间得计算

Time: 存储时间部分得数据

注意: 不要使用字符串类型来存储日期时间数据（通常比字符串占用得储存空间小，在进行查找过滤可以
利用日期得函数）

使用 int 存储日期时间不如使用 timestamp 类型


#### 36 、对于关系型数据库而言，索引是相当重要的概念，请

#### 回答有关索引的几个问题：

**（ 1 ）索引的目的是什么？**

快速访问数据表中的特定信息，提高检索速度

创建唯一性索引，保证数据库表中每一行数据的唯一性。

加速表和表之间的连接

使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间

**（ 2 ）索引对数据库系统的负面影响是什么？**

负面影响：

创建索引和维护索引需要耗费时间，这个时间随着数据量的增加而增加；索引需要占用物理空间，不光
是表需要占用数据空间，每个索引也需要占用物理空间；当对表进行增、删、改、的时候索引也要动态
维护，这样就降低了数据的维护速度。

**（ 3 ）为数据表建立索引的原则有哪些？**

在最频繁使用的、用以缩小查询范围的字段上建立索引。

在频繁使用的、需要排序的字段上建立索引

**（ 4 ）什么情况下不宜建立索引？**

对于查询中很少涉及的列或者重复值比较多的列，不宜建立索引。

对于一些特殊的数据类型，不宜建立索引，比如文本字段（text）等

#### 37 、解释 MySQL 外连接、内连接与自连接的区别

先说什么是交叉连接: 交叉连接又叫笛卡尔积，它是指不使用任何条件，直接将一个表的所有记录和另
一个表中的所有记录一一匹配。

内连接则是只有条件的交叉连接，根据某个条件筛选出符合条件的记录，不符合条件的记录不会出现在
结果集中，即内连接只连接匹配的行。

外连接其结果集中不仅包含符合连接条件的行，而且还会包括左表、右表或两个表中的所有数据行，这
三种情况依次称之为左外连接，右外连接，和全外连接。

左外连接，也称左连接，左表为主表，左表中的所有记录都会出现在结果集中，对于那些在右表中并没
有匹配的记录，仍然要显示，右边对应的那些字段值以 NULL 来填充。右外连接，也称右连接，右表为
主表，右表中的所有记录都会出现在结果集中。左连接和右连接可以互换，MySQL 目前还不支持全外
连接。


#### 38 、Myql 中的事务回滚机制概述

事务是用户定义的一个数据库操作序列，这些操作要么全做要么全不做，是一个不可分割的工作单位，

事务回滚是指将该事务已经完成的对数据库的更新操作撤销。

要同时修改数据库中两个不同表时，如果它们不是一个事务的话，当第一个表修改完，可能第二个表修
改过程中出现了异常而没能修改，此时就只有第二个表依旧是未修改之前的状态，而第一个表已经被修
改完毕。

而当你把它们设定为一个事务的时候，当第一个表修改完，第二表修改出现异常而没能修改，第一个表
和第二个表都要回到未修改的状态，这就是所谓的事务回滚

#### 39 、SQL 语言包括哪几部分？每部分都有哪些操作关键

#### 字？

SQL 语言包括数据定义 (DDL)、数据操纵 (DML), 数据控制 (DCL) 和数据查询（DQL） 四个部分。

数据定义：Create Table, Alter Table, Drop Table, Craete/Drop Index 等

数据操纵：Select ,insert, update, delete,

数据控制：grant, revoke

数据查询：select

#### 40 、完整性约束包括哪些？

数据完整性 (Data Integrity) 是指数据的精确 (Accuracy) 和可靠性 (Reliability)。

分为以下四类：

（ 1 ）实体完整性：规定表的每一行在表中是惟一的实体。

（ 2 ）域完整性：是指表中的列必须满足某种特定的数据类型约束，其中约束又包括取值范围、精度等
规定。

（ 3 ）参照完整性：是指两个表的主关键字和外关键字的数据应一致，保证了表之间的数据的一致性，
防止了数据丢失或无意义的数据在数据库中扩散。

（ 4 ）用户定义的完整性：不同的关系数据库系统根据其应用环境的不同，往往还需要一些特殊的约束
条件。用户定义的完整性即是针对某个特定关系数据库的约束条件，它反映某一具体应用必须满足的语
义要求。

与表有关的约束：包括列约束 (NOT NULL（非空约束）) 和表约束 (PRIMARY KEY、foreign key、
check、UNIQUE) 。


#### 41 、什么是锁？

数据库是一个多用户使用的共享资源。

当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。

若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据库的一致性。

加锁是实现数据库并发控制的一个非常重要的技术。

当事务在对某个数据对象进行操作前，先向系统发出请求，对其加锁。

加锁后事务就对该数据对象有了一定的控制，在该事务释放锁之前，其他的事务不能对此数据对象进行
更新操作。

基本锁类型：锁包括行级锁和表级锁

#### 42 、什么叫视图？游标是什么？

视图是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，视图通常是有一
个表或者多个表的行或列的子集。对视图的修改不影响基本表。它使得我们获取数据更容易，相比多表
查询。

游标：是对查询出来的结果集作为一个单元来有效的处理。游标可以定在该单元中的特定行，从结果集
的当前行检索一行或多行。可以对结果集当前行做修改。一般不使用游标，但是需要逐条处理数据的时
候，游标显得十分重要。

#### 43 、什么是存储过程？用什么来调用？

存储过程是一个预编译的 SQL 语句，优点是允许模块化的设计，就是说只需创建一次，以后在该程序
中就可以调用多次。如果某次操作需要执行多次 SQL，使用存储过程比单纯 SQL 语句执行要快。可以
用一个命令对象来调用存储过程。

#### 44 、如何通俗地理解三个范式？

第一范式：1 NF 是对属性的原子性约束，要求属性具有原子性，不可再分解；

第二范式：2 NF 是对记录的惟一性约束，要求记录有惟一标识，即实体的惟一性；

第三范式：3 NF 是对字段冗余性的约束，即任何字段不能由其他字段派生出来，它要求字段没有冗
余。。

###### 范式化设计优缺点:

优点: 可以尽量得减少数据冗余，使得更新快，体积小


缺点: 对于查询需要多个表进行关联，减少写得效率增加读得效率，更难进行索引优化

###### 反范式化:

优点: 可以减少表得关联，可以更好得进行索引优化

缺点: 数据冗余以及数据异常，数据得修改需要更多的成本

#### 45 、什么是基本表？什么是视图？

基本表是本身独立存在的表，在 SQL 中一个关系就对应一个表。视图是从一个或几个基本表导出的
表。视图本身不独立存储在数据库中，是一个虚表

#### 46 、试述视图的优点？

(1) 视图能够简化用户的操作

(2) 视图使用户能以多种角度看待同一数据；

(3) 视图为数据库提供了一定程度的逻辑独立性；

(4) 视图能够对机密数据提供安全保护。

#### 47 、 NULL 是什么意思

NULL 这个值表示 UNKNOWN (未知): 它不表示“”(空字符串)。对 NULL 这个值的任何比较都会生产一个
NULL 值。您不能把任何值与一个 NULL 值进行比较，并在逻辑上希望获得一个答案。

使用 IS NULL 来进行 NULL 判断

#### 48 、主键、外键和索引的区别？

主键、外键和索引的区别

**定义：**

主键——唯一标识一条记录，不能有重复的，不允许为空

外键——表的外键是另一表的主键, 外键可以有重复的, 可以是空值

索引——该字段没有重复值，但可以有一个空值

**作用：**

主键——用来保证数据完整性


外键——用来和其他表建立联系用的

索引——是提高查询排序的速度

**个数：**

主键—— 主键只能有一个

外键—— 一个表可以有多个外键

索引—— 一个表可以有多个唯一索引

#### 49 、你可以用什么来确保表格里的字段只接受特定范围里

#### 的值?

Check 限制，它在数据库表格里被定义，用来限制输入该列的值。

触发器也可以被用来限制数据库表格里的字段能够接受的值，但是这种办法要求触发器在表格里被定
义，这可能会在某些情况下影响到性能。

#### 50 、说说对 SQL 语句优化有哪些方法？（选择几条）

（ 1 ）Where 子句中：where 表之间的连接必须写在其他 Where 条件之前，那些可以过滤掉最大数量
记录的条件必须写在 Where 子句的末尾. HAVING 最后。

（ 2 ）用 EXISTS 替代 IN、用 NOT EXISTS 替代 NOT IN。

（ 3 ） 避免在索引列上使用计算

（ 4 ）避免在索引列上使用 IS NULL 和 IS NOT NULL

（ 5 ）对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索
引。

（ 6 ）应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫
描

（ 7 ）应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描

#### 51 面试官：听说你 sql 写的挺溜的，你说一说查询 sql 的执

#### 行过程

当希望 Mysql 能够高效的执行的时候，最好的办法就是清楚的了解 Mysql 是如何执行查询的，只有更加
全面的了解 SQL 执行的每一个过程，才能更好的进行 SQl 的优化。

当执行一条查询的 SQl 的时候大概发生了一下的步骤：

```
1. 客户端发送查询语句给服务器。
```

```
2. 服务器首先检查缓存中是否存在该查询，若存在，返回缓存中存在的结果。若是不存在就进行下一
步。
3. 服务器进行SQl的解析、语法检测和预处理，再由优化器生成对应的执行计划。
4. Mysql的执行器根据优化器生成的执行计划执行，调用存储引擎的接口进行查询。
5. 服务器将查询的结果返回客户端。
```
###### Mysql 的执行的流程

Mysql 的执行的流程图如下图所示：

再来一个图


这里以一个实例进行说明 Mysql 的的执行过程，新建一个 User 表，如下：

```
// 新建一个表
DROP TABLE IF EXISTS User;
```

现在针对这个表发出一条 SQl 查询：

查询每个部门中 25 岁以下的员工个数大于 3 的员工个数和部门编号，并按照人工个数降序排序和部门编号升序排序
的前两个部门。

###### 执行连接器

开始执行这条 sql 时，会检查该语句是否有权限，若是没有权限就直接返回错误信息，有权限会进行下
一步，校验权限的这一步是在图一的连接器进行的，对连接用户权限的校验。

###### 执行检索内存

相连建立之后，履行查询语句的时候，会先行检索内存，Mysql 会先行冗余这个 sql 与否履行过，以此
Key-Value 的形式平缓适用内存中，Key 是检索预定，Value 是结果集。

假如内存 key 遭击中，便会间接回到给客户端，假如没命中，便会履行后续的操作，完工之后亦会将结
果内存上去，当下一次进行查询的时候也是如此的循环操作。

###### 执行分析器

```
CREATE TABLE `User` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`name` varchar(10) DEFAULT NULL,
`age` int DEFAULT 0,
`address` varchar(255) DEFAULT NULL,
`phone` varchar(255) DEFAULT NULL,
`dept` int,
PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=40 DEFAULT CHARSET=utf8;
```
```
// 并初始化数据，如下
INSERT INTO User(name,age,address,phone,dept)VALUES('张三',24,'北
京','13265543552',2);
INSERT INTO User(name,age,address,phone,dept)VALUES('张三三',20,'北
京','13265543557',2);
INSERT INTO User(name,age,address,phone,dept)VALUES('李四',23,'上
海','13265543553',2);
INSERT INTO User(name,age,address,phone,dept)VALUES('李四四',21,'上
海','13265543556',2);
INSERT INTO User(name,age,address,phone,dept)VALUES('王五',27,'广
州','13265543558',3);
INSERT INTO User(name,age,address,phone,dept)VALUES('王五五',26,'广
州','13265543559',3);
INSERT INTO User(name,age,address,phone,dept)VALUES('赵六',25,'深
圳','13265543550',3);
INSERT INTO User(name,age,address,phone,dept)VALUES('赵六六',28,'广
州','13265543561',3);
INSERT INTO User(name,age,address,phone,dept)VALUES('七七',29,'广
州','13265543562',4);
INSERT INTO User(name,age,address,phone,dept)VALUES('八八',23,'广
州','13265543563',4);
INSERT INTO User(name,age,address,phone,dept)VALUES('九九',24,'广
州','13265543564',4);
```
```
SELECT dept,COUNT(phone) AS num FROM User WHERE age< 25 GROUP BY dept HAVING num
>= 3 ORDER BY num DESC,dept ASC LIMIT 0,2;
```

分析器主要有两步：（ 1 ）词法分析（ 2 ）语法分析

```
词法分析主要执行提炼关键性字，比如select，提交检索的表，提交字段名，提交检索条件。
```
```
语法分析主要执行辨别你输出的sql与否准确，是否合乎mysql的语法。
```
当 Mysql 没有命中内存的时候，接着执行的是 FROM student 负责把数据库的表文件加载到内存中去，
WHERE age< 60，会把所示表中的数据进行过滤，取出符合条件的记录行，生成一张临时表，如下图
所示。

GROUP BY dept 会把上图的临时表分成若干临时表，切分的过程如下图所示：

查询的结果只有部门 2 和部门 3 才有符合条件的值，生成如上两图的临时表。接着执行 SELECT 后面的字
段，SELECT 后面可以是表字段也可以是聚合函数。

这里 SELECT 的情况与是否存在 GROUP BY 有关，若是不存在 Mysql 直接按照上图内存中整列读取。若是
存在分别 SELECT 临时表的数据。


最后生成的临时表如下图所示：

紧接着执行 HAVING num>2 过滤员工数小于等于 2 的部门，对于 WHERE 和 HAVING 都是进行过滤，那么
这两者有什么不同呢？

第一点是 WHERE 后面只能对表字段进行过滤，不能使用聚合函数，而 HAVING 可以过滤表字段也可以使
用聚合函数进行过滤。

第二点是 WHERE 是对执行 from USer 操作后，加载表数据到内存后，WHERE 是对原生表的字段进行过
滤，而 HAVING 是对 SELECT 后的字段进行过滤，也就是 WHERE 不能使用别名进行过滤。

因为执行 WHERE 的时候，还没有 SELECT，还没有给字段赋予别名。接着生成的临时表如下图所示：

最后在执行 ORDER BY 后面的排序以及 limit 0,2 取得前两个数据，因为这里数据比较少，没有体现出
来。最后生成得结果也是如上图所示。接着判断这个 sql 语句是否有语法错误，关键性词与否准确等等。

###### 执行优化器

查询优化器会将解析树转化成执行计划。一条查询可以有多种执行方法，最后都是返回相同结果。优化
器的作用就是找到这其中最好的执行计划。

生成执行计划的过程会消耗较多的时间，特别是存在许多可选的执行计划时。如果在一条 SQL 语句执行
的过程中将该语句对应的最终执行计划进行缓存。

当相似的语句再次被输入服务器时，就可以直接使用已缓存的执行计划，从而跳过 SQL 语句生成执行计划
的整个过程，进而可以提高语句的执行速度。


MySQL 使用基于成本的查询优化器。它会尝试预测一个查询使用某种执行计划时的成本，并选择其中
成本最少的一个。

###### 执行执行器

由优化器生成得执行计划，交由执行器进行执行，执行器调用存储引擎得接口，存储引擎获取数据并返
回，结束整个查询得过程。

这里之讲解了 select 的过程，对于 update 这些修改数据或者删除数据的操作，会涉及到事务，会使用两
个日志模块，redo log 和 binlog 日志。具体对这两个日志的介绍请看着一篇文章。

以前的 Mysql 的默认存储引擎 MyISAM 引擎是没 redo log 的，而现在的默认存储引擎 InnoDB 引擎便是透
过 redo 复杂度来拥护事务的，保证事务能够准确的回滚或者提交，保证事务的 ACID。

#### 52 ：必备：从千万级数据查询来聊一聊索引结构和数据库

#### 原理

在日常工作中我们不可避免地会遇到慢 SQL 问题，比如笔者在之前的公司时会定期收到 DBA 彪哥发来的
Oracle AWR 报告，并特别提示我某条 sql 近阶段执行明显很慢，可能要优化一下等。对于这样的问题通
常大家的第一反应就是看看 sql 是不是写的不合理啊诸如： _“_ 避免使用 _in_ 和 _not in_ ，否则可能会导致全表扫
描 _”“_ 避免在 _where_ 子句中对字段进行函数操作 _”_ 等等，还有一种常见的反应就是这个表有没有加索引？绝
大部分情况下，加了个索引基本上就搞定了。


既然题目是《从千万级数据查询来聊一聊索引结构和数据库原理》，首先就来构造一个千万级的表直观
感受下。我们创建了一张 user 表，然后插入了 1000 万条数据，查询一下：

用了近 30 秒的时间，这还是单表查询，关联查询明显会更让人无法忍受。接下来，我们只是对 id 增加一
个索引，再来验证一把：

从 30 s 到 0.02 s，提升了足足 1500 倍。为什么加了索引之后，速度嗖地一下子就上去了呢？我们从【索
引数据结构】、【Mysql 原理】两个方面入手。

###### 一、索引数据结构

我们先来看下 MySQL 官方对索引的定义：

```
索引（Index）是帮助MySQL高效获取数据的数据结构。
```
这里面有 2 个关键词：高效查找、数据结构。对于数据库来说，查询是我们最主要的使用功能，查询速
度肯定是越快越好。最基本的查找是顺序查找，更高效的查找我们很自然会想到二叉树、红黑树、
Hash 表、BTree 等等。

###### 1.1 二叉树

这个大家很熟悉了，他有一个很重要的特点：左边节点的键值小于根的键值，右边节点的键值大于根的
键值。比如图 1 ，它确实能明显提高我们的搜索性能。但如果用来作为数据库的索引，明显存在很大的
缺陷，但对于图 2 这种递增的 id，存储后索引近似于变成了单边的链表，肯定是不合适的。

###### 1.2 红黑树


也称之为平衡二叉树。在 JDK 1.8 后，HashMap 对底层的链表也优化成了红黑树（后续文章我们可以讲
讲 Hashmap 1.8 之后的调整）。平衡二叉树的结构使树的结构较好，明显提高查找运算的速度。但是缺
陷也同样很明显，插入和删除运算变得复杂化，从而降低了他们的运算速度。对大数据量的支撑很不
好，当数据量很大时，树的高度太高，如果查找的数据是叶子节点，依然会超级慢。

###### 1.3 BTree

B-Tree 是为磁盘等外存储设备设计的一种平衡查找树。系统从磁盘读取数据到内存时是以磁盘块
（block）为基本单位的，位于同一个磁盘块中的数据会被一次性读取到内存中。在 Mysql 存储引擎中有
页（Page）的概念，页是其磁盘管理的最小单位。Mysql 存储引擎中默认每个页的大小为 16 KB，查看
方式：

我们也可以将它修改为 4 K、8 K、16 K。系统一个磁盘块的存储空间往往没有 16 K，因此 Mysql 每次申请
磁盘空间时都会将若干地址连续磁盘块来达到页的大小 16 KB。Mysql 在把磁盘数据读入到磁盘时会以页
为基本单位，在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘
I/O 次数，提高查询效率。

```
mysql> show variables like 'innodb_page_size';
```

如上图所示，一棵 B 树包含有键值、存储子节点的指针信息、及除主键外的数据。相对于普通的树 BTree
将横向节点的容量变大，从而存储更多的索引。

###### 1.4 B+Tree

在 B-Tree 的基础上大牛们又研究出了许多变种，其中最常见的是 B+Tree，MySQL 就普遍使用 B+Tree 实
现其索引结构。

与 B-Tree 相比，B+Tree 做了以下一些改进：
1 、非叶子节点，只存储键值信息，这样极大增加了存放索引的数据量。
2 、所有叶子节点之间都有一个链指针。对于区间查询时，不需要再从根节点开始，可直接定位到数
据。
3 、数据记录都存放在叶子节点中。根据二叉树的特点，这个是顺序访问指针，提升了区间访问的性
能。
通过这样的设计，一张千万级的表最多只需要 3 次磁盘交互就可以找出数据。

###### 二、Mysql 部分原理说明

这一部分我们选举几个日常面试过程中或者使用过程中比较常见的问题通过问答的形式来进行讲解。

###### 2.1、数据库引擎 MyISAM 和 InnoDB 有什么区别

```
MyISAM：
在Mysql8之前，默认引擎是MyISAM，其目标是快速读取。
特点：
1 、读取非常快，如果频繁插入和更新的话，因为涉及到数据全表锁，效率并不高
2 、保存了数据库行数，执行count时，不需要扫描全表；
3 、不支持数据库事务；
4 、不支持行级锁和外键；
5 、不支持故障恢复。
6 、支持全文检索FullText，压缩索引。
```

```
建议使用场景：
1 、做很多count计算的，（如果count计算后面有where还是会全表扫描）
2 、插入和更新较少，查询比较频繁的
InnoDB：
在Mysql8里，默认存储引擎改成了InnoDB。
特点
1 、支持事务处理、ACID事务特性
2 、实现了SQL标准的四种隔离级别
3 、支持行级锁和外键约束
4 、可以利用事务日志进行数据恢复
5 、不支持FullText类型的索引，没有保存数据库行数，计算count(*)需要全局扫描
6 、支持自动增加列属性auto_increment
7 、最后也是非常重要的一点：InnerDB是为了处理大量数据时的最大性能设计，其CPU效率可能
是其他基于磁盘的关系型数据库所不能匹敌的。
建议使用场景
1 、可靠性高或者必须要求事务处理
2 、表更新和查询相当的频繁，并且表锁定的机会比较大的情况下，指定InnerDB存储引擎。
```
###### 2.2 表和数据等在 Mysql 中是如何存储的

我们新建一个数据库 mds_demo，里面有两张表：order_info, user

我们找到 mysql 存放数据的 data 目录，存在一个 mds_demo 的文件夹，同时我们也找到了 order_info 和
user 的文件。

为什么两张表产生了不同的文件呢？原因很简单，因为创建这两张表时使用了不同的引擎


```
MyISAM引擎在创建表的时候，会创建三个文件
.MYD文件：存放表里的数据
.MYI文件：存放索引数据
.sdi文件： Serialized Dictionary Information的缩写。在Mysql5里没有sdi文件，但会有一个FRM
文件，用户存放表结构信息。在MySQL8.0中重新设计了数据字典，改为sdi。
MyISAM的索引和数据是分开的，并且索引是有压缩的，所以存储文件就会小很多，MyISAM应对
错误码导致的数据恢复的速度很快。
InnerDB引擎在创建表的时候，只有 1 个文件.ibd，即存放了索引又存放了文件，参见B+Tree。所
以它也被称之为聚集索引，即叶子节点包含完整的索引和数据，对应的MyISAM为非聚集索引。
补充说明一下：存储引擎是针对表的，而不是针对数据库，同一个库的不同的表可以使用不同的引
擎。
```
###### 2.3 为什么 InnoDB 必须要有主键，并且推荐使用整型的自增主键？

通过上面的讲解这个问题其实已经很清楚了，为了满足 MySQL 的索引数据结构 B+树的特性，必须要有
索引作为主键，可以有效提高查询效率。有的童鞋可能会说我创建表的时候可以没有主键啊，这个其实
和 Oracle 的 rownum 一样，如果不指定主键，InnoDB 会从插入的数据中找出不重复的一列作为主键索
引，如果没找到不重复的一列，InnoDB 会在后台增加一列 rowId 做为主键索引。所以不如我们自己创建
一个主键。

将索引的数据类型是设置为整型，一来占有的磁盘空间或内存空间更少，另一方面整型相对于字符串比
较更快速，而字符串需要先转换为 ASCII 码然后再一个个进行比较的。

参见 B+树的图它本质上是多路多叉树，如果主键索引不是自增的，那么后续插入的索引就会引起 B+树
的其他节点的分裂和重新平衡，影响数据插入的效率，如果是自增主键，只用在尾节点做增加就可以。

最后特别强调一点：不管当前是否有性能要求或者数据量多大，千万不要使用 UUID 作为索引。

###### 2.4 为什么 Mysql 存储引擎中默认每个页的大小为 16 KB？

假设我们一行数据大小为 1 K，那么一页就能存 16 条数据，包含指针+数据+索引。假设一行数据大小为
1 K，那么一页（ 1 个叶子节点）就能存 16 条数据；对于非叶子节点，假设 ID 为 bigint 类型那么长度为
8 B，指针大小在 Innodb 源码中为 6 B，一共就是 14 B，那么一页里就可以存储 16 K/14=1170 个 (主键+指
针)，这样一颗高度为 3 的 B+树能存储的数据为： 1170 _1170_ 16=2 千万级别。所以我们前面 1000 万的数据
只有 0.02 s。

###### 2.5 HASH 算法的使用场景


Hash 算法是一种散列算法，就是计算出某个字段的 hash，然后存放在对应的地址中，查找数据时只需
要 1 次定位而不像 BTree 那样从根节点找到叶子节点经过多次 IO 操作，所以查询效率非常地高。但同样也
有很多的弊端，讲一下最重要的两条。

1 、很明显 hash 只支持=、IN 等查询，而不支持范围查询
2 、 Hash 索引在任何时候都不能避免表扫描。

所以使用时务必注意。

#### 53: 非关系型数据库和关系型数据库区别，优势比较

非关系型数据库（感觉翻译不是很准确）称为 NoSQL，也就是 Not Only SQL，不仅仅是 SQL。非关系
型数据库不需要写一些复杂的 SQL 语句，其内部存储方式是以 key-value 的形式存在可以把它想象成
电话本的形式，每个人名（key）对应电话（value）。常见的非关系型数据库主要有 **Hbase、Redis、
MongoDB** 等。非关系型数据库不需要经过 SQL 的重重解析，所以性能很高；非关系型数据库的可扩
展性比较强，数据之间没有耦合性，遇见需要新加字段的需求，就直接增加一个 key-value 键值对即
可。

关系型数据库以表格的形式存在，以行和列的形式存取数据，关系型数据库这一系列的行和列被称为
表，无数张表组成了数据库，常见的关系型数据库有 **Oracle、DB 2、Microsoft SQL Server、
MySQL** 等。关系型数据库能够支持复杂的 SQL 查询，能够体现出数据之间、表之间的关联关系；关系
型数据库也支持事务，便于提交或者回滚。

它们之间的劣势都是基于对方的优势来满足的。

#### 54: MySQL 事务四大特性

一说到 MySQL 事务，你肯定能想起来四大特性：原子性、一致性、隔离性、持久性，下面再对这事
务的四大特性做一个描述


```
事务隔离级别 脏读 不可重复读 幻读
```
```
读未提交 允许 允许 允许
```
```
读已提交 不允许 允许 允许
```
```
可重复读 不允许 不允许 允许
```
```
串行化 不允许 不允许 不允许
```
```
原子性(Atomicity): 原子性指的就是 MySQL 中的包含事务的操作要么全部成功、要么全部失败
回滚，因此事务的操作如果成功就必须要全部应用到数据库，如果操作失败则不能对数据库有任
何影响。
这里涉及到一个概念，什么是 MySQL 中的事务？
```
```
事务是一组操作，组成这组操作的各个单元，要不全都成功要不全都失败，这个特性就是事务。
在 MySQL 中，事务是在引擎层实现的，只有使用 innodb 引擎的数据库或表才支持事务。
```
```
一致性(Consistency)：一致性指的是一个事务在执行前后其状态一致。比如 A 和 B 加起来的钱
一共是 1000 元，那么不管 A 和 B 之间如何转账，转多少次，事务结束后两个用户的钱加起来还
得是 1000 ，这就是事务的一致性。
持久性(Durability): 持久性指的是一旦事务提交，那么发生的改变就是永久性的，即使数据库
遇到特殊情况比如故障的时候也不会产生干扰。
隔离性(Isolation)：隔离性需要重点说一下，当多个事务同时进行时，就有可能出现脏读
(dirty read)、不可重复读(non-repeatable read)、幻读(phantom read) 的情况，为了解决
这些并发问题，提出了隔离性的概念。
```
```
脏读：事务 A 读取了事务 B 更新后的数据，但是事务 B 没有提交，然后事务 B 执行回滚操作，那
么事务 A 读到的数据就是脏数据
不可重复读：事务 A 进行多次读取操作，事务 B 在事务 A 多次读取的过程中执行更新操作并提
交，提交后事务 A 读到的数据不一致。
```
```
幻读：事务 A 将数据库中所有学生的成绩由 A -> B，此时事务 B 手动插入了一条成绩为 A 的记
录，在事务 A 更改完毕后，发现还有一条记录没有修改，那么这种情况就叫做出现了幻读。
```
SQL 的隔离级别有四种，它们分别是读未提交 (read uncommitted)、读已提交 (read committed)、
可重复读 (repetable read) 和串行化 (serializable)。下面分别来解释一下。

读未提交：读未提交指的是一个事务在提交之前，它所做的修改就能够被其他事务所看到。

读已提交：读已提交指的是一个事务在提交之后，它所做的变更才能够让其他事务看到。

可重复读：可重复读指的是一个事务在执行的过程中，看到的数据是和启动时看到的数据是一致的。未
提交的变更对其他事务不可见。

串行化：顾名思义是对于同一行记录，写会加写锁，读会加读锁。当出现读写锁冲突的时候，后访问
的事务必须等前一个事务执行完成，才能继续执行。

这四个隔离级别可以解决脏读、不可重复读、幻象读这三类问题。总结如下

其中隔离级别由低到高是：读未提交 < 读已提交 < 可重复读 < 串行化

隔离级别越高，越能够保证数据的完整性和一致性，但是对并发的性能影响越大。大多数数据库的默认
级别是读已提交 (Read committed)，比如 Sql Server、Oracle ，但是 MySQL 的默认隔离级别是可重
复读 (repeatable-read)。


#### 55 MySQL 存储引擎架构了解吗？

https://dev.mysql.com/doc/refman/5.7/en/innodb-architecture.html

下面是官方的 InnoDB 引擎结构图，主要分为内存结构和磁盘结构两大部分。

**内存区域**

**Buffer Pool:** 在 InnoDB 访问表记录和索引时会在 Buffer Pool 的页中缓存，以后使用可以减少磁盘 IO 操
作，提升效率。主要用来缓存热的数据页和索引页。

**Log Buffer** ：用来缓存 redolog

**Adaptive Hash Index** ：自适应哈希索引

**Change Buffer** ：它是一种应用在非唯一普通索引页（non-unique secondary index page）不在缓冲
池中，对页进行了写操作，并不会立刻将磁盘页加载到缓冲池，而仅仅记录缓冲变更（Buffer
Changes），等未来数据被读取时，再将数据合并（Merge）恢复到缓冲池中的技术。写缓冲的目的是
降低写操作的磁盘 IO，提升数据库性能。

**磁盘区域**

磁盘中的结构分为两大类：表空间和重做日志。

```
表空间：分为系统表空间(MySQL 目录的 ibdata1 文件)，临时表空间，常规表空间，Undo 表空间
以及 file-per-table 表空间(MySQL5.7默认打开file_per_table 配置）。系统表空间又包括了
InnoDB数据字典，双写缓冲区(Doublewrite Buffer)，修改缓存(Change Buffer），Undo日志
等。
Redo日志：存储的就是 Log Buffer 刷到磁盘的数据。
```
官方文档：


https://dev.mysql.com/doc/refman/8.0/en/innodb-storage-engine.html

#### 56 能否单独为一张表设置存储引擎？

方法 1 ：

设置默认存储引擎：

方法 2 ：

或者修改 my. cnf 文件：vim /etc/my. cnf

新增一行：default-storage-engine=MyISAM

重启 MySQL：systemctl restart mysqld

方法 3 ：

我们可以为不同的表设置不同的存储引擎

#### 57 MySQL 支持哪些存储引擎？默认使用哪个？

查看 MySQL 提供什么存储引擎

下面的结果表示 MySQL 中默认使用的存储引擎是 InnoDB，支持事务，行锁，外键，支持分布式事务
(XA)，支持保存点 (回滚)

也可以通过以下语句查看默认的存储引擎：

```
SET DEFAULT_STORAGE_ENGINE=MyISAM;
```
```
CREATE TABLE 表名( 建表语句; ) ENGINE = 存储引擎名称;
ALTER TABLE 表名 ENGINE = 存储引擎名称;
```
```
SHOW ENGINES;
```
```
SHOW VARIABLES LIKE '%default_storage_engine%';
```

#### 58 Mysql 8.0 自带哪些存储引擎？分别是做什么的？

```
1. InnoDB存储引擎
```
```
InnoDB是MySQL的默认事务型引擎，它被设计用来 处理大量的短期(short-lived)事务 。可以确保
事务的完整提交(Commit)和回滚(Rollback)。
除非有非常特别的原因需要使用其他的存储引擎，否则 应该优先考虑InnoDB引擎 。
数据文件结构：
表名.frm 存储表结构（MySQL8.0时，合并在表名.ibd中）
表名.ibd 存储数据和索引
InnoDB不仅缓存索引还要缓存真实数据， 对内存要求较 高 ，而且内存大小对性能有决定性的影
响。
```
```
2. MyISAM存储引擎
MyISAM提供了大量的特性，包括全文索引、压缩、空间函数(GIS)等，但 MyISAM不支持事务和行
级锁 ，有一个毫无疑问的缺陷就是崩溃后无法安全恢复。
优势是访问的 速度快 ，对事务完整性没有要求或者以SELECT、INSERT为主的应用。
数据文件结构：
表名.frm 存储表结构
表名.MYD 存储数据
表名.MYI 存储索引
MyISAM只缓存索引，不缓存真实数据。
```
```
3. Archive引擎
Archive档案存储引擎只支持INSERT和SELECT操作 。
Archive表适合日志和数据采集（档案）类应用。
根据英文的测试结论来看，Archive表比MyISAM表要小大约75%，比支持事务处理的InnoDB表小
大约83%。
```
```
4. Blackhole引擎
Blackhole引擎没有实现任何存储机制，它会丢弃所有插入的数据，不做任何保存 。
但服务器会记录Blackhole表的日志，所以可以用于复制数据到备库，或者简单地记录到日志。但
这种应用方式会碰到很多问题，因此并不推荐。
```
```
5. CSV引擎
```
```
CSV引擎可以将普通的CSV文件作为MySQL的表来处理，但不支持索引 。
CSV引擎可以作为一种数据交换的机制，非常有用。
CSV存储的数据直接可以在操作系统里，用文本编辑器，或者excel读取。
6. Memory引擎
```
```
如果需要快速地访问数据，并且这些数据不会被修改，重启以后丢失也没有关系，那么使用
Memory表是非常有用。
Memory表至少比MyISAM表要快一个数量级。
```

```
7. Federated引擎
```
```
Federated引擎是访问其他MySQL服务器的一个代理（跨库关联查询） ，尽管该引擎看起来提供
了一种很好的跨服务器的灵活性，但也经常带来问题，因此默认是禁用的。
```
#### 59 阿里、京东等大厂都有自研的存储引擎，如何开发一套

#### 自己的？

开发存储引擎并不难，难的是开发出来高效的有意义的存储引擎。

简单例子可以看一下官方源码中的示例，可以实现一个什么也没做的存储引擎。

有兴趣可以参考官方文档：https://dev.mysql.com/doc/dev/mysql-server/latest/

#### 55: MySQL 常见存储引擎的区别

MySQL 常见的存储引擎，可以使用

命令，来列出所有的存储引擎

可以看到，InnoDB 是 MySQL 默认支持的存储引擎，支持 **事务、行级锁定和外键** 。

###### MyISAM 存储引擎的特点

在 5.1 版本之前，MyISAM 是 MySQL 的默认存储引擎，MyISAM 并发性比较差，使用的场景比较少，
主要特点是

```
不支持事务操作，ACID 的特性也就不存在了，这一设计是为了性能和效率考虑的。
不支持外键操作，如果强行增加外键，MySQL 不会报错，只不过外键不起作用。
MyISAM 默认的锁粒度是表级锁，所以并发性能比较差，加锁比较快，锁冲突比较少，不太容易
发生死锁的情况。
MyISAM 会在磁盘上存储三个文件，文件名和表名相同，扩展名分别是 .frm(存储表定
义)、.MYD(MYData,存储数据)、MYI(MyIndex,存储索引)。这里需要特别注意的是 MyISAM 只
缓存索引文件，并不缓存数据文件。
MyISAM 支持的索引类型有 全局索引(Full-Text)、B-Tree 索引、R-Tree 索引
Full-Text 索引：它的出现是为了解决针对文本的模糊查询效率较低的问题。
```
```
SHOW ENGINES
```

```
B-Tree 索引：所有的索引节点都按照平衡树的数据结构来存储，所有的索引数据节点都在叶节点
R-Tree索引：它的存储方式和 B-Tree 索引有一些区别，主要设计用于存储空间和多维数据的字段
做索引,目前的 MySQL 版本仅支持 geometry 类型的字段作索引，相对于 BTREE，RTREE 的优势
在于范围查找。
数据库所在主机如果宕机，MyISAM 的数据文件容易损坏，而且难以恢复。
增删改查性能方面：SELECT 性能较高，适用于查询较多的情况
```
###### InnoDB 存储引擎的特点

自从 MySQL 5.1 之后，默认的存储引擎变成了 InnoDB 存储引擎，相对于 MyISAM，InnoDB 存储引擎
有了较大的改变，它的主要特点是

```
支持事务操作，具有事务 ACID 隔离特性，默认的隔离级别是可重复读(repetable-read)、通过
MVCC（并发版本控制）来实现的。能够解决脏读和不可重复读的问题。
InnoDB 支持外键操作。
InnoDB 默认的锁粒度行级锁，并发性能比较好，会发生死锁的情况。
和 MyISAM 一样的是，InnoDB 存储引擎也有 .frm文件存储表结构 定义，但是不同的是，InnoDB
的表数据与索引数据是存储在一起的，都位于 B+ 数的叶子节点上，而 MyISAM 的表数据和索引
数据是分开的。
InnoDB 有安全的日志文件，这个日志文件用于恢复因数据库崩溃或其他情况导致的数据丢失问
题，保证数据的一致性。
InnoDB 和 MyISAM 支持的索引类型相同，但具体实现因为文件结构的不同有很大差异。
增删改查性能方面，果执行大量的增删改操作，推荐使用 InnoDB 存储引擎，它在删除操作时是
对行删除，不会重建表。
```
###### MyISAM 和 InnoDB 存储引擎的对比

```
锁粒度方面：由于锁粒度不同，InnoDB 比 MyISAM 支持更高的并发；InnoDB 的锁粒度为行锁、
MyISAM 的锁粒度为表锁、行锁需要对每一行进行加锁，所以锁的开销更大，但是能解决脏读和
不可重复读的问题，相对来说也更容易发生死锁
可恢复性上：由于 InnoDB 是有事务日志的，所以在产生由于数据库崩溃等条件后，可以根据日志
文件进行恢复。而 MyISAM 则没有事务日志。
查询性能上：MyISAM 要优于 InnoDB，因为 InnoDB 在查询过程中，是需要维护数据缓存，而且
查询过程是先定位到行所在的数据块，然后在从数据块中定位到要查找的行；而 MyISAM 可以直
接定位到数据所在的内存地址，可以直接找到数据。
表结构文件上： MyISAM 的表结构文件包括：.frm(表结构定义),.MYI(索引),.MYD(数据)；而
InnoDB 的表数据文件为:.ibd和.frm(表结构定义)；
```
#### 56: MySQL 基础架构

这道题应该从 MySQL 架构来理解，我们可以把 MySQL 拆解成几个零件，如下图所示


大致上来说，MySQL 可以分为 Server 层和存储引擎层。

Server 层包括连接器、查询缓存、分析器、优化器、执行器，包括大多数 MySQL 中的核心功能，所有
跨存储引擎的功能也在这一层实现，包括 **存储过程、触发器、视图等** 。

存储引擎层包括 MySQL 常见的存储引擎，包括 **MyISAM、InnoDB 和 Memory** 等，最常用的是
InnoDB，也是现在 MySQL 的默认存储引擎。存储引擎也可以在创建表的时候手动指定，比如下面

然后我们就可以探讨 MySQL 的执行过程了

###### 连接器

首先需要在 MySQL 客户端登陆才能使用，所以需要一个连接器来连接用户和 MySQL 数据库，我们一
般是使用

来进行 MySQL 登陆，和服务端建立连接。在完成 TCP 握手后，连接器会根据你输入的用户名和密码
验证你的登录身份。如果用户名或者密码错误，MySQL 就会提示 **Access denied for user** ，来结束执
行。如果登录成功后，MySQL 会根据权限表中的记录来判定你的权限。

```
CREATE TABLE t (i INT) ENGINE = <Storage Engine>;
```
```
mysql -u 用户名 -p 密码
```

###### 查询缓存

连接完成后，你就可以执行 SQL 语句了，这行逻辑就会来到第二步：查询缓存。

MySQL 在得到一个执行请求后，会首先去查询缓存中查找，是否执行过这条 SQL 语句，之前执行过
的语句以及结果会以 key-value 对的形式，被直接放在内存中。key 是查询语句，value 是查询的结
果。如果通过 key 能够查找到这条 SQL 语句，就直接返回 SQL 的执行结果。

如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果就会被放入查询缓存中。
可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，效率会很
高。

**但是查询缓存不建议使用**

为什么呢？因为只要在 MySQL 中对某一张表执行了更新操作，那么所有的查询缓存就会失效，对于更
新频繁的数据库来说，查询缓存的命中率很低。

###### 分析器

如果没有命中查询，就开始执行真正的 SQL 语句。

```
首先，MySQL 会根据你写的 SQL 语句进行解析，分析器会先做 词法分析，你写的 SQL 就是由多
个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串是什么，代表什么。
然后进行 语法分析，根据词法分析的结果， 语法分析器会根据语法规则，判断你输入的这个 SQL
语句是否满足 MySQL 语法。如果 SQL 语句不正确，就会提示 You have an error in your SQL
syntax
```
###### 优化器

经过分析器的词法分析和语法分析后，你这条 SQL 就合法了，MySQL 就知道你要做什么了。但是在执
行前，还需要进行优化器的处理，优化器会判断你使用了哪种索引，使用了何种连接，优化器的作用就
是确定效率最高的执行方案。

###### 执行器


```
关键字或 解释 执行顺序
```
```
select 查询列表（字段） 第七步
```
```
from 表 第一步
```
```
连接类型 join 表 2 第二步
```
```
on 连接条件 第三步
```
```
where 筛选条件 第四步
```
```
group by 分组列表 第五步
```
```
having 分组后的筛选条件 第六步
```
```
order by 排序列表 第八步
```
```
limit 偏移 ，条目数 第九步
```
MySQL 通过分析器知道了你的 SQL 语句是否合法，你想要做什么操作，通过优化器知道了该怎么做效
率最高，然后就进入了执行阶段，开始执行这条 SQL 语句

在执行阶段，MySQL 首先会判断你有没有执行这条语句的权限，没有权限的话，就会返回没有权限的
错误。如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引
擎提供的接口。对于有索引的表，执行的逻辑也差不多。

至此，MySQL 对于一条语句的执行过程也就完成了。

#### 57: SQL 的执行顺序面试题

问题：

mysql 的解析 select * from A left join B on xxx where Axx group by xx order by xx；

问这个 sql 的执行顺序是啥

答案：

from --> join 表 2 --> on --> where --> groupby --> having --> select --> distinct --> order by --> limit


#### 57: SQL 的执行顺序详解

我们在编写一个查询语句的时候

它的执行顺序你知道吗？ 这道题就给你一个回答。

```
SELECT DISTINCT
< select_list >
FROM
< left_table > < join_type >
JOIN < right_table > ON < join_condition >
WHERE
< where_condition >
GROUP BY
< group_by_list >
HAVING
< having_condition >
ORDER BY
< order_by_condition >
LIMIT < limit_number >
```

###### FROM 连接

首先，对 SELECT 语句执行查询时，对 FROM 关键字两边的表执行连接，会形成笛卡尔积，这时候会产
生一个虚表 VT 1 (virtual table)

```
首先先来解释一下什么是笛卡尔积
```
```
现在我们有两个集合 A = {0,1} , B = {2,3,4}
```
```
那么，集合 A * B 得到的结果就是
```
```
A * B = {(0,2)、(1,2)、(0,3)、(1,3)、(0,4)、(1,4)};
```
```
B * A = {(2,0)、{2,1}、{3,0}、{3,1}、{4,0}、(4,1)};
上面 A * B 和 B * A 的结果就可以称为两个集合相乘的 笛卡尔积
```
```
我们可以得出结论，A 集合和 B 集合相乘，包含了集合 A 中的元素和集合 B 中元素之和，也就是
A 元素的个数 * B 元素的个数
```
再来解释一下什么是虚表

```
在 MySQL 中，有三种类型的表
一种是永久表，永久表就是创建以后用来长期保存数据的表
```
```
一种是临时表，临时表也有两类，一种是和永久表一样，只保存临时数据，但是能够长久存在
的；还有一种是临时创建的，SQL 语句执行完成就会删除。
```
```
一种是虚表，虚表其实就是视图，数据可能会来自多张表的执行结果。
```
###### ON 过滤

然后对 FROM 连接的结果进行 ON 筛选，创建 VT 2，把符合记录的条件存在 VT 2 中。

###### JOIN 连接

第三步，如果是 OUTER JOIN (left join、right join) ，那么这一步就将添加外部行，如果是 left
join 就把 ON 过滤条件的左表添加进来，如果是 right join ，就把右表添加进来，从而生成新的虚拟表
VT 3。

###### WHERE 过滤

第四步，是执行 WHERE 过滤器，对上一步生产的虚拟表引用 WHERE 筛选，生成虚拟表 VT 4。

WHERE 和 ON 的区别

```
如果有外部列，ON 针对过滤的是关联表，主表(保留表)会返回所有的列;
如果没有添加外部列，两者的效果是一样的;
```
应用

```
对主表的过滤应该使用 WHERE;
对于关联表，先条件查询后连接则用 ON，先连接后条件查询则用 WHERE;
```
###### GROUP BY

根据 group by 字句中的列，会对 VT 4 中的记录进行分组操作，产生虚拟机表 VT 5。果应用了 group
by，那么后面的所有步骤都只能得到的 VT 5 的列或者是聚合函数（count、sum、avg 等）。


###### HAVING

紧跟着 GROUP BY 字句后面的是 HAVING，使用 HAVING 过滤，会把符合条件的放在 VT 6

###### SELECT

第七步才会执行 SELECT 语句，将 VT 6 中的结果按照 SELECT 进行刷选，生成 VT 7

###### DISTINCT

在第八步中，会对 TV 7 生成的记录进行去重操作，生成 VT 8。事实上如果应用了 group by 子句那么
distinct 是多余的，原因同样在于，分组的时候是将列中唯一的值分成一组，同时只为每一组返回一行
记录，那么所以的记录都将是不相同的。

###### ORDER BY

应用 order by 子句。按照 order_by_condition 排序 VT 8，此时返回的一个游标，而不是虚拟表。sql
是基于集合的理论的，集合不会预先对他的行排序，它只是成员的逻辑集合，成员的顺序是无关紧要
的。

SQL 语句执行的过程如下

#### 58: 什么是临时表，何时删除临时表

什么是临时表？

MySQL 在执行 SQL 语句的过程中，通常会临时创建一些存储中间结果集的表，临时表只对当前连接可
见，在连接关闭时，临时表会被删除并释放所有表空间。

临时表分为两种：一种是内存临时表，一种是磁盘临时表，

什么区别呢？


内存临时表使用的是 MEMORY 存储引擎，而临时表采用的是 MyISAM 存储引擎。

```
MEMORY 存储引擎：
```
```
memory 是 MySQL 中一类特殊的存储引擎，它使用存储在内容中的内容来创建表，而且 数据全
部放在内存中 。每个基于 MEMORY 存储引擎的表实际对应一个磁盘文件。该文件的文件名与表
名相同，类型为 frm 类型。而其数据文件，都是存储在内存中，这样有利于数据的快速处理，
提高整个表的效率。MEMORY 用到的很少，因为它是把数据存到内存中，如果内存出现异常就
会影响数据。如果重启或者关机，所有数据都会消失。因此，基于 MEMORY 的表的生命周期很
短，一般是一次性的。
```
MySQL 会在下面这几种情况产生临时表

```
使用 UNION 查询：UNION 有两种，一种是UNION ，一种是 UNION ALL ，它们都用于联合查
询；区别是 使用 UNION 会去掉两个表中的重复数据，相当于对结果集做了一下去重
(distinct)。使用 UNION ALL，则不会排重，返回所有的行。使用 UNION 查询会产生临时表。
使用 TEMPTABLE 算法或者是 UNION 查询中的视图。TEMPTABLE 算法是一种创建临时表的算
法，它是将结果放置到临时表中，意味这要 MySQL 要先创建好一个临时表，然后将结果放到临时
表中去，然后再使用这个临时表进行相应的查询。
ORDER BY 和 GROUP BY 的子句不一样时也会产生临时表。
DISTINCT 查询并且加上 ORDER BY 时；
SQL中用到 SQL_SMALL_RESULT 选项时；如果查询结果比较小的时候，可以加上
SQL_SMALL_RESULT 来优化，产生临时表
FROM 中的子查询；
EXPLAIN 查看执行计划结果的 Extra 列中，如果使用 Using Temporary 就表示会用到临时表。
```
#### 59: MySQL 常见索引类型

索引是存储在一张表中特定列上的数据结构，索引是在列上创建的。并且，索引是一种数据结构。

在 MySQL 中，主要有下面这几种索引

```
全局索引(FULLTEXT)：全局索引，目前只有 MyISAM 引擎支持全局索引，它的出现是为了解决针
对文本的模糊查询效率较低的问题。
哈希索引(HASH)：哈希索引是 MySQL 中用到的唯一 key-value 键值对的数据结构，很适合作为索
引。HASH 索引具有一次定位的好处，不需要像树那样逐个节点查找，但是这种查找适合应用于查
找单个键的情况，对于范围查找，HASH 索引的性能就会很低。
B-Tree 索引：B 就是 Balance 的意思，BTree 是一种平衡树，它有很多变种，最常见的就是 B+
Tree，它被 MySQL 广泛使用。
R-Tree 索引：R-Tree 在 MySQL 很少使用，仅支持 geometry 数据类型，支持该类型的存储引擎
只有MyISAM、BDb、InnoDb、NDb、Archive几种，相对于 B-Tree 来说，R-Tree 的优势在于范
围查找。
```
#### 60: varchar 和 char 的区别和使用场景

MySQL 中没有 nvarchar 数据类型，所以直接比较的是 varchar 和 char 的区别


char ：表示的是定长的字符串，当你输入小于指定的数目，比如你指定的数目是 char (6)，当你输
入小于 6 个字符的时候，char 会在你最后一个字符后面补空值。当你输入超过指定允许最大长度后，
MySQL 会报错

varchar： varchar 指的是长度为 n 个字节的可变长度，并且是非 Unicode 的字符数据。n 的值是介
于 1 - 8000 之间的数值。存储大小为实际大小。

```
Unicode 是一种字符编码方案，它为每种语言中的每个字符都设定了统一唯一的二进制编码，以
实现跨语言、跨平台进行文本转换、处理的要求
```
使用 char 存储定长的数据非常方便、char 检索效率高，无论你存储的数据是否到了 10 个字节，都要
去占用 10 字节的空间

使用 varchar 可以存储变长的数据，但存储效率没有 char 高。

#### 60: 什么是内连接、外连接、交叉连接、笛卡尔积

连接的方式主要有三种： **外连接、内链接、交叉连接**

```
外连接(OUTER JOIN)：外连接分为三种，分别是左外连接(LEFT OUTER JOIN 或 LEFT JOIN)
、右外连接(RIGHT OUTER JOIN 或 RIGHT JOIN) 、全外连接(FULL OUTER JOIN 或 FULL
JOIN)
左外连接：又称为左连接，这种连接方式会显示左表不符合条件的数据行，右边不符合条件的数据
行直接显示 NULL
```
右外连接：也被称为右连接，他与左连接相对，这种连接方式会显示右表不符合条件的数据行，左表不
符合条件的数据行直接显示 NULL


**MySQL 暂不支持全外连接**

```
内连接(INNER JOIN)：结合两个表中相同的字段，返回关联字段相符的记录。
```
```
笛卡尔积(Cartesian product)： 我在上面提到了笛卡尔积，为了方便，下面再列出来一下。
```
```
现在我们有两个集合 A = {0,1} , B = {2,3,4}
```
```
那么，集合 A * B 得到的结果就是
```
```
A * B = {(0,2)、(1,2)、(0,3)、(1,3)、(0,4)、(1,4)};
B * A = {(2,0)、{2,1}、{3,0}、{3,1}、{4,0}、(4,1)};
```
```
上面 A * B 和 B * A 的结果就可以称为两个集合相乘的 笛卡尔积
```
```
我们可以得出结论，A 集合和 B 集合相乘，包含了集合 A 中的元素和集合 B 中元素之和，也就是
A 元素的个数 * B 元素的个数
交叉连接的原文是Cross join ，就是笛卡尔积在 SQL 中的实现，SQL中使用关键字CROSS
JOIN来表示交叉连接，在交叉连接中，随便增加一个表的字段，都会对结果造成很大的影响。
```
```
或者不用 CROSS JOIN，直接用 FROM 也能表示交叉连接的效果
```
```
如果表中字段比较多，不适宜用交叉连接，交叉连接的效率比较差。
全连接：全连接也就是 full join，MySQL 中不支持全连接，但是可以使用其他连接查询来模
拟全连接，可以使用 UNION 和 UNION ALL 进行模拟。例如
```
```
SELECT * FROM t_Class a CROSS JOIN t_Student b WHERE a.classid=b.classid
```
```
SELECT * FROM t_Class a ,t_Student b WHERE a.classid=b.classid
```

```
使用 UNION 和 UNION ALL 的注意事项
```
```
通过 union 连接的 SQL 分别单独取出的列数必须相同
使用 union 时，多个相等的行将会被合并，由于合并比较耗时，一般不直接使用 union 进
行合并，而是通常采用 union all 进行合并
```
#### 61: 谈谈 SQL 优化的经验

```
查询语句无论是使用哪种判断条件 等于、小于、大于 ， WHERE 左侧的条件查询字段不要使用函数
或者表达式
使用 EXPLAIN 命令优化你的 SELECT 查询，对于复杂、效率低的 sql 语句，我们通常是使用
explain sql 来分析这条 sql 语句，这样方便我们分析，进行优化。
当你的 SELECT 查询语句只需要使用一条记录时，要使用 LIMIT 1
不要直接使用 SELECT *，而应该使用具体需要查询的表字段，因为使用 EXPLAIN 进行分析时，
SELECT * 使用的是全表扫描，也就是 type = all。
为每一张表设置一个 ID 属性
避免在 WHERE 字句中对字段进行 NULL 判断
避免在 WHERE 中使用 != 或 <> 操作符
使用 BETWEEN AND 替代 IN
为搜索字段创建索引
选择正确的存储引擎，InnoDB 、MyISAM 、MEMORY 等
使用 LIKE %abc% 不会走索引，而使用 LIKE abc% 会走索引
对于枚举类型的字段(即有固定罗列值的字段)，建议使用ENUM而不是VARCHAR，如性别、星期、
类型、类别等
拆分大的 DELETE 或 INSERT 语句
选择合适的字段类型，选择标准是 尽可能小、尽可能定长、尽可能使用整数 。
字段设计尽可能使用 NOT NULL
进行水平切割或者垂直分割
```
```
水平分割：通过建立结构相同的几张表分别存储数据
垂直分割：将经常一起使用的字段放在一个单独的表中，分割后的表记录之间是一一对应关系。
```
#### 63 ：说几个常见的影响 MYSQL 性能的案例

```
参考文献 ：https://www.cnblogs.com/zhiqian-ali/p/6336521.html
```
**大规模数据导出功能**

相信很多业务都遇到过数据导出，明细展示这方面的需求，sql 基本上都是先求一个数据的总和然后，
limit n, m 分页查询，这样的问题就在于，在扫描前面的数据时是不会有性能问题的，当 n 值越大，偏移
量越多，扫描的数据就越多，这个时候就会产生问题，一个本来不的 sql 就会变成慢 sql，导致 DB 性能下
降。针对这种问题 DBA 都会建议开发将 limit n, m 改为 id 范围的查询，或者进行业务改造对于一些不必要
的场景只展示前几百条，只需要进行一次分页即可。

类似 sql 模式：

```
(select colum1,colum2...columN from tableA ) union (select
colum1,colum2...columN from tableB )
```
```
或
(select colum1,colum2...columN from tableA ) union all (select
colum1,colum2...columN from tableB )；
```

**ERP 类系统使用聚合函数或者分组排序**

类似仓库内管理系统会需要展示很多统计信息，很多开发会选择在 DB 端计算出结果直接展示，问题在
于 sum，max，min 类的聚合函数在 DB 端执行会消耗到 CPU 资源，如果这个时候还遇到索引不合理的情
况，往往会带来灾难性的后果。这种情况 DB 端除了增加索引，对 CPU 的消耗是无法优化的，所以 DB 性
能必然下降。一般这种情况 DBA 会建议能在程序端计算的就不要放在 DB 端，或者直接接搜索引擎。

类似 sql 模式：

**错误使用子查询**

在 DB 端执行去重， **join 以及子查询等操作的时候，mysql 会自动创建临时表** 。

DB 自动创建临时表的情况有如下几种

在 mysql 中，对于子查询，外层每执行一次，内层子查询要重复执行一次，所以一般建议用 join 代替子
查询。

**下面举一个子查询引起 DB 性能问题的例子**

```
Query1：select count(*) from wd_order_late_reason_send wrs left join
wd_order_detail_late_send wds on wrs.store_code = wds.store_code;
```
下面是执行计划：

```
select count(*) from table_name_1;
select * from table_name_1 limit n,m;(n值越大性能越差)
建议改造成：
select * from table_name_1 where id>? and id<?
```
```
select sum(column_name) as column_1 from table_name_1;
or
select distinct cloumn_name from table_name_1 group by column_name_1 order
by column_name_1;
```
1. Evaluation of UNION statements.
2. Evaluation of some views, such those that use the TEMPTABLE algorithm,
UNION, or aggregation.
3. Evaluation of derived tables (subqueries in the FROM clause).（这个是本节关注的
重点）
4. Tables created for subquery or semi-join materialization (see Section
8.2.1.18, “Subquery Optimization”).
5. Evaluation of statements that contain an ORDER BY clause and a different
GROUP BY clause, or for which the ORDER BY or GROUP BY contains columns from
tables other than the first table in the join queue.
6. Evaluation of DISTINCT combined with ORDER BY may require a temporary table.
7. For queries that use the SQL_SMALL_RESULT option, MySQL uses an in-memory
temporary table, unless the query also contains elements (described later) that
require on-disk storage.
8. Evaluation of multiple-table UPDATE statements.
9. Evaluation of GROUP_CONCAT () or COUNT (DISTINCT) expressions.

```
*************************<strong> 1. row </strong>***********************
<strong>
id: 1
select_type: SIMPLE
table: wrs
```

```
Query2：select count(*) from (select wrs.store_code from wd_order_late_reason_send wrs
left join wd_order_detail_late_send wds on wrs.store_code = wds.store_code) tb；
```
执行计划如下

```
type: ALL
possible_keys: NULL
key: NULL
key_len: NULL
ref: NULL
rows: 836846
Extra: NULL
</strong>***********************<strong> 2. row
</strong>*************************
id: 1
select_type: SIMPLE
table: wds
type: ALL
possible_keys: NULL
key: NULL
key_len: NULL
ref: NULL
rows: 670612
Extra: Using where; Using join buffer (Block Nested Loop)
```
```
*************************<strong> 1. row </strong>***********************
<strong>
id: 1
select_type: PRIMARY
table: <derived2>
type: ALL
possible_keys: NULL
key: NULL
key_len: NULL
ref: NULL
rows: 561198969752
Extra: NULL
</strong>***********************<strong> 2. row
</strong>***********************<strong>
id: 2
select_type: DERIVED
table: wrs
type: ALL
possible_keys: NULL
key: NULL
key_len: NULL
ref: NULL
rows: 836846
Extra: NULL
</strong>***********************<strong> 3. row
</strong>*************************
id: 2
select_type: DERIVED
table: wds
type: ALL
```

```
类别 吞吐量 响应时间
```
```
访问L1 Cache 0.5ns
```
```
访问L2 Cache 7ns
```
```
内存访问 800M/s 100ns
```
```
机械盘 300M/s 10ms
```
```
SSD 300M/s 0.1~0.2ms
```
这两个 sql 结果相同， **唯一不同的是第二条 sql 使用了子查询。通过执行计划可以看出（排除没有索引部
分）两个 sql 最大的差别就是第二个 sql 有 derived table 并且 rows 是 561198969752 ，出现这个数值是
因为在 select count（*）*** *每次计数的时候子查询的 sql 都会执行一遍，所以最后是子查询 join 的笛卡
尔积**。因为内存中用于进行 join 操作的空间有限，这个时候就会使用磁盘空间来创建临时表，所以当
第二种 sql 频繁执行的时候会有磁盘被撑爆的风险。想要了解更多关于子查询的优化可以参考下面这个
链接 link

**慢 sql**

这里我们所说的慢 sql 主要指那些由于索引使用不正确或没有使用索引产生的，一般可以通过增加索
引。一个合理的索引对一条 sql 性能的影响是非常巨大的。索引的主要目的是为了减少读取的数据块，
也就是我们常说的逻辑读，读取的数据块越少，sql 效率越高。另外索引在一定程度上也可以减少 CPU 的
消耗，例如排序，分组，因为索引本来就是有序的。

说到逻辑读，对应的就会有物理读，在 mysql 服务端是有 buffer pool 来缓存硬盘中的数据，但是这个
buffer pool 的大小跟磁盘中数据文件的大小是不等的，往往 buffer pool 会远远小于磁盘中数据的大
小。buffer pool 会有一个 LRU 链表，当从磁盘中加载数据块到内存中（这个就是物理读）发现没有空间
的时候会优先覆盖 LRU 链表中的数据块。当一条 sql 没有合理的索引需要扫描大量的数据的时候，不光要
扫描内存中的许多数据块，还可能需要从磁盘中加载不同不存在的数据块到内存中进行判断，当这种情
况频繁发生的时候，sql 性能就会急剧下降，因而也影响了 DB 实例的性能。

以下表格是访问不同存储设备的 rt，由此可见一个合理的索引的重要性。

**日志刷盘策略不合理**

目前集团 mysql 大部分使用的都是 innodb 存储引擎，因此在每条 DML 语句执行时不光会记如 binlog 还有
记录 innodb 特有的 redo log 和 undo log。这些日志文件都是先写入内存中然后在刷新到磁盘中。 **在
server 端有两个参数分别控制他们的写入速度。innodb_flush_log_at_trx_commit 控制 redo log 写入
模式，sync_binlog 控制 binlog 写入模式。**

```
possible_keys: NULL
key: NULL
key_len: NULL
ref: NULL
rows: 670612
Extra: Using where; Using join buffer (Block Nested Loop)
```

通过以上表格可以了解到，在使用线上默认配置的情况下每次 commit 都会刷 redo log 到磁盘，也就是
说每次写入都会伴随着日志刷盘的操作，需要消耗磁盘 IO，所以在高 TPS 或者类似业务大促情况下，
DBA 可以调整这个参数，来提升 DB 支撑 TPS 的能力。

**BP 设置过小**

前面已经提到 sql 在读写数据的时候不会直接跟磁盘交互，而是先读写内存数据，因为这样最快。但是
考虑到成本问题 BP（buffer pool）大小是有限的，不可能跟数据文件同等大小，所以如果 BP 设置不合
理就会导致 DB 的 QPS TPS 始终上不去。下面我们具体分析一下。

mysql buffer pool 中包含 undo page，insert buffer page，adaptive hash index，index page，lock
info，data dictionary 等等 DB 相关信息，但是这些 page 都可以归为三类 free page, clean page, dirty
page. buffer pool 中维护了三个链表：free list, dirty list, lru list

```
free page:此page未被使用，此种类型page位于free链表中
clean page:此page被使用，对应数据文件中的一个页面，但是页面没有被修改，此种类型page位
于lru链表中
dirty page:此page被使用，对应数据文件中的一个页面，但是页面被修改过，此种类型page位于
lru链表和flush链表中
```
当 BP 设置过小的时候， **比如 BP 10 g 数据文件有 200 g 这个时候有大量的 select 或者 dml 语句，mysql 就
会频繁的刷新 lru list 或者 dirty list 到磁盘，大部分时间消耗在刷磁盘上，而不是业务 sql 处理上，这个
时候就会导致业务 TPS QPS 始终上不去，伴随着 DB 内存命中率降低** 。通常这个时候的解决办法是需要
DBA 调整一下实例 BP 的大小。

**硬件问题**

就像生活中会有意外一样，在排除了之前那些因素之后，还会存在因为硬件故障或者参数设置不合理导
致 DB 性能抖动的情况，如果不能立即修复，DBA 一般只能通过迁移实例的方式来消除影响。


#### 64 、如何进行 MySQL OOM（内存溢出）的排查和优化？

OOM 全称"Out Of Memory"，即内存溢出。

```
内存溢出已经是软件开发历史上存在了近 40 年的“老大难”问题。在操作系统上运行各种软件时，
软件所需申请的内存远远超出了物理内存所承受的大小，就叫内存溢出。
```
内存溢出产生原因多种多样，当内存严重不足时，内核有两种选择：

```
1. 直接panic
2. 杀掉部分进程，释放一些内核。
```
大部分情况下，会杀掉导致 OOM 的进程，然后系统恢复。

通常我们会添加对内存的监控报警，例如：当 memory 或 swap 使用超过 90%时，触发报警通知，需要
及时介入排查。

如果已经出现 OOM，则可以通过 dmesg 命令查看，CentOS 7 版本以上支持 -T 选项，能将时间戳转成时
间格式，方便查看具体时间：

通过日志可以看出哪些进程、占用多少内存等信息，并会 Kill 掉占用内存较大的进程。

**内存问题的排查思路**

**一、操作系统内存检查**

以 MySQL 为例，OOM 后，mysqld 进程被 Killed，内存会被释放。

mysqld_safe 安全进程会将 mysqld 拉起，此时查看到的系统内存会是一个正常值。

如果内存使用很高，但还未 OOM，系统内存使用情况可能为下面情况：

可以看出此时的内存使用已经很高了，物理内存和 swap 虚拟内存几乎都被用完，buffers 和 cached 也不
多，随时可能出现 OOM 的情况。

```
[root@localhost ~]# dmesg -T[Tue Mar 26 10:23:41 2019] memory: usage 25164884kB,
limit 25165824kB, failcnt 172713[Tue Mar 26 10:23:41 2019] memory+swap: usage
25165824kB, limit 25165824kB, failcnt 6632[Tue Mar 26 10:23:41 2019] kmem: usage
0kB, limit 9007199254740988kB, failcnt 0[Tue Mar 26 10:23:41 2019] Memory cgroup
stats for host: cache:4184KB rss:25160700KB rss_huge:0KB mapped_file:1968KB
swap:940KB inactive_anon:1744836KB active_anon:23417824KB inactive_file:76KB
active_file:1112KB unevictable:0KB[Tue Mar 26 10:23:41 2019] [ pid ] uid tgid
total_vm rss nr_ptes swapents oom_score_adj name[Tue Mar 26 10:23:41 2019]
[12910] 997 12910 16489842 6282421 28321 0 809 mysqld[Tue
Mar 26 10:23:41 2019] Memory cgroup out of memory: Kill process 548216
(ParalInputsProc) score 1812 or sacrifice child[Tue Mar 26 10:23:41 2019] Killed
process 12910 (mysqld) total-vm:65959368kB, anon-rss:25129684kB, file-rss:0kB,
shmem-rss:0kB
```
```
[root@localhost ~]# free -m total used free shared
buffers cachedMem: 128937 128527 409 1 166
1279-/+ buffers/cache: 127081 1855Swap: 16383 16252
131
```

首先，通过 top 命名查看占用内存最大的进程：

shift+o 可以选择排序方式，n 代表%MEM。

可以看出 mysqld 进程占用内存最大，也可以这样查：

**RSZ** 为进程占用私有内存大小，单位 Kb。

**VSZ** 为映射的虚拟内存大小，单位 Kb。

通过 **RSZ/total** 也可以算出占用总内存比例。

**二、查看给 mysql 分配的内存**

mysql 内部主要内存可通过下面语句查出：

每个参数配置大小：

每个参数配置说明：

```
[root@localhost ~]# topMem: 132031556k total, 131418864k used, 612692k free,
212104k buffersSwap: 16777212k total, 0k used, 16777212k free,
14648144k cached
PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND14920 mysql
20 0  125 g 109 g 6164 S 6.6 87.0  27357 : 08 mysqld
```
```
[root@localhost ~]# ps -e -o 'pid,comm,args,pcpu,rsz,vsz,stime,user,uid' | grep
-E 'PID|mysql' |grep -v grep PID COMMAND COMMAND
%CPU RSZ VSZ STIME USER UID25339 mysqld
/export/servers/mysql/bin/m 9.4 115001324 130738976 2017 mysql 50032070
mysqld_safe /bin/sh /export/servers/mys 0.0 296 106308 2017 root
0
```
```
MYSQL >SET @giga_bytes = 1024 * 1024 * 1024 ;SELECT (@@key_buffer_size +
@@query_cache_size + @@tmp_table_size + @@innodb_buffer_pool_size +
@@innodb_additional_mem_pool_size + @@innodb_log_buffer_size + (select
count(HOST) from information_schema.processlist)/*@@max_connections*/*
(@@read_buffer_size + @@read_rnd_buffer_size + @@sort_buffer_size +
@@join_buffer_size + @@binlog_cache_size + @@thread_stack)) / @giga_bytes AS
MAX_MEMORY_GB;
```
```
*************************** 1. row ***************************
@@key_buffer_size: 67108864 @@query_cache_size: 0
@@tmp_table_size: 268435456 @@innodb_buffer_pool_size: 38654705664
@@innodb_additional_mem_pool_size: 134217728 @@innodb_log_buffer_size:
8388608 @@max_connections: 3000 @@read_buffer_size: 4194304
@@read_rnd_buffer_size: 4194304 @@sort_buffer_size:
2097152 @@join_buffer_size: 2097152
@@binlog_cache_size: 32768 @@thread_stack: 262144
```

```
innodb_buffer_pool_size 占用内存最大的参数
```
```
innodb_additional_mem_pool_size 额外内存，mysql5.7以后移除
```
```
innodb_log_buffer_size 重做日志缓存大小
```
```
key_buffer_size 只用于MyISAM引擎，不需要太大
```
```
tmp_table_size 临时表缓存大小
```
```
query_cache_size 查询缓存，建议关闭
```
```
max_connections 最大连接数
```
```
read_buffer_size
read_rnd_buffer_size
sort_buffer_size join_buffer_size
binlog_cache_size thread_stack
```
```
这些参数都跟线程有关，所占内存为这些参数的和*最大
连接数。连接数越多占用内存也就越多，建议不超过
512K，binlog_cache_size采用系统默认32K，
thread_stack默认256K即可。
```
需要给 mysql 分配多大内存，直接跟以上参数有关。

太大会导致内存不足，太小会影响性能，如何分配合理值，还需根据业务情况来定。

但业务场景较多，每个业务配置都不一样，会造成运维成本较高。所以能定制出一套适用于绝大多数场
景的配置模板就可以了。

**1 、如果 mysql 分配的内存比系统内存大**

比如系统内存 128 G，mysql 分配的内存已经大于 128 G，但是系统本身和其它程序也需要内存，甚至
mysqldump 同样需要内存，所以这样就很容易造成系统内存不足，从而导致 OOM。

这时我们要查出哪些参数设置比较大，适当降低内存分配。

innodb_buffer_pool 在 mysql 中占有最大内存，将 innodb_buffer_pool_size 调小可以有效降低 OOM 问
题。

但如果设置太小会导致内存刷脏页频率增加，IO 增多，从而降低性能。通常我们认为
innodb_buffer_pool_size 为系统内存的 60%~75%最优。

查看 buffer_pool 的使用情况：


可以看出 buffer_pool 分了 3 个实例，POOL_SIZE 为每个实例大小，这里为页个数，我们知道 mysql 页的
默认大小为 16 K，所以单个实例的真正大小为 611669*16 K，5.6 以后要求 FREE_BUFFERS 至少保留 1024
个页，少于 1024 个页时会强制刷脏数据，后面的值可以看出脏页情况。

另外如果 PAGES_MADE_YOUNG 远大于 PAGES_NOT_MADE_YOUNG 页数，那么此时内存使用就可能比
较大，可以适当降低 innodb_buffer_pool_size 大小。

**2 、如果 innodb_buffer_pool_size 不是很大，但内存还是很高，**

如果 innodb_buffer_pool_size 不是很大，但内存还是很高，也可能是由于并发线程太多导致，需要确
认是不是应用异常，还是需要调整 max_connections 最大连接。

如果连接太多，每个连接也会占用独立的内存，read、sort、join 缓存都是 session 级别，连接越多需要
内存就越多，所以这些参数也不能设置太大。

需要注意的是一些参数不支持动态修改，只能先修改配置文件然后重启 mysql 才能生效，所以在 mysql
启动之前，一定要把参数值确认好。

**3 、如果 mysql 参数设置都比较合理，但是仍然出现 oom**

如果 mysql 分配的内存比系统内存小, mysql 参数设置都比较合理，但是仍然出现 oom，那么可能是 **由
于 mysql 在系统层面所需内存不足导致** ，

因为 mysql 读取表时，如果同时有 **多个 session 引用一个表则会创建多个表对象** ， **这样虽然减少了内部
表锁的争用，但是会加大内存使用量** 。

首先，可以通过 lsof -p pid 查看进程打开的系统文件数，pid 为 mysqld 的进程号。

```
MYSQL >select
POOL_ID,POOL_SIZE,FREE_BUFFERS,DATABASE_PAGES,OLD_DATABASE_PAGES,MODIFIED_DATABA
SE_PAGES,PAGES_MADE_YOUNG,PAGES_NOT_MADE_YOUNG from
information_schema.INNODB_BUFFER_POOL_STATS;+---------+-----------+-------------
-+----------------+--------------------+-------------------------+--------------
----+----------------------+| POOL_ID | POOL_SIZE | FREE_BUFFERS |
DATABASE_PAGES | OLD_DATABASE_PAGES | MODIFIED_DATABASE_PAGES | PAGES_MADE_YOUNG
| PAGES_NOT_MADE_YOUNG |+---------+-----------+--------------+----------------+-
-------------------+-------------------------+------------------+---------------
-------+| 0 |  611669 | 1024 | 610644 |
225393 | 0 | 309881 |  0 ||
1 |  611669 | 1024 | 610645 | 225393 |
0 | 309816 |  0 || 2 |  611669
| 1024 | 610645 | 225393 | 0 |
309756 |  0 |+---------+-----------+--------------+-
---------------+--------------------+-------------------------+-----------------
-+----------------------+
```

查看 mysql 服务打开文件数限制：

查看操作系统打开文件数限制：

**如果此时打开的文件很多，内存也会占用很多** 。

其次，还需看一下 table_open_cache，当打开一个表后会把这个表的文件描述符缓存下来。

```
[root@localhost ~]# ps -ef | grep mysqld[root@localhost ~]# lsof -p 3455COMMAND
PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEmysqld 30012 mysql
cwd DIR 8,3 12288 58982404 /mysql/datamysqld 30012 mysql mem
REG 8,1 599392 272082 /lib64/libm-2.12.somysqld 30012 mysql mem
REG 8,1 91096 272089 /lib64/libz.so.1.2.3mysqld 30012 mysql mem
REG 8,1 93320 272083 /lib64/libgcc_s-4.4.7-20120601.so.1mysqld
30012 mysql mem REG 8,1 43392 272095 /lib64/libcrypt-
2.12.somysqld 30012 mysql 10uW REG 8,3 536870912 59015176
/mysql/data/ib_logfile0mysqld 30012 mysql 11uW REG 8,3 536870912
59015177 /mysql/data/ib_logfile1mysqld 30012 mysql 12uW REG 8,3
536870912 59015178 /mysql/data/ib_logfile2mysqld 30012 mysql 13uW REG
8,3 675282944 59001816 /mysql/data/test/table6.ibdmysqld 30012 mysql 14uW
REG 8,3 2155872256 58985613 /mysql/data/test/table487.ibdmysqld 30012
mysql 15u REG 8,3 0 58982414 /mysql/tmp/ibhNDzPM
(deleted)mysqld 30012 mysql 16uW REG 8,3 2306867200 58983861
/mysql/data/test/table327.ibdmysqld 30012 mysql 17uW REG 8,3 4169138176
58985467 /mysql/data/test/table615.ibdmysqld 30012 mysql 18uW REG 8,3
79691776 59020641 /mysql/data/test/table_v199_20170920.ibdmysqld 30012 mysql
19uW REG 8,3 67108864 59015043
/mysql/data/test/table_v39_20170920.ibdmysqld 30012 mysql 20uW REG 8,3
75497472 59014992 /mysql/data/test/table_v7_20170920.ibdmysqld 30012 mysql
21uW REG 8,3 83886080 59019735
/mysql/data/test/table_v167_20170920.ibdmysqld 30012 mysql 22uW REG 8,3
1367343104 58997684 /mysql/data/popfin6/table_uuid6.ibdmysqld 30012 mysql
23uW REG 8,3 1275068416 58984491 /mysql/data/test/table_uuid7.ibd...
[root@localhost ~]# lsof -p 3455 |grep ibd|wc -l54869
```
```
MySQL >show global variables like 'open_files_limit';+------------------+-------
+| Variable_name | Value |+------------------+-------+| open_files_limit |
65535 |+------------------+-------+
```
```
[root@localhost ~]# ulimit -amax memory size (kbytes, -m) unlimitedopen
files (-n) 65535
```
```
MYSQL >show global variables like 'table_open_cache';+------------------+-------
+| Variable_name | Value |+------------------+-------+| table_open_cache |
16384 |+------------------+-------+MYSQL >show global status like
'%open%tables%';+------------------------+--------+| Variable_name |
Value |+------------------------+--------+| Open_tables | 16384 ||
Opened_tables | 401374 |+------------------------+--------+
```

通过以上两个值来判断 table_open_cache 是否到达瓶颈。

**当缓存中的值 open_tables 临近到了 table_open_cache 值的时候，说明表缓存池快要满了** ，但
Opened_tables 还在一直有新的增长，这说明还有很多未被缓存的表。

用 show open tables from schema 命令，可以查看 table_open_cache 中缓存的表，重复打开的表仅显
示一个。

In_use 显示当前正在使用此表的线程数，如果大于 0 也意味着此表被锁。

Name_locked 只适用于 DROP 和 RENAME，在执行 DROP 或 RENAME 时，table_open_cache 中的表文件
描述符会被移除，所以不会看到除 0 以外的其他值。

一般在库表比较多的情况下（分库分表）很容易出现内存占用较大的情况。

如果要解决根源，还是需要对库表进行拆分。

**4 、MYSQL 内部其他内存**

information_schema 下的表都使用的都是 MEMORY 存储引擎，数据只在内存中保留，启动时加载，关
闭后释放。

查看除系统库外是否有 MEMORY 引擎表：

如果 **业务有使用 MEMORY 存储引擎的，尽量改成 innodb 引擎** 。

**5 、MYSQL 事件内存指标**

从 MySQL 5.7 开始，在 performance_schema 中会记录内存分配。

查看哪些指标启动了内存收集功能：

复制

启动需要收集内存的指标：

复制

```
MYSQL >show open tables from sysbenchtest;+--------------+----------+--------+--
-----------+| Database | Table | In_use | Name_locked |+--------------+--
--------+--------+-------------+| sysbenchtest | sbtest1 |  1 | 0
|| sysbenchtest | sbtest2 |  0 | 0 || sysbenchtest | sbtest3 |
 0 | 0 || sysbenchtest | sbtest4 |  0 | 0 ||
sysbenchtest | sbtest5 |  0 | 0 |
```
```
MySQL >select * from information_schema.tables where engine='MEMORY' and
TABLE_SCHEMA !='information_schema';
```
```
MySQL >select * from performance_schema.setup_instruments where NAME LIKE
'memory/%';
```
```
MySQL >UPDATE performance_schema.setup_instruments SET ENABLED = 'YES' WHERE
NAME LIKE 'memory/%';
```

指标的内存收集结果会汇总到到 sys 库下的视图中：

这些视图总结了内存使用情况，按事件类型分组，默认降序排列：

**总结：**

通过以上排查能大体知道哪些占用内存较多，针对内存占用较多的地方再做具体优化。正像文章开头所
说的，内存溢出已经是软件开发历史上存在了近 40 年的“老大难”问题，更何况数据库环境更加复杂，
SQL 语法、数据类型、数据大小等这些因素都与内存有关，所以在设计使用上更要多想内存溢出问题。

#### 65 、批量向 MySQL 导 1000 W 数据，如何优化？

```
1. 直接导入
2. 使用存储过程循环拼接
3. 使用load data infile
4. 修改ENGINE=InnoDB为MyISAM（v5.1之前是MyISAM，之后是InnoDB）
5. 减少IO次数
6. SQL写法优化（一条SQL语句插入多条数据）
7. 合理设置批量大小（在事务中进行插入处理，不要一条数据提交一次，肯定要分批处理）
8. 尽量顺序插入（减少索引的维护压力）
```
```
MySQL root@[sys]>show tables like 'memory%';+-----------------------------------
+| Tables_in_sys (memory%) |+-----------------------------------+|
memory_by_host_by_current_bytes || memory_by_thread_by_current_bytes ||
memory_by_user_by_current_bytes || memory_global_by_current_bytes ||
memory_global_total |+-----------------------------------+
```
```
MySQL >select event_name,current_count,current_alloc,high_alloc from
sys.memory_global_by_current_bytes where current_count > 0 ;+--------------------
------------------------------------------------------------+---------------+---
------------+-------------+| event_name
| current_count | current_alloc | high_alloc |+----
----------------------------------------------------------------------------+---
------------+---------------+-------------+|
memory/performance_schema/table_handles |
 10 | 90.62 MiB | 90.62 MiB ||
memory/performance_schema/events_statements_summary_by_thread_by_event_name |
3 | 26.01 MiB | 26.01 MiB ||
memory/performance_schema/memory_summary_by_thread_by_event_name |
3 | 16.88 MiB | 16.88 MiB ||
memory/performance_schema/events_statements_history_long |
1 | 13.66 MiB | 13.66 MiB ||
memory/performance_schema/events_statements_history |
3 | 10.49 MiB | 10.49 MiB ||
memory/performance_schema/events_statements_current |
3 | 10.49 MiB | 10.49 MiB |...
```

```
9. 合并事务+事务+有序数据的优化插入方式
10. 注意SQL批量插入的大小必须合理
11. 事务执行时间不要太长
12. 合理设置MySQL相应配置参数，增加缓存或减少不必要日志磁盘读写
```
**合并 SQL 语句**

一条 SQL 插入一条数据

一条 SQL 插入多条数据

合并 SQL 语句能够提高程序的插入效率（原因在于合并后日志量减少了，降低日志刷盘的数据量和频
率），也能减少 SQL 语句解析的次数，减少网络传输的 IO

**在事务中插入处理**

将插入修改为

在事务中可以提高数据的插入效率，因为在进行一个 insert 操作时，MySQL 内部会建立一个事务，在事
务内才能进行真正的插入处理操作，这样可以减少创建事务的消耗，让所有插入都在执行后才进行提交
操作

**数据有序插入**

数据有序插入是指插入记录在主键上是有序排列
无序插入：

有序插入：

由于数据库在插入时，需要维护索引数据，无序的记录会增大维护索引的成本（参照 innodb 使用的
B+tree 索引，如果每次插入记录都在索引的最后面，索引的定位效率很高，并且对索引调整较小；

```
insert into tb1(id,name,age,sex) values( 1 ,'khue', 25 ,'m');
insert into tb1(id,name,age,sex) values( 2 ,'green', 24 ,'w');
```
```
insert into tb1(id,name,age,sex) values( 1 ,'khue', 25 ,'m'),( 2 ,'green', 24 ,'w');
```
```
start transaction;
insert into tb1(id,name,age,sex) values( 1 ,'khue', 25 ,'m');
insert into tb1(id,name,age,sex) values( 2 ,'green', 24 ,'w');
commit;
```
```
insert into tb1(id,name,age,sex) values( 3 ,'joo', 26 ,'m');
insert into tb1(id,name,age,sex) values( 5 ,'green' 25 ,'w');
insert into tb1(id,name,age,sex) values( 1 ,'khue', 25 ,'m');
```
```
insert into tb1(id,name,age,sex) values( 1 ,'khue', 25 ,'m');
insert into tb1(id,name,age,sex) values( 3 ,'joo', 26 ,'m');
insert into tb1(id,name,age,sex) values( 5 ,'green' 25 ,'w');
123
```

如果插入的记录在索引中间，则需要 B+tree 进行分裂合并等操作，会消耗比较多的计算资源，并且插入
记录的索引定位效率会下降，数据量较大时会有频繁的磁盘操作）

###### 总结

合并数据+事务的方法在较小数据量时，性能提高很明显；

数据量较大时（ 1000 万以上），性能急剧下降，因为此时数据量超过了 innodb_buffer 的容量，每次定
位索引涉及较多的磁盘读写操作；

使用合并数据+事务+有序数据则表现良好，因为有序数据索引定位较为方便，无需频繁对磁盘进行读写
操作

注意：

```
1. SQL语句是有长度限制的，在合并数据时，不能超过SQL长度限制（通过max_allowed_packet配
置可以修改，默认1M）
2. 事务也需要控制大小，事务过大会影响执行效率（MySQL有innodb_log_buffer_size配置项，超过
值会把innodb数据刷到磁盘，此时效率会有所下降）
```
#### 66 、数据库中事务的隔离级别有哪些？各自有什么特点？

事务 (Transaction) 是操作数据库中某个数据项的一个程序执行单元 (unit)。

事务应该具有 4 个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为 ACID 特性。

###### 事务的四个特征：

**1 、Atomic 原子性**

事务必须是一个原子的操作序列单元，事务中包含的各项操作在一次执行过程中，要么全部执行成功，
要么全部不执行，任何一项失败，整个事务回滚，只有全部都执行成功，整个事务才算成功。

**2 、Consistency 一致性**

事务的执行不能破坏数据库数据的完整性和一致性，事务在执行之前和之后，数据库都必须处于一致性
状态。

**3 、Isolation 隔离性**

在并发环境中，并发的事务是相互隔离的，一个事务的执行不能被其他事务干扰。

即不同的事务并发操纵相同的数据时，每个事务都有各自完整的数据空间，即一个事务内部的操作及使
用的数据对其他并发事务是隔离的，并发执行的各个事务之间不能相互干扰。

**4 、Durability 持久性**

持久性（durability）：持久性也称永久性（permanence），指一个事务一旦提交，它对数据库中对应
数据的状态变更就应该是永久性的。

即使发生系统崩溃或机器宕机，只要数据库能够重新启动，那么一定能够将其恢复到事务成功结束时的
状态。


```
隔离级别
```
```
脏读（Dirty
Read）
```
```
不可重复读
（NonRepeatable Read）
```
```
幻读（Phantom
Read）
```
```
读未提交(Read
uncommitted） 可能 可能 可能
```
```
读已提交（Read
committed）
```
```
不可能 可能 可能
```
```
可重复读
（Repeatable
read）
```
```
不可 能 不可 能 可能
```
```
可串行化
（Serializable ） 不可能 不可能 不可能
```
```
比方说：一个人买东西的时候需要记录在账本上，即使老板忘记了那也有据可查。
```
###### SQL 中的 4 个事务隔离级别：

在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是
为了构建这些隔离级别存在的。

###### （ 1 ）读未提交

**允许脏读。** 在 **读未提交** 隔离级别下，允许 **脏读** 的情况发生。

脏读指的是读到了其他事务未提交的数据，

未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。

读到了并一定最终存在的数据，这就是脏读。

```
如果一个事务正在处理某一数据，并对其进行了更新，
但同时尚未完成事务，或者说事务没有提交，
与此同时，允许另一个事务也能够访问该数据。
例如A将变量n从 0 累加到 10 才提交事务，此时B可能读到n变量从 0 到 10 之间的所有中间值。
```

脏读最大的问题就是可能会读到不存在的数据。

比如在上图中，事务 B 的更新数据被事务 A 读取，但是事务 B 回滚了，更新数据全部还原，也就是说事务
A 刚刚读到的数据并没有存在于数据库中。

###### （ 2 ）读已提交

在 **读已提交** 隔离级别下， **禁止了脏读** ，但是允许 **不可重复读** 的情况发生

**不可重复读** 指的是在一个事务内，最开始读到的数据和事务结束前的任意时刻读到的同一批数据出现不
一致的情况。

```
只允许读到已经提交的数据。
即事务A在将n从 0 累加到 10 的过程中，B无法看到n的中间值，之中只能看到 10 。
```
```
事务A在将n从 0 累加到 10 的过程中，B无法看到n的中间值，之中只能看到 10 。
同时，
有事务C进行从 10 到 20 的累加，此时B在同一个事务内再次读时，读到的是 20 。
```

**事务 A 多次读取同一数据，但事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A
多次读取同一数据时，结果不一致。**

不可重复读一词，有点反人类，不好记忆。是从 **Nonrepeatable read** 翻译过来的，感觉英文的，好
记忆一点。

###### （ 3 ）可重复读

在 **可重复读** 隔离级别下，禁止了： **脏读、不可重复读** 。

但是，允许 **幻读** 。

在可重复读中，该 sql 第一次读取到数据后，就将这些数据加锁（悲观锁），其它事务无法修改这些数
据，就可以实现可重复读了。

但这种方法却无法锁住 insert 的数据，所以当事务 A 先前读取了数据，或者修改了全部数据，事务 B 还是
可以 insert 数据提交，

这时事务 A 就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。

###### （ 4 ）串行化

如果不对事务进行并发控制，我们看看数据库并发操作是会有那些异常情形

```
（ 1 ）一类丢失更新：两个事物读同一数据，一个修改字段 1 ，一个修改字段 2 ，后提交的恢复了先
提交修改的字段。
（ 2 ）二类丢失更新：两个事物读同一数据，都修改同一字段，后提交的覆盖了先提交的修改。
（ 3 ）脏读：读到了未提交的值，万一该事物回滚，则产生脏读。
（ 4 ）不可重复读：两个查询之间，被另外一个事务修改（update）了数据的内容，产生内容的不
一致。
```
```
保证在事务处理过程中，多次读取同一个数据时，其值都和事务开始时刻时是一致的。
```
```
最严格的事务，要求所有事务被串行执行，不能并发执行。
```

```
（ 5 ）幻读：两个查询之间，被另外一个事务插入或删除了（insert、delete）记录，产生结果集
的不一致。
```
###### 数据一致性和高性能，是天生的矛盾

无论是尼恩的葵花宝典视频、还是 rocketmq 视频、还是推送中台实操视频，无一例外，都揭示了一个
硬道理： **数据一致性和高性能，是天生的矛盾**

在事务领域，也是如此：

**场景一：性能最好的，一致性最差**

读未提交的级别，它是性能最好，也可以说它是最野蛮的方式，因为它压根儿就不加锁，所以根本谈不
上什么隔离效果，可以理解为没有隔离。

**场景二：一致性最好的，性能最差**

串行化就一致性性最强。串行化相当于处理一个人请求的时候，别的人都等着。

读的时候加共享锁，也就是其他事务可以并发读，但是不能写。写的时候加排它锁，其他事务不能并发
写也不能并发读。

需要 Serializable 隔离级别，读用读锁，写用写锁，读锁和写锁互斥，这都是非常悲观的悲观锁策略，
这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。

**然后，就是场景三： 在高性能和数据一致性中间，寻找平衡。**

数据库的事务隔离越严格，并发副作用越小，但付出的代价越大；

**读提交和可重复读** , 都是在寻找平衡

这两种隔离级别是比较复杂的，既要允许一定的并发，又想要解决数据一致性问题。

oracle 默认事务隔离级别为读已提交 (RC), 说明可以 **不可重复读** ，（不可重复读这个词用的反人类，）

MySQL 默认事务隔离级别为可重复读 (RR),

ORACLE、MySQL、PostgreSQL 等成熟的数据库中的读已提交、可重复读隔离级别，并没有使用
Serializable 隔离级别中的悲观锁，都是使用了以乐观锁为理论基础的 MVCC（多版本并发控制）来实
现。

**补充 : 基于元数据的 Spring 声明性事务 :**

Isolation 属性一共支持五种事务设置，具体介绍如下：

```
DEFAULT 使用数据库设置的隔离级别 ( 默认 ) ，由 DBA 默认的设置来决定隔离级别.
READ_UNCOMMITTED 会出现脏读、不可重复读、幻读 ( 隔离级别最低，并发性能高 )
READ_COMMITTED 会出现不可重复读、幻读问题（锁定正在读取的行）
REPEATABLE_READ 会出幻读（锁定所读取的所有行）
SERIALIZABLE 保证所有的情况不会发生（锁表）
```
**不可重复读和幻读的区别：**

不可重复读的重点是修改：同样的条件，你读取过的数据，再次读取出来发现值不一样了。


幻读的重点在于新增或者删除：同样的条件, 第 1 次和第 2 次读出来的记录数不一样。

**事务传播行为种类**

Spring 在 TransactionDefinition 接口中规定了 7 种类型的事务传播行为，它们规定了事务方法和事务方
法发生嵌套调用时事务如何进行传播：

PROPAGATION_REQUIRED

如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选
择。

PROPAGATION_SUPPORTS

支持当前事务，如果当前没有事务，就以非事务方式执行。

PROPAGATION_MANDATORY

使用当前的事务，如果当前没有事务，就抛出异常。

PROPAGATION_REQUIRES_NEW

新建事务，如果当前存在事务，把当前事务挂起。

PROPAGATION_NOT_SUPPORTED

以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。

PROPAGATION_NEVER

以非事务方式执行，如果当前存在事务，则抛出异常。

PROPAGATION_NESTED

如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与 PROPAGATION_REQUIRED 类
似的操作。

#### 67 、Mysql 如何实现无数据插入，有数据更新？

mysql 的语法与 sql server 有很多不同，sql server 执行插入更新时可以 update 后使用 if 判断返回的
@@rowcount 值，然后确定是否插入，

mysql 在语句中无法使用类似 sql server 的@@rowcount，但是有另外一些方式也能达到无数据插入，
有数据更新的目的

**mysql 无数据插入，有数据更新**

一种方法是通过 REPLACE 语句实现。

一种方法是结合 INSERT 语句和 ON DUPLICATE KEY UPDATE 语句实现，

###### 1 、REPLACE 语句

REPLACE 相当于如果数据存在，先按主键删除原记录后再添加记录新记录，此方式会更新索引

语法和 INSERT 非常的相似，如下面的 REPLACE 语句是插入或更新一条记录。


使用 REPLACE 的最大好处就是可以将 DELETE 和 INSERT 合二为一，形成一个原子操作。

这样就可以不必考虑在同时使用 DELETE 和 INSERT 时添加事务等复杂操作了。

在使用 REPLACE 时，表中必须有唯一索引，而且这个索引所在的字段不能允许空值，否则 REPLACE 就和
INSERT 完全一样的。

在执行 REPLACE 后，系统返回了所影响的行数，如果返回 1 ，说明在表中并没有重复的记录，如果返回
2 ，说明有一条重复记录，系统自动先调用了 DELETE 删除这条记录，然后再记录用 INSERT 来插入这条
记录。

###### 2 、ON DUPLICATE KEY UPDATE

ON DUPLICATE KEY UPDATE 先更新数据，如果数据不存在时进行 insert

如果指定了 ON DUPLICATE KEY UPDATE，并且插入行后会导致在一个 UNIQUE 索引或 PRIMARY KEY 中
出现重复值，则执行 UPDATE。

#### 68 ：说说有哪些分库分表的思路和技巧？

###### 为什么要分库分表

在业务场景中，mysql 的单表数据量出现在千万级左右查询数据就会出现瓶颈。在各种优化后，可以考
虑对数据库进行拆分。

1. 水平拆分：

```
REPLACE INTO t_param (param_name,param_value) select #{paramName}, #{paramValue}
```
```
insert into t_param (param_name, param_value) values (#{paramName}, #
{paramValue}) ON DUPLICATE KEY UPDATE param_name = #{paramName},param_value = #
{paramValue}
```

就是把一张表的数据拆成两张表

2. 垂直拆分：

将表中的某些字段，单独拆分出来，并通过某个字段和原表建立联系

###### 分库分表常用思路

**1. hash 取模方案**
hash 的方案就是对指定的路由 key（通常为主键 id）对分表总数进行取模。
优点：某个表不会出现热点问题（某个表被频繁访问，其他表访问较少的情况）
缺点：对于以后要扩容会比较麻烦，增加表的时候要对所有数据重新 hash

**2. range 范围方案**
简单来说就是 1-1000 万的数据放在 1 数据库并以此类推。


**优点：** 有利于将来扩容，有新数据就直接加入就好了
**缺点：** 缺点也很明显，有可能出现热点问题。比如刚加进来的数据经常使用之类的问题

###### 两者结合的方案

那能不能考虑把两个的有点结合过来，又把去掉两者的缺点？ **即不容进行数据迁移，又不存在热点问
题？其实还有一个现实需求，能否根据服务器的性能以及存储高低，适当均匀调整存储呢？**

我们定义一个 Group 的概念，一个 Group 包含了一些分库和分表，如下图：

上图有几个关键点：
一个 Group 01 可以存放 4000 w 个数据，Group 01 有三个 DB，共 10 table。那么可以通过 id 范围确定在哪
个 group，然后 id%10 哪个 db。再根据范围确定哪个 table。简单来说就是 **整体有序，局部用 hash 打
乱** ，当扩容的时候只要加 group 就好了。


上图中 id 一千万以内的都被我们设计的流程均匀的分在了三个的 table 0 中。然后就是为什么对表的总数
取模？因为每台服务器的性能有差异，对表取模，那表多的服务器就更能被使用到。

图中我们对 10 进行取模，如果值为【 0 ， 1 ， 2 ， 3 】就路由到 DB_0，【 4 ， 5 ， 6 】路由到 DB_1，【 7 ，
8 ， 9 】路由到 DB_2。现在小伙伴们有没有理解，这样的设计就可以把多一点的数据放到 DB_0 中，其他
2 个 DB 数据量就可以少一点。DB_0 承担了 4/10 的数据量，DB_1 承担了 3/10 的数据量，DB_2 也承担了
3/10 的数据量。整个 Group 01 承担了【 0 ， 4000 万】的数据量


其实上面设计思路理解了，扩容就已经出来了；那就是扩容的时候再设计一个 group 02 组，定义好此
group 的数据范围就 ok 了。

###### 分库分表实操：

请参见《尼恩 java 架构班，10 Wqps 推送中台视频》

#### 聊聊：为什么要分库分表？

**「分表」**

比如单表都几千万数据了，确定能扛住么？

绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。

一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。

分表是啥意思？

就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。


比如按照用户 id 来分表，将一个用户的数据就放在一个表中。

然后操作的时候对一个用户就操作那个表就好了。

这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。

**「分库」**

分库是啥意思？

就是一个库一般我们经验而言，最多支撑到并发 2000 ，一定要扩容了，而且一个健康的单库并发值你
最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访
问一个库好了。

#### 聊聊：说说分库与分表的设计？

**「分库分表方案:」**

```
水平分库 ：以字段为依据，按照一定策略（hash、range等），将一个库中的数据拆分到多个库
中。
水平分表 ：以字段为依据，按照一定策略（hash、range等），将一个表中的数据拆分到多个表
中。
垂直分库 ：以表为依据，按照业务归属不同，将不同的表拆分到不同的库中。
垂直分表 ：以字段为依据，按照字段的活跃性，将表中字段拆到不同的表（主表和扩展表）中。
```
#### 聊聊：分库分表数据分片规则？

我们在考虑去水平切分表，将一张表水平切分成多张表，这就涉及到数据分片的规则，比较常见的有：
**Hash 取模分表** 、 **数值 Range 分表** 、 **一致性 Hash 算法分表** 。

1)、 **「Hash 取模分表」**

概念一般采用 Hash 取模的切分方式，例如：假设按 goods_id 分 4 张表。（goods_id%4 取整确定表）

**「优点」**

```
数据分片相对比较均匀，不容易出现热点和并发访问的瓶颈。
```
**「缺点」**

```
后期分片集群扩容时，需要迁移旧的数据很难。
容易面临跨分片查询的复杂问题。比如上例中，如果频繁用到的查询条件中不带goods_id时,将会
导致无法定位数据库,从而需要同时向 4 个库发起查询， 再在内存中合并数据，取最小集返回给应
用，分库反而成为拖累。
```
**「2)、数值 Range 分表」**

概念按照时间区间或 ID 区间来切分。例如：将 goods_id 为 11000 的记录分到第一个表， 10012000 的分
到第二个表，以此类推。

**「优点」**

```
单表大小可控
天然便于水平扩展，后期如果想对整个分片集群扩容时，只需要添加节点即可，无需对其他分片的
数据进行迁移
使用分片字段进行范围查找时，连续分片可快速定位分片进行快速查询，有效避免跨分片查询的问
题。
```
**「缺点」**


```
热点数据成为性能瓶颈。
例如按时间字段分片，有些分片存储最近时间段内的数据，可能会被频繁的读写，而有些分片存储
的历史数据，则很少被查询
```
**「3)、一致性 Hash 算法」**

一致性 Hash 算法能很好的解决因为 Hash 取模而产生的分片集群扩容时，需要迁移旧的数据的难题。至
于具体原理这里就不详细说。

#### 聊聊：分库分表带来的问题有哪些？

**「 1 、分布式事务问题」**

使用分布式事务中间件解决，具体是通过最终一致性还是强一致性分布式事务，看业务需求，这里就不
多说。

**「 2 、跨节点关联查询 Join 问题」**

切分之前，我们可以通过 Join 来完成。而切分之后，数据可能分布在不同的节点上，此时 Join 带来的问
题就比较麻烦了，考虑到性能，尽量避免使用 Join 查询。

解决这个问题的一些方法：

**全局表**

全局表，也可看做是 "数据字典表"，就是系统中所有模块都可能依赖的一些表，为了避免跨库 Join 查
询，可以将这类表在每个数据库中都保存一份。这些数据通常很少会进行修改，所以也不担心一致性
的问题。

**字段冗余**

利用空间换时间，为了性能而避免 join 查询。例：订单表保存 userId 时候, 也将 userName 冗余保存一
份，这样查询订单详情时就不需要再去查询"买家 user 表"了。

**数据组装**

在系统层面，分两次查询。第一次查询的结果集中找出关联数据 id，然后根据 id 发起第二次请求得到关
联数据。最后将获得到的数据进行字段拼装。

**「 3 、跨节点分页、排序、函数问题」**

跨节点多库进行查询时，会出现 Limit 分页、Order by 排序等问题。分页需要按照指定字段进行排序，
当排序字段就是分片字段时，通过分片规则就比较容易定位到指定的分片；

当排序字段非分片字段时，就变得比较复杂了。需要先在不同的分片节点中将数据进行排序并返回，然
后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户。

**「 4 、全局主键避重问题」**

如果都用主键自增肯定不合理，如果用 UUID 那么无法做到根据主键排序，所以我们可以考虑通过雪花
ID 来作为数据库的主键，

**「 5 、数据迁移问题」**

采用双写的方式，修改代码，所有涉及到分库分表的表的增、删、改的代码，都要对新库进行增删改。
同时，再有一个数据抽取服务，不断地从老库抽数据，往新库写，边写边按时间比较数据是不是最新
的。

#### 聊聊：如何以最效率从 MySQL 中随机查询一条记录？


在很多应用场景下，我们需要从数据库表中随机获取一条或者多条记录。

这是最原始最直观的语法，如下：

通过 EXPLAIN 来分析这个语句，会发现虽然 MySQL 通过建立一张临时表来排序，但由于 ORDER BY 和
LIMIT 本身的特性，在排序未完成之前，我们还是无法通过 LIMIT 来获取需要的记录。

也就是说，你的记录有多少条，就必须首先对这些数据进行排序。

如果最高的效率的从 MySQL 中随机查询一条记录呢？这里主要介绍对比两个方法。

###### 1 ：ORDER BY RAND ()

获取一条记录，最简单的方法，但经过测试认证，效率也是最低，查询时间是最长。

使用中 RAND () 函数调用可以在 0 和 1 之间产生一个随机数。

如果你想要查询多个记录可以将 LIMIT 1 改为 LIMIT n , n 代表你要返回的数据量。

官方手册讲解：在 MYSQL 的官方手册，里面针对 RAND () 的提示大概意思就是，在 ORDER BY 从句里面
不能使用 RAND () 函数，因为这样会导致数据列被多次扫描。

###### 2 ：用 JOIN

此方法经过测试速度非常快，在数万个记录中随机获取数据并且加上条件筛选也能在 0.02 s 左右的时间
返回，可以说速度是非常快，效率非常高的。

LIMIT 5 表示取出 5 条记录，可根据需要对 SQL 语句进行修改即可使用。

在数据量大的情况下，使用 JOIN，也避免了 ORDER BY 所造成的所有记录的排序过程，

#### 聊聊：什么是数据库事务？ 聊聊事务的特性？

事务 (Transaction) 是操作数据库中某个数据项的一个程序执行单元 (unit)。

事务应该具有 4 个属性：

原子性、一致性、隔离性、持久性。

这四个属性通常称为 ACID 特性。

```
SELECT * FROM foo ORDER BY RAND() LIMIT 1
```
```
SELECT * FROM table_name ORDER BY RAND() LIMIT 1；
```
```
SELECT * FROM t1
JOIN
(SELECT ROUND(RAND() * ((SELECT MAX(id) FROM t1 )-(SELECT MIN(id) FROM t1))+
(SELECT MIN(id) FROM t1)) AS id) AS t2
WHERE t1.id >= t2.id
ORDER BY t1.id LIMIT 5 ;
```

###### 事务的四个特征：

**1 、Atomic 原子性**

事务必须是一个原子的操作序列单元，事务中包含的各项操作在一次执行过程中，要么全部执行成功，
要么全部不执行，任何一项失败，整个事务回滚，只有全部都执行成功，整个事务才算成功。

**2 、Consistency 一致性**

事务的执行不能破坏数据库数据的完整性和一致性，事务在执行之前和之后，数据库都必须处于一致性
状态。

**3 、Isolation 隔离性**

在并发环境中，并发的事务是相互隔离的，一个事务的执行不能被其他事务干扰。

即不同的事务并发操纵相同的数据时，每个事务都有各自完整的数据空间，即一个事务内部的操作及使
用的数据对其他并发事务是隔离的，并发执行的各个事务之间不能相互干扰。

**4 、Durability 持久性**

持久性（durability）：持久性也称永久性（permanence），指一个事务一旦提交，它对数据库中对应
数据的状态变更就应该是永久性的。

即使发生系统崩溃或机器宕机，只要数据库能够重新启动，那么一定能够将其恢复到事务成功结束时的
状态。

```
比方说：一个人买东西的时候需要记录在账本上，即使老板忘记了那也有据可查。
```
#### 聊聊：MySQL 的事务 ACID 是如何实现的？

大多数场景下，我们的应用都只需要操作单一的数据库，这种情况下的事务称之为本地事务 (Local
Transaction)。本地事务的 ACID 特性是数据库直接提供支持。

了解过 MySQL 事务的同学，就会知道，为了达成本地事务，MySQL 做了很多的工作，比如回滚日志，
重做日志，MVCC，读写锁等。

**MySQL 数据库的事务实现原理**

以 MySQL 的 InnoDB （InnoDB 是 MySQL 的一个存储引擎）为例，介绍一下单一数据库的事务实现原
理。

InnoDB 是通过 **日志和锁** 来保证的事务的 ACID 特性，具体如下：

（ 1 ）通过数据库锁的机制，保障事务的隔离性；

（ 2 ）通过 Redo Log（重做日志）来，保障事务的持久性；

（ 3 ）通过 Undo Log （撤销日志）来，保障事务的原子性；

（ 4 ）通过 Undo Log （撤销日志）来，保障事务的一致性；

**Undo Log 如何保障事务的原子性呢？**


具体的方式为：在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为
Undo Log），然后进行数据的修改。如果出现了错误或者用户执行了 Rollback 语句，系统可以利用
Undo Log 中的备份将数据恢复到事务开始之前的状态。

**Redo Log 如何保障事务的持久性呢？**

具体的方式为：Redo Log 记录的是新数据的备份（和 Undo Log 相反）。在事务提交前，只要将 Redo
Log 持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是 Redo Log 已经持
久化。系统可以根据 Redo Log 的内容，将所有数据恢复到崩溃之前的状态。

#### 聊聊：什么是 XA 协议？

MySQL :: MySQL 8.0 Reference Manual :: 13.3.8 XA Transactions

```
AP（Application Program）：应用程序，定义事务边界（定义事务开始和结束）并访问事务边界
内的资源。
RM（Resource Manger）资源管理器: 管理共享资源并提供外部访问接口。供外部程序来访问数
据库等共享资源。此外，RM还具有事务的回滚能力。
```

```
TM（Transaction Manager）事务管理器：TM是分布式事务的协调者，TM与每个RM进行通信，
负责管理全局事务，分配事务唯一标识，监控事务的执行进度，并负责事务的提交、回滚、失败恢
复等。
```
```
应用程序AP向事务管理器TM发起事务请求
TM调用xa_open()建立同资源管理器的会话
TM调用xa_start()标记一个事务分支的开头
AP访问资源管理器RM并定义操作，比如插入记录操作
TM调用xa_end()标记事务分支的结束
TM调用xa_prepare()通知RM做好事务分支的提交准备工作。其实就是二阶段提交的提交请求阶
段。
TM调用xa_commit()通知RM提交事务分支，也就是二阶段提交的提交执行阶段。
TM调用xa_close管理与RM的会话。
这些接口一定要按顺序执行，比如xa_start接口一定要在xa_end之前。此外，这里千万要注
意的是事务管理器只是标记事务分支并不执行事务，事务操作最终是由应用程序通知资源管
理器完成的。另外，我们来总结下XA的接口
xa_start:负责开启或者恢复一个事务分支，并且管理XID到调用线程
xa_end:负责取消当前线程与事务分支的关系
xa_prepare:负责询问RM 是否准备好了提交事务分支 xa_commit:通知RM提交事务分支
xa_rollback:通知RM回滚事务分支
```
#### 聊聊：什么是 mysql xa 事务？

mysql 的 xa 事务分为两部分：

```
1. InnoDB内部本地普通事务操作协调数据写入与log写入两阶段提交
2. 外部分布式事务
```
XA 事务语法示例如下：

XA PREPARE 执行成功后，事务信息将被持久化。

即使会话终止甚至应用服务宕机，只要我们将【自定义事务 id】记录下来，后续仍然可以使用它对事务
进行 rollback 或者 commit。

#### 聊聊：xa 事务与普通事务区别是什么？

xa 事务可以跨库或跨服务器，属于分布式事务，

同时 xa 事务还支撑了 InnoDB 内部日志两阶段记录

```
5.7 SHOW VARIABLES LIKE '%innodb_support_xa%';
8.0 默认开启无法关闭
```
```
XA START '自定义事务id';
SQL语句...XA
END '自定义事务id';
XA PREPARE '自定义事务id';
XA COMMIT\ROLLBACK '自定义事务id';
```

普通事务只能在单库中执行

Mysql 的 XA 事务分为内部 XA 和外部 XA。

```
外部XA可以参与到外部的分布式事务中，需要应用层介入作为协调者；
内部XA事务用于同一实例下, 跨多引擎 事务，由Binlog作为协调者，
```
比如在一个存储引擎提交时，需要将提交信息写入二进制日志，这就是一个分布式内部 XA 事务，只不过
二进制日志的参与者是 MySQL 本身的多引擎。

MySQL 从 5.0.3 开始支持 XA 分布式事务，且只有 InnoDB 存储引擎支持。

MySQL ConnectorJ 从 5.0.0 版本之后开始直接提供对 XA 的支持。

#### 聊聊：什么是脏读、幻读、不可重复读？

在多个事务并发操作时，数据库中会出现下面三种问题： **脏读，幻读，不可重复读** 。

**脏读（Dirty Read）**

事务 A 读到了事务 B 还未提交的数据：

事务 A 读取的数据，事务 B 对该数据进行修改还未提交数据之前，事务 A 再次读取数据会读到事务 B 已经
修改后的数据，如果此时事务 B 进行回滚或再次修改该数据然后提交，事务 A 读到的数据就是脏数据，这
个情况被称为脏读（Dirty Read）。

**幻读（Phantom Read）**

事务 A 进行范围查询时，事务 B 中新增了满足该范围条件的记录，当事务 A 再次按该条件进行范围查询，
会查到在事务 B 中提交的新的满足条件的记录（ **幻行** Phantom Row）。


**不可重复读（Unrepeatable Read）**

事务 A 在读取某些数据后，再次读取该数据，发现读出的该数据已经在事务 B 中发生了变更或删除。


```
隔离级别
```
```
脏读（Dirty
Read）
```
```
不可重复读
（NonRepeatable Read）
```
```
幻读（Phantom
Read）
```
```
读未提交(Read
uncommitted）
```
```
可能 可能 可能
```
```
读已提交（Read
committed）
```
```
不可能 可能 可能
```
```
可重复读
（Repeatable
read）
```
```
不可 能 不可 能 可能
```
```
可串行化
（Serializable ） 不可能 不可能 不可能
```
```
幻读和不可重复度的区别：
```
```
幻读 ：在同一事务中，相同条件下，两次查询出来的 记录数 不一样；
不可重复读 ：在同一事务中，相同条件下，两次查询出来的 数据 不一样；
```
总之：

```
（ 1 ）脏读：A事务读到了B事务未提交的值，万一B事物回滚，则A产生脏读。
（ 2 ）不可重复读：同一事务A两个查询之间，被另外一个事务B修改（update）了数据的内容，
产生内容的不一致。
（ 3 ）幻读：同一事务A两个查询之间，被另外一个事务B插入或删除了（insert、delete）记录，
产生结果集的不一致。
```
#### 聊聊：如何实现 Transaction 的隔离性？Mysql 事务、

#### Oracle 事务的默认隔离级别？

通过数据库的 **锁机制** ，保障事务的隔离性；

并且，在数据库操作中，为了 **有效管理事务的隔离性** ，并且，有效保证高并发场景的数据正确性，提出
的 **事务隔离级别** 。

我们的 **数据库锁** 的类型和应用，和不同的隔离级别，是有紧密关系。

###### SQL 中的 4 个事务隔离级别：

###### （ 1 ）读未提交


**允许脏读。** 在 **读未提交** 隔离级别下，允许 **脏读** 的情况发生。

脏读指的是读到了其他事务未提交的数据，

未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。

读到了并一定最终存在的数据，这就是脏读。

脏读最大的问题就是可能会读到不存在的数据。

比如在上图中，事务 B 的更新数据被事务 A 读取，但是事务 B 回滚了，更新数据全部还原，也就是说事务
A 刚刚读到的数据并没有存在于数据库中。

###### （ 2 ）读已提交

在 **读已提交** 隔离级别下， **禁止了脏读** ，但是允许 **不可重复读** 的情况发生

换成简单的表达：

**就是不能读到中间值，但是允许在事务中间数据被修改，也允许在事务期间增加和删除数据**

**不可重复读** 指的是在一个事务 A 内多次读取一批数据，最开始读到的数据和事务结束前的任意时刻读到
的同一批数据出现不一致的情况。

```
如果一个事务正在处理某一数据，并对其进行了更新，
但同时尚未完成事务，或者说事务没有提交，
与此同时，允许另一个事务也能够访问该数据。
例如A将变量n从 0 累加到 10 才提交事务，此时B可能读到n变量从 0 到 10 之间的所有中间值。
```
```
只允许读到已经提交的数据。
即事务A在将n从 0 累加到 10 的过程中，B无法看到n的中间值，之中只能看到 10 。
```
```
事务A在将n从 0 累加到 10 的过程中，B无法看到n的中间值，之中只能看到 10 。
同时，
有事务C进行从 10 到 20 的累加，此时B在同一个事务内再次读时，读到的是 20 。
```

**事务 A 多次读取同一数据，但事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A
多次读取同一数据时，结果不一致。**

不可重复读一词，有点反人类，不好记忆。

不可重复读一词是从 **Nonrepeatable read** 翻译过来的，感觉英文的，好记忆一点。

###### （ 3 ）可重复读

在 **可重复读** 隔离级别下，禁止了： **脏读、不可重复读** 。但是，允许 **幻读** 。

换成简单的表达：

**就是不能读到中间值，不允许在事务中间数据被修改，但是，允许在事务期间增加和删除数据**

在可重复读中，该 sql 第一次读取到数据后，就将这些数据 **加锁（悲观锁, 如行锁）** ，其它事务无法修改
这些数据，就可以实现可重复读了。

但这种方法却无法锁住 insert 的数据，所以当事务 A 先前读取了数据，或者修改了全部数据，事务 B 还是
可以 insert 数据提交，

这时事务 A 就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过 **行锁** 来避免。

###### （ 4 ）串行化

如果不对事务进行并发控制，我们看看数据库并发操作是会有那些异常情形

```
（ 1 ）一类丢失更新：两个事物读同一数据，一个修改字段 1 ，一个修改字段 2 ，后提交的恢复了先
提交修改的字段。
（ 2 ）二类丢失更新：两个事物读同一数据，都修改同一字段，后提交的覆盖了先提交的修改。
（ 3 ）脏读：读到了未提交的值，万一该事物回滚，则产生脏读。
```
```
保证在事务处理过程中，多次读取同一个数据时，其值都和事务开始时刻时是一致的。
```
```
最严格的事务，要求所有事务被串行执行，不能并发执行。
```

```
（ 4 ）不可重复读：两个查询之间，被另外一个事务修改（update）了数据的内容，产生内容的不
一致。
（ 5 ）幻读：两个查询之间，被另外一个事务插入或删除了（insert、delete）记录，产生结果集
的不一致。
```
###### 数据一致性和高性能，是天生的矛盾

无论是尼恩的葵花宝典视频、还是 rocketmq 视频、还是推送中台实操视频，无一例外，都揭示了一个
硬道理： **数据一致性和高性能，是天生的矛盾**

在事务领域，也是如此：

**场景一：性能最好的，一致性最差**

读未提交的级别，它是性能最好，也可以说它是最野蛮的方式，因为它压根儿就不加锁，所以根本谈不
上什么隔离效果，可以理解为没有隔离。

**场景二：一致性最好的，性能最差**

串行化就一致性性最强。串行化相当于处理一个人请求的时候，别的人都等着。

读的时候加共享锁，也就是其他事务可以并发读，但是不能写。写的时候加排它锁，其他事务不能并发
写也不能并发读。

需要 Serializable 隔离级别， **读用读锁，写用写锁** ，读锁和写锁互斥，这都是非常悲观的悲观锁策略，
这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。

**然后，就是场景三： 在高性能和数据一致性中间，寻找平衡。**

数据库的事务隔离越严格，并发副作用越小，但付出的代价越大；

**读已提交和可重复读** , 都是在寻找平衡

这两种隔离级别是比较复杂的，既要允许一定的并发，又想要解决数据一致性问题。

oracle 默认事务隔离级别为 **读已提交 (RC)** , 说明可以 **不可重复读** ，（不可重复读这个词用的反人类，）

换成简单的表达：

**就是不能读到中间值，但是允许在事务中间数据被修改，也允许在事务期间增加和删除数据**

**MySQL 默认事务隔离级别为可重复读 (RR),**

换成简单的表达：

**就是不能读到中间值，不允许在事务中间数据被修改，但是，允许在事务期间增加和删除数据**

ORACLE、MySQL、PostgreSQL 等成熟的数据库中的读已提交、可重复读隔离级别，并没有使用
Serializable 隔离级别中的悲观锁，都是使用了以乐观锁为理论基础的 MVCC（多版本并发控制）来实
现。

但是，别着急，MySQL 用自己的方式，解决了事务期间增加和删除数据的问题，也就是解决了幻读问
题。

#### 聊聊：Mysql 如何的控制事务的隔离级别？


**MySQL 事务隔离级别** ：https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-lev
els. html

通过修改 MySQL 系统参数来控制事务的隔离级别，

在 MySQL 8 中该参数为 **transaction_isolation** ，在 MySQL 5 中该参数为 **tx_isolation** :

**事务的四个隔离级别：**

```
未提交读（READ UNCOMMITTED） ：所有事务都可以看到其他事务未提交的修改。一般很少使
用；
读已提交（READ COMMITTED） ：Oracle默认隔离级别，事务之间只能看到彼此已提交的变更
修改；
可重复读（REPEATABLE READ） ：MySQL默认隔离级别，同一事务中的多次查询会看到相同的
数据行；可以解决不可重复读，但可能出现幻读；
可串行化（SERIALIZABLE） ：最高的隔离级别，事务串行的执行，前一个事务执行完，后面的事
务会执行。读取每条数据都会加锁，会导致大量的超时和锁争用问题；
```
提交读（READ COMMITTED）隔离级别：

采用行锁（Record Lock）, 不会出现脏读，但是会出现可重复读、"幻读"问题.

#### 聊聊：Mysql 如何实现 RR 级隔离时，不会幻读？

同下题

#### 聊聊：如何保证 REPEATABLE READ 级别不产生幻读？

什么是幻读？ 就是 A 事务两个查询之间，被另外一个事务 B 插入或删除了（insert、delete）记录，产
生结果集的不一致。

```
MySQL8：
-- 查看系统隔离级别：
SELECT @@global.transaction_isolation;
```
```
-- 查看当前会话隔离级别
SELECT @@transaction_isolation;
```
```
-- 设置当前会话事务隔离级别
SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;
```
```
-- 设置全局事务隔离级别
SET GLOBAL TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
```

MySQL Innodb 存储引擎中，通过 **间隙锁防止幻读** 的产生

MySQL 默认的隔离级别是 Repeatable Read，同时知道这种隔离级别是有幻读产生的，但是真的会出现
幻读吗？

###### 那么，如何保证 REPEATABLE READ 级别不产生幻读呢？两种方

###### 案。

在 RR 的隔离级别下，Innodb 使用 MVCC 和 next-key locks (行锁和间隙锁的组合) 解决幻读，

```
MVCC解决的是 普通读（快照读） 的幻读，
next-key locks解决的是 当前读 情况下的幻读。
```
###### 普通读（快照读）和当前读

在 MVCC 并发控制中，读操作可以分成两类：快照读 (snapshot read) 与当前读 (current read)。

```
快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。
当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再
并发修改这条记录。
```
在一个支持 MVCC 并发控制的系统中，哪些读操作是快照读？哪些操作又是当前读呢？

以 MySQL InnoDB 为例：

```
快照读：简单的select操作，属于快照读，不加锁。(当然，也有例外，下面会分析)
select * from table where ?;
当前读：
特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。
```
###### 间隙锁：

间隙锁在 InnoDB 的唯一作用就是防止其它事务的插入操作，以此来达到防止幻读的发生，

所以间隙锁不分什么共享锁与排它锁。

默认情况下，InnoDB 工作在 Repeatable Read 隔离级别下，并且以 Next-Key Lock 的方式对数据行进行
加锁，这样可以有效防止幻读的发生。

Next-Key Lock 是行锁与间隙锁的组合：

```
当对数据进行条件，范围检索时，对其范围内也许并存在的值进行加锁！
当查询的索引含有唯一属性（唯一索引，主键索引）时，Innodb存储引擎会对next-key lock进行
优化，将其降为record lock,即仅锁住索引本身，而不是范围！
若是普通辅助索引，则会使用传统的next-key lock进行范围锁定！
```
要禁止间隙锁的话，可以把隔离级别降为 Read Committed，或者开启参数
innodb_locks_unsafe_for_binlog。

```
select * from table where? lock in share mode;
select * from table where? for update;
insert into table values (...);
update table set? where ?;
delete from table where ?;
```

###### Innodb 自动使用间隙锁的条件：

Innodb 自动使用间隙锁的条件：

（ 1 ）必须在 Repeatable Read 级别下

（ 2 ）检索条件必须有索引（没有索引的话，mysql 会全表扫描，那样会锁定整张表所有的记录，包括
不存在的记录，此时其他事务不能修改不能删除不能添加）

#### 聊聊：什么是间隙锁？

MySQL InnoDB 支持三种 **行锁定** 方式：

```
记录锁（Record Lock）: 锁直接加在索引记录上面，锁住的是key。
间隙锁（Gap Lock）: 锁定索引记录间隙，确保索引记录的间隙不变。
```
间隙锁是针对事务隔离级别为可重复读或以上级别而已的。

```
临键锁 Next-Key Lock ：行锁和间隙锁组合起来就叫Next-Key Lock。
```
###### 记录锁（Record Locks）

记录锁其实很好理解，对表中的记录加锁，叫做记录锁，简称行锁。比如

它会在 id=1 的记录上加上记录锁，以阻止其他事务插入，更新，删除 id=1 这一行。

需要注意的是：

```
id 列必须为唯一索引列或主键列，否则上述语句加的锁就会变成临键锁(有关临键锁下面会讲)。
同时查询语句必须为精准匹配（=），不能为 >、<、like等，否则也会退化成临键锁。
```
其他实现

在通过 **主键索引** 与 **唯一索引** 对数据行进行 UPDATE 操作时，也会对该行数据加记录锁：

**记录锁是锁住记录，锁住索引记录，而不是真正的数据记录**.

如果要锁的列没有索引，进行全表记录加锁

记录锁也是排它 (X) 锁, 所以会阻塞其他事务对其 **插入、更新、删除** 。

###### 间隙锁（Gap Locks）

间隙锁是 **Innodb 在 RR (可重复读) 隔离级别** 下为了解决 **幻读问题** 时引入的锁机制。

```
SELECT * FROM `test` WHERE `id`= 1 FOR UPDATE;
```
```
-- id 列为主键列或唯一索引列
UPDATE SET age = 50 WHERE id = 1 ;
```

**间隙锁是 innodb 中行锁的一种** 。

请务必牢记： **使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据** 。

举例来说，假如 emp 表中只有 101 条记录，其 empid 的值分别是 1,2,..., 100,101，

下面的 SQL：

当我们用条件检索数据，并请求共享或排他锁时，InnoDB 不仅会对符合条件的 empid 值为 101 的记录加
锁，也会对 empid 大于 101 （这些记录并不存在）的“间隙”加锁。

这个时候如果你插入 empid 等于 102 的数据的，如果那边事物还没有提交，那你就会处于等待状态，无
法插入数据。

###### 临键锁（Next-Key Locks）

**Next-key 锁是记录锁和间隙锁的组合，它指的是加在某条记录以及这条记录前面间隙上的锁** 。

也可以理解为一种特殊的 **间隙锁** 。

通过 **临建锁** 可以解决幻读的问题。

每个数据行上的 **非唯一索引列** 上都会存在一把 **临键锁** ，当某个事务持有该数据行的 **临键锁** 时，会锁住一
段 **左开右闭区间** 的数据。

需要强调的一点是，InnoDB 中 **行级锁** 是基于索引实现的。

比如一个 age 索引包含值， 10 ， 11 ， 13 和 20 。那么，间隙锁的范围如下

Next-Key Locks 的范围是 **左开右闭** 。默认右边的记录也会加锁。

在事务 A 中执行如下命令：

不管执行了上述 SQL 中的哪一句，之后如果在事务 B 中执行以下命令，则该命令会被阻塞：

```
SELECT * FROM emp WHERE empid > 100 FOR UPDATE
```
```
临键锁只与 非唯一索引列 有关，在 唯一索引列（包括主键列）上不存在临键锁。
```
```
(negative infinity, 10 ]
(10, 11 ]
(11, 13 ]
(13, 20 ]
(20, positive infinity)
```
```
-- 根据非唯一索引列 UPDATE 某条记录
UPDATE table SET name = Vladimir WHERE age = 11 ;
-- 或根据非唯一索引列 锁住某条记录
SELECT * FROM table WHERE age = 11 FOR UPDATE;
```
```
INSERT INTO table VALUES( 100 , 12 , 'tianqi');
```

很明显，事务 A 在对 age 为 11 的列进行 UPDATE 操作的同时，也获取了 (11, 13] 这个区间内的临键
锁。

行级锁实际上是索引记录锁。

索引记录上的临键锁（Next-Key Locks）也会影响该索引记录之前的“间隔”。

也就是说，临键锁（Next-Key Locks）是索引标准记录锁（Record Locks）加上位于索引记录右边的间
隙上的间隙锁（Gap Locks）。

**InnoDB 的默认加锁方式是 next-key 锁。**

默认情况下，InnoDB 工作在可重复读 (Repeatable Read) 隔离级别下，并且会以 Next-Key Lock 的方式
对数据行进行加锁，这样可以有效防止幻读的发生。

Next-Key Lock 是行锁和间隙锁的组合，当 InnoDB 扫描索引记录的时候，会首先对索引记录加上行锁
（Record Lock），再对索引记录两边的间隙加上间隙锁（Gap Lock）。

加上间隙锁之后，其他事务就不能在这个间隙修改或者插入记录。

在可重复读 (Repeatable Read) 隔离级别下， Gap Lock 在 InnoDB 的唯一作用就是防止其他事务的插入
操作，以此防止幻读的发生。

Mysql 的间隙锁工作在 Repeatable Read 隔离级别下面，可以防止幻读,

Innodb 自动使用间隙锁的条件：
（ 1 ）必须在 Repeatable Read 级别下
（ 2 ）检索条件必须有索引（没有索引的话，mysql 会全表扫描，那样会锁定整张表所有的记录，包括
不存在的记录，此时其他事务不能修改、不能删除、不能添加）

#### 聊聊：什么是 MVCC 多版本并发控制协议？

###### MVCC 的设计思想：copyonwrite 思想

在并发读写数据库时，读操作可能会不一致的数据（脏读）。

为了避免这种情况，需要实现数据库的并发访问控制，最简单的方式就是加锁访问。

由于，加锁访问，不光阻塞了写，也阻塞了读，会将读写操作串行化，当然，不会出现不一致的状态。

但是，读操作会被写操作阻塞，大幅降低读性能。

###### copyonwrite 思想

在 java concurrent 包中，有 copyonwrite 系列的类，专门用于 **读多写少** 场景。

copyonwrite 的思想是：

```
在进行写操作时，将数据copy一份，不会影响原有数据，然后进行修改，修改完成后原子替换掉
旧的数据，而读操作只会读取原有数据。
```

copyonwrite 的优势是：

```
通过这种方式实现写操作不会阻塞读操作，从而优化读效率。
```
copyonwrite 的缺点是：

```
写操作之间是要互斥的，并且每次写操作都会有一次copy，所以，copyonwrite，更多只适合 读操
作远多于写操作 场景。
```
###### MVCC 的原理

MVCC 的原理与 copyonwrite 类似，全称是 Multi-Version Concurrent Control，即多版本并发控制。

在 MVCC 协议下，每个读操作, 会看到一个一致性的 snapshot 快照，并且可以实现非阻塞的读。

读这个 snapshot 快照，不用加锁。

除了 snapshot 快照版本之外， MVCC 允许数据具有多个版本，版本编号可以是时间戳，或者是全局递
增的事务 ID，在同一个时间点，不同的事务看到的数据是不同的。

下面是一个简单的例子，以事务 ID 为版本号：

------------------------------------------------------------------------------------------> 时间轴

|-------Read (T 1)-----|

|-----------Update (T 2)-----------|

如上图，假设有两个并发操作 Read (T 1) 和 Update (T 2):

```
T1和T2是事务ID，T1小于T2， (T1) 操作中包含的数据a = 1，
```
Read (T 1) 和 Update (T 2) 的操作如下：

Read：read a =1 (T 1)

Update：a = 2 (T 2)

Read：read （读操作）的版本 T 1 表示要读取数据的版本，

在时间轴上，Read 早于 Update，由于 Update 在 Read 开始之后，所以 Update 提交的数据，所以对于
Read 是不可见的。

所以，Read 只会读取 T 1 版本的数据，即 a = 1。读不到 a =2

Update 写操作才会更新数据的版本，读操作不会。

**关于 MVCC 数据的一致性**

而对于读操作而言，只能读到在自己之前开始的，所有已经提交的写操作，正在执行中的写操作对其是
不可见的。

由于在 update 操作提交之前，不能影响已有数据的一致性，所以不会改变旧的数据，

另外，update 操作会被拆分成 insert + delete。

update 操作需要标记删除旧的数据，insert 新的数据。只有 update 提交之后，才会影响后续的读操作。

**mysql 的 innodb 引擎是如何实现 MVCC 的**

什么是 MVCC 多版本并发控制（Multi-Version Concurrent Control）呢 ？


```
id name create version delete version
```
```
1 test 1
```
```
id name create version delete version
```
```
1 test 1 2
```
```
1 new_value 2
```
```
id name create version delete version
```
```
1 new_value 2 3
```
其实就是 innodb 会为每一行添加两个字段，注意是在每一行记录的后面增加两个隐藏列：

```
创建版本号
删除版本号，
```
分别表示该行 **创建的版本** 和 **删除的版本** ，填入的是事务版本号（事务的编号)，

```
事务版本号随着事务的创建不断递增。
```
**下面是一个简单的例子：**

1 、在插入操作时 ： 记录的创建版本号就是事务版本号（事务 id ）。

插入一条记录, 事务版本号（事务 id ）假设是 1 ，那么记录如下：

也就是说，创建版本号就是事务版本号（事务 id ）。

2 、在更新操作的时候，采用的是先标记旧的那行记录为已删除，就是说，删除版本号是事务版本号，
然后插入一行新的记录的方式。

比如，针对上面那行记录，事务 Id 为 2 要把 name 字段更新

update table set name= 'new_value' where id=1;

3 、删除操作的时候，就把事务版本号作为删除版本号。比如

delete from table where id=1;

4 、查询操作：

select from table where id=1;

从上面的描述可以看到，在查询时要符合以下两个条件的记录才能被事务查询出来：


```
id name create version delete version
```
```
1 test 1 2
```
```
1 new_value 2 3
```
1 ）该行的创建版本号小于等于当前版本号（也就是，行的创建版本本号小于或等于事务版本号），

这样可以确保事务读取的行：

```
只么是在事务开始前已经存在的，
要么是事务自身插入或者修改过的。
```
2 ）该行的删除版本号大于当前版本或者为空。

这可以确保事务读取到的行，在事务开始之前未被删除。

通过前面的 3 个事务，目前的版本记录，具体如下：

假设执行这个查询的 select from table where id=1; 事务编号为 4

那么该行的创建版本号 1/2 小于等于当前版本号 4 的记录，有两条。

该行的删除版本号 2/3 大于当前版本或者为空，的， 0 条。

所以， select from table where id=1; 查不到数据。

###### 在 repeated read 的隔离级别下，MVCC 具体的实现：

**select：**

满足以下两个条件 innodb 会返回该行数据：

```
该行的创建版本号小于等于当前版本号， 保证改行在当前版本之前已经被插入 。
该行的删除版本号大于当前版本或者为空。 删除版本号大于当前版本意味着当前版本并未执行该行
的删除操作，是之后才删除的 。
```
**insert：**

```
将新插入的行的创建版本号设置为当前系统的版本号。 说明该行在当前版本被插入 。
```
**delete：**

```
将要删除的行的删除版本号设置为当前系统的版本号。 说明该行在当前版本被删除 。
```
**update：**

```
不执行原地update，而是转换成insert + delete。 将旧行的删除版本号设置为当前版本号，并将新
行insert同时设置创建版本号为当前版本号 。
将旧行的删除版本号设置为当前版本号，并将新行insert同时设置创建版本号为当前版本号。
```
**其他规则**

```
执行insert、delete和update都要将系统版本号递增。 执行select的版本号为系统版本号。
由于旧数据并不真正的删除，所以必须对这些数据进行清理，innodb会开启一个后台线程执行清
理工作，具体的规则是将删除版本号小于当前系统版本的行删除，这个过程叫做purge。
```

###### 总之：通过 MVCC 就保证了各个事务互不影响。

数据库并发场景有三种，分别为：

1 、读读：不存在任何问题，也不需要并发控制

2 、读写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读、幻读、不可重复读

3 、写写：有线程安全问题，可能存在更新丢失问题

MVCC 是一种用来解决读写冲突的 **无锁并发控制** ，目标是高并发

MVCC 为事务分配单项增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读
该事务开始前的数据库的快照，所以 MVCC 可以为数据库解决以下问题：

1 、在并发读写数据库时，可以做到在 **读操作时不用阻塞写操作** ，写操作也不用阻塞读操作，提高了数
据库并发读写的性能
2 、解决脏读、幻读、不可重复读等事务隔离问题，但是不能解决更新丢失问题

从这里也可以体会到一种提高系统性能的思路，就是： **通过版本号+副本的方式，来减少锁的争用。**

MVCC 的实现，通过保存数据在某个时间点的快照来实现的。

这意味着一个事务无论运行多长时间，在同一个事务里能够看到数据一致的视图。

根据事务开始的时间不同，同时也意味着在同一个时刻，不同事务看到的相同表里的数据可能是不同
的。

MVCC 的基本特征：

```
每行数据都存在一个版本，每次数据更新时，都更新该版本。
修改时Copy出当前版本随意修改，各个事务之间无干扰。
保存时比较版本号，如果成功（commit），则覆盖原记录；失败则放弃copy（rollback）
```
MVCC 并不是 MySql 独有的，Oracle，PostgreSQL 等都在使用。

上面是 MVCC 的基础流程，在不同的数据库中，实现的方案都是不同的。

**在 Mysql 中，是使用 undo log 版本链+ ReadView 快照读试图，实现 MVCC 机制的。**

#### 聊聊：MVCC 工作的事务隔离级别是啥？

**MVCC 只是工作在两种事务隔离级别底下：**

```
(2) Read Committed
(3) Repeatable Read;
```
其他两种：

(1) READ UNCOMMITTED，可行读取到最新的没有提交的中间数据，而那些中间数据，不符合当前数
据的事务版本要求

(3) Serializable 则会对所有的行加锁。

(1) 、(3) 这两种事务隔离级别，都不需要 MVCC。


#### 聊聊：Mysql 中 Undo Log 机制与 MVCC 的关系？

###### Undo Log 机制

Undo log 的字面意思是撤销操作的日志，指的是使 mysql 中的数据回到某个状态。

在事务开启中（Undo Log 是 InnerDB 独有的），mysql 会将待修改的记录保存到 Undo Log 中。

如果数据库崩溃或者事务需要回滚时，mysql 可以通过利用 Undo log 日志，将数据库中的事务回滚到之
前的状态。

mysql 新增、修改、删除数据时，在事务开启前，就会将信息写入 Undo Log 中，事务提交时，并不会立
刻删除 Undo Log，InnoDB 存储引擎会将事务对应的 Undo Log 放入待删除列表中，之后会通过后台的
purge thread 对待删除的列表进行删除操作处理。

注意的是 Undo log 是一种逻辑日志，记录的是一种逻辑过程。比如：

```
mysql执行delete操作，Undo log就会记录一个insert操作；
mysql执行一个insert操作，Undo Log就会记录一个delete操作；
mysql执行update操作，Undo Log记录一个相反的update操作。
```
Undo Log 以段的方式来管理记录日志信息，在 InnoDB 存储引擎的数据文件中，包含了一种 rollback
segment 的回滚段，内部包含了 1024 个 undo log segment 。

Undo Log 实现了事务的 **原子性** 和 **多版本并发控制（MVCC）** :

```
Undo Log实现了事务的 原子性
Mysql出现了错误、或者手动执行了事务的回滚，Undo Log会将数据库中的数据恢复到之前的状
态。
```
```
Undo Log实现了事务的 多版本并发控制（MVCC）
Mysql 基于 Undo log实现多版本并发控制
事务A未提交之前，undo log保存了未提交之前的版本，事务B读取的是之前的版本信息和旧数据
的副本，
这个旧数据的副本，是从 undo log 的Buffer中获取的。
```

###### undo log 的版本链模式

我们每条数据其实都有两个隐藏字段，一个是 trx_id，一个是 roll_pointer，

```
trx_id就是最近一次更新这条数据的事务id，
roll_pointer就是指向更新了这个事务之前生成的undo log。
```
多个事务串行执行的时候，每个人修改了一行数据，都会更新隐藏字段 txr_id 和 roll_pointer，

同时之前多个数据快照对应的 undo log，会通过 roll_pinter 指针串联起来，形成一个重要的 undo log 版
本链！

最终 undo log 版本链，链首存储的是最新的旧记录，链尾存储的是最旧的旧记录。

undo log 版本链不会无限膨胀，会存在一个后台清除线程，purge 线程，发现当前记录不需要回滚且不
需要参与 MVCC 的时候就会吧数据清理掉。

MySQL 采用从新到旧（Newest To Oldest）的版本链。

如下图， V 1 被一个事务更新为 V 2，V 2 被另一个事务更新为 V 3，Δ1 存储 V 1 到 V 2 的更新，Δ2 存储 V 2 到 V 3
的更新。

此时，如果一个事条定位到 B+Tree 叶子节点的记录 V 3，则通过 V 3+Δ2 可以还原出 V 2，通过 V 3+Δ2+Δ1 可
以还原出 V 1。


B+Tree 叶结点上，始终存储的是最新的数据（可能是还未提交的数据）。

而旧版本数据，通过 UNDO 记录（做 DELTA）存储在回滚段（Rollback Segment）里。

每一条记录都会维护一个 ROW HEADER 元信息，存储有创建这条记录的事务 ID，一个指向 UNDO 记录
的指针。

通过最新记录和 UNDO 信息，可以还原出旧版本的记录。

###### ReadView 机制

这个 ReadView 呢，简单来说，每执行一个事务的时候，就给这个事务生成一个 ReadView，

ReadView 是某一个时间点，事务执行状态的一个快照，可以用来判断事务的可见性。ReadView 的基本
结构如下：

**creator_trx_id** 创建这个 ReadView 的事务 ID

**low_limit_id** 所有事务 ID 大于或等于 low_limit_id 对当前事务都不可见

**up_limit_id** 所有事务 ID 严格小于 up_limit_id 的事务对当前事务可见

**ids** 未提交的事务 ID 列表

ReadView 比较关键的东西有 4 个

```
一个是 trx_ids ，这个就是说此时有哪些事务在MySQL里执行还没提交的, 保存这些事务id 的集合
一个是 up_limit_id ，就是 trx_ids 里最小的值；
一个是 low_limit_id ，这是说mysql下一个要生成的事务id，就是最大事务id；
一个是 creator_trx_id ，就是你这个事务的id
```
简单的说，这些记录，类似于乐观锁中的版本号的作用。

有了这个 ReadView，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见。

```
如果被访问版本的trx_id属性值与ReadView中的 creator_trx_id 值相同，
意味着当前事务在访问 它自己修改过的记录，所以该版本可以被当前事务访问。
```
```
ReadView { creator_trx_id low_limit_id up_limit_id ids ...}
```

```
如果被访问版本的trx_id属性值小于ReadView中的 up_limit_id 值，
表明生成该版本的事务，在当前 事务生成ReadView前已经提交，所以该版本可以被当前事务访
问。
如果被访问版本的trx_id属性值，大于或等于ReadView中的 low_limit_id 值，表明生成该版本的
事 务在当前事务生成ReadView后才开启，所以该版本不可以被当前事务访问。
```
如果被访问版本的 trx_id 属性值在 ReadView 的 up_limit_id 和 low_limit_id 之间，那就需要判断一下
trx_id 属性值是不是在 trx_ids 列表中。

```
如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问。
如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问。
```
#### 聊聊：Mysql 中 MVCC 是如何实现的？

在 Mysql 中，是使用 undo log 版本链+ ReadView 快照读试图，实现 MVCC 机制的。

详细的答案，参考上题。

#### 聊聊：MVCC 机制如何解决幻读？

在 RR 的隔离级别下，Innodb 使用 MVCC 和 next-key locks (行锁和间隙锁的组合) 解决幻读，

```
MVCC解决的是 普通读（快照读） 的幻读，
next-key locks解决的是 当前读 情况下的幻读。
```
所以，来看看 MVCC 机制如何 **普通读（快照读）** 的幻读？

了解了这些概念之后，我们来看下当查询一条记录的时候，系统如何通过 MVCC 找到它：

```
1. 首先获取事务自己的版本号，也就是事务 ID；
2. 获取 ReadView 读试图；
3. 查询得到的数据，然后与 ReadView 中的事务版本号进行比较；
4. 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照；
5. 最后返回符合规则的数据。
```
不同的隔离级别，或者的 Read View 数量不同：

```
在隔离级别为RC 读已提交（Read Committed）时，一个事务中的每一次 SELECT 查询都会重新
获取一次Read View。如表所示：
```

注意，此时同样的查询语句都会重新获取一次 Read View，这时如果 Read View 不同，就可能产生不
可重复读或者幻读的情况。

```
当隔离级别为 RR 可重复读的时候，就避免了不可重复读，这是因为一个事务只在第一次 SELECT
的时候会 获取一次 Read View，而后面所有的 SELECT 都会复用这个 Read View，如下表所示：
```
###### 举例说明 ：READ COMMITTED 隔离级别下

READ COMMITTED ：每次读取数据前都生成一个 ReadView。

现在有两个事务 id 分别为 10 、 20 的事务在执行：

**此刻，表 student 中 id 为 1 的记录得到的版本链表如下所示：**

**说明:**

```
# Transaction 10 BEGIN;
UPDATE student SET name="李四" WHERE id= 1 ;
UPDATE student SET name="王五" WHERE id= 1 ;
```
```
# Transaction 20 BEGIN;
# 更新了一些别的表的记录 ...
```

事务执行过程中，只有在第一次真正修改记录时（比如使用 INSERT、DELETE、UPDATE 语句)，才会被
分配一个单独的事务 id，这个事务 id 是递增的。所

以我们才在事务 2 中更新一些别的表的记录，目的是让它分配事务 id。

假设现在有一个使用 READ COMMITTED 隔离级别的事务开始执行：

**之后，我们把事务 id 为 10 的事务提交一下：**

**然后再到事务 id 为 20 的事务中更新一下表 student 中 id 为 1 的记录：**

**此刻，表 student 中 id 为 1 的记录的版本链就长这样：**

**然后再到刚才使用 READ COMMITTED 隔离级别的事务中，继续查找这个 id 为 1 的记录，如下：**

```
# 使用 READ COMMITTED 隔离级别的事务
BEGIN;
# SELECT 1：Transaction 10 、 20 未提交
```
```
SELECT * FROM student WHERE id = 1 ; # 得到的列 name 的值为'张三'
```
```
# Transaction 10
BEGIN; UPDATE student SET name="李四" WHERE id= 1 ;
UPDATE student SET name="王五" WHERE id= 1 ;
COMMIT;
```
```
# Transaction 20
BEGIN; # 更新了一些别的表的记录 ...
UPDATE student SET name="钱七" WHERE id= 1 ;
UPDATE student SET name="宋八" WHERE id= 1 ;
```

**这个 SELECT 2 的执行过程如下:**

**步骤 1 ∶** 在执行 SELECT 语句时会又会单独生成一个 ReadView，该 ReadView 的 trx_ids 列表的内容就是
[20]，up_limit_id 为 20 ，low_limit_id 为 21, creator_trx_id 为 0 。
**步骤 2:** 从版本链中挑选可见的记录，从图中看出，最新版本的列 name 的内容是‘宋八’，该版本的 tr×_id
值为 20 ，在 trx_ids 列表内，所以不符合可见性要求，根据 roll. pointer 跳到下一个版本。
**步骤 3 ∶** 下一个版本的列 name 的内容是’钱七’，该版本的 trx_id 值为 20 ，也在 trx_ids 列表内，所以也不符
合要求，继续跳到下一个版本。
**步骤 4 ∶** 下一个版本的列 name 的内容是’王五’，该版本的 trx_id 值为 10 ，小于 ReadView 中的 up_limit. id
值 20 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列 name 为’王五’的记录。

以此类推，如果之后事务 id 为 20 的记录也提交了，再次在使用 READ CONMMITTED 隔离级别的事务中
查询表 student 中 id 值为 1 的记录时，得到的结果就是‘宋八’了，具体流程我们就不分析了。

###### REPEATABLE READ 隔离级别下

使用 REPEATABLE READ 隔离级别的事务来说，只会在第一次执行查询语句时生成一个 ReadView ，之
后的查询就不会重复生成了。

**比如，系统里有两个事务 id 分别为 10 、 20 的事务在执行：**

**此刻，表 student 中 id 为 1 的记录得到的版本链表如下所示：**

**假设现在有一个使用 REPEATABLE READ 隔离级别的事务开始执行：**

```
# 使用 READ COMMITTED 隔离级别的事务
BEGIN;
```
```
# SELECT 1：Transaction 10 、 20 均未提交
SELECT * FROM student WHERE id = 1 ;
```
```
# 得到的列 name 的值为'张三'
# SELECT 2：Transaction 10 提交，Transaction 20 未提交
```
```
SELECT * FROM student WHERE id = 1 ;
# 得到的列 name 的值为'王五'
```
```
# Transaction 10
BEGIN;
UPDATE student SET name="李四" WHERE id= 1 ;
UPDATE student SET name="王五" WHERE id= 1 ;
```
```
# Transaction 20 BEGIN; # 更新了一些别的表的记录 ...
```

**之后，我们把事务 id 为 10 的事务提交一下，就像这样：**

**然后再到事务 id 为 20 的事务中更新一下表 student 中 id 为 1 的记录：**

**此刻，表 student 中 id 为 1 的记录的版本链长这样：**

**然后再到刚才使用 REPEATABLE READ 隔离级别的事务中继续查找这个 id 为 1 的记录，如下：**

**这个 SELECT 2 的执行过程如下:**

```
# 使用 REPEATABLE READ 隔离级别的事务
BEGIN;
# SELECT 1：Transaction 10 、 20 未提交
SELECT * FROM student WHERE id = 1 ; # 得到的列 name 的值为'张三'
```
```
# Transaction 10
BEGIN;
UPDATE student SET name="李四" WHERE id= 1 ;
UPDATE student SET name="王五" WHERE id= 1 ;
```
```
COMMIT;
```
```
# Transaction 20
BEGIN;
```
```
# 更新了一些别的表的记录 ...
UPDATE student SET name="钱七" WHERE id= 1 ;
UPDATE student SET name="宋八" WHERE id= 1 ;
```
```
# 使用 REPEATABLE READ 隔离级别的事务
```
```
BEGIN;
# SELECT 1：Transaction 10 、 20 均未提交
SELECT * FROM student WHERE id = 1 ; # 得到的列 name 的值为'张三'
```
```
# SELECT 2：Transaction 10 提交，Transaction 20 未提交
SELECT * FROM student WHERE id = 1 ; # 得到的列 name 的值仍为'张三'
```

**步骤 1:** 因为当前事务的隔离级别为 REPEATABLE READ，而之前在执行 SELECT 1 时已经生成过 ReadView
了，所以此时直接复用之前的 ReadView，之前的 ReadView 的 trx_ids 列表的内容就是[10，20]，
up_limit_id 为 10, low_limit_id 为 21 , creator_trx_id 为 0 。
**步骤 2:** 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列 name 的内容是’宋八’trx_id 值
为 20 ，在 trx_ids 列表内，所以不符合可见性要求，根据 roll_pointer 跳到下一个版本。
**步骤 3:** 下一个版本的列 name 的内容是’钱七’，该版本的 trx_id 值为 20 ，也在 trx_ids 列表内合要求，继续
跳到下一个版本。
**步骤 4:** 下一个版本的列 name 的内容是’王五’，该版本的 trx_id 值为 10 ，而 trx_ids 列表中是包含值为 10 的
事务 id 的，所以该版本也不符合要求，同理下一个列 name 的内容是’李四’的版本也不符合要求。继续跳
到下个版本。
**步聚 5 ∶** 下一个版本的列 name 的内容是’张三’，该版本的 trx_id 值为 80 ，小于 Readview 中的 up_limit_id
值 10 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列 c 为‘张三’的记录。

两次 SELECT 查询得到的结果是重复的，记录的列 c 值都是’张三’，这就是可重复读的含义。

如果我们之后再把事务 id 为 20 的记录提交了，然后再到刚才使用 REPEATABLE READ 隔离级刷的事务中
继续查找这个 id 为 1 的记录，得到的结果还是’张三’，具体执行过程大家可以自己分析一下。

###### 如何解决幻读

接下来说明 InnoDB 是如何解决幻读的。

假设现在表 student 中只有一条数据，数据内容中，主键 id=1，隐藏的 trx_id=10，它的 undo log 如
下图所示。

假设现在有事务 A 和事务 B 并发执行，事务 A 的事务 id 为 20 ，事务 B 的事务 id 为 30 。

步骤 1 ：事务 A 开始第一次查询数据，查询的 SQL 语句如下。

在开始查询之前，MySQL 会为事务 A 产生一个 ReadView，此时 ReadView 的内容如下： trx_ids=
[20,30] ， up_limit_id=20 ， low_limit_id=31 ， creator_trx_id=20 。

由于此时表 student 中只有一条数据，且符合 where id>=1 条件，因此会查询出来。

然后根据 ReadView 机制，发现该行数据的 trx_id=10，小于事务 A 的 ReadView 里 up_limit_id，这表
示这条数据是事务 A 开启之前，其他事务就已经提交了的数据，因此事务 A 可以读取到。

结论：事务 A 的第一次查询，能读取到一条数据，id=1。

步骤 2 ：接着事务 B (trx_id=30)，往表 student 中新插入两条数据，并提交事务。

此时表 student 中就有三条数据了，对应的 undo 如下图所示：

```
select * from student where id >= 1 ;
```
```
insert into student (id, name) values ( 2 ,'李四'); insert into student (id, name)
values ( 3 ,'王五');
```

步骤 3 ：接着事务 A 开启第二次查询，根据可重复读隔离级别的规则，此时事务 A 并不会再重新生成
ReadView。此时表 student 中的 3 条数据都满足 where id>=1 的条件，因此会先查出来。然后根据
ReadView 机制，判断每条数据是不是都可以被事务 A 看到。

1 ）首先 id=1 的这条数据，前面已经说过了，可以被事务 A 看到。

2 ）然后是 id=2 的数据，它的 trx_id=30，此时事务 A 发现，这个值处于 up_limit_id 和 low_limit_id
之间，因此还需要再判断 30 是否处于 trx_ids 数组内。由于事务 A 的 trx_ids=[20,30]，因此在数组
内，这表示 id=2 的这条数据是与事务 A 在同一时刻启动的其他事务提交的，所以这条数据不能让事务
A 看到。

3 ）同理，id=3 的这条数据，trx_id 也为 30 ，因此也不能被事务 A 看见。

**结论：最终事务 A 的第二次查询，只能查询出 id=1 的这条数据。这和事务 A 的第一次查询的结果是一
样的，因此没有出现幻读现象，所以说在 MySQL 的可重复读隔离级别下，不存在幻读问题。**

###### 总结

这里介绍了 MVCC 在 READ COMMITTD 、 REPEATABLE READ 这两种隔离级别的事务在执行快照读操
作时访问记录的版本链的过程。这样使不同事务的读-写、写-读操作并发执行，从而提升系统性能。

核心点在于 ReadView 的原理， READ COMMITTD 、 REPEATABLE READ 这两个隔离级别的一个很大
不同就是生成 ReadView 的时机不同：

```
READ COMMITTD 在每一次进行普通 SELECT 操作前都会生成一个 ReadView
REPEATABLE READ 只在第一次进行普通 SELECT 操作前生成一个 ReadView，之后的查询操作都重
复使用这个 ReadView 就好了。
```
**通过 MVCC 我们可以解决:**

1. 读写之间阻塞的问题。

通过 MVcc 可以让读写互相不阻塞，即读不阻塞写，写不阻塞读，这样就可以提升事务并发处理能力。


2. 降低了死锁的概率。

这是因为 MVCC 采用了乐观锁的方式，读取数据时并不需要加锁，对于写操作，也只锁定必要的行。

3. 解次快照读的问题。

当我们查询数据库在某个时间点的快照时，只能看到这个时间点之前事务提交更新的结果，而不能看到
这个时间点之后事务提交的更新结果。

#### 聊聊：MySQL 间隙锁，如何解决幻读？

在 RR 的隔离级别下，Innodb 使用 MVCC 和 next-key locks (行锁和间隙锁的组合) 解决幻读，

```
MVCC 解决的是普通读（快照读） 的幻读，
next-key locks 解决的是当前读情况下的幻读。
```
MySQL 间隙锁 + 记录锁，组合起来，解决的是 **当前读** 情况下的幻读

###### 间隙锁

InnoDB 支持三种锁定方式：

```
记录锁（Record Lock）：锁直接加在索引记录上面。
间隙锁（Gap Lock）：锁加在不存在的空闲空间，可以是两个索引记录之间，也可能是第一个索
引记录之前或最后一个索引之后的空间。
Next-Key Lock：记录锁与间隙锁组合起来用就叫做 Next-Key Lock。
```
间隙锁：当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB 会给符合条件
的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，

InnoDB 也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Gap Lock 锁）。

举例来说，假如 user 表中只有 101 条记录，其 empid 的值分别是 1,2,..., 100,101，下面的 SQL：

是一个范围条件的检索，InnoDB 不仅会对符合条件的 user_id 值为 101 的记录加锁，也会对 user_id 大于
101 （这些记录并不存在）的“间隙”加锁。

产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。

因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。

InnoDB 使用间隙锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求，

对于上面的例子，要是不使用间隙锁，如果其他事务插入了 user_id 大于 100 的任何记录，那么本事务如
果再次执行上述语句，就会发生幻读；

另外一方面，是为了满足其恢复和复制的需要。

###### 快照读和当前读

###### 快照读（历史数据）－mvcc

innodb 的默认事务隔离级别是 rr（可重复读）。它的实现技术是 mvcc（MVCC 只在读提交可重复读两种
隔离级别下工作）。

```
select * from user where user_id > 100 for update;
```

基于版本的控制协议。该技术不仅可以保证 innodb 的可重复读，而且可以防止幻读。

但是它防止的是快照读，也就是读取的数据虽然是一致的，但是数据是历史数据。

快照读对应的 sql 语法：简单的 select 操作 (不包括 select ... lock in share mode, select ... for update)

###### 当前读（最新数据）－next-key lock

如何做到保证数据是一致的（也就是一个事务，其内部读取对应某一个数据的时候，数据都是一样
的），同时读取的数据是最新的数据。

innodb 提供了 next－key lock，也就是结合 **gap 锁与行锁** ，达到最终目的。

当前读（最新数据）对应的 sql 语法：

特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。

总之：

在 RR 级别下，快照读是通过 MVCC (多版本控制) 和 undo log 来实现的，当前读是通过加 record lock (记录
锁) 和 gap lock (间隙锁) 来实现的。

#### 聊聊：什么是表级锁、行级锁、页级锁？

数据库锁定机制简单来说，就是 **数据库为了保证数据的一致性，而使各种共享资源在被并发访问变得有
序所设计的一种规则** 。

MySQL 数据库由于其自身架构的特点，存在多种数据存储引擎，每种存储引擎的锁定机制都是为各自所
面对的特定场景而优化设计，所以各存储引擎的锁定机制也有较大区别。

MySQL 各存储引擎使用了三种类型（级别）的锁定机制：表级锁定，行级锁定和页级锁定。

**1 、表级锁**

表级别的锁定是 MySQL 各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简
单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。

当然， **锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并发度大打折
扣** 。

使用表级锁定的主要是 MyISAM，MEMORY，CSV 等一些非事务性存储引擎。

**2 、行级锁**

```
select * from table where? lock in share mode;
select * from table where? for update;
insert into table values (...);
update table set? where ?;
delete from table where ?;
```

行级锁定最大的特点就是 **锁定对象的颗粒度很小** ，由于锁定颗粒度很小，所以发生锁定资源争用的概率
也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。

虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。

**由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大
了。此外，行级锁定也最容易发生死锁** 。

```
使用行级锁定的主要是 InnoDB 存储引擎。
```
**3 、页级锁**

页级锁定是 MySQL 中比较独特的一种锁定级别。

页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能
提供的并发处理能力也同样是介于上面二者之间。

使用页级锁定的主要是 BerkeleyDB 存储引擎。

**4 、总结**

总的来说，MySQL 这 3 种锁的特性可大致归纳如下：

```
表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低；
```
```
行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高；
```
页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一
般。

#### 聊聊：什么是共享锁、排它锁？

InnoDB 实现了标准的 **行级锁** ，包括两种：

```
共享锁 （简称 s 锁）、
排它锁 （简称 x 锁）。
```
对于共享锁而言，对当前行加 **共享锁** ，不会阻塞其他事务对同一行的读请求，但会阻塞对同一行的写请
求。

只有当读锁释放后，才会执行其它事物的写操作。

对于排它锁而言，会阻塞其他事务对同一行的读和写操作，只有当写锁释放后，才会执行其它事务的读
写操作。

简而言之，就是：

```
读锁会阻塞写 (X)，但是不会堵塞读 (S)
而写锁则会把读 (S) 和写 (X) 都堵塞
```

对于 InnoDB 在 RR (MySQL 默认隔离级别) 而言，对于 update、delete 和 insert 语句，会自动给涉及数
据集加排它锁（X）；

对于普通 select 语句，innodb 不会加任何锁。

如果想在 select 操作的时候加上 S 锁或者 X 锁，需要我们手动加锁。

**读的时候，用 select... in share mode 获得共享锁，**

主要用在需要数据依存关系时来确认某行记录是否存在，并确保没有人对这个记录进行 update 或者
delete 操作。

**读的时候，用 select... for update** 获得排他锁

但是如果当前事务也需要对该记录进行更新操作, 则有可能造成死锁，对于锁定行记录后需要进行更新
操作的应用，应该使用 **select... for update** 方式获得排他锁。

#### 聊聊：什么是记录锁（Record Locks）？

记录锁其实很好理解，对表中的记录加锁，叫做记录锁，简称行锁。

比如

它会在 id=1 的记录上加上记录锁，以阻止其他事务插入，更新，删除 id=1 这一行。

需要注意的是：

```
id 列必须为唯一索引列或主键列，否则上述语句加的锁就会变成临键锁 (有关临键锁下面会讲)。
同时查询语句必须为精准匹配（=），不能为 >、<、like 等，否则也会退化成临键锁。
```
其他实现

在通过 **主键索引** 与 **唯一索引** 对数据行进行 UPDATE 操作时，也会对该行数据加记录锁：

**记录锁是锁住记录，锁住索引记录，而不是真正的数据记录**.

如果要锁的列没有索引，进行全表记录加锁

记录锁也是排它 (X) 锁, 所以会阻塞其他事务对其 **插入、更新、删除** 。

#### 聊聊：什么是间隙锁（Gap Locks）？

间隙锁是 **Innodb 在 RR (可重复读) 隔离级别** 下为了解决 **幻读问题** 时引入的锁机制。

```
-- 加共享锁（S）
select * from table_name where ... lock in share mode
```
```
-- 加排它锁（X)
select * from table_name where ... for update
```
```
SELECT * FROM `test` WHERE `id`= 1 FOR UPDATE;
```
```
-- id 列为主键列或唯一索引列
UPDATE SET age = 50 WHERE id = 1 ;
```

**间隙锁是 innodb 中行锁的一种** 。

请务必牢记： **使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据** 。

举例来说，假如 emp 表中只有 101 条记录，其 empid 的值分别是 1,2,..., 100,101，下面的 SQL：

当我们用条件检索数据，并请求共享或排他锁时，InnoDB 不仅会对符合条件的 empid 值为 101 的记录加
锁，也会对 empid 大于 101 （这些记录并不存在）的“间隙”加锁。

这个时候如果你插入 empid 等于 102 的数据的，如果那边事物还没有提交，那你就会处于等待状态，无
法插入数据。

有关间隙锁所需讲的东西还是蛮多的，我会单独写一篇文章来分析间隙锁，并在文章中附上完整的示
例。

#### 聊聊：什么是临键锁（Next-Key Locks）？

Next-key 锁是记录锁和间隙锁的组合，它指的是加在某条记录以及这条记录前面间隙上的锁。

也可以理解为一种特殊的 **间隙锁** 。

通过 **临建锁** 可以解决幻读的问题。

每个数据行上的 **非唯一索引列** 上都会存在一把 **临键锁** ，当某个事务持有该数据行的 **临键锁** 时，会锁住一
段 **左开右闭区间** 的数据。

需要强调的一点是，InnoDB 中 **行级锁** 是基于索引实现的。

假设有如下表：

id 主键, age 普通索引

该表中 age 列潜在的临键锁有：
(-∞, 10],
(10, 24],
(24, 32],

```
SELECT * FROM emp WHERE empid > 100 FOR UPDATE
```
```
临键锁只与非唯一索引列有关，在唯一索引列（包括主键列）上不存在临键锁。
```

(32, 45],
(45, +∞],

在事务 A 中执行如下命令：

不管执行了上述 SQL 中的哪一句，之后如果在事务 B 中执行以下命令，则该命令会被阻塞：

很明显，事务 A 在对 age 为 24 的列进行 UPDATE 操作的同时，也获取了 (24, 32] 这个区间内的临键
锁。

这里对 **记录锁** 、 **间隙锁** 、 **临键锁** 做一个总结

```
InnoDB 中的行锁的实现依赖于索引，一旦某个加锁操作没有使用到索引，那么该锁就会退化为
表锁。
记录锁存在于包括主键索引在内的唯一索引中，锁定单条索引记录。
间隙锁存在于非唯一索引中，锁定开区间范围内的一段间隔。
临键锁存在于非唯一索引中，该类型的每条记录的索引上都存在这种锁，它是一种特殊的间隙锁，
锁定一段左开右闭的索引区间。
```
#### 聊聊：什么是意向锁？

意向锁又分为意向共享锁（IS）和意向排他锁（IX）

```
意向共享 (IS) 锁 ：事务有意向对表中的某些行加共享锁 （S 锁）
```
```
意向排他 (IX) 锁 ：事务有意向对表中的某些行加排他锁 （X 锁）
```
首先我们要明白四点

```
意向共享锁（IS）和意向排他锁（IX）都是表锁。
意向锁是一种不与行级锁冲突的表级锁，这一点非常重要。
意向锁是 InnoDB 自动加的，不需用户干预。
意向锁是在 InnoDB 下存在的内部锁，对于 MyISAM 而言没有意向锁之说。
```
这里就会有疑惑，既然前面已经有了共享锁（S 锁）、排它锁（X 锁）。

那么为什么需要引入意向锁呢？它能解决什么问题呢？

```
-- 根据非唯一索引列 UPDATE 某条记录
UPDATE table SET name = Vladimir WHERE age = 24 ;
-- 或根据非唯一索引列锁住某条记录
SELECT * FROM table WHERE age = 24 FOR UPDATE;
```
```
INSERT INTO table VALUES ( 100 , 26 , 'tianqi');
```
```
-- 事务要获取某些行的 S 锁，必须先获得表的 IS 锁。
SELECT column FROM table ... LOCK IN SHARE MODE;
```
```
-- 事务要获取某些行的 X 锁，必须先获得表的 IX 锁。
SELECT column FROM table ... FOR UPDATE;
```

```
我们可以理解意向锁存在的目的就是为了让 InnoDB 中的行锁和表锁更高效的共存。
```
为什么这么说，我们来举一个例子。

**举例**

下面有一张表 **InnoDB RR 隔离级别 id 是主键**

事务 A 获取了 **某一行的排他锁** ，并未提交：

事务 B 想要获取 users 表的 **表锁** ：

因为共享锁与排他锁互斥，所以事务 B 在视图对 users 表加共享锁的时候，必须保证：

```
当前没有其他事务持有 users 表的排他锁。
当前没有其他事务持有 users 表中任意一行的排他锁。
```
为了检测是否满足第二个条件，事务 B 必须在确保 users 表不存在任何 **排他锁** 的前提下，去检测表中的
**每一行是否存在排他锁** 。很明显这是一个效率很差的做法，但是有了 **意向锁** 之后，情况就不一样了：事
务 B 只要看表上有没有

意向共享锁，有则说明表中有些行被 **共享行锁** 锁住了，因此，事务 B 申请表的写锁会被阻塞。这样是不
是就高效多了。

这也解释就应该清楚，为什么有意向锁这个东西存在了。

我们可以举个生活中的例子，再来理解下为什么需要存在意向锁。

打个比方，就像有个游乐场，很多小朋友进去玩，看门大爷如果要下班锁游乐场的门 ( **加表锁** )，他必须
确保每个角落都要去检查一遍，确保每个小朋友都离开了 ( **释放行锁** )，才可以锁门。

假设锁门是件频繁发生的事情，大爷就会非常崩溃。那大爷想了一个办法，每个小朋友进入，就把自己
的名字写在本子上，小朋友离开，就把自己的名字划掉，那大爷就能方便掌握有没有小朋友在游乐场
里，不必每个角落都去寻找一遍。

例子中的“小本子”，就是 **意向锁** ，他记录的信息并不精细，他只是提醒大爷，有人在屋里。

这里我们再来看下共享 (S) 锁、排他 (X) 锁、意向共享锁（IS）、意向排他锁（IX）的兼容性

```
SELECT * FROM users WHERE id = 6 FOR UPDATE;
```
```
LOCK TABLES users READ;
```

可以看出 **意向锁之间是互相兼容的** .那你存在的意义是啥？

意向锁不会为难意向锁。也不会为难行级排他 (X)/共享 (X) 锁,它的存在是为难表级排他 (X)/共享 (X) 锁。

注意 **这里的排他 (X)/共享 (S) 锁指的都是表锁！意向锁不会与行级的共享/排他锁互斥！** 行级别的 X 和 S 按
照上面的兼容性规则即可。

意向锁与意向锁之间永远是兼容的，因为当你不论加行级的 X 锁或 S 锁，都会自动获取表级的 IX 锁或者 IS
锁。也就是你有 10 个事务，对不同的 10 行加了行级 X 锁，那么这个时候就存在 10 个 IX 锁。

这 10 IX 存在的目的是啥呢，就是假如这个时候有个事务，想对整个表加排它 X 锁, 那它不需要遍历每一行
是否存在 S 或 X 锁，而是看有没有存在意向锁，只要存在一个意向锁，那这个事务就加不了表级排它 X
锁，要等上面 10 个 IX 全部释放才行。

#### 聊聊：什么是插入意向锁？

在讲解插入意向锁之前，先来思考一个问题

下面有张表 id 主键，age 普通索引

首先 **事务 A** 插入了一行数据，并且没有 **commit** ：

随后 **事务 B** 试图插入一行数据：

```
INSERT INTO users SELECT 4 , 'Bill', 15 ;
```

请问：

1 、事务 A 使用了什么锁？

2 、 **事务 B** 是否会被 **事务 A** 阻塞？

**插入意向锁** 是在插入一条记录行前，由 **INSERT** 操作产生的一种间隙锁。

该锁用以表示插入 **意向** ，当多个事务在 **同一区间** （gap）插入 **位置不同** 的多条数据时，事务之间 **不需要
互相等待** 。

假设存在两条值分别为 4 和 7 的记录，两个不同的事务分别试图插入值为 5 和 6 的两条记录，每个事
务在获取插入行上独占的（排他）锁前，都会获取（ 4 ， 7 ）之间的 **间隙锁** ，但是因为数据行之间并不冲
突，所以两个事务之间

并 **不会产生冲突** （阻塞等待）。

总结来说， **插入意向锁** 的特性可以分成两部分：

```
插入意向锁是一种特殊的间隙锁 —— 间隙锁可以锁定开区间内的部分记录。
插入意向锁之间互不排斥，所以即使多个事务在同一区间插入多条记录，只要记录本身（ 主键、唯
一索引 ）不冲突，那么事务之间就不会出现冲突等待。
```
需要强调的是，虽然 **插入意向锁** 中含有 **意向锁** 三个字，但是它并不属于 **意向锁** 而属于 **间隙锁** ，因为 **意向
锁** 是 **表锁** 而插入意向锁是行锁。

现在我们可以回答开头的问题了：

1 、使用 **插入意向锁** 与 **记录锁** 。

2 、 **事务 A** 不会阻塞 **事务 B** 。

为什么不用间隙锁

如果只是使用普通的 **间隙锁** 会怎么样呢？

我们在看事务 A, 其实它一共获取了 3 把锁

```
id 为 4 的记录行的记录锁。
age 区间在（ 10 ， 15 ）的间隙锁。
age 区间在（ 15 ， 20 ）的间隙锁。
```
最终， **事务 A** 插入了该行数据，并锁住了（ 10 ， 20 ）这个区间。

随后 **事务 B** 试图插入一行数据：

因为 16 位于（ 15 ， 20 ）区间内，而该区间内又存在一把 **间隙锁** ，所以 **事务 B** 别说想申请自己的 **间隙锁**
了，它甚至不能获取该行的 **记录锁** ，自然只能乖乖的等待 **事务 A** 结束，才能执行插入操作。

很明显，这样做事务之间将会频发陷入 **阻塞等待** ， **插入的并发性** 非常之差。

这时如果我们再去回想我们刚刚讲过的 **插入意向锁** ，就不难发现它是如何优雅的解决了 **并发插入** 的问
题。

###### 总结

```
InnoDB 在 RR 的事务隔离级别下，使用插入意向锁来控制和解决并发插入。
```
```
INSERT INTO users SELECT 5 , 'Louis', 16 ;
```
```
INSERT INTO users SELECT 5 , 'Louis', 16 ;
```

```
插入意向锁是一种特殊的间隙锁。
插入意向锁在锁定区间相同但记录行本身不冲突的情况下互不排斥。
```
#### 聊聊：Mysql 的锁你了解哪些

按锁粒度分类：

```
1. 行锁：锁某行数据，锁粒度最小，并发度高
2. 表锁：锁整张表，锁粒度最大，并发度低
3. 间隙锁：锁的是一个区间
```
还可以分为：

```
1. 共享锁：也就是读锁，一个事务给某行数据加了读锁，其他事务也可以读，但是不能写
2. 排它锁：也就是写锁，一个事务给某行数据加了写锁，其他事务不能读，也不能写
```
还可以分为：

```
1. 乐观锁：并不会真正的去锁某行记录，而是通过一个版本号来实现的
2. 悲观锁：上面所的行锁、表锁等都是悲观锁
```
在事务的隔离级别实现中，就需要利用所来解决幻读

#### 美团索命一问：一个 SQL ，怎么分析加了哪些锁？

###### 背景说明：

美团问数据库应该是非常多的，尤其喜欢考手写 SQL 然后问你这个 SQL 语句上面加了哪些锁，

你会发现其他厂面试基本很少会这样考，所以很多小伙伴遇到这种问题的时候都是一脸懵逼，

可以说是“夺命一问”

此文， 40 岁老架构师尼恩，用自己的深厚内功，为大家梳理了加锁的四大场景、八大规则，

把这个 “夺命一问”，彻底攻克。

###### 加锁的场景分析

```
场景 1 ：唯一索引等值查询
场景 1.1、查询的记录存在
场景 1.2、查询的记录不存在
场景 2 ：唯一索引范围查询
场景 3 ：非唯一索引等值查询
场景 3.1、查询的记录存在
场景 3.2、查询的记录不存在
场景 4 ：非唯一索引范围查询
```

```
id (唯一索引) a (非唯一索引) b
```
```
10 4 Alice
```
```
15 8 Bob
```
```
20 16 Cilly
```
```
25 32 Druid
```
```
30 64 Erik
```
接下来，按照以上 4 大场景，进行 sql 加锁的分析。

###### 回顾一下，InnoDB 三种行锁：

```
Record Lock（记录锁）：锁住某一行记录
Gap Lock（间隙锁）：锁住一段左开右开的区间
Next-key Lock（临键锁）：锁住一段左开右闭的区间
```
哪些语句上面会加行锁？

1 ）对于常见的 DML 语句（如 UPDATE、DELETE 和 INSERT ），InnoDB 会自动给相应的记录行加写
锁

2 ）默认情况下对于普通 SELECT 语句，InnoDB 不会加任何锁，但是在 Serializable 隔离级别下会加
行级读锁

上面两种是隐式锁定，InnoDB 也支持通过特定的语句进行显式锁定：

3 ）SELECT * FROM table_name WHERE ... FOR UPDATE，加行级写锁

4 ）SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE，加行级读锁

###### 两条最为基础的规则

在学习具体行锁加锁规则之前，小伙伴们需要记住两条最为基础的规则：

**规则 1 ：** 查找过程中访问到的对象才会加锁

这句话该怎么理解？

比如有主键 id 为 1 2 3 4 5 ... 10 的 10 条记录，我们要找到 id = 7 的记录。

注意，查找并不是从第一行开始一行一行地进行遍历，而是根据 B+ 树的特性进行二分查找，所以一般
存储引擎只会访问到要找的记录行（id = 7）的相邻区间

**规则 2 ：** 加锁的基本单位是 Next-key Lock （临键锁）：锁住一段 **左开右闭** 的区间

**实例分析**

下面结合实例，分析一条 SQL 语句上面究竟被 InnoDB 自动加上了多少个锁

假设有这么一张 user 表:

```
id 为主键（唯一索引），a 是普通索引（非唯一索引），b 都是普通的列，b 上没有任何索引：
```
###### 场景 1 ：唯一索引等值查询


当我们用唯一索引进行等值查询的时候，根据查询的记录是否存在，加锁的规则会有所不同：

**规则 3 ：** 当查询的记录是存在的，Next-key Lock 临键锁会退化成 Record Lock 记录锁

**规则 4 ：** 当查询的记录是不存在的，Next-key Lock 临键锁会退化 Gap Lock 成间隙锁

**场景 1.1、查询的记录存在**

先来看个查询的记录存在的案例：

**结合加锁的规则 1 、规则 2 ：**

**规则 1 ：** 查找过程中访问到的对象才会加锁

**规则 2 ：** 加锁的基本单位是 Next-key Lock 临键锁（左开右闭）

两条要点, 我们可以分析出，这条语句的加锁范围是临键锁 (20, 25]

不过，按照 **规则 3** ，由于这个唯一索引等值查询的记录 id = 25 是存在的，因此，Next-key Lock 临键
锁会退化成 Record Lock 记录锁，因此最终的加锁范围是 id = 25 这一行

**场景 1.2、查询的记录不存在**

再来看查询的记录不存在的案例：

**结合加锁的两条要点（重复一下，表示强调）：**

**规则 1 ：** 查找过程中访问到的对象才会加锁

**规则 2 ：** 加锁的基本单位是 Next-key Lock 临键锁（左开右闭）

我们可以分析出，这条语句的加锁范围是 (20, 25]

这里为什么是 (20，25] 而不是 (20, 22]？

因为 id = 22 的记录不存在呀，InnoDB 先找到 id = 20 的记录，发现不匹配，于是继续往下找，发现 id
= 25，因此，id = 25 的这一行被扫描到了，所以整体的加锁范围是 (20, 25]

不过，按照 **规则 4** ，由于这个唯一索引等值查询的记录 id = 22 是不存在的，因此，Next-key Lock 会
退化成间隙锁，因此最终在主键 id 上的加锁范围是 Gap Lock (20, 25)

###### 场景 2 ：唯一索引范围查询

唯一索引范围查询的规则和等值查询的规则一样，

**规则 3 ：** 当查询的记录是存在的，Next-key Lock 会退化成记录锁

**规则 4 ：** 当查询的记录是不存在的，Next-key Lock 会退化成间隙锁

只有一个区别，就是唯一索引的范围查询，需要一直向右遍历到第一个不满足条件的记录，相当于增加
了一条规则：

**规则 5 ：** 唯一索引的范围查询，一直向右遍历到第一个不满足条件的记录

下面结合案例来分析：

```
select * from user  where id = 25 for update;
```
```
select * from user  where id = 22 for update;
```

```
id (唯一索引) a (非唯一索引) b
```
```
10 4 Alice
```
```
15 8 Bob
```
```
20 16 Cilly
```
```
25 32 Druid
```
```
30 64 Erik
```
先来看语句查询条件的前半部分 id >= 20，

**结合加锁的规则 1 、规则 2 ：**

**规则 1 ：** 查找过程中访问到的对象才会加锁

**规则 2 ：** 加锁的基本单位是 Next-key Lock 临键锁（左开右闭）

这条语句最开始要找的第一行是 id = 20，需要加上 Next-key Lock (15,20]。

**根据规则 3 、规则 4 ：**

又由于 id 是唯一索引，且 id = 20 的这行记录是存在的，因此会退化成记录锁，也就是只会对 id =
20 这一行加锁。

**根据规则 5 ：**

再来看语句查询条件的后半部分 id < 22，由于是范围查找，就会继续往后找第一个不满足条件的记
录，也就是会找到 id = 25 这一行停下来，然后加 Next-key Lock 临界锁 (20, 25]，

**根据规则 4 ：**

重点来了，但由于 id = 25 不满足 id < 22，因此会退化成间隙锁，加锁范围变为 (20, 25)。

所以，上述语句在主键 id 上的最终的加锁范围是 Record Lock id = 20 以及 Gap Lock (20, 25)

###### 场景 3 ：非唯一索引等值查询

当我们用非唯一索引进行等值查询的时候，根据查询的记录是否存在，加锁的规则会有所不同：

**规则 6 ：** 当查询的记录是存在的，除了会加 Next-key Lock 外，还会额外加间隙锁（规则是向下遍历到
第一个不符合条件的值才能停止），也就是会加两把锁

很好记忆，就是要查找记录的左区间加 Next-key Lock 锁住一段 **左开右闭** 的区间，右区间加 Gap lock
锁住一段 **左开右开** 的区间

**规则 7 ：** 当查询的记录是不存在的，Next-key Lock 会退化成 Gap lock 间隙锁（这个规则和唯一索引的
等值查询是一样的）

**场景 3.1、查询的记录存在**

先来看个查询的记录存在的案例：

**结合加锁的规则 1 、规则 2 ：**

```
select * from user  where id >= 20 and id < 22 for update;
```
```
select * from user  where a = 16 for update;
```

**规则 1 ：** 查找过程中访问到的对象才会加锁

**规则 2 ：** 加锁的基本单位是 Next-key Lock 临键锁（左开右闭）

这条语句首先会对普通索引 a 加上 Next-key Lock，范围是 (8,16]

根据 **规则 6** ，又因为是非唯一索引等值查询，且查询的记录 a= 16 是存在的，所以还会加上间隙锁，规
则是向下遍历到第一个不符合条件的值才能停止，因此间隙锁的范围是 (16,32)

所以，上述语句在普通索引 a 上的最终加锁范围是 Next-key Lock (8,16] 以及 Gap Lock (16,32)

**场景 3.2、查询的记录不存在**

再来看查询的记录不存在的案例：

结合加锁的两条核心，这条语句首先会对普通索引 a 加上 Next-key Lock，范围是 (16,32]

但是由于查询的记录 a = 18 是不存在的，因此 Next-key Lock 会退化为间隙锁，即最终在普通索引 a
上的加锁范围是 (16,32)。

###### 场景 4 ：非唯一索引范围查询

回顾一下，唯一索引范围查询，其查询的规则和唯一索引等值查询的规则一样，

**规则 3 ：** 当查询的记录是存在的，Next-key Lock 会退化成记录锁

**规则 4 ：** 当查询的记录是不存在的，Next-key Lock 会退化成间隙锁

**规则 5 ：** 唯一索引的范围查询，一直向右遍历到第一个不满足条件的记录

非唯一索引的范围查询和唯一索引范围查询不同的是，

**规则 8 ：** 非唯一索引的范围查询并不会退化成 Record Lock 或者 Gap Lock。

先来看语句查询条件的前半部分 a >= 16，因此，这条语句最开始要找的第一行是 a = 16，结合加锁
的两个核心，需要加上 Next-key Lock (8,16]。

虽然非唯一索引 a = 16 的这行记录是存在的，但此时并不会像唯一索引那样退化成记录锁。

再来看语句查询条件的后半部分 a < 18，由于是范围查找，就会继续往后找第一个不满足条件的记
录，也就是会找到 id = 32 这一行停下来，然后加 Next-key Lock (16, 32]。

虽然 id = 32 不满足 id < 18，但此时并不会向唯一索引那样退化成间隙锁。

所以，上述语句在普通索引 a 上的最终的加锁范围是 Next-key Lock (8, 16] 和 (16, 32]，也就是
(8, 32]。

```
select * from user  where a = 18 for update;
```
```
select * from user  where a >= 16 and a < 18 for update;
```

```
事务隔离级别脏读不可重复读幻读
```
```
读未提交 RU 允许允许允许
```
```
读已提交 RC 不允许允许允许
```
```
可重复读 RR 不允许不允许允许
```
```
串行化不允许不允许不允许
```
## 阿里二面：为什么 MySQL 默认的 Repeatable

## Read 隔离级别，被改成了 RC？

前段时间，一位小伙伴面试了阿里，遇到了一个 MySQL 日志的面试题：

**为什么 MySQL 默认的 Repeatable Read 隔离级别，被改成了 RC?**

这个问题，背后的原理比较复杂，而且又很重要。

现在把这个题目，以及参考答案，收入咱们的《尼恩 Java 面试宝典》，供后面的小伙伴参考，前车之
鉴啊

###### ISO SQL 定义的标准隔离级别有四种

我们知道，我们可以通过这个命令查看数据库当前的隔离级别，MySQL 默认隔离级别是 RR.

**ANSI/ISO SQL 定义的标准隔离级别有四种，从高到底依次为：**

**可序列化 (Serializable)、可重复读 (Repeatable Reads)、提交读 (Read Committed)、未提交读
(Read Uncommitted)。**

这四个隔离级别可以解决脏读、不可重复读、幻象读这三类问题。总结如下

```
RU 隔离级别下，可能发生脏读、幻读、不可重复读等问题。
未提交读的数据库锁情况（实现原理）
```
```
事务在读数据的时候并未对数据加锁。
事务在修改数据的时候只对数据增加行级共享锁。
```
```
RC 隔离级别下，解决了脏读的问题，存在幻读、不可重复读的问题。
提交读的数据库锁情况
```
```
事务对当前被读取的数据加行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行
级共享锁；
事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁，直到事务结
束才释放。
```
```
select @@tx_isolation;
```

```
RR 隔离级别下，解决了脏读、不可重复读的问题，存在幻读的问题。
可重复读的数据库锁情况
事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加行级共享锁，直到事务结
束才释放；
事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁，直到事务结
束才释放。
```
```
Serializable 隔离级别下，解决了脏读、幻读、不可重复读的问题。
可序列化的数据库锁情况
```
```
事务在读取数据时，必须先对其加表级共享锁，直到事务结束才释放；
事务在更新数据时，必须先对其加表级排他锁，直到事务结束才释放。
```
虽然可序列化解决了脏读、不可重复读、幻读等读现象。但是序列化事务会产生以下效果：

```
1. 无法读取其它事务已修改但未提交的记录。
2. 在当前事务完成之前，其它事务不能修改目前事务已读取的记录。
3. 在当前事务完成之前，其它事务所插入的新记录，其索引键值不能在当前事务的任何语句所读取
的索引键范围中。
```
这四种隔离级别是 ANSI/ISO SQL 定义的标准定义的，我们比较常用的 MySQL 对这四种隔离级别是都支
持的。

其中隔离级别由低到高是：读未提交 < 读已提交 < 可重复读 < 串行化。隔离级别越高，越能够保证数据
的完整性和一致性，但是对并发的性能影响越大。

注意，Mysql 通过间隙锁解决了 RR 级别下的幻读问题，所以，Mysql 的 RR 级别，也解决了脏读、幻读、
不可重复读的问题。

大多数数据库的默认级别是读已提交 (Read committed)，比如 Sql Server、Oracle ，

但是 MySQL 的默认隔离级别是可重复读 (repeatable-read)。

**大家经常说：Oracle 默认的隔离级别是 RC，而 MySQL 默认的隔离级别是 RR。**

那么，为什么 MySQL 默认隔离级别是 RR，为什么阿里等大厂会改成 RC？

**根本的原因：提升 MYSQL 吞吐量、并发量。允许很短的时间内数据不可重读、幻读，或者在业务维度
去规避。**

互联网公司和传统企业最大的区别是什么？高并发！互联网业务的并发度比传统企业要高处很多。

这点，从一个简单的数据可以看出: 2020 年双十一当天，订单创建峰值达到 58.3 万笔/秒。

MySQL 的 RR”可重复读“这种隔离级别，带来了很大的性能损耗。

无论是超高并发读的场景，还是超高并发写的场景，带来了一些的性能损耗。

###### RR 在高并发写场景的性能损耗

在 MySQL 中，有三种类型的锁，分别是 Record Lock、Gap Lock 和 Next-Key Lock。


```
Record Lock 表示记录锁，锁的是索引记录。
```
```
Gap Lock 是间隙锁，锁的是索引记录之间的间隙。
```
```
Next-Key Lock 是 Record Lock 和 Gap Lock 的组合，同时锁索引记录和间隙。他的范围是左开右闭
的。
```
在 RC 中，只会对索引增加 Record Lock，不会添加 Gap Lock 和 Next-Key Lock。

在 RR 中，为了解决幻读的问题，在支持 Record Lock 的同时，还支持 Gap Lock 和 Next-Key Lock；

间隙锁的触发条件：

```
事务隔离级别为 RR 。因为间隙锁只有在事务隔离级别 RR 中才会产生，隔离级别级别是 RC 的话，间
隙锁将会失效
显式加锁。比如使用了类似于 select... for update 这样的加锁语句
查询条件必须有索引 ：
```
间隙锁属于 **排它锁** ，它会把查询 sql 中最靠近检索条件的左右两边的数据间隙锁住，防止其它事务在这
个间隙内插入、修改、删除数据，从而保证该事务内，任何时候以相同检索条件读取的数据都是相同，
也即保证可重复读。

在高并发下，由于间隙被锁住，导致需要往间隙内插入、删除、修改数据的并发线程必须等待，会带来
一定性能问题，并且 **最终锁影响的范围可能远远超过我们想要操作的数据** 。

间隙锁主要是解决 RR 级别的幻读问题而来。但在现实中，一个事务里重复同一条 sql 再次查询的场景极
低，且出于性能的考虑，一般也会尽量避免在同一事务内对同一数据进行多次查询。

在高并发、分布式 RPC 应用场景，通过加 Redis 分布式 Cache 的方式，其实也就避免了多次查询。没有
多次查询，或者避免多次查询，也就无所谓幻读。

如果实际业务场景中，如果无需锁住数据间隙，建议关闭间隙锁，或者将 MySQL 隔离级别由 RR 改为
RC，否则会带来无谓的性能开销，甚至会 **引发死锁** ，影响业务运行。

###### RR 在高并发读场景的性能损耗

为了提升性能，RR 与 RC 下的普通读都是快照读，这里提到的普通读, 是指除了如下 2 种之外的 select 都
是普通读

所以普通读的范围，是非常大的。

快照读，又称为一致性读。快照即当前行数据之前的历史版本。快照读就是使用快照信息显示基于某个
时间点的查询结果，而不考虑与此同时运行的其他事务所执行的更改。

```
(1) 若查询条件走唯一索引：只有锁住多条记录或者一条不存在的记录的时候，才会产生间隙锁；如果
查询单条存在的记录，不会产生间隙锁
```
```
(2) 若查询条件走普通索引：不管是锁住单条，还是多条记录，都会产生间隙锁
```
```
select * from table where ... for update;
select * from table where ... lock in share mode;
```

在 MySQL 中，只有 READ COMMITTED 和 REPEATABLE READ 这两种事务隔离级别才会使用一致性读。

RR 与 RC 下的普通读都是快照读，但两者的快照读有所不同：

```
RC 下，事务内每次都是读最新版本的快照数据
RR 下，事务内每次都是读同一版本的快照数据 (即首次 read 时的版本)
```
RR 下，事务会以第一次普通读时快照数据为准，该事务后续其他的普通读都是读的该份快照数据，也即
事务内是同一份快照读。这就意味着，Mysql 为了维护同一版本的快照数据，需要额外的资源耗损，含
计算耗损、内存耗损。

但在现实中，一个事务里重复同一条 sql 再次查询的场景极低，且出于性能的考虑，一般也会尽量避免
在同一事务内对同一数据进行多次查询。因此，RR 下所谓的事务内同一份快照读意义并不大。

###### 总之：

MySQL 的 RR”可重复读“这种隔离级别，带来了很大的性能损耗。

无论是超高并发读的场景，还是超高并发写的场景，带来了一些的性能损耗。

MySQL 默认隔离级别是 RR，为什么阿里等大厂会改成 RC，主要是出于性能考虑。

另外，通过程序手段，去规避幻读、可重复读的问题。

#### 聊聊：count (1)、count (*) 与 count (列名) 的区别？

```
count (*) 包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为 NULL
count (1) 包括了忽略所有列，用 1 代表代码行，在统计结果的时候，不会忽略列值为 NULL
count (列名) 只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空表示 null）的计
数，即某个字段值为 NULL 时，不统计。
```
阿里巴巴 Java 开发手册有一条强制建议： **不要使用 count (列名) 或 count (常量) 来代替 count (*)** 。
count (*) 就是 SQL 92 定义的标准统计行数语法，跟数据库无关。

#### 聊聊：exist 和 in 的区别？

使用 in 时, sql 语句是先执行子查询，也就是先查询子表 b，再查主表 a。而使用 exists 是先查主表 a ,再查询
子表 b。

根据小表驱动大表 (即小的数据集驱动大的数据集) 的原则, 如果主查询中的表较大且又有索引时应该用
in。反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用 exists。

还有一点注意的是用 exists 该子查询实际上并不返回任何数据，而是返回值 True 或 False。

**「not in 和 not exists」**

如果查询语句使用了 not in 那么内外表都进行全表扫描，没有用到索引；而 not extsts 的子查询依然能
用到表上的索引。所以无论那个表大，用 not exists 都比 not in 要快。

```
-- in
select * from a where id in (select id from b);
-- exists
select * from A where exists (select 1 from B where B.id = A.id);
```

```
delete truncate drop
```
```
类
型
```
```
DML DDL DDL
```
```
回
滚
```
```
可回滚不可回滚不可回滚
```
```
删
除
内
容
```
```
表结构还在，删除表的全
部或者一部分数据行
```
```
表结构还在，删除
表中的所有数据
```
```
表结构也会删除，所有的数据
行，索引和权限也会被删除
```
```
删
除
速
度
```
```
删除速度慢，逐行删除删除速度快删除速度最快
```
#### 聊聊：truncate、delete 与 drop 区别？

#### 聊聊：union 与 union all 的区别？

```
Union ：对两个结果集进行并集操作，不包括重复行，同时进行默认规则的排序；
Union All ：对两个结果集进行并集操作，包括重复行，不进行排序；
```
从效率上说，union all 要比 union 快很多，所以，如果可以确认合并的两个结果集中不包含重复的数据
的话，那么就使用 union all

#### 聊聊：group by 和 distinct 的区别？

它们本质语言逻辑上的数据处理动作先后是不一样，distinct 是先获取结果集，再去重复记录。group
by 是基于 KEY 先分组，再返回计算结果。

从效率上来讲 group by 和 distinct 都能使用索引, 在相同语义下，从执行效率上也看不到明显的差异;

那为什么，大家都更推崇使用 group by？

```
group by 语义更为清晰，灵活
group by 可对数据进行更为复杂的一些处理
```
由于 distinct 关键字会对所有字段生效，在进行复合业务处理时，group by 的使用灵活性更高，group
by 能根据分组情况，对数据进行更为复杂的处理，例如通过 having 对数据进行过滤，或通过聚合函数对
数据进行运算。

#### 聊聊：Blob 和 text 有什么区别？

```
Blob 用于存储二进制数据，而 Text 用于存储大字符串。
Blob 值被视为二进制字符串（字节字符串）, 它们没有字符集，并且排序和比较基于列值中的字节
的数值。
text 值被视为非二进制字符串（字符字符串）。它们有一个字符集，并根据字符集的排序规则对值
进行排序和比较。
```

#### 聊聊：数据库的三大范式？

```
第一范式 ：数据表中的每一列（每个字段）都不可以再拆分。
第二范式 ：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。
第三范式 ：在满足第二范式的基础上，表中的非主键只依赖于主键，而不依赖于其他非主键。
```
#### 聊聊：什么是存储过程？有哪些优缺点？

**「存储过程」** ，就是一些编译好了的 SQL 语句，这些 SQL 语句代码像一个方法一样实现一些功能（对单
表或多表的增删改查），然后给这些代码块取一个名字，在用到这个功能的时候调用即可。

**「优点：」**

```
存储过程是一个预编译的代码块，执行效率比较高
存储过程在服务器端运行，减少客户端的压力
允许模块化程序设计，只需要创建一次过程，以后在程序中就可以调用该过程任意次，类似方法的
复用
一个存储过程替代大量 SQL 语句，可以降低网络通信量，提高通信速率
可以一定程度上确保数据安全
```
**「缺点：」**

```
调试麻烦
可移植性不灵活
重新编译问题
```
#### 聊聊：主键使用自增 ID 还是 UUID?

推荐使用自增 ID, 不要使用 UUID。

因为在 InnoDB 存储引擎中, 主键索引是作为聚簇索引存在的, 也就是说, 主键索引的 B+树叶子节点上存储
了主键索引以及全部的数据 (按照顺序), 如果主键索引是自增 ID, 那么只需要不断向后排列即可, 如果是
UUID, 由于到来的 ID 与原来的大小不确定, 会造成非常多的数据插入, 数据移动, 然后导致产生很多的内存
碎片, 进而造成插入性能的下降。

#### 聊聊：超大分页怎么处理?

```
「用 id 优化」
```
先找到上次分页的最大 ID, 然后利用 id 上的索引来查询, 类似于

这样的效率非常快, 因为主键上是有索引的, 但是这样有个缺点, 就是 ID 必须是连续的, 并且查询不能有
where 语句, 因为 where 语句会造成过滤数据。

```
「用覆盖索引优化」
```
Mysql 的查询完全命中索引的时候, 称为覆盖索引, 是非常快的, 因为查询只需要在索引上进行查找, 之后可
以直接返回, 而不用再回表拿数据. 因此我们可以先查出索引的 ID, 然后根据 Id 拿数据.

```
「在业务允许的情况下限制页数」
```
```
select * from user where id>1000000 limit 100
```
```
select * from table where id in (select id from table where age > 20 limit
1000000,10)
```

建议跟业务讨论，有没有必要查这么后的分页啦。因为绝大多数用户都不会往后翻太多页。

#### 网易一面：select 分页要调优 100 倍，说说你的思路？

#### （内含 Mysql 的 36 军规）

###### 背景说明：

Mysql 调优，是大家日常常见的调优工作。所以 **Mysql 调优** 是一个 **非常、非常核心的面试知识点** 。

在 40 岁老架构师尼恩的 **读者交流群** (50+) 中，其相关面试题是一个非常、非常高频的交流话题。

近段时间，有小伙伴面试网易，说遇到一个 **SQL 深度分页查询** 调优的面试题：

```
MySQL 百万级数据，怎么做分页查询？说说你的思路？
```
社群中，还遇到过大概的变种：

```
形式 1 ：如何解决 Mysql 深分页问题？
```
```
形式 2 ：mysql 如何实现高效分页
```
```
形式 3 ：后面的变种，应该有很多变种........，会收入《尼恩 Java 面试宝典》。
```
这里尼恩给大家调优，做一下系统化、体系化的梳理，使得大家可以充分展示一下大家雄厚的 “技术肌
肉”， **让面试官爱到 “不能自已、口水直流”** 。

也一并把这个 SQL 深度分页题目以及参考答案，收入咱们的《尼恩 Java 面试宝典》，供后面的小伙伴
参考，提升大家的 3 高架构、设计、开发水平。

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从这里获取：码云
```
###### 深挖问题：MySQL 分页起点越大查询速度越慢

在数据库开发过程中我们经常会使用分页，核心技术是使用用 limit start, count 分页语句进行数据的读
取。

我们分别看下从 10 ， 1000 ， 10000 ， 100000 开始分页的执行时间（每页取 20 条）。

我们已经看出随着起始记录的增加，时间也随着增大，

这说明分页语句 limit 跟起始页码是有很大关系的，那么我们把起始记录改为 100 w 看下：

我们惊讶的发现 MySQL 在数据量大的情况下分页起点越大，查询速度越慢，

300 万条起的查询速度已经需要 1.368 秒钟。

这是为什么呢?

因为 limit 3000000,10 的语法实际上是 mysql 扫描到前 3000020 条数据, 之后丢弃前面的 3000000 行，

这个步骤其实是浪费掉的。

```
select * from product limit 10 , 20 0.002 秒
select * from product limit 1000 , 20 0.011 秒
select * from product limit 10000 , 20 0.027 秒
select * from product limit 100000 , 20 0.057 秒
```
```
select * from product limit 1000000 , 20 0.682 秒
```

从中我们也能总结出两件事情：

```
limit 语句的查询时间与起始记录的位置成正比
mysql 的 limit 语句是很方便，但是对记录很多的表并不适合直接使用。
```
###### 基础知识：mysql 中 limit 的用法

**语法** ：

**注释** ：Limit 子句可以被用于强制 SELECT 语句返回指定的记录数。

Limit 接受一个或两个数字参数, 参数必须是一个整数常量。

如果给定两个参数，

```
第一个参数指定第一个返回记录行的偏移量，
第二个参数指定返回记录行的最大数目。
```
1. m 代表从 m+1 条记录行开始检索，n 代表取出 n 条数据。(m 可设为 0)

表示：从第 7 条记录行开始算，取出 5 条数据

2. 值得注意的是，n 可以被设置为-1，当 n 为-1 时，表示从 m+1 行开始检索，直到取出最后一条数据。

表示：取出第 6 条记录行以后的所有数据。

3. 若只给出 m，则表示从第 1 条记录行开始算一共取出 m 条

以年龄倒序后取出前 3 行：

```
select * from product limit 3000000 , 20 1.368 秒
```
```
SELECT * FROM 表名 limit m, n;
SELECT * FROM table LIMIT [offset,] rows;
```
```
如：SELECT * FROM 表名 limit 6 , 5 ;
```
```
如：SELECT * FROM 表名 limit 6 ,- 1 ;
```
```
如：SELECT * FROM 表名 limit 6 ;
```
```
mysql> select * from student order by age desc;
+-----+--------+------+------+
| SNO | SNAME | AGE | SEX |
+-----+--------+------+------+
| 1 | 换换 | 23 | 男 |
| 2 | 刘丽 | 22 | 女 |
| 5 | 张友 | 22 | 男 |
| 6 | 刘力 | 22 | 男 |
| 4 | NULL | 10 | NULL |
+-----+--------+------+------+
5 rows in set (0.00 sec)
```
```
mysql> select * from student order by age desc limit 3 ;
```

跳过前 3 行后再 2 取行.

###### 回到问题：MySQL 百万级数据大分页查询优化

我们惊讶的发现 MySQL 在数据量大的情况下分页起点越大，查询速度越慢，

300 万条起的查询速度已经需要 1.368 秒钟。

###### 那么，该如何优化呢？

###### 方法 1: 直接使用数据库提供的 SQL 语句

**语句样式** ：

MySQL 中, 可用如下方法:

**功能**

Limit 限制的是从结果集的 start 位置处取出 count 条输出, 其余抛弃.

**原因/缺点** ：

全表扫描, 速度会很慢

而且，有的数据库结果集返回不稳定 (如某次返回 1,2,3, 另外的一次返回 2,1,3).

**适应场景** ：

适用于数据量较少的情况

元祖数量、记录数量级别：百/千级

###### 方法 2: 建立主键或唯一索引, 利用索引 (假设每页 10 条)

```
+-----+--------+------+------+
| SNO | SNAME | AGE | SEX |
+-----+--------+------+------+
| 1 | 换换 | 23 | 男 |
| 2 | 刘丽 | 22 | 女 |
| 6 | 刘力 | 22 | 男 |
+-----+--------+------+------+
3 rows in set (0.00 sec)
```
```
mysql> select * from student order by age desc limit 3 , 2 ;
+-----+--------+------+------+
| SNO | SNAME | AGE | SEX |
+-----+--------+------+------+
| 6 | 刘力 | 22 | 男 |
| 4 | NULL | 10 | NULL |
+-----+--------+------+------+
```
```
SELECT * FROM 表名称 LIMIT start, count
```

**语句样式** ：

除了主键，也可以利用唯一键索引快速定位部分元组，避免全表扫描

比如: 读第 1000 到 1019 行元组 (pk 是唯一键).

**原因** ：

索引扫描, 速度会很快.

**缺点** ：

如果数据查询出来并不是按照 pk_id 排序，并且 pk_id 全部数据都存在没有缺失可以作为序号使用，不
然，分页会有漏掉数据，

**适应场景** ：

```
适用于数据量多的情况 (元组数上万)
id 数据没有缺失，可以作为序号使用
```
###### 方法 3: 基于索引再排序

**语句样式** ：

MySQL 中, 可用如下方法:

**适应场景** ：

适用于数据量多的情况 (元组数上万).

最好 ORDER BY 后的列对象是主键或唯一索引,

id 数据没有缺失，可以作为序号使用

使得 ORDERBY 操作能利用索引被消除但结果集是稳定的

**原因** ：

索引扫描, 速度会很快.

但 MySQL 的排序操作, 只有 ASC 没有 DESC

mysql 中，索引存储的排序方式是 ASC 的，没有 DESC 的索引。

这就能够理解为啥 order by 默认是按照 ASC 来排序的了吧

虽然索引是 ASC 的，但是也可以反向进行检索，就相当于 DESC 了

###### 方法 4: 基于索引使用 prepare

**语句样式** ：MySQL 中, 可用如下方法:

```
SELECT * FROM 表名称 WHERE id_pk > (pageNum* 10 ) LIMIT M
```
```
SELECT * FROM 表名称 WHERE pk>= 1000 ORDER BY pk ASC LIMIT 0 , 20
```
```
SELECT * FROM 表名称 WHERE id_pk > (pageNum* 10 ) ORDER BY id_pk ASC LIMIT M
```

第一个问号表示 pageNum

**适应场景** ：

大数据量

**原因** ：

索引扫描，速度会很快.

prepare 语句又比一般的查询语句快一点。

###### 方法 5: 利用"子查询+索引"快速定位元组

利用"子查询+索引"快速定位元组的位置, 然后再读取元组.

比如 (id 是主键/唯一键)

利用子查询示例:

###### 方法 6: 利用"连接+索引"快速定位元组的位置, 然后再读取元组.

比如 (id 是主键/唯一键, 蓝色字体时变量)

利用连接示例:

###### 方法 7: 利用表的索引覆盖来调优

我们都知道，利用了索引查询的语句中如果只包含了那个索引列（也就是 **索引覆盖** ），那么这种情况会
查询很快。

为什么呢？

因为利用索引查找有优化算法，且数据就在查询索引上面，不用再去找相关的数据地址了，这样节省了
很多时间。另外 Mysql 中也有相关的索引缓存，在并发高的时候利用缓存就效果更好了。

在我们的例子中，我们知道 id 字段是主键，自然就包含了默认的主键索引。现在让我们看看利用覆盖索
引的查询效果如何。

这次我们之间查询最后一页的数据（利用覆盖索引，只包含 id 列），如下：

```
PREPARE stmt_name FROM SELECT * FROM 表名称 WHERE id_pk > (？* 10 ) ORDER BY id_pk
ASC LIMIT M
```
```
SELECT * FROM your_table WHERE id <= (SELECT id FROM your_table ORDER BY id desc
LIMIT ($page- 1 )*$pagesize ORDER BY id desc LIMIT $pagesize
```
```
SELECT * FROM your_table AS t 1 JOIN (SELECT id FROM your_table ORDER BY id desc
LIMIT ($page- 1 )*$pagesize AS t 2 WHERE t 1. id <= t 2. id ORDER BY t 1. id desc LIMIT
$pagesize;
```
```
select id from product limit 866613 , 20 0.2 秒
```

如果查询了所有列的 37.44 秒，这里只要 0.2 秒，提升了大概 **100 多倍** 的速度

那么如果我们也要查询所有列，有两种方法，一种是 id>=的形式，另一种就是利用 join，看下实际情
况：

查询时间为 0.2 秒！

另一种写法

查询时间也很短！

###### 方法 8 ：利用复合索引进行优化

假设数据表 collect ( id, title ,info ,vtype) 就这 4 个字段，其中 title 用定长，info 用 text, id 是逐渐，
vtype 是 tinyint，vtype 是索引。这是一个基本的新闻系统的简单模型。

现在往里面填充数据，填充 10 万篇新闻。

最后 collect 为 10 万条记录，数据库表占用硬 1.6 G。

看下面这条 sql 语句：

很快；基本上 0.01 秒就 OK，再看下面的

从 9 万条开始分页，结果？

8-9 秒完成，my god 哪出问题了？

看下面一条语句:

很快，0.04 秒就 OK。

为什么？

因为用了 id 主键做索引, 这里实现了索引覆盖（方法 7 ），当然快。

所以，可以按照方法 7 进行优化，具体如下：

再看下面的语句，带上 where

```
SELECT * FROM product WHERE ID > =(select id from product limit 866613 , 1 ) limit
20
```
```
SELECT * FROM product a JOIN (select id from product limit 866613 , 20 ) b ON a.ID
= b.id
```
```
select id, title from collect limit 1000 , 10 ;
```
```
select id, title from collect limit 90000 , 10 ;
```
```
select id from collect order by id limit 90000 , 10 ;
```
```
select id, title from collect where id>=(select id from collect order by id limit
90000 , 1 ) limit 10 ;
```

很慢，用了 8-9 秒！

注意：vtype 做了索引了啊？怎么会慢呢？vtype 做了索引是不错，如果直接对 vtype 进行过滤，比如

是很快的，基本上 0.05 秒，可是提高 90 倍，从 9 万开始，那就是 0.05*90=4.5 秒的速度了。

和测试结果 8-9 秒到了一个数量级。

其实加了 where 就不走索引，这样做还是全表扫描，解决的办法是： **复合索引** ！

加一个复合索引， search_index (vtype, id) 这样的索引。

然后测试

非常快！0.04 秒完成！再测试:

非常遗憾，8-9 秒，没走 search_index **复合索引** ，不是索引覆盖！

**综上** ：

**如果对于有 where 条件，又想走索引用 limit 的，**

**必须设计一个索引，将 where 放第一位，limit 用到的主键放第 2 位，而且只能 select 主键！**

按这样的逻辑，百万级的 limit 在 0.0 x 秒就可以分完。完美解决了分页问题了。

看来 mysql 语句的优化和索引时非常重要的！

像这种分页最大的页码页显然这种时间是无法忍受的。

**40 岁老架构师尼恩提示**

回答到了这里，已经接近满分了

但是面试，是一个需要 120 分的活儿

怎么得到 120 分呢？

可以告诉面试官：如何提升 SQL 的性能，还是要从表设计、索引设计、SQL 设计等全方位解决，具体
请看 MySQL 数据库开发的三十六条军规

接下来，就给面试官介绍一下，MySQL 数据库开发的三十六条军规

**开扯：MySQL 数据库开发的三十六条军规**

附在本文的末尾

**40 岁老架构师尼恩提示**

问题回答到这里，已经 20 分钟过去了，面试官已经爱到 “不能自已、口水直流” 啦。

```
select id from collect where vtype= 1 order by id limit 90000 , 10 ;
```
```
select id from collect where vtype= 1 limit 1000 , 10 ;
```
```
select id from collect where vtype= 1 limit 90000 , 10 ;
```
```
select id ,title from collect where vtype= 1 limit 90000 , 10 ;
```

#### 滴滴一面：order by 调优 10 倍，思路是啥？

###### 背景说明：

Mysql 调优，是大家日常常见的调优工作。

所以， **Mysql 调优** 是一个 **非常、非常核心的面试知识点** 。

在 40 岁老架构师尼恩的 **读者交流群** (50+) 中，其相关面试题是一个非常、非常高频的交流话题。

近段时间，有小伙伴面试滴滴，说遇到一个 order by 调优的面试题：

```
order by 线上的查询速度太慢，需要优化 10 倍以上，说说你的思路？
```
社群中，还遇到过大概的变种：

```
形式 1 ：order by 是怎么实现排序的？
形式 2 ：order by 是怎么实现优化的？
```
```
形式 3 ： 后面的变种，应该有很多变种........，会收入《尼恩 Java 面试宝典》。
```
这里尼恩给大家 order by 调优，做一下系统化、体系化的梳理，使得大家可以充分展示一下大家雄厚
的 “技术肌肉”， **让面试官爱到 “不能自已、口水直流”** 。

也一并把这个题目以及参考答案，收入咱们的《尼恩 Java 面试宝典》，供后面的小伙伴参考，提升大家
的 3 高架构、设计、开发水平。

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从这里获取：码云
```
回答这个 order by 的优化之前，首先要给面试官介绍一下 order by 的底层原理。

###### 首先、什么是 Order by 工作原理？

假设，有一个用户表为例，表结构如下：

表数据示例如下：

```
create table `user` (
`id` int ( 11 ) not null auto_increment COMMENT 'id',
`city` varchar ( 16 ) not null COMMENT '城市',
`name` varchar ( 16 ) not null COMMENT '姓名',
`age` int ( 11 ) not null COMMENT '年龄',
`sex` int ( 1 ) default 1 COMMENT '性别',
primary key (`id`),
key `city` (`city`)
) engine = InnoDB comment '用户表';
```

现在假定有个需求：查询前 5 个来自北京的用户姓名、年龄、城市，并且按照年龄升序排序。

那么相应的 SQL 如下：

这条 SQL 语句逻辑简单清晰，要点有 3 个：

```
city = '北京' ：有 where 查询条件
order by age ：根据 age 排序，默认是 asc
limit 5 ：取得 top 5
```
那么 mysql 底层是如何执行的呢？

首先，给大家介绍一下宏观的思路。

###### Order by 执行的两步

总体来说，Order by 执行流程，分为两步，具体如下：

```
select name, age, city from user where city = '北京' order by age limit 5 ;
```

**第 1 步：索引的查找**

根据 where 后面的字段，进行二级索引的查找，找到后再回表聚集索引，拿到需要的字段

**第 2 步：原始数据的排序**

原始数据的数据，并不是按照 order by 有序的。

所以，需要按照 order by 字段，进行排序。

接下来，排序的地点在哪里呢？

```
优先选择内存。因为内存的性能高。
如果原始的数据实在规模太大，就借助磁盘进行排序。
```
用于排序的内存，称为 sort_buffer。其实 MySQL 会给每个线程分配一块内存用于排序的
**sort_buffer** 。

了解了整个步骤后，开始来看看执行计划。

然后再看怎么优化。

###### Explain 查看执行计划

我们先用 **Explain** 关键字查看一下执行计划。

那么相应的 SQL 如下：

```
select name, age, city from user where city = '北京' order by age limit 5 ;
```

可以看到：

```
key 字段表示使用到 city 索引，
Extra 字段中的 Using index condition 表示用到了索引条件,
Using filesort 表示用到了文件排序。
```
用到文件排序，说明第一次查出来的原始数据，在内存放不下，需要借助磁盘空间进行排序，

磁盘 IO 的性能比较低的，所以，需要进行调优。

再调优之前，首先图解一下 order

###### 图解一下 Order by 执行的两步

**第 1 步：索引的查找**

根据 where 后面的字段，进行二级索引的查找，找到后再回表聚集索引，拿到需要的字段

回顾 SQL 如下：

首先从二级索引 city 索引树的查找 city = '北京' 的索引叶子。

在 city 索引树中是非聚簇索引树，叶子节点存储的是主键 ID。city 索引树如下：

聚簇索引树的叶子节点则存放的是每行数据，如下图：

```
select name, age, city from user where city = '北京' order by age limit 5 ;
```

查询语句的执行流程就是先通过 city 索引树，找到对应的主键 ID，然后再搜索主键索引树，找到对应
的行数据。

这些数据是原始数据，放在内存 sort_buffer 中。

**第 2 步：原始数据的排序**

原始数据的数据，并不是按照 order by 有序的。所以，需要按照 order by 字段，进行排序。

加上 order by 排序之后，整体的执行流程就是：

Order by 执行完整流程，如下：

```
1. 当前线程首先初始化 sort_buffer 块，
2. 然后从 city 索引树从查询一条满足 city='北京'的主键 ID, 比如图中的 id=2,
3. 接着在聚簇索引树中查询 id=2 的一行数据，将 name、age、city 三个字段的值，存到
sort_buffer ，
4. 继续重复前两个步骤，直到 city 索引树中找不到 city='北京'的主键 ID。
5. 最后在 sort_buffer 中，将所有数据根据 age 进行排序，取前 5 行返回给客户端。
```
全字段排序就是将查询所需的字段，如 name、age、city 三个字段数据全部存到 sort_buffer 中。

###### 3 个核心概念

接下来开始给面试官介绍如何调优。


不过不急，调优涉及到 3 个核心概念

```
全字段排序
外部排序
rowid 排序
```
**全字段排序**

**sort_buffer** 是 MySQL 为每个任务线程维护的一块内存区域，用于进行排序。

**sort_buffer** 的大小可以通过 **sort_buffer_size** 来设置。

那这种处理方式会存在一个问题, sort_buffer 是一块固定大小的内存，如果数据量太大，sort_buffer
放不下怎么办呢？

**sort_buffer_size** 是一个用于控制 **sort_buffer** 内存大小的参数。

**外部排序**

如果要排序的数据小于 sort_buffer_size，那在 sort_buffer 内存中就可以完成排序，如果要排序的数
据大于 sort_buffer_size，则需要外部排序，借助磁盘文件来进行排序。

通过执行一下命令，可以查看 SQL 语句执行中是否采用了磁盘文件辅助排序。

可以从 **number_of_tmp_files** 中看出，是否使用了临时文件。

```
set optimizer_trace = "enabled=on";
```
```
select name, age, city from user where city = '北京' order by age limit 5 ;
```
```
select * from information_schema. optimizer_trace;
```
```
{
"join_execution": {
"select#": 1 ,
"steps": [
{
"filesort_information": [
{
"direction": "asc",
"table": "`user`",
"field": "age"
}
],
"filesort_priority_queue_optimization": {
"limit": 5 ,
"rows_estimate": 992 ,
"row_size": 112 ,
"memory_available": 262144 ,
"chosen": true
},
"filesort_execution": [
],
"filesort_summary": {
"rows": 3 ,
```

number_of_tmp_files 表示使用来排序的磁盘临时文件数。如果 number_of_tmp_files>0，则表示使用
了磁盘文件来进行排序。

使用了磁盘临时文件后，当 sort_buffer 内存不足时，先进行排序，将排序后的数据存放到一个小磁盘
文件中，清空 sort_buffer。

然后继续存放数据到 sort_buffer，重复以上步骤。最后将多个磁盘小文件合并成一个有序的大文件。

```
Tips: 磁盘小文件合并排序，使用的是归并排序算法
```
这样依然会存在问题，数据存放到临时磁盘小文件，然后还需要归并排序为大文件，再读入到内存中，
返回结果集，整体排序效率低下。

**rowid 排序**

要解决上述问题，可以将只需要用于排序的字段和主键 ID 放入 sort_buffer 中，也就是 rowid 排序，
这样就可以在 sort_buffer 中完成排序。

**max_length_for_sort_data** 是一个用于表示 Mysql 用于排序行数据长度的一个参数，如果单行数据的
长度超过了这个值，那么可能会导致采用临时文件排序，mysql 会换用 rowid 排序。

可以通过命令看下这个参数取值。

**max_length_for_sort_data** 默认值是 1024 。本文中 name, age, city 长度=64+4+64=132<1024, 所以
走的是全字段排序。

我们执行以下命令，改下参数值，再重新执行 SQL。

使用 rowid 排序的后，执行示意图如下：

```
"examined_rows": 28 ,
"number_of_tmp_files": 0 ,
"sort_buffer_size": 720 ,
"sort_mode": "<sort_key, additional_fields>"
}
}
]
}
}
```
```
show variables like 'max_length_for_sort_data';
```
```
set max_length_for_sort_data = 32 ;
```
```
select name, age, city from user where city = '北京' order by age limit 5 ;
```

对比全字段排序，rowid 排序最后需要根据主键 ID 获取对应字段数据即多了回表查询。

```
当需要查询的数据在索引树中不存在的时候，需要再次到聚集索引中去获取，这个过程叫做回表
```
我们通过执行以下命令，可以看到是否使用了 rowid 排序的：

```
set optimizer_trace = "enabled=on";
```
```
select name, age, city from user where city = '北京' order by age limit 5 ;
```
```
select * from information_schema. optimizer_trace;
```
```
{
"join_execution": {
"select#": 1 ,
"steps": [
{
"filesort_information": [
{
"direction": "asc",
"table": "`user`",
"field": "age"
}
],
"filesort_priority_queue_optimization": {
"limit": 5 ,
"rows_estimate": 992 ,
"row_size": 8 ,
"memory_available": 262144 ,
"chosen": true
},
"filesort_execution": [],
"filesort_summary": {
"rows": 3 ,
"examined_rows": 28 ,
"number_of_tmp_files": 0 ,
"sort_buffer_size": 96 ,
"sort_mode": "<sort_key, rowid>"
}
}
]
}
```

**sort_mode** 表示排序模式为 rowid 排序。

###### order by 的优化思路

重点来了

如何调优呢？两大措施：

```
联合索引优化
如果数据本身是有序的，那就不需要排序，而索引数据本身是有序的，所以，我们可以通过建立联
合索引，跳过排序步骤。
参数优化
可以通过调整 max_length_for_sort_data 等参数优化排序算法；
```
**联合索引**

基于查询条件和排序条件，给表加个联合索引 idx_city_age。然后再查看执行计划：

从执行计划 **Extra** 字段中可以发现没有再使用 **Using filesort** 排序， **keys** 使用了 idx_city_age 索引。

```
idx_city_age 联合索引示意图，如下：
```
整个 SQL 执行流程图如下所示：

任务线程从 idx_city_age 索引树中可以有序的获取满足条件的主键 ID, 然后根据主键 ID 查询对应字段
数据，不需要在 sort_buffer 中排序了。

```
}
```
```
alter table user add index idx_city_age (city, age);
```
```
explain select name, age, city from user where city = '北京' order by age limit 5 ;
```

从示意图看来，还是有一次回表操作，那可以通过覆盖索引来解决，给 city，name，age 组成一个
联合索引，省去回表操作。

```
覆盖索引是 select 的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所
建的索引覆盖。
```
**参数优化**

通过调整参数，也可以优化 order by 的执行。

```
1. 调整 sort_buffer_size 参数的值。如果 sort_buffer 值太小而数据量大的话，MySQL 会采用磁盘
临时文件辅助排序。MySQL 服务器配置高的情况下，可以将参数调大些。
2. 调整 max_length_for_sort_data 的值，值太小的话 MySQL 会采用 rowid 排序，会多一次回表操
作导致查询性能降低。同样可以适当调大些。
```
#### 京东太狠：100 W 数据去重，该用 distinct 还是 group by，

#### 说说理由？

###### 背景说明：

Mysql 调优，是大家日常常见的调优工作。所以， **Mysql 调优** 是一个 **非常、非常核心的面试知识点** 。在
40 岁老架构师尼恩的 **读者交流群** (50+) 中，其相关面试题是一个非常、非常高频的交流话题。

近段时间，有小伙伴面试京东，说遇到一个去重调优的面试题：

```
100 W 记录去重，是用 distinct 还是 group by，说说理由？
```
MYsql 设计的时候，如何高性能进行数据去重，也是调优的重点和难点，社群中，还遇到过大概的变
种：

```
形式 1 ：distinct 效率更高还是 group by 效率更高？？
```
```
形式 2 ：为啥 group by 会导致慢 mysql 产生？
```
```
形式 3 ：........，后面的变种很多，都会收入《尼恩 Java 面试宝典》。
```
这里尼恩给大家对数据去重的调优，做一下系统化、体系化的梳理，使得大家可以充分展示一下大家
雄厚的 “技术肌肉”， **让面试官爱到 “不能自已、口水直流”** 。

也一并把这个题目以及参考答案，收入咱们的《尼恩 Java 面试宝典》V 73，供后面的小伙伴参考，提升
大家的 3 高架构、设计、开发水平。

```
注：本文以 PDF 持续更新，最新尼恩架构笔记、面试题的 PDF 文件，请从这里获取：[码云](
```
接下来，我们先来看一下 distinct 和 group by 的功能、使用和底层原理。

###### 首先：来看 distinct 功能和用法


DISTINCT 是一种用于去除 SELECT 语句返回结果中重复行的关键字。在使用 SELECT 语句查询数据
时，如果结果集中包含重复的行，可以使用 SELECT DISTINCT 语句来去除这些重复的行。例如，以下
语句返回一个包含重复行的结果集：

可以使用以下语句来去除重复的行：

这样就可以得到一个不包含重复行的结果集。需要注意的是，DISTINCT 关键字会对查询的性能产生一
定的影响，因为它需要对结果集进行排序和去重的操作。因此，在使用 DISTINCT 关键字时需要谨慎，
尽可能地使用索引来优化查询，以提高查询的性能。

###### distinct 用法

例如：

DISTINCT 关键词用于返回唯一不同的值。放在查询语句中的第一个字段前使用，且 **作用于主句所有
列** 。

注意：DISTINCT 子句将所有 NULL 值视为相同的值

```
如果列具有 NULL 值，并且对该列使用 DISTINCT 子句，MySQL 将保留一个 NULL 值，并删除其它的
NULL 值。
```
###### distinct 多列去重

distinct 多列的去重，则是根据指定的去重的列信息来进行，

即只有 **所有指定的列信息都相同** ，才会被认为是重复的信息。

```
SELECT name FROM users;
```
```
SELECT DISTINCT name FROM users;
```
```
SELECT DISTINCT columns FROM table_name WHERE where_conditions;
```
```
mysql> select distinct age from student;
+------+
| age |
+------+
| 10 |
| 12 |
| 11 |
| NULL |
+------+
4 rows in set (0.01 sec)
```
```
SELECT DISTINCT column 1, column 2 FROM table_name WHERE where_conditions;
```
```
mysql> select distinct sex, age from student;
+--------+------+
```

###### 其次：来看 GROUP BY 的使用场景

GROUP BY 主要的使用场景是在 **分组聚合** 。

具体来说，GROUP BY 子句通常用于将查询结果按照一个或多个列进行分组，然后对每个组进行聚合计
算。

例如，假设一个表存储了每个人的姓名、年龄和所在城市，可以使用 GROUP BY 子句按照城市对人进
行分组，并计算每个城市的平均年龄或人口数量等统计信息。

GROUP BY 子句通常与聚合函数（例如 COUNT、SUM、AVG、MAX 和 MIN）一起使用，以计算每个
组的聚合值。例如，可以使用 GROUP BY 子句和 COUNT 函数来计算每个城市中的人数。

但是，除了分组聚合，GROUP BY 还可以用来进行数据去重，并且在某些特定场景下， **性能超过
distinct**

需要注意的是：GROUP BY 子句会对结果集进行排序，因此可能会导致使用临时文件排序。如果查询中
包含 ORDER BY 子句，使用不当会产生临时文件排序，容易产生慢 SQL 问题。

具体该怎么优化呢？ 请持续跟踪尼恩的《尼恩 _Java_ 面试宝典》，后面结合其他的面试题进行介绍。

###### Group by 底层原理

group by 语句根据一个或多个列对结果集进行分组。在分组的列上通常配合 COUNT, SUM, AVG 等函
数一起使用。

假定有个需求：统计每个城市的用户数量。

对应的 SQL 语句如下：

执行部分结果如下：

```
| sex | age |
+--------+------+
| male | 10 |
| female | 12 |
| male | 11 |
| male | NULL |
| female | 11 |
+--------+------+
5 rows in set (0.02 sec)
```
```
select city ,count (*) as num from user group by city;
```

先用 **Explain** 查看一下执行计划

在 Extra 字段里面，我们可以看到以下信息：

```
用到了 Using temporary , 表示执行时创建了一个内部临时表。
注意这里的临时表可能是内存上的临时表，也有可能是硬盘上的临时表，当然，如果临时表比较
小，就是基于内存的，可以肯定的是： 基于内存的临时表的性能高，时间消耗肯定要比基于硬盘
的临时表的实际消耗小。
用到了 Using filesort ，表示执行过程中没有使用索引的排序，而是使用临时文件。
"Using filesort"是 MySQL 的 EXPLAIN 输出中的一个短语，表示查询需要使用临时文件对结果集进
行排序。
这可能发生在查询包括 ORDER BY 子句或 GROUP BY 子句时，而数据库无法使用索引满足排序顺
序。
使用临时文件对大型结果集进行排序可能会导致磁盘 I/O 和内存使用方面的昂贵开销，因此最好尽
可能避免“Using filesort”。
一些避免文件排序的策略包括使用适当的索引优化查询，限制结果集的大小或修改查询以使用不同
的排序算法。
```
那么 group by 语句为啥会同时用到临时表和临时文件排序呢？

首先看下整个执行流程：

```
1. 在执行过程中首先创建内存临时表，表里有 city, num 两个字段，city 为主键。
2. 扫描 user 表，依次取出一行数据，数据中 city 字段的值为 c;
```
```
如果临时表中没有主键为 c 的行，则插入一条新纪录（c ， 1 ）；
如果存在，则更新该行为 （c, num + 1)；
```
```
3. 遍历完后，再根据 city 进行排序，最后将结果集返回给客户端。
```
```
explain select city ,count (*) as num from user group by city;
```

这个流程的执行示意图如下：

然后进入到第二阶段，进行内存排序。其中对内存临时表的排序执行步骤，本质上和的 order by 流
程基本一致。

```
1. 数据从内存临时表中拷贝到 sort buffer 中
2. sort buffer 进行排序，根据实际情况采用全字段排序或 rowid 排序，
3. 排序结果写回到内存临时表中，
4. 从内存临时表中返回结果集。
```
流程示意图如下所示：

**group by 在去重场景的使用**


分为两个去重场景进行介绍：

```
单列去重
多列去重
```
单列去重和多列去重的区别在于去重的依据不同。

单列去重是指针对某一列数据进行去重，即将该列中重复的值只保留一个。例如，如果有一个包含重复
数据的姓名列表，可以使用单列去重将重复的姓名去除，只保留一个。

多列去重是指针对多列数据进行去重，即将多列数据中重复的行只保留一行。例如，如果有一个包含姓
名、年龄和所在城市的列表，可以使用多列去重将重复的姓名、年龄和城市都相同的行去除，只保留一
行。

**单列去重**

GROUP BY 子句，大部分都是用于单列去重。

例如，假设有一个表包含学生姓名和所在城市，可以使用以下 SQL 语句进行单列去重，以获取不同城
市的学生数量：

在上述语句中，使用 GROUP BY 子句对城市进行分组，并使用 COUNT 和 DISTINCT 函数计算每个城市
中不同姓名的数量。这将返回一个结果集，其中每个行包含一个城市和该城市中不同姓名的数量，从而
达到了单列去重的目的。

对于单列去重来说，group by 的使用和 distinct 类似:

单列去重语法：

执行：

注意：和 DISTINCT 子句一样，group by 将所有 NULL 值视为相同的值

```
SELECT city, COUNT (DISTINCT name) FROM student GROUP BY city;
```
```
SELECT columns FROM table_name WHERE where_conditions GROUP BY columns;
```
```
mysql> select age from student group by age;
+------+
| age |
+------+
| 10 |
| 12 |
| 11 |
| NULL |
+------+
4 rows in set (0.02 sec)
```

```
如果列具有 NULL 值，并且对该列使用 group by 子句，MySQL 将保留一个 NULL 值，并删除其它的
NULL 值。
```
**多列去重**

GROUP BY 子句也可以用于多列去重。

例如，假设有一个表包含学生姓名、所在城市和年龄，可以使用以下 SQL 语句进行多列去重，以获取
不同城市、不同年龄的学生数量：

在上述语句中，使用 GROUP BY 子句对城市和年龄进行分组，并使用 COUNT 和 DISTINCT 函数计算每
个城市、年龄组合中不同姓名的数量。这将返回一个结果集，其中每个行包含一个城市、一个年龄和该
城市、年龄组合中不同姓名的数量，从而达到了多列去重的目的。

多列去重语法：

多列去重例子：

###### group by 多列去重和单列去重的区别

GROUP BY 子句用于将结果集按照指定的列进行分组，并对每个组进行聚合操作。在 GROUP BY 子句
中指定的列将成为聚合键，用于将结果集中的行分组。 **在聚合操作之后，可以使用 GROUP BY 子句来
去重** 。

单列去重是指使用 GROUP BY 子句将结果集 **按照单个列** 进行分组，并对每个组进行聚合操作。例如，
以下查询将返回一个包含不同城市名称的列表：

```
SELECT city, age, COUNT (DISTINCT name)
FROM student
GROUP BY city, age;
```
```
SELECT column 1, column 2 FROM table_name WHERE where_conditions GROUP BY column 1,
column 2;
```
```
mysql> select sex, age from student group by sex, age;
+--------+------+
| sex | age |
+--------+------+
| male | 10 |
| female | 12 |
| male | 11 |
| male | NULL |
| female | 11 |
+--------+------+
5 rows in set (0.03 sec)
```

多列去重是指使用 GROUP BY 子句将结果集 **按照多个列** 进行分组，并对每个组进行聚合操作。例如，
以下查询将返回一个包含不同城市和州的列表：

group by 的原理是先对结果进行分组排序，然后返回 **每组中的第一条** 数据。所以，区别在于： **单列去重
只按照一个列进行分组，而多列去重则按照多个列进行分组** 。

###### distinct 和 group by 去重原理分析

在大多数例子中，DISTINCT 可以被看作是特殊的 GROUP BY，它们的实现都基于分组操作，且都可以
通过 **松散索引扫描、紧凑索引扫描** 来实现。

```
MySQL 中的松散索引扫描和紧凑索引扫描是两种不同的索引扫描方式。
```
```
松散索引扫描是指 MySQL 在查询索引时，若索引满足查询条件的最左前缀匹配，则会使用该索引
进行扫描，但是在扫描过程中可能会跳过一些索引记录，因为这些记录中并不满足查询条件的其
余部分。
```
```
紧凑索引扫描则是指 MySQL 在查询索引时，必须扫描满足查询条件的全部索引记录，不能跳过任
何记录。
```
```
松散索引扫描可以更快地定位到记录的位置，因为它可以在跳过不符合条件的记录后，直接进入
满足条件的记录范围进行搜索。但是由于可能跳过一些索引记录，因此可能会导致查询结果不准
确。
```
```
紧凑索引扫描虽然会扫描更多的索引记录，但是由于必须匹配满足条件的全部查询条件，因此可
以保证查询结果的准确性。
在实际应用中，应根据具体情况选择合适的索引扫描方式。
```
```
如果需要保证查询结果的准确性，则应使用紧凑索引扫描；
如果查询效率更为重要，可以考虑使用松散索引扫描。
```
DISTINCT 和 GROUP BY 都是 **可以使用索引进行扫描搜索** 的。

例如以下两条 sql，我们对这两条 sql 进行分析，可以看到，在 extra 中，这两条 sql 都使用了紧凑索引扫
描 Using index for group-by。

所以，在一般情况下，对于相同语义的 DISTINCT 和 GROUP BY 语句，我们可以对其使用相同的索引优化
手段来进行优化。

```
SELECT city FROM mytable GROUP BY city;
```
```
SELECT city, state FROM mytable GROUP BY city, state;
```
```
mysql> explain select int 1_index from test_distinct_groupby group by int 1_index;
+----+-------------+-----------------------+------------+-------+---------------
+---------+---------+------+------+----------+--------------------------+
| id | select_type | table | partitions | type | possible_keys
| key | key_len | ref | rows | filtered | Extra |
+----+-------------+-----------------------+------------+-------+---------------
+---------+---------+------+------+----------+--------------------------+
```

但对于 GROUP BY 来说，在 MYSQL 8.0 之前，GROUP Y 默认会依据字段进行 **隐式排序** 。

可以看到，下面这条 sql 语句在使用了临时表的同时，还进行了 filesort。

**Group by 的隐式排序**

对于隐式排序，我们可以参考 Mysql 官方的解释：

```
GROUP BY 默认隐式排序（指在 GROUP BY 列没有 ASC 或 DESC 指示符的情况下也会进行排
序）。然而，GROUP BY 进行显式或隐式排序已经过时（deprecated）了，要生成给定的排序顺
序，请提供 ORDER BY 子句。
```
```
参考连接：
MySQL :: MySQL 5.7 Reference Manual :: 8.2.1.14 ORDER BY Optimization
```
```
| 1 | SIMPLE | test_distinct_groupby | NULL | range | index_1
| index_1 | 5 | NULL | 955 | 100.00 | Using index for group-by |
+----+-------------+-----------------------+------------+-------+---------------
+---------+---------+------+------+----------+--------------------------+
1 row in set (0.05 sec)
```
```
mysql> explain select distinct int 1_index from test_distinct_groupby;
+----+-------------+-----------------------+------------+-------+---------------
+---------+---------+------+------+----------+--------------------------+
| id | select_type | table | partitions | type | possible_keys
| key | key_len | ref | rows | filtered | Extra |
+----+-------------+-----------------------+------------+-------+---------------
+---------+---------+------+------+----------+--------------------------+
| 1 | SIMPLE | test_distinct_groupby | NULL | range | index_1
| index_1 | 5 | NULL | 955 | 100.00 | Using index for group-by |
+----+-------------+-----------------------+------------+-------+---------------
+---------+---------+------+------+----------+--------------------------+
1 row in set (0.05 sec)
```
```
mysql> explain select int 6_bigger_random from test_distinct_groupby GROUP BY
int 6_bigger_random;
+----+-------------+-----------------------+------------+------+---------------
+------+---------+------+-------+----------+---------------------------------+
| id | select_type | table | partitions | type | possible_keys |
key | key_len | ref | rows | filtered | Extra |
+----+-------------+-----------------------+------------+------+---------------
+------+---------+------+-------+----------+---------------------------------+
| 1 | SIMPLE | test_distinct_groupby | NULL | ALL | NULL |
NULL | NULL | NULL | 97402 | 100.00 | Using temporary; Using filesort |
+----+-------------+-----------------------+------------+------+---------------
+------+---------+------+-------+----------+---------------------------------+
1 row in set (0.04 sec)
```

所以，在 Mysql 8.0 之前, Group by 会 **默认** 根据作用字段（Group by 的后接字段）对结果进行 **排序** 。在能
利用索引的情况下，Group by 不需要额外进行排序操作；但当无法利用索引排序时，Mysql 优化器就不
得不选择通过使用临时表然后再排序的方式来实现 GROUP BY 了。要命的是， **当临时结果集的大小超出
系统设置临时表大小时，Mysql 会将临时表数据 copy 到磁盘上面再进行操作** ，语句的执行效率会变得极
低。这也是 Mysql 选择将此操作（隐式排序） **弃用** 的原因。

基于上述原因，Mysql 在 8.0 时，对此进行了优化更新：

```
从前（Mysql 5.7 版本之前），Group by 会根据确定的条件进行隐式排序。
```
```
在 mysql 8.0 中，已经移除了这个功能，所以不再需要通过添加 order by null 来禁止隐式排序了，
但是，查询结果可能与以前的 MySQL 版本不同。
如果要生成给定顺序的结果，请按通过 ORDER BY 指定需要进行排序的字段。
```
MySQL :: MySQL 8.0 Reference Manual :: 8.2.1.16 ORDER BY Optimization

因此，我们的结论也出来了：

```
在语义相同，有索引的情况下：group by 和 distinct 都能使用索引，效率相同。因为 group by 和
distinct 近乎等价，distinct 可以被看做是特殊的 group by。
在语义相同，无索引的情况下：distinct 效率高于 group by。原因是： distinct 和 group by 都会进
行分组操作，但在 Mysql 8.0 之前 group by 会进行隐式排序，导致触发 filesort，sql 执行效率低下。
从 Mysql 8.0 开始，Mysql 就删除了隐式排序，所以，此时在语义相同，无索引的情况下，group
by 和 distinct 的执行效率也是近乎等价的。
```
总之，从 Mysql 8.0 开始，不管有索引、还是索引， group by 和 distinct 的执行效率也是近乎等价的

但是， 100 W 级数据去重场景，优先推荐使用 group by。

###### 那么，为啥要优先推荐 group by 呢？

相比于 distinct 来说，group by 的语义明确。

```
1. group by 语义更为清晰
2. group by 可对数据进行更为复杂的一些处理
3. 由于 distinct 关键字会对所有字段生效，在进行复合业务处理时，group by 的使用灵活性更高，
4. group by 能根据分组情况，对数据进行更为复杂的处理，例如通过 having 对数据进行过滤，或通
过聚合函数对数据进行运算。
```
所以，不论是 100 W 级数据去重场景，还是普通数据去重场景，建议优先选用 group by

#### 聊聊：一个 6 亿的表 a，一个 3 亿的表 b，通过外间 tid 关联，

#### 你如何最快的查询出满足条件的第 50000 到第 50200 中的这

#### 200 条数据记录。

这是一道腾讯的面试题，其实这个问题和上面是同一个问题，都是超大分页的问题, 这就像读书的时候做
数学题一样，上面是公式、定理，下面是题目，所以要学会举一反三。

1 、如果 A 表 TID 是自增长, 并且是连续的, B 表的 ID 为索引


2 、如果 A 表的 TID 不是连续的, 那么就需要使用覆盖索引. TID 要么是主键, 要么是辅助索引, B 表 ID 也需要有
索引。

###

#### 聊聊：如何选择合适的分布式主键方案呢？

```
数据库自增长序列或字段。
UUID。
Redis 生成 ID
Twitter 的 snowflake 算法
利用 zookeeper 生成唯一 ID
MongoDB 的 ObjectId
```
#### 聊聊：说一下大表查询的优化方案

```
优化 shema、sql 语句+索引；
可以考虑加缓存，memcached, redis，或者 JVM 本地缓存；
主从复制，读写分离；
分库分表；
```
#### 聊聊：百万级别或以上的数据，你是如何删除的？

```
我们想要删除百万数据的时候可以先删除索引
然后批量删除其中无用数据
删除完成后重新创建索引。
```
#### 聊聊：InnoDB 与 MyISAM 的区别？

MyISAM 和 InnoDB 两者之间还是有着明显区别

```
1. 「事务支持」
```
MyISAM 不支持事务，而 InnoDB 支持。

```
1. 「表锁差异」
```
**MyISAM** ：只支持表级锁。

**InnoDB** ：支持事务和行级锁，是 innodb 的最大特色。行锁大幅度提高了多用户并发操作的新能。但是
InnoDB 的行锁, 只有在索引上才可能是行锁，否则还是表锁。

3 ） **「索引结构」**

MyISAM 引擎使用 B+Tree 作为索引结构，叶节点的 data 域存放的是数据记录的地址，即：MyISAM 索引
文件和数据文件是分离的，MyISAM 的索引文件仅仅保存数据记录的地址。MyISAM 中索引检索的算法
为首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值
为地址，读取相应数据记录。MyISAM 的索引方式也叫做“非聚集”的。

```
select * from a, b where a.tid = b.id and a.tid>500000 limit 200;
```
```
select * from b , (select tid from a limit 50000,200) a where b.id = a .tid;
```

InnoDB 引擎也使用 B+Tree 作为索引结构，但是 InnoDB 的数据文件本身就是索引文件，叶节点 data 域保
存了完整的数据记录。这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。这种
索引叫做“聚焦索引”。InnoDB 的辅助索引的 data 域存储相应记录主键的值而不是地址。

4 ） **「表主键」**

**MyISAM** ：允许没有任何索引和主键的表存在，索引都是保存行的地址。

**InnoDB** ：如果没有设定主键或者非空唯一索引，就会自动生成一个 6 字节的主键 (用户不可见)，数据是
主索引的一部分，其它索引保存的是主索引的值。

5 ） **「表的具体行数」**

**MyISAM** ：保存有表的总行数，如果 select count (*) from table; 会直接取出出该值。

**InnoDB** ：没有保存表的总行数 (只能遍历)，如果使用 select count (*) from table；就会遍历整个表，消
耗相当大，但是在加了 where 条件后，myisam 和 innodb 处理的方式都一样。

6 ） **「外键」**

**MyISAM** ：不支持

**InnoDB** ：支持

#### 聊聊: Innodb 中文件存储分为了四个级别?

Innodb 中将文件存储分为了四个级别

```
Pages,
Extents,
Segments,
and Tablespaces
```
它们的关系是：

默认的 extent 大小为 1 M 即 64 个 16 KB 的 Page。

平常我们文件系统所说的页大小是 4 KB，包含 8 个 512 Byte 的扇区。


**插入数据**

如果我们在一个有序的字段上，建立索引，然后插入数据。 '

'在存储的时候，innodb 就会按照顺序一个个存储到页上，存满一个页再去申请新的页，然后接着存。

但如果我们的字段是无序的，存储的位置就会在不同的页上。

当我们的数据存储到一个已经被存满的页上时，就会造成页分裂，从而形成碎片。

**索引结构 B+树**

#### 聊聊：什么是索引？ Mysql 支持那些类型索引结构？

索引其实是一种 **数据结构** ，能够帮助我们快速的检索数据库中的数据。

Mysql 支持 B-tree、Hash、Full-text 等索引；

不同的存储引擎支持的索引类型也不一样：

InnoDB 支持事务，支持行级别锁定，支持 B-tree、Full-text 等索引，不支持 Hash 索引；

MyISAM 不支持事务，支持表级别锁定，支持 B-tree、Full-text 等索引，不支持 Hash 索引；

Memory 不支持事务，支持表级别锁定，支持 B-tree、Hash 等索引，不支持 Full-text 索引；

NDB 支持事务，支持行级别锁定，支持 Hash 索引，不支持 B-tree、Full-text 等索引；

Archive 不支持事务，支持表级别锁定，不支持 B-tree、Hash、Full-text 等索引；

#### 聊聊：几种不同的索引组织形式？

```
1. 聚簇索引，
如上面 B+树图所示，子节点上存储行数据，并且索引的排列的顺序和索引键值顺序一致的话就是
聚簇索引。主键索引就是聚簇索引，除了主键索引，其他所以都是辅助索引
2. 辅助索引，
如果我们创建了一个辅助索引，它的叶子节点上只存储自己的值和主键索引的值。
这就意味着，如果我们通过辅助索引查询所有数据，就会先去查找辅助索引中的主键键值，然后再
去主键索引里面，查到相关数据。这个过程称为回表
3. rowid 如果没有主键索引怎么办呢？
```

```
没有主键，但是有一个 Unique key 而且都不是 null 的，则会根据这个 key 来创建聚簇索引。
那上面两种都没有呢，别担心，innodb 自己维护了一个叫 rowid 的东西，根据这个 id 来创建聚簇
索引
```
###### 聊聊：索引如何起作用？

搞清楚什么是索引，结构是什么之后。

我们来看看，什么时候我们要用到索引，理解了这些能更好地帮助我们创建正确高效的索引

```
1. 离散度低不建索引，也就是数据之间相差不大的就没必要建立索引。
（因为建立索引，在查询的时候，innodb 大多数据都是相同的，我走索引和全表没什么差别就会
直接全表查询）。比如性别字段。这样反而浪费了大量的存储空间。
2. 联合字段索引，
比如 idx (name, class_name)
当执行 select * from stu where class_name = xx and name = lzw 查询时，也能走 idx 这个索引
的，
因为优化器将 SQL 优化为了 name = lzw and class_name = xx
当需要有 select ··· where name = lzw 的时候，不需要创建一个单独的 name 索引，会直接走 idx
这个索引覆盖索引。
如果我们此次查询的所有数据全都包含在索引里面了，就不需要再回表去查询了。
比如：select class_name from stu where name =lzw
3. 索引条件下推 (index_condition_pushdown)
有这样一条 SQL，select * from stu where name = lzw and class_name like '%xx'
如果没有索引条件下推，因为后面是 like '%xx'的查询条件，所以这里首先根据 name 走 idx 联合
索引查询到几条数据后，再回表查询到全量 row 数据，
然后在 server 层进行 like 过滤找到数据如果有，则直接在引擎层对 like 也进行过滤了，
相当于把 server 层这个过滤操作下推到引擎层了。
如图所示：
```

###### 聊聊：我们执行一句 update SQL 会发生什么？

```
1. 查询到我们要修改的那条数据，我们这里称做 origin，返给执行器
2. 在执行器中，修改数据，称为 modification
3. 将 modification 刷入内存，Buffer Pool 的 Change Buffer
4. 引擎层：记录 undo log （实现事务原子性）
5. 引擎层：记录 redo log （崩溃恢复使用）
6. 服务层：记录 bin log（记录 DDL）
7. 返回更新成功结果
8. 数据等待被工作线程刷入磁盘
```
#### 聊聊：Mysql InnoDB 引擎支持那些索引？

Mysql 支持 B-tree、Hash、Full-text 等索引；

InnoDB 引擎的索引类型有 B+树索引和 Hash 索引，默认的索引类型为 B+树索引。

InnoDB 不支持 Hash 索引；

###### Hash 索引

哈希索引是基于哈希表实现的，

Hash 索引是将一列或者多列数据值, 进行 hash 运算, 并将结果映射到数组的某个位置上.

当 hash 值冲突时, 会追加一个链表存储数据.

Hash 索引整个结构与 Java 中的 HashMap 的结果是类似的.


当我们要给某张表某列增加索引时，存储引擎会对这列进行哈希计算得到哈希码，

将哈希码的值作为哈希表的 key 值， **将指向数据行的指针作为哈希表的** value 值。

这样查找一个数据的时间复杂度就是 O (1)，一般多用于精确查找。

所以在如"=", IN, "<=>" 等值查找场景效率是非常，

由于 Hash 索引本身的特殊性, 也带来了很多限制和弊端.

比如：用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置;

多个 key 值经过哈希函数的换算，会出现同一个值的情况：处理这种情况的一种方法是，拉出一个链
表, 值得注意的是这个链表中的值并不是有序的。

所以，我们开发一般会选择 Btree，因为 Hash 会存在如下一些缺点。

Hash 索引的缺点主要如下：

1. Hash 索引只适合等值比较查找, 如"=", IN, "<=>", 不适合范围查找和匹配查找, 如 like, ">", "<", ">"等;

2. Hash 索引适合区分度很高的列, 反之, 会造成 hash 值的大量冲突, 大量数据在一个无序链表中比较查询,
性能可想而知.

3. 因为散列数组的长度是有限的, 所以 Hash 索引也只适合数据量不是很大的情况下使用.

4. Mysql 中的 Innodb 引擎是不支持手动创建 Hash 索引的, 只提供了内部优化使用的自适应哈希索引
(Adaptive Hash Index).

5. Hash 索引无法被用来避免数据的排序操作。

###### Hash 表和 B+树

hash 表通过 key 值经过 hash 算法，直接定位到数据存储地址，然后取出 value 值。

时间复杂度 O（ 1 ），找数据和存数据就需要那么一下子，就给找到了

hash 存储方式 **支持增、删、改以及随机读取操作** ，但不支持 **顺序扫描** ，对应的存储系统为 key-value 存
储系统。

对于 key-value 的插入以及查询，哈希表的复杂度都是 O (1)，明显比树的操作 O (n) 快,

简单来说，


```
如果不需要有序的遍历数据，哈希表就是最佳选择
如果需要有序的遍历数据，B+树就是最佳选择
```
###### 索引能提升与磁盘 IO 优化

另外，相对于 cpu 和内存操作，磁盘 IO 开销是非常大的，磁盘 IO 非常容易成为系统的性能瓶颈。如何减
少磁盘 IO，成为索引优化的一个重要方向。

而为什么索引能提升数据库查询效率呢？ 根本原因就在于，经过索引机制的不断优化，减少了查询过
程中的 IO 次数。

那么它是如何做到的呢？

使用 B+树。下面先简单了解一下 B 树和 B+树。

###### 从二叉树，到 B 树和 B+树的演进

要说清楚 B+树，首先从二叉树说起，一颗非常普通的树，非常容易退化为一张链表。

因为二叉树会产生退化现象，提出了平衡二叉树，

在平衡二叉树基础上，需要减少遍历高度，怎样让每一层放的节点多一些数据，来引申出 m 叉树，

m 叉搜索树同样会有退化现象，引出 **m 叉平衡树** ，也就是 B 树，

###### B 树 (平衡多路二叉树)


B 就是 Balance 平衡的第一个字母。

**B 树，又叫平衡多路查找树。一棵 m 阶的 B~树 (m 叉树) 的特性如下：**

1) 树中每个结点至多有 m 个孩子；

2) 除根结点和叶子结点外，其它每个结点至少有[m/2]个孩子；

3) 若根结点不是叶子结点，则至少有 2 个孩子；

4) 所有叶子结点都出现在同一层，叶子结点不包含任何关键字信息 (可以看做是外部接点或查询失败的
接点，实际上这些结点不存在，指向这些结点的指针都为 null)；

5) 每个非终端结点中包含有 n 个关键字信息： (n，A 0，K 1，A 1，K 2，A 2，......，Kn，An)。

其中，

```
a) Ki (i=1... n) 为关键字，且关键字按顺序排序 Ki < K (i-1)。
b) Ai 为指向子树根的接点，且指针 A (i-1) 指向子树种所有结点的关键字均小于 Ki，但都大于 K (i-
1)。
c) 关键字的个数 n 必须满足： [m/2]-1 <= n <= m-1
```
B 树是一种多叉的 AVL 树。

B-Tree 减少了 AVL 数的高度，增加了每个节点的 KEY 数量。

B 树的问题： 每个节点既放了 key 也放了 value，怎样使每个节点放尽可能多的 key 值， **以减少遍历高度
呢（访问磁盘次数）** ，

具体如何优化呢？

可以将每个节点只放 key 值，将 value 值放在叶子结点，在叶子结点的 value 值增加指向相邻节点指针，
这就是优化后的 **B+树** 。

实际应用中，每个节点的最小单元不是 KEY，而一般是按照块（BLOCK）来算。

比如磁盘文件系统 EXT 4 每块 4 KB；

数据库比如 PostgreSQL 是 8 KB，MySQL InnoDB 是 16 KB， MySQL NDB 是 32 KB 等。

上图每个节点的基本单元是一个磁盘块（BLOCK，默认 4 KB），根节点含有一个键值，其他节点含有 3
个键值，每个磁盘块包含对应的键值与数据。

比如下面的 B 树，现在要读取 KEY 为 31 的记录：


先找到根节点磁盘块（ 1 ），读入内存。（第一次 IO）；

关键字 31 大于区间（ 16 ，），根据指针 P 2 找到磁盘块 3 ，读入内存（第二次 IO）；

31 大于区间（ 20 ， 24 ， 28 ），根据指针 P 4 读取磁盘块 11 （第三次 IO），在磁盘块 11 中找到 KEY 为
31 的记录，返回结果。

实际应用中，每个节点的最小单元不是 KEY，而一般是按照块（BLOCK）来算。

三次 IO，前两次 IO 其实从磁盘读取了不必要的数据，因为只用比较 KEY，所以非叶子节点对应的
DATA 完全没有必要，如果 DATA 很大，那完全是浪费内存资源。

考虑下能否把非叶子节点的 DATA 拿掉？

###### B+树

B+Tree 是在 B-Tree 基础上的一种优化，使其更适合实现外存储索引结构，

**InnoDB 存储引擎就是用 B+Tree 实现其索引结构。**

从上一节中的 B-Tree 结构图中可以看到每个节点中不仅包含数据的 key 值，还有 data 值。

而每一个页的存储空间是有限的，如果 data 数据较大时将会导致每个节点（即一个页）能存储的 key 的
数量很小，当存储的数据量很大时同样会导致 B-Tree 的深度较大，增大查询时的磁盘 I/O 次数，进而影
响查询效率。

在 B+Tree 中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只
存储 key 值信息，这样可以大大加大每个节点存储的 key 值数量，降低 B+Tree 的高度。

**B+树：是应文件系统所需而产生的一种 B 树的变形树。**


一棵 m 阶的 B+树和 m 阶的 B-树的差异在于：

1) 有 n 棵子树的结点中含有 n 个关键字； (B~树是 n 棵子树有 n+1 个关键字)

2) 所有的叶子结点中包含了全部关键字的信息，及指向含有这些关键字记录的指针，且叶子结点本身依
关键字的大小自小而大的顺序链接。 (B~树的叶子节点并没有包括全部需要查找的信息)

3) 所有的非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键字。 (B~
树的非终节点也包含需要查找的有效信息)

B+Tree 相对于 B-Tree 有几点不同：

```
1. 非叶子节点只存储键值。
2. 所有叶子节点之间都有一个链指针。
3. 数据记录都存放在叶子节点中。
```
将上一节中的 B-Tree 优化，由于 B+Tree 的非叶子节点只存储键值信息，假设每个磁盘块能存储 4 个键值
及指针信息，则变成 B+Tree 后其结构如下图所示：

假设每个磁盘块能存储 3 个键值及指针信息，则变成 B+Tree 后其结构如下图所示：


上图一棵 B+ 树。

非叶子节点不再包含除了主键外的数据（卫星数据），数据全部放在叶子节点，并且所有叶子节点存放
在一个单向链表里，当然也可以双向链表。

可以看到， **B+ 树同时具有平衡多叉树和链表的优点，**

即可兼顾 B 树对范围查找的高效，又可兼顾链表随机写入的高效，这也是大部分数据库都用 B+ 树来存
储索引的原因。

###### B+Tree 的根本优势

```
B+Tree 的本质: 通过减少磁盘寻道次数来提高查询性能
```
通常在 B+Tree 上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节
点（即数据节点）之间是一种链式环结构。

因此可以对 B+Tree 进行两种查找运算：

```
一种是对于主键的范围查找和分页查找
另一种是从根节点开始，进行随机查找。
```
下面做一个推算：

```
InnoDB 存储引擎中页的大小为 16 KB，一般表的主键类型为 INT（占用 4 个字节）或 BIGINT（占用
8 个字节），指针类型也一般为 4 或 8 个字节，也就是说一个页（B+Tree 中的一个节点）中大概存
储 16 KB/(8 B+8 B)=1 K 个键值（因为是估值，为方便计算，这里的 K 取值为 10^3）。
```
```
也就是说一个深度为 3 的 B+Tree 索引可以维护 10^3 * 10^3 * 10^3 = 10 亿条记录。
```
实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree 的高度一般都在 2~4 层。

InnoDB 存储引擎在设计时是将根节点常驻内存的，也就是说: **查找某一键值的行记录时最多只需要 1~3
次磁盘 I/O 操作** 。

#### 聊聊: 为什么 B+树比 B 树更适合实现数据库索引？

B+Tree 相对于 B-Tree 有几点不同：

```
1. 非叶子节点只存储键值信息。
2. 所有叶子节点之间都有一个链指针。
3. 数据记录都存放在叶子节点中。
```
为什么 B+树比 B 树更适合实现数据库索引呢？原因有三：

```
1. B+ 树叶子结点之间用链表有序连接，所以扫描全部数据只需扫描一遍叶子结点，利于扫库和范围
查询；
```
B 树由于非叶子结点也存数据，所以只能通过中序遍历按序来扫。

也就是说，对于范围查询和有序遍历而言，B+ 树的效率更高。


```
2. B+ 树更相比 B 树减少了 I/O 读写的次数。
```
由于索引文件很大因此索引文件存储在磁盘上，B+ 树的非叶子结点只存关键字不存数据，因而单个页
可以存储更多的关键字，即一次性读入内存的需要查找的关键字也就越多，磁盘的随机 I/O 读取次数相
对就减少了。

```
3. B+树的查询效率更加稳定，
```
任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个
数据的查询效率相当。

#### 聊聊：索引的优缺点

**优点**

```
提高数据检索的效率，降低数据库 IO 成本。
通过索引对数据进行排序，降低数据的排序成本，降低 CPU 的消耗。
```
**缺点**

```
建立索引需要占用物理空间
会降低表的增删改的效率，因为每次对表记录进行增删改，需要进行动态维护索引，导致增删改时
间变长
```
#### 聊聊：什么情况下需要建索引？

```
1. 主键自动创建唯一索引
2. 较频繁的作为查询条件的字段
3. 查询中排序的字段，查询中统计或者分组的字段
```
#### 聊聊：什么情况下不建索引？

```
1. 表记录太少的字段
2. 经常增删改的字段
3. 唯一性太差的字段，不适合单独创建索引。比如性别，民族，政治面貌
```
#### 聊聊：MySQL 索引主要有哪几种分类？

MySQL 主要的几种索引类型：

```
普通索引
唯一索引
主键索引
组合索引
全文索引。
1. 普通索引 : 是最基本的索引，它没有任何限制
2. 唯一索引 : 索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一
3. 主键索引 : 是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。
4. 组合索引 : 一个索引包含多个列，实际开发中推荐使用组合索引。
5. 全文索引 (FULLTEXT ) : 全文搜索的索引。FULLTEXT 用于搜索很长一篇文章的时候，效果最好。
只能用于 InnoDB 或 MyISAM 表，只能为 CHAR、VARCHAR、TEXT 列创建。
```
**主键索引和唯一索引的区别** ：


主键必唯一，但是唯一索引不一定是主键；

一张表上只能有一个主键，但是可以有一个或多个唯一索引。

#### 聊聊：聚集索引、非聚集索引的区别？

###### 聚集索引介绍

**聚簇索引就是按照每张表的主键构造一颗 B+树** ，同时叶子节点中存放的就是整张表的行记录数据，也将
聚集索引的叶子节点称为数据页。这个特性决定了索引组织表中数据也是索引的一部分， **每张表只能拥
有一个聚簇索引** 。

```
如果表设置了主键，则主键就是聚簇索引
如果表没有主键，则会默认第一个 NOT NULL，且唯一（UNIQUE）的列作为聚簇索引
以上都没有，则会默认创建一个隐藏的 row_id 作为聚簇索引
```
聚集索引的叶子节点就是整张表的行记录。InnoDB 主键使用的是聚簇索引。聚集索引要比非聚集索引
查询效率高很多。

###### 非聚集索引介绍

普通索引也叫二级索引，除聚簇索引外的索引，即非聚簇索引。

对于 InnoDB 而言，普通索引的 **叶子节点** 存储的是主键（聚簇索引）的值，

对于 MyISAM 而言，普通索引的 **叶子节点** 存储的是记录指针。

**示例**

```
id 字段是聚簇索引，age 字段是普通索引（二级索引）
```
**填充数据**

```
create table user (
id int ( 10 ) auto_increment,
name varchar ( 30 ),
age tinyint ( 4 ),
primary key (id),
index idx_age (age)
) engine=innodb charset=utf 8 mb 4;
```
```
insert into user (name, age) values ('张三', 30 );
insert into user (name, age) values ('李四', 20 );
insert into user (name, age) values ('王五', 40 );
insert into user (name, age) values ('赵六', 10 );
insert into user (name, age) values ('田七', 20 );
```
```
mysql> select * from user;
+----+--------+------+
| id | name | age |
+----+--------+------+
| 1 | 张三 | 30 |
| 2 | 李四 | 20 |
| 3 | 王五 | 40 |
| 4 | 赵六 | 10 |
```

id 是主键，所以是聚簇索引，其叶子节点存储的是对应行记录的数据

###### 聚簇索引查询

如果查询条件为主键（聚簇索引），则只需扫描一次 B+树，即可通过聚簇索引定位到要查找的行记录数
据。

如：select * from user where id = 3;

```
| 5 | 田七 | 20 |
......
```

###

###### 非聚簇索引存储结构

age 是普通索引（二级索引），非聚簇索引，

非聚簇索引，其叶子节点存储的是聚簇索引的的值

###### 非聚簇索引查询

如果查询条件为普通索引（非聚簇索引），需要扫描两次 B+树，

第一次扫描通过普通索引定位到聚簇索引的值，然后第二次扫描通过聚簇索引的值定位到要查找的行记
录数据。

如：select * from user where age = 40;


```
1. 先通过普通索引 age=40 定位到主键值 id=4
2. 再通过聚集索引 id=3 定位到行记录数据 (就是上面这一步)
```
#### 聊聊：什么是回表查询？

先通过非聚簇索引定位到主键，再根据主键通过聚簇索引的定位行记录数据，就是回表查询。

先通过普通索引的值定位聚簇索引值，再通过聚簇索引的值定位行记录数据，需要扫描两次索引 B+树，
它的性能较扫一遍索引树更低。

InnoDB 聚集索引的叶子节点存储行记录，因此， InnoDB 必须要有，且只有一个聚集索引：

（ 1 ）如果表定义了主键，则 PK 就是聚集索引；
（ 2 ）如果表没有定义主键，则第一个非空唯一索引（not NULL unique）列是聚集索引；
（ 3 ）否则，InnoDB 会创建一个隐藏的 row-id 作为聚集索引；

先创建一张表，sql 语句如下：

然后，我们再执行下面的 SQL 语句，插入几条测试数据。

```
create table user (
id int ( 10 ) auto_increment,
name varchar ( 30 ),
age tinyint ( 4 ),
primary key (id),
index idx_age (age)
) engine=innodb charset=utf 8 mb 4;
```
```
insert into user (name, age) values ('张三', 30 );
insert into user (name, age) values ('李四', 20 );
insert into user (name, age) values ('王五', 40 );
```

假设，现在我们要查询出 id 为 2 的数据。

那么执行 select * from user where ID = 2; 这条 SQL 语句就不需要回表。

原因是根据主键的查询方式，则只需要搜索 ID 这棵 B+ 树。

主键是唯一的，根据这个唯一的索引，MySQL 就能确定搜索的记录。

但当我们使用 age 这个索引来查询 age = 40 的记录时就要用到回表。

原因是通过 age 这个普通索引查询方式，则需要先搜索 age 索引树，然后得到主键 ID 的值为 3 ，再到
ID 索引树搜索一次。

**这个过程虽然用了索引，但实际上底层进行了两次索引查询，这个过程就称为回表。**

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查
询。

这就是所谓的回表查询，先定位主键值，再定位行记录，它的性能较扫一遍索引树更低。

**使用聚集索引（主键或第一个唯一索引）就不会回表，普通索引就会回表。**

#### 聊聊：什么是索引覆盖？

只需要在一棵索引树上就能获取 SQL 所需的所有列数据，无需回表，速度更快。

如果我们把上面的非聚簇索引查询的 sql 改下

这个 sql 我们是不是就不用回表查询了，因为在非聚簇索引的叶子节点上已经有 id 和 age 的值。

所以根本不需要拿着 id 的值再去聚簇索引定位行记录数据了。

也就是在这一颗索引树上就可以完成对数据的检索，这样就实现了覆盖索引。

如果这个 sql 是

```
insert into user (name, age) values ('赵六', 10 );
insert into user (name, age) values ('田七', 20 );
```
```
mysql> select * from user;
+----+--------+------+
| id | name | age |
+----+--------+------+
| 1 | 张三 |  30 |
| 2 | 李四 |  20 |
| 3 | 王五 |  40 |
| 4 | 赵六 |  10 |
| 5 | 田七 |  20 |
......
```
```
select * from user where age = 40 ;
```
```
select id, age from user where age = 30 ;
```

那就不能实现索引覆盖了，因为 name 的值在 age 索引树上是没有的，还是需要拿着 id 的值再去聚簇索引
定位行记录数据。

但是如果我们对 age 和 name 做一个组合索引 idx_age_name (age, name), 那就又可以实现索引覆盖了。

#### 聊聊：如何实现索引覆盖？

**常见的方法是：将被查询的字段，建立到联合索引里去。**

例子

注意，语句中通过 index (name) 创建了 name 索引

第一个 sql：

select id, name from user where name='shenjian';

**能够命中 name 索引，索引叶子节点存储了主键 id，通过 name 的索引树即可获取 id 和 name** ，

无需回表，符合索引覆盖，效率较高。

Extra：Using index。

第二个 sql：

能够命中 name 索引， **索引叶子节点存储了主键 id，没有储存 sex，sex 字段必须回表查询才能获取到** ，

不符合索引覆盖，需要再次通过 id 值扫描聚集索引获取 sex 字段，效率会降低。

```
select id, age, name from user where age = 30 ;
```
```
create table user (
id int primary key,
name varchar ( 20 ),
sex varchar ( 5 ),
index (name)
) engine=innodb;
```
```
select id, name, sex from user where name='shenjian';
```

Extra：Using index condition。

如果把 (name) 单列索引升级为联合索引 (name, sex) 就不同了。

可以看到：

select id, name ... where name='shenjian';
select id, name, sex ... where name='shenjian';

**单列索升级为联合索引 (name, sex) 后，索引叶子节点存储了主键 id，name，sex** ，都能够命中索引覆
盖，无需回表。

画外音，Extra：Using index。

#### 聊聊：哪些场景可以利用索引覆盖来优化 SQL？

**场景 1 ：全表 count 查询优化**

原表为：
user (PK id, name, sex)；

直接：
select count (name) from user;

不能利用索引覆盖。

```
create table user 1 (
id int primary key,
name varchar ( 20 ),
sex varchar ( 5 ),
index (name, sex)
) engine=innodb;
```

添加索引：
alter table user add key (name);

就能够利用索引覆盖提效。

**场景 2 ：列查询回表优化**

这个例子不再赘述，将单列索引 (name) 升级为联合索引 (name, sex)，即可避免回表。

**场景 3 ：分页查询**

将单列索引 (name) 升级为联合索引 (name, sex)，也可以避免回表。

#### 聊聊：什么是最左匹配原则？

最左匹配原则就是指在联合索引中，如果你的 SQL 语句中用到了联合索引中的最左边的索引，那么这
条 SQL 语句就可以利用这个联合索引去进行匹配。

例如某表现有索引 (a, b, c)，现在你有如下语句：

也就是说通过最左匹配原则你可以定义一个联合索引，但是使得多数查询条件都可以用到该索引。

值得注意的是，当遇到范围查询 (>、<、between、like) 就会停止匹配。也就是：

这条语句只有 a, b 会用到索引，c 都不能用到索引。

这个原因可以从联合索引的结构来解释。

但是如果是建立 (a, c, b) 联合索引，则 a, b, c 都可以使用索引，因为优化器会自动改写为最优查询语句

```
select * from t where a= 1 and b= 1 and c = 1 ; #这样可以利用到定义的索引 （a, b, c）, 用
上 a, b, c
```
```
select * from t where a= 1 and b= 1 ; #这样可以利用到定义的索引 （a, b, c）, 用上 a, b
```
```
select * from t where b= 1 and a= 1 ; #这样可以利用到定义的索引 （a, b, c）, 用上 a, b，
mysql 有查询优化器）
```
```
select * from t where a= 1 ; #这样也可以利用到定义的索引 （a, b, c）, 用上 a
```
```
select * from t where b= 1 and c= 1 ; #这样不可以利用到定义的索引 （a, b, c）
```
```
select * from t where a= 1 and c= 1 ; #这样可以利用到定义的索引 （a, b, c），但只用上 a 索
引，b, c 索引用不到
```
```
select * from t where a= 1 and b> 1 and c = 1 ; #这样a , b 可以用到（a, b, c），c 用不到索引
```
```
select * from t where a= 1 and b > 1 and c= 1 ;  #如果是建立 (a, c, b) 联合索引，则 a, b, c 都可
以使用索引
#优化器改写为
select * from t where a= 1 and c= 1 and b > 1 ;
```

这也是最左前缀原理的一部分，索引 index 1: (a, b, c)，只会走 a, a, b, a, b, c 三类型的查询，

其实这里说的有一点问题，a, c 也走，但是只走 a 字段索引，不会走 c 字段。

另外还有一个特殊情况说明下，

这种类型的也只会有 a 与 b 走索引，c 不会走。

这种类型的 sql 语句，在 a、b 走完索引后，c 肯定是无序了，所以 c 就没法走索引，

为啥呢？ 数据库会觉得还不如全表扫描 c 字段来的快。

以 index （a, b, c）为例建立这样的索引相当于建立了索引 a、ab、abc 三个索引。

一个索引顶三个索引当然是好事，毕竟每多一个索引，都会增加写操作的开销和磁盘空间的开销。

#### 聊聊：为什么要用最左匹配呢？

就是为了使用联合索引。

最左匹配原则都是针对联合索引来说的，那么为什么要使用联合索引呢？

**一、为什么要使用联合索引？**

1 、减少开销。

建一个联合索引 (col 1, col 2, col 3)，实际相当于建了 (col 1)，(col 1, col 2)，(col 1, col 2, col 3) 三个索引。

每多一个索引，都会增加写操作的开销和磁盘空间的开销，所以，对于大量数据的表，使用 **联合索引** 会
大大的减少开销！

2 、覆盖索引。

对联合索引 (col 1, col 2, col 3)，

如果有如下的 SQL:

那么 MySQL 可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机 io 操作。

减少 io 操作，特别的随机 io 其实是 dba 主要的优化策略。

所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。

3 、效率高。

索引列越多，通过索引筛选出的数据越少。

有 1000 W 条数据的表，有如下 SQL:

```
select * from table where a = '1' and b > ‘2’ and c='3'
```
```
select * from table where a = '1' and b > ‘2’ and c='3'
```
```
select col 1, col 2, col 3 from test where col 1 =1 and col 2=2
```
```
select from table where col 1=1 and col 2=2 and col 3=3
```

假设假设每个条件可以筛选出 10%的数据，

然后再回表从 100 w 条数据中找到符合 col 2=2 and col 3=3 的数据，然后再排序，再分页；

如果是联合索引，

效率提升可想而知！

**二、最左前缀匹配原则**

对于联合索引，MySQL 会一直向右匹配，直到遇到范围查询 (＞、＜、between、like) 就停止匹配，

比如

如果建立 (a, b, c, d) 顺序的索引，d 是用不到索引的，

如果建立 (a, b, d, c) 顺序的索引，则都可以用到，a, b, d 的顺序可以任意调整。

实际上：where a=1 and b=2 等价于 where b=2 and a=1 ，

两个语句都可以使用到联合索引 (a, b)。

这是因为 MySQL 的查询优化器会判断 SQL 语句以什么样的顺序执行效率最高，当然能尽量利用索引时查
询效率最高，所以 MySQL 查询优化器会以最合理的顺序执行查询。

**三、最左匹配原则的原理**

我们都知道联合索引当然还是一颗 B+树，只不过联合索引的健值数量不是一个，而是多个。

构建一颗 B+树只能根据一个值来构建，因此数据库依据联合索引最左的字段来构建 B+树。

例子：假如创建一个 (a, b, c) 的联合索引，那么它的索引树是这样的：

```
如果只有单值索引，那么通过该索引能筛选出 1000 W*10%=100 w 条数据，
```
```
通过索引筛选出 1000 w*10%*10%*10%=1 w 条数据，
```
```
a=1 and b=2 and c＞3 and d=4
```
```
误区：
对于联合索引 (a, b)，只有 where a=1 and b=2 可以使用到索引，
而 where b=2 and a=1 无法使用索引。
```

该图就是一个形如 (a, b, c) 联合索引的 b+树，其中:

```
非叶子节点存储的是第一个关键字的索引 a，
而叶子节点存储的是三个关键字的数据。
```
**总的原则：联合索引就是按照第一列进行排序，然后第一列排好序的基础上再对第二列进行排序，以此
类推。**

```
这里可以看出 a 是有序的，
在 a 不同的前提下，而 b, c 都是无序的。
但是当在 a 相同的时候，b 是有序的，b 相同的时候，c 又是有序的。
```
通过对联合索引的结构的了解，那么就可以很好的了解为什么最左匹配原则中如果遇到范围查询就会停
止了。

以 select * from t where a=5 and b＞0 and c=1; 为例

这样 a, b 可以用到 (a, b, c)，c 不可以，

当查询到 b 的值以后，这 b 是一个范围值，在一个范围里边， c 是无序的，

所以，就不能根据联合索引，来确定到底该取哪一行。

**联合索引就是按照第一列进行排序，然后第一列排好序的基础上再对第二列进行排序，以此类推。**

**如果没有第一列直接访问第二列，第二列肯定是无序的，直接访问后面的列就用不到索引了。**

#### 聊聊：为什么要使用联合索引 ？

1 、减少开销。


建一个联合索引 (col 1, col 2, col 3)，实际相当于建了 (col 1), (col 1, col 2), (col 1, col 2, col 3) 三个索引。每多一个
索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开
销！

2 、覆盖索引。

对联合索引 (col 1, col 2, col 3)，如果有如下的 sql: select col 1, col 2, col 3 from test where col 1=1 and
col 2=2。那么 MySQL 可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机 io 操作。减少
io 操作，特别的随机 io 其实是 dba 主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提
升性能的优化手段之一。

3 、效率高。

索引列越多，通过索引筛选出的数据越少。有 1000 W 条数据的表，有如下 sql: select from table where
col 1=1 and col 2=2 and col 3=3, 假设假设每个条件可以筛选出 10%的数据，如果只有单值索引，那么通
过该索引能筛选出 1000 W 10%=100 w 条数据，然后再回表从 100 w 条数据中找到符合 col 2=2 and col 3=
3 的数据，然后再排序，再分页；如果是联合索引，通过索引筛选出 1000 w 10% 10% *10%=1 w，效率
提升可想而知！

#### 聊聊：索引失效的主要场景有哪些？

理解了上面聚集索引相对于非聚集索引的树的结构，对于什么时候索引会失效，理解起来就不那么难
了。

```
1. 组合索引未使用最左前缀，
例如组合索引（age，name），
where name='张三’不会使用索引;
2. or 会使索引失效。
如果查询字段相同，也可以使用索引。
例如 where age=20 or age=30（索引生效），
where age=20 or name=‘张三’（这里就算你 age 和 name 都单独建索引，还是一样失效）;
3. 如果列类型是字符串，不使用引号。
例如 where name=张三 (索引失效），
改成 where name=‘张三’(索引有效);
4. like 未使用最左前缀，
where A like ‘%China’;
5. 在索引列上做任何操作计算、函数，会导致索引失效而转向全表扫描;
6. 如果 mysql 估计使用全表扫描要比使用索引快, 则不使用索引;
```
#### 实操学习：MySQL 索引失效的 15 种场景

无论你是技术大佬，还是刚入行的小白，时不时都会踩到 Mysql 数据库不走索引的坑。

常见的现象就是：明明在字段上添加了索引，但却并未生效。


前些天就遇到一个稍微特殊的场景，同一条 SQL 语句，在某些参数下生效，在某些参数下不生效，这是
为什么呢？

为了方便学习和记忆，这篇文件将常见的 15 种不走索引情况进行汇总，并以实例展示，

帮助大家更好地避免踩坑。

###### 数据库及索引准备

**创建表结构**

为了逐项验证索引的使用情况，我们先准备一张表 t_user：

在上述表结构中有三个索引：

```
id：为数据库主键；
union_idx：为 id_no、username、age 构成的联合索引；
create_time_idx：是由 create_time 构成的普通索引；
```
###### 初始化数据

初始化数据分两部分：基础数据和批量导入数据。

基础数据 insert 了 4 条数据，其中第 4 条数据的创建时间为未来的时间，用于后续特殊场景的验证：

除了基础数据，还有一条存储过程及其调用的 SQL，方便批量插入数据，用来验证数据比较多的场景：

```
CREATE TABLE `t_user` (
`id` int ( 11 ) unsigned NOT NULL AUTO_INCREMENT COMMENT 'ID',
`id_no` varchar ( 18 ) CHARACTER SET utf 8 mb 4 COLLATE utf 8 mb 4_bin DEFAULT NULL
COMMENT '身份编号',
`username` varchar ( 32 ) CHARACTER SET utf 8 mb 4 COLLATE utf 8 mb 4_bin DEFAULT NULL
COMMENT '用户名',
`age` int ( 11 ) DEFAULT NULL COMMENT '年龄',
`create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
PRIMARY KEY (`id`),
KEY `union_idx` (`id_no`,`username`,`age`),
KEY `create_time_idx` (`create_time`)
) ENGINE=InnoDB DEFAULT CHARSET=utf 8 mb 4 COLLATE=utf 8 mb 4_bin;
```
```
INSERT INTO `t_user` (`id`, `id_no`, `username`, `age`, `create_time`) VALUES
(null, '1001', 'Tom 1', 11 , '2022-02-27 09:04:23');
INSERT INTO `t_user` (`id`, `id_no`, `username`, `age`, `create_time`) VALUES
(null, '1002', 'Tom 2', 12 , '2022-02-26 09:04:23');
INSERT INTO `t_user` (`id`, `id_no`, `username`, `age`, `create_time`) VALUES
(null, '1003', 'Tom 3', 13 , '2022-02-25 09:04:23');
INSERT INTO `t_user` (`id`, `id_no`, `username`, `age`, `create_time`) VALUES
(null, '1004', 'Tom 4', 14 , '2023-02-25 09:04:23');
```
```
-- 删除历史存储过程
DROP PROCEDURE IF EXISTS `insert_t_user`
```
```
-- 创建存储过程
delimiter $
```
```
CREATE PROCEDURE insert_t_user (IN limit_num int)
BEGIN
```

关于存储过程的创建和存储，可暂时不执行，当用到时再执行。

**数据库版本及执行计划**

查看当前数据库的版本：

上述为本人测试的数据库版本：8.0.18。

当然，以下的所有示例，大家可在其他版本进行执行验证。

查看 SQL 语句执行计划，一般我们都采用 explain 关键字，通过执行结果来判断索引使用情况。

执行示例：

执行结果：

explain

可以看到上述 SQL 语句使用了主键索引（PRIMARY），key_len 为 4 ；

其中 key_len 的含义为：表示索引使用的字节数，

根据这个值可以判断索引的使用情况，

特别是在组合索引的时候，判断该索引有多少部分被使用到，非常重要。

做好以上数据及知识的准备，下面就开始讲解具体索引失效的实例了。

###### 1 联合索引不满足最左匹配原则

联合索引遵从最左匹配原则，顾名思义， **在联合索引中，最左侧的字段优先匹配** 。

因此，在创建联合索引时，where 子句中使用最频繁的字段放在组合索引的最左侧。

```
DECLARE i INT DEFAULT 10 ;
DECLARE id_no varchar ( 18 ) ;
DECLARE username varchar ( 32 ) ;
DECLARE age TINYINT DEFAULT 1 ;
WHILE i < limit_num DO
SET id_no = CONCAT ("NO", i);
SET username = CONCAT ("Tom", i);
SET age = FLOOR ( 10 + RAND ()* 2 );
INSERT INTO `t_user` VALUES (NULL, id_no, username, age, NOW ());
SET i = i + 1 ;
END WHILE;
```
```
END $
-- 调用存储过程
call insert_t_user ( 100 );
```
```
select version ();
8.0.18
```
```
explain select * from t_user where id = 1 ;
```

而在查询时，要想让查询条件走索引，则需满足：最左边的字段要出现在查询条件中。

实例中，union_idx 联合索引组成：

最左边的字段为 id_no，一般情况下，只要保证 id_no 出现在查询条件中，则会走该联合索引。

**示例一** ：

explain 结果：

通过 explain 执行结果可以看出，上述 SQL 语句走了 union_idx 这条索引。

这里再普及一下 key_len 的计算：

```
id_no 类型为 varchar (18)，字符集为 utf 8 mb 4_bin，也就是使用 4 个字节来表示一个完整的 UTF-
8 。
此时，key_len = 18* 4 = 72；
由于该字段类型 varchar 为变长数据类型，需要再额外添加 2 个字节。
此时，key_len = 72 + 2 = 74；
由于该字段允许为 NULL（default NULL），需要再添加 1 个字节。
此时，key_len = 74 + 1 = 75；
```
上面演示了 key_len 一种情况的计算过程，后续不再进行逐一推演，知道基本组成和原理即可，更多情
况大家可自行查看。

**示例二** ：

explain 结果：

很显然，依旧走了 union_idx 索引，根据上面 key_len 的分析，

大胆猜测，在使用索引时，不仅使用了 id_no 列，还使用了 username 列。

**示例三** ：

explain 结果：

```
KEY `union_idx` (`id_no`,`username`,`age`)
```
```
explain select * from t_user where id_no = '1002';
```
```
explain select * from t_user where id_no = '1002' and username = 'Tom 2';
```
```
explain select * from t_user where id_no = '1002' and age = 12 ;
```

走了 union_idx 索引，但跟示例一一样，只用到了 id_no 列。

当然，还有三列都在查询条件中的情况，就不再举例了。

上面都是走索引的正向例子，也就是满足最左匹配原则的例子，下面来看看，不满足该原则的反向例
子。

**反向示例** ：

explain 结果：

explain-04

此时，可以看到未走任何索引，也就是说索引失效了。

同样的，下面只要没出现最左条件的组合，索引也是失效的：

那么，第一种索引失效的场景就是： **在联合索引的场景下，查询条件不满足最左匹配原则** 。

###### 2 使用了 select *

在《阿里巴巴开发手册》的 **ORM 映射** 章节中有一条【强制】的规范：

```
【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。说
明:1) 增加查询分析器解析成本。2) 增减字段容易与 resultMap 配置不一致。3) 无用字段增加网络
消耗，尤其是 text 类型的字段。
```
虽然在规范手册中没有提到索引方面的问题，但禁止使用 select * 语句可能会带来的附带好处就是：
某些情况下可以走覆盖索引。

比如，在上面的联合索引中，如果查询条件是 age 或 username，当使用了 select * ，肯定是不会走
索引的。

但如果希望根据 username 查询出 id_no、username、age 这三个结果（均为索引字段），明确查询结
果字段，是可以走覆盖索引的：

explain 结果：

```
explain select * from t_user where username = 'Tom 2' and age = 12 ;
```
```
explain select * from t_user where age = 12 ;
explain select * from t_user where username = 'Tom 2';
```
```
explain select id_no, username, age from t_user where username = 'Tom 2'; explain
select id_no, username, age from t_user where age = 12 ;
```

覆盖索引

无论查询条件是 username 还是 age，都走了索引，根据 key_len 可以看出使用了索引的所有列。

第二种索引失效场景： **在联合索引下，尽量使用明确的查询列来趋向于走覆盖索引** ；

这一条不走索引的情况属于优化项，如果业务场景满足，则进来促使 SQL 语句走索引。至于阿里巴巴开
发手册中的规范，只不过是两者撞到一起了，规范本身并不是为这条索引规则而定的。

**3 索引列参与运算**

直接来看示例：

explain 结果：

索引列计算

可以看到，即便 id 列有索引，由于进行了计算处理，导致无法正常走索引。

针对这种情况，其实不单单是索引的问题，还会增加数据库的计算负担。就以上述 SQL 语句为例，数据
库需要全表扫描出所有的 id 字段值，然后对其计算，计算之后再与参数值进行比较。如果每次执行都经
历上述步骤，性能损耗可想而知。

建议的使用方式是：先在内存中进行计算好预期的值，或者在 SQL 语句条件的右侧进行参数值的计算。

针对上述示例的优化如下：

第三种索引失效情况： **索引列参与了运算，会导致全表扫描，索引失效** 。

**4 索引列参使用了函数**

示例：

explain 结果：

索引-函数

上述示例中，索引列使用了函数（SUBSTR，字符串截取），导致索引失效。

此时，索引失效的原因与第三种情况一样，都是因为数据库要先进行全表扫描，获得数据之后再进行截
取、计算，导致索引索引失效。同时，还伴随着性能问题。

示例中只列举了 SUBSTR 函数，像 CONCAT 等类似的函数，也都会出现类似的情况。解决方案可参考第
三种场景，可考虑先通过内存计算或其他方式减少数据库来进行内容的处理。

第四种索引失效情况： **索引列参与了函数处理，会导致全表扫描，索引失效** 。

```
explain select * from t_user where id + 1 = 2 ;
```
```
-- 内存计算，得知要查询的 id 为 1 explain select * from t_user where id = 1 ;-- 参数侧计算
explain select * from t_user where id = 2 - 1 ;
```
```
explain select * from t_user where SUBSTR (id_no, 1 , 3 ) = '100';
```

**5 错误的 Like 使用**

示例：

explain 结果：

针对 like 的使用非常频繁，但使用不当往往会导致不走索引。

常见的 like 使用方式有：

```
方式一：like '%abc'；
方式二：like 'abc%'；
方式三：like '%abc%'；
```
其中方式一和方式三，由于占位符出现在首部，导致无法走索引。

这种情况不做索引的原因很容易理解，索引本身就相当于目录，从左到右逐个排序。

而条件的左侧使用了占位符，导致无法按照正常的目录进行匹配，导致索引失效就很正常了。

第五种索引失效情况： **模糊查询时（like 语句），模糊匹配的占位符位于条件的首部** 。

**6 类型隐式转换**

示例：

explain 结果：

```
id_no 字段类型为 varchar，但在 SQL 语句中使用了 int 类型，导致全表扫描。
```
出现索引失效的原因是：varchar 和 int 是两个种不同的类型。

解决方案就是将参数 1002 添加上单引号或双引号。

第六种索引失效情况： **参数类型与字段类型不匹配，导致类型发生了隐式转换，索引失效** 。

这种情况还有一个特例，如果字段类型为 int 类型，而查询条件添加了单引号或双引号，则 Mysql 会参数
转化为 int 类型，虽然使用了单引号或双引号：

上述语句是依旧会走索引的。

**7 、使用 OR 操作**

```
explain select * from t_user where id_no like '%00%';
```
```
explain select * from t_user where id_no = 1002 ;
```
```
explain select * from t_user where id = '2';
```

OR 是日常使用最多的操作关键字了，但使用不当，也会导致索引失效。

示例：

explain 结果：

看到上述执行结果是否是很惊奇啊，明明 id 字段是有索引的，由于使用 or 关键字，索引竟然失效了。

其实，换一个角度来想，如果单独使用 username 字段作为条件很显然是全表扫描，既然已经进行了全
表扫描了，前面 id 的条件再走一次索引反而是浪费了。所以，在使用 or 关键字时，切记两个条件都要
添加索引，否则会导致索引失效。

但如果 or 两边同时使用“>”和“<”，则索引也会失效：

explain 结果：

第七种索引失效情况： **查询条件使用 or 关键字，其中一个字段没有创建索引，则会导致整个查询语句索
引失效；or 两边为“>”和“<”范围查询时，索引失效** 。

**8 两列做比较**

如果两个列数据都有索引，但在查询条件中对两列数据进行了对比操作，则会导致索引失效。

这里举个不恰当的示例，比如 age 小于 id 这样的两列（真实场景可能是两列同维度的数据比较，这里迁
就现有表结构）：

explain 结果：

这里虽然 id 有索引，age 也可以创建索引，但当两列做比较时，索引还是会失效的。

第八种索引失效情况： **两列数据做比较，即便两列都创建了索引，索引也会失效** 。

**9 不等于比较**

示例：

```
explain select * from t_user where id = 2 or username = 'Tom 2';
```
```
explain select * from t_user where id > 1 or id < 80 ;
```
```
explain select * from t_user where id > age;
```

explain 结果：

当查询条件为字符串时，使用”<>“或”!=“作为条件查询，有可能不走索引，但也不全是。

上述 SQL 中，由于“2022-02-27 09:56:42”是存储过程在同一秒生成的，大量数据是这个时间。执行之后
会发现，当查询结果集占比比较小时，会走索引，占比比较大时不会走索引。此处与结果集与总体的占
比有关。

需要注意的是：上述语句如果是 id 进行不等操作，则正常走索引。

explain 结果：

第九种索引失效情况： **查询条件使用不等进行比较时，需要慎重，普通索引会查询结果集占比较大时索
引会失效** 。

**10 is not null**

示例：

explain 结果：

第十种索引失效情况： **查询条件使用 is null 时正常走索引，使用 is not null 时，不走索引** 。

**11 not in 和 not exists**

在日常中使用比较多的范围查询有 in、exists、not in、not exists、between and 等。

```
explain select * from t_user where id_no <> '1002';
```
```
explain select * from t_user where create_time != '2022-02-27 09:56:42';
```
```
explain select * from t_user where id != 2 ;
```
```
explain select * from t_user where id_no is not null;
```
```
explain select * from t_user where id in ( 2 , 3 );
explain select * from t_user where id_no in ('1001','1002');
explain select * from t_user u 1 where exists (select 1 from t_user u 2 where
u 2. id = 2 and u 2. id = u 1. id);
explain select * from t_user where id_no between '1002' and '1003';
```

上述四种语句执行时都会正常走索引，具体的 explain 结果就不再展示。主要看不走索引的情况：

explain 结果：

当使用 not in 时，不走索引？把条件列换成主键试试：

explain 结果：

如果是主键，则正常走索引。

第十一种索引失效情况： **查询条件使用 not in 时，如果是主键则走索引，如果是普通索引，则索引失
效** 。

再来看看 not exists：

explain 结果：

当查询条件使用 not exists 时，不走索引。

第十二种索引失效情况： **查询条件使用 not exists 时，索引失效** 。

**12 order by 导致索引失效**

示例：

explain 结果：

```
explain select * from t_user where id_no not in ('1002' , '1003');
```
```
explain select * from t_user where id not in ( 2 , 3 );
```
```
explain select * from t_user u 1 where not exists (select 1 from t_user u 2 where
u 2. id = 2 and u 2. id = u 1. id);
```
```
explain select * from t_user order by id_no ;
```

其实这种情况的索引失效很容易理解，毕竟需要对全表数据进行排序处理。

那么，添加删 limit 关键字是否就走索引了呢？

explain 结果：

结果依旧不走索引。在网络上看到有说如果 order by 条件满足最左匹配则会正常走索引，在当前
8.0.18 版本中并未出现。所以，在基于 order by 和 limit 进行使用时，要特别留意。是否走索引不仅
涉及到数据库版本，还要看 Mysql 优化器是如何处理的。

这里还有一个特例，就是主键使用 order by 时，可以正常走索引。

explain 结果：

可以看出针对主键，还是 order by 可以正常走索引。

另外，笔者测试如下 SQL 语句：

上述三条 SQL 语句都是走索引的，也就是说覆盖索引的场景也是可以正常走索引的。

现在将 id 和 id_no 组合起来进行 order by：

explain 结果：

上述两个 SQL 语句，都未走索引。

第十三种索引失效情况： **当查询条件涉及到 order by、limit 等条件时，是否走索引情况比较复杂，而
且与 Mysql 版本有关，通常普通索引，如果未使用 limit，则不会走索引。order by 多个索引字段时，
可能不会走索引。其他情况，建议在使用时进行 expain 验证。**

```
explain select * from t_user order by id_no limit 10 ;
```
```
explain select * from t_user order by id desc;
```
```
explain select id from t_user order by age; explain select id , username from
t_user order by age;
explain select id_no from t_user order by id_no;
```
```
explain select * from t_user order by id, id_no desc;
explain select * from t_user order by id, id_no desc limit 10 ;explain select *
from t_user order by id_no desc, username desc;
```

**13 参数不同导致索引失效**

此时，如果你还未执行最开始创建的存储过程，建议你先执行一下存储过程，然后执行如下 SQL：

其中，时间是未来的时间，确保能够查到数据。

explain 结果：

可以看到，正常走索引。

随后，我们将查询条件的参数换个日期：

explain 结果：

此时，进行了全表扫描。这也是最开始提到的奇怪的现象。

为什么同样的查询语句，只是查询的参数值不同，却会出现一个走索引，一个不走索引的情况呢？

答案很简单： **上述索引失效是因为 DBMS 发现全表扫描比走索引效率更高，因此就放弃了走索引** 。

也就是说，当 Mysql 发现通过索引扫描的行记录数超过全表的 10%-30%时，优化器可能会放弃走索引，
自动变成全表扫描。某些场景下即便强制 SQL 语句走索引，也同样会失效。

类似的问题，在进行范围查询（比如>、< 、>=、<=、in 等条件）时往往会出现上述情况，而上面提到
的临界值根据场景不同也会有所不同。

第十四种索引失效情况： **当查询条件为大于等于、in 等范围查询时，根据查询结果占全表数据比例的不
同，优化器有可能会放弃索引，进行全表扫描。**

**14 其他**

当然，还有其他一些是否走索引的规则，这与索引的类型是 B-tree 索引还是位图索引也有关系，就不再
详细展开。

这里要说的其他，可以总结为第十五种索引失效的情况： **Mysql 优化器的其他优化策略，比如优化器认
为在某些情况下，全表扫描比走索引快，则它就会放弃索引。**

针对这种情况，一般不用过多理会，当发现问题时再定点排查即可。

**小结**

本篇文章为大家总结了 15 个常见的索引失效的场景，由于不同的 Mysql 版本，索引失效策略也有所不
同。

大多数索引失效情况都是明确的，有少部分索引失效会因 Mysql 的版本不同而有所不同。

```
explain select * from t_user where create_time > '2023-02-24 09:04:23';
```
```
explain select * from t_user where create_time > '2022-02-27 09:04:23';
```

因此，建议收藏本文，当在实践的过程中进行对照，如果没办法准确把握，则可直接执行 explain 进行
验证。

#### 聊聊：索引的设计原则

```
索引列的区分度越高，索引的效果越好。
比如使用性别这种区分度很低的列作为索引，效果就会很差。
尽量使用短索引，对于较长的字符串进行索引时，应该指定一个较短的前缀长度，因为较小的索引
涉及到的磁盘 I/O 较少，查询速度更快。
索引不是越多越好，每个索引都需要额外的物理空间，维护也需要花费时间。
利用最左前缀原则。
长字符串创建索引时，最好是前缀索引
```
#### 聊聊：如何给长字符串, 创建如何更好的创建索引?

MySQL 中，数据和索引都是在一颗 B+树上，

我们建立索引的时候，这棵树所占用的空间越小，检索速度就会越快，

而 varchar 格式的字符串有些会很长，那么在效率为上的今天，我们如何更加合理的建立字符串的索引
呢？

假如说我们一张表中存在 email 字段，现在要给 email 字段创建索引，

```
email 字段值的格式为： zhangsan@qq.com 。
```
有 2 种建立索引的方式：

1 、直接给 email 字段建立索引：alter table t add index index 1 (email);

索引树结构为：

2 、建立 email 的前缀索引：alter table t add index index 2 (email (6));

索引数据结构为：


此时我们的查询语句为：

```
select id, name, email from t where email=' zhangsh123@xxx.com ';
```
当使用 index 1 索引时其执行步骤为：

1 、从 index 1 索引树查找索引值为 zhangsh123@xxx.com 的主键值 ID 1；

2 、根据 ID 1 回表查到该行数据确实为 zhangsh123@xxx.com ，将结果加入结果集；

3 、继续查找 index 1 索引树下一个索引值是否满足 zhangsh123@xxx.com ，不满足则结束查询。

当使用 index 2 索引时其执行步骤为：

1 、从 index 2 索引树查找索引值为 zhangs 的主键值 ID 1；

2 、根据 ID 1 回表查到该行数据确实为 zhangsh123@xxx.com ，将结果加入结果集；

3 、继续查找 index 2 索引树下一个索引值是否满足 zhangs，满足则继续回表查询该行数据是否为
zhangsh123@xxx.com ，不是则跳过继续查找；

4 、持续查找 index 2 索引树，直到索引值不是 zhangs 为止。

**从以上分析中我们可以看出，**

全字段索引相比前缀索引来说，减少了回表的次数，但是如果我们将前缀从 6 个增加到 7 个 8 个的话，前
缀索引回表的次数就会减少，

也就是说，只要定义好前缀的长度，我们就能既节省空间又保证效率。

那么问题来了，我们怎么衡量使用前缀索引的长度呢？

1 、使用 select count (distinct email) as L from t; 查询字段不同值的个数；

2 、依次选取不同的前缀长度查看不同值的个数：


然后根据实际可接受的损失比例，选取适合的最短的前缀长度。

前缀的长度问题我们解决了，但是一个问题是，如果使用前缀索引，那我们索引覆盖的特性就用不到
了。
用全字段索引时，当我们查询 select id, email from t where email=' zhangsh123@xxx.com ';
时，不用回表直接就能查到 id 和 email 字段。
但是用前缀索引时，MySQL 并不清楚前缀是否会整个覆盖 email 的值，无论是否全包含都会根据主键值
回表查询判断。
所以说，使用前缀索引虽然能节省空间保证效率但是却不能用到覆盖索引的特性，是否使用就在于具体
考虑了。

**其他字符串索引创建方式**

实际情况实际考虑，并不是所有的字符串都能使用前缀截取的方式创建索引，如身份证号或者 ip 这些字
符串使用前缀索引就不合理了，身份证号一般同一个地区的人前几位都是一模一样的，使用前缀索引就
不合理了，而 ip 值我们一般在实际中将其转化为数字去存储。

针对身份证号，我们可以使用倒叙存储，取前缀创建索引或者使用 crc 32 () 函数来获取一个 hash 校验码
（int 值）当做索引。

倒叙：select field_list from t where id_card = reverse ('input_id_card_string');
crc 32：select field_list from t where id_card_crc=crc 32 ('input_id_card_string') and
id_card='input_id_card_string'

这两种方式相对来说效率都差不多，都不支持范围查找，支持等值查找。

在倒叙方式中，需要使用 reverse 函数，但是回表次数可能比 hash 方式多。

在 hash 方式中，需要新建一个索引字段并调用 crc 32 () 函数。（注意：crc 32 () 函数获取的结果不保证能
唯一，可能存在重复的情况，但是这种情况概率较小），回表次数少，几乎 1 次就行。

**最后**

针对字符串索引，一般有以下几种创建方式：
1 、字符串较短，直接全字段索引
2 、字符串较长，且前缀区分度较好，创建前缀索引
3 、字符串较长，前缀区分度不好，倒叙或 hash 方式创建索引（这种方式范围查询就不行了）
4 、根据实际情况，遇到特殊字符串，特殊对待，如 ip。？为什么？

#### 聊聊：什么是索引下推？

**索引下推** 也被称为 **索引条件下推 （Index Condition Pushdown）ICP**

```
select
count (distinct left (email, 4)）as L 4,
count (distinct left (email, 5)）as L 5,
count (distinct left (email, 6)）as L 6,
count (distinct left (email, 7)）as L 7,
from t;
```

```
id name level tool
```
```
1 大王 1 电话
```
```
2 小王 2 手机
```
```
3 小李 3 BB 机
```
```
4 大李 4 马儿
```
MySQL 5.6 新添加的特性，用于优化数据查询的。

5.6 之前通过非主键索引查询时，存储引擎通过索引查询数据，然后将结果返回给 MySQL server 层，
在 server 层判断是否符合条件，

在以后的版本可以使用索引下推，当存在索引列作为判断条件时，Mysql server 将这一部分判断条件传
递给存储引擎，

然后 **存储引擎会筛选出符合传递传递条件的索引项** ，即在存储引擎层 **根据索引条件过滤掉不符合条件的
索引项** ，然后回表查询得到结果，将结果再返回给 Mysql server，

有了索引下推的优化，在满足一定条件下，存储引擎层会在回表查询之前对数据进行过滤，可以减少 **存
储引擎回表查询的次数** 。

假如有一张表 user, 表有四个字段 id, name, level, tool

建立联合索引（name, level）

匹配姓名第一个字为“大”，并且 level 为 1 的用户，sql 语句为

在 5.6 之前，执行流程是如下图

根据前面说的“最左前缀原则”，该语句在搜索索引树的时候，只能匹配到名字第一个字是‘大的记录，接
下来是怎么处理的呢？

当然是 ID 1 、ID 4 开始，逐个回表，到主键索引上找出相应的记录，再比对 level 这个字段的值是否符
合。

图 1 中，在 (name, level) 索引里，只是按顺序把“name 第一个字是’大’”的记录一条条取出来回表。

因此，需要回表 2 次。

```
select * from user where name like "大%" and level = 1 ；
```

但是！MySQL 5.6 引入了索引下推优化，可以在索引遍历过程中， **对索引中包含的字段先做判断，过滤
掉不符合条件的记录，减少回表字数** 。
下面图 1 、图 2 分别展示这两种情况。

5.6 及之后，执行流程图如下

图 2 跟图 1 的区别是，InnoDB 在 (name, level) 索引内部就判断了 level 是否等于 1 ，对于不等于 1 的记
录，直接判断并跳过。

在我们的这个例子中，只需要对 ID 1 、ID 4 这两条记录回表取数据判断，就只需要回表 1 次。

使用索引下推后由两次回表变为一次，提高了查询效率。

###### 总结

如果没有索引下推优化（或称 ICP 优化），

当进行索引查询时， **首先根据索引来查找记录，然后再根据 where 条件来过滤记录** ；

在支持 ICP 优化后，MySQL 会在取出索引的同时， **判断是否可以进行 where 条件过滤再进行索引查询** ，

也就是说提前执行 where 的部分过滤操作，在某些场景下，可以大大减少回表次数，从而提升整体性
能。

## 京东、阿里一面：Mysql 索引 15 个夺命连环炮

在疯狂创客圈的社群里边，经常有小伙伴 **面试 JD、阿里、美团、滴滴、网易等大厂，面完之后做面试
题交流。**

**其中，Mysql 索引，是面试中的绝对重点和难点**


现围绕这个重点，梳理了 15 个夺命连环炮，

以后遇到这个主题，大家再也不用怕了。

从下一题开始，夺命连环炮开始：

#### 001 聊聊: Mysql 如何实现的索引机制？

MySQL 中索引分三类：B+树索引、Hash 索引、全文索引

#### 002 聊聊: InnoDB 索引与 MyISAM 索引实现的区别是什

#### 么？

MyISAM 的索引方式都是非聚簇的，与 InnoDB 包含 1 个聚簇索引是不同的。

```
在 InnoDB 存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找就能找到对应的记录，
而在 MyISAM 中却需要进行一次回表操作，意味着 MyISAM 中建立的索引相当于全部都是二级索引
。
```
InnoDB 的数据文件本身就是索引文件，而 MyISAM 索引文件和数据文件是分离的，索引文件仅保存数
据记录的地址。

MyISAM 的表在磁盘上存储在以下文件中：

```
*. sdi（描述表结构） 、
.MYD （数据） ，
.MYI（索引）
```
InnoDB 的表在磁盘上存储在以下文件中： .ibd（表结构、索引和数据都存在一起）

InnoDB 的非聚簇索引 data 域存储相应记录主键的值，而 MyISAM 索引记录的是地址。

换句话说，InnoDB 的所有非聚簇索引都引用主键作为 data 域。

MyISAM 的回表操作是十分快速的，因为是拿着地址偏移量直接到文件中取数据的，反观 InnoDB 是通过
获取主键之后再去聚簇索引里找记录，虽然说也不慢，但还是比不上直接用地址去访问。

InnoDB 要求表必须有主键 （ MyISAM 可以没有 ）。

如果没有显式指定，则 MySQL 系统会自动选择一个可以非空且唯一标识数据记录的列作为主键。

如果不存在这种列，则 MySQL 自动为 InnoDB 表生成一个隐含字段作为主键，这个字段⻓度为 6 个字
节，类型为⻓整型。


#### 003 一个表中如果没有创建索引，那么还会创建 B+树吗？

会

如果有主键会创建聚簇索引

如果没有主键会生成 rowid 作为隐式主键

#### 004 说一下 B+树索引实现原理（数据结构）

假设有一个表 index_demo，表中有 2 个 INT 类型的列， 1 个 CHAR (1) 类型的列，c 1 列为主键：

index_demo 表的简化的行格式示意图如下：

```
CREATE TABLE index_demo (c 1 INT, c 2 INT, c 3 CHAR ( 1 ), PRIMARY KEY (c 1)) ;
```

我们只在示意图里展示记录的这几个部分：

```
record_type： 表示记录的类型， 0 是普通记录、 2 是最小记录、 3 是最大记录、 1 是 B+树非叶
子节点记录。
next_record： 表示下一条记录的相对位置，我们用箭头来表明下一条记录。
各个列的值： 这里只记录在 index_demo 表中的三个列，分别是 c 1 、 c 2 和 c 3 。
其他信息： 除了上述 3 种信息以外的所有信息，包括其他隐藏列的值以及记录的额外信息。
```
将其他信息项暂时去掉并把它竖起来的效果就是这样：

把一些记录放到页里的示意图就是（这里一页就是一个磁盘块，代表一次 IO）：


name age sex

```
MySQL InnoDB 的默认的页大小是 16 KB ，因此数据存储在磁盘中，可能会占用多个数据页。
```
如果各个页中的记录没有规律，我们就不得不依次遍历所有的数据页。

如果我们想快速的定位到需要查找的记录在哪些数据页中，我们可以这样做 ：

```
下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值
给所有的页建立目录项
```
以页 28 为例，它对应目录项 2 ，这个目录项中包含着该页的页号 28 以及该页中用户记录的最小主键
值 5 。

我们只需要把几个目录项在物理存储器上连续存储（比如：数组），就可以实现根据主键值快速查找某
条记录的功能了。

比如：查找主键值为 20 的记录，具体查找过程分两步：

```
1. 先从目录项中根据二分法快速确定出主键值为 20 的记录在目录项 3 中 （因为 12 ≤ 20 < 209），对
应页 9 。
2. 再到页 9 中根据二分法快速定位到主键值为 20 的用户记录。
```

至此，针对数据页做的简易目录就搞定了。这个目录有一个别名，称为索引。

**InnoDB 中的索引方案**

我们新分配一个编号为 30 的页来专⻔存储目录项记录，页 10 、 28 、 9 、 20 专⻔存储用户记录 ：

目录项记录和普通的用户记录的不同点：

```
目录项记录的 record_type 值是 1 ，而普通用户记录的 record_type 值是 0 。
目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，包含很多
列，另外还有 InnoDB 自己添加的隐藏列。
```
现在查找主键值为 20 的记录，具体查找过程分两步：

```
1. 先到页 30 中通过二分法快速定位到对应目录项，因为 12 ≤ 20 < 209 ，就是页 9 。
2. 再到页 9 中根据二分法快速定位到主键值为 20 的用户记录。
```
更复杂的情况如下：

我们生成了一个存储更高级目录项的页 33 ，这个页中的两条记录分别代表页 30 和页 32 ，如果用户记录
的主键值在 [1, 320) 之间，则到页 30 中查找更详细的目录项记录，如果主键值不小于 320 的话，就到页
32 中查找更详细的目录项记录。这个数据结构，它的名称是 B+树。


#### 005 聚簇索引与非聚簇索引 b+树实现有什么区别？

**聚簇索引**

特点：

```
索引和数据保存在同一个 B+树中
页内的记录是按照主键的大小顺序排成一个单向链表。
页和页之间也是根据页中记录的主键的大小顺序排成一个双向链表。
非叶子节点存储的是记录的主键+页号。
叶子节点存储的是完整的用户记录。
```
**优点** ：

```
数据访问更快，因为索引和数据保存在同一个 B+树中，因此从聚簇索引中获取数据比非聚簇索引
更快。
聚簇索引对于主键的排序查找和范围查找速度非常快。
```
按照聚簇索引排列顺序，查询显示一定范围数据的时候，由于数据都是紧密相连，数据库可以从更少的
数据块中提取数据，节省了大量的 IO 操作。

**缺点** ：

```
插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重
影响性能。因此，对于 InnoDB 表，我们一般都会定义一个自增的 ID 列为主键。
更新主键的代价很高，因为将会导致被更新的行移动。因此，对于 InnoDB 表，我们一般定义主键
为不可更新。
```
限制：

```
只有 InnoDB 引擎支持聚簇索引， MyISAM 不支持聚簇索引。
由于数据的物理存储排序方式只能有一种，所以每个 MySQL 的表只能有一个聚簇索引。
如果没有为表定义主键，InnoDB 会选择非空的唯一索引列代替。如果没有这样的列，InnoDB 会
隐式的定义一个主键作为聚簇索引。
```

```
为了充分利用聚簇索引的聚簇特性，InnoDB 中表的主键应选择有序的 id ，不建议使用无序的 id，
比如 UUID、MD 5、HASH、字符串作为主键，无法保证数据的顺序增⻓。
```
**非聚簇索引**

（二级索引、辅助索引）

聚簇索引，只能在搜索条件是主键值时才发挥作用，因为 B+树中的数据都是按照主键进行排序的，如
果我们想以别的列作为搜索条件，那么需要创建非聚簇索引。

例如，以 c 2 列作为搜索条件，那么需要使用 c 2 列创建一棵 B+树，如下所示：

这个 B+树与聚簇索引有几处不同：

```
页内的记录是按照从 c 2 列的大小顺序排成一个单向链表。
页和页之间也是根据页中记录的 c 2 列的大小顺序排成一个双向链表。
非叶子节点存储的是记录的 c 2 列+页号。
叶子节点存储的并不是完整的用户记录，而只是 c 2 列+主键这两个列的值。
```
一张表可以有多个非聚簇索引：


#### 006 说一下 B+树中聚簇索引的查找（匹配）逻辑

如果查询条件为主键（聚簇索引），则只需扫描一次 B+树，即可通过聚簇索引定位到要查找的行记录数
据。

如：select * from index_demo where c 1= 12;


#### 007 说一下 B+树中非聚簇索引的查找（匹配）逻辑

需要扫描两次 B+树，

第一次扫描通过普通索引定位到聚簇索引的值，

然后第二次扫描通过聚簇索引的值定位到要查找的行记录数据。

如：select * from index_demo where c 2= 4;

```
1. 先通过普通索引 c 2= 4 定位到主键值 4 、10.
2. 再通过聚集索引 c 1=4、 10 定位到行记录数据.
```
**下面的图中，展示在 C 2 列的非聚簇索引** ：根据 c 2 列的值查找 c 2=4 的记录，查找过程如下：

```
1. 根据根页面 44 定位到页 42 （因为 2 ≤ 4 < 9 ）
2. 由于 c 2 列没有唯一性约束，所以 c 2=4 的记录可能分布在多个数据页中，又因为 2 ≤ 4 ≤ 4 ，所以
确定实际存储用户记录的页在页 34 和页 35 中。
3. 在页 34 和 35 中定位到具体的记录。
4. 但是这个 B+树的叶子节点只存储了 c 2 和 c 1（主键） 两个列，所以我们必须再根据主键值去聚簇
索引中再查找一遍完整的用户记录。
```

#### 008 平衡二叉树，红黑树，B 树和 B+树的区别是什么？都有

#### 哪些应用场景？

**平衡二叉树**

```
基础数据结构
左右平衡
高度差大于 1 会自旋
每个节点记录一个数据
```
**平衡二叉树（AVL）**

AVL 树全称G.M. Adelson-Velsky 和E.M. Landis，这是两个人的人名。

平衡二叉树也叫平衡二叉搜索树（Self-balancing binary search tree）又被称为 AVL 树，可以保证查询
效率较高。

具有以下特点：

```
它是一棵空树或它的左右两个子树的高度差的绝对值不超过 1
并且左右两个子树都是一棵平衡二叉树。
```
AVL 的生成演示：https://www.cs.usfca.edu/~galles/visualization/AVLtree.html


**AVL 的问题**

众所周知，IO 操作的效率很低，在大量数据存储中，查询时我们不能一下子将所有数据加载到内存中，
只能逐节点加载（一个节点一次 IO）。如果我们利用二叉树作为索引结构，那么磁盘的 IO 次数和索引
树的高度是相关的。平衡二叉树由于树深度过大而造成磁盘 IO 读写过于频繁，进而导致效率低下。

为了提高查询效率，就需要减少磁盘 IO 数。为了减少磁盘 IO 的次数，就需要尽量降低树的高度，需要
把原来“瘦高”的树结构变的“矮胖”，树的每层的分叉越多越好。针对同样的数据，如果我们把二叉树改
成三叉树：

上面的例子中，我们将二叉树变成了三叉树，降低了树的高度。如果能够在一个节点中存放更多的数
据，我们还可以进一步减少节点的数量，从而进一步降低树的高度。这就是多叉树。

**普通树的问题**

```
左子树全部为空，从形式上看，更像一个单链表，不能发挥 BST 的优势。
解决方案：平衡二叉树 (AVL)
```

**红黑树**

```
hashmap 存储
两次旋转达到平衡
分为红黑节点
```
在这个棵严格的平台树上又进化为“红黑树”{是一个非严格的平衡树左子树与右子树的高度差不能超过
1}，红黑树的⻓子树只要不超过短子树的两倍即可！

当再次插入 7 的时候，这棵树就会发生旋转


**B+ 树和 B 树的差异** ：

B+树中非叶子节点的关键字也会同时存在在子节点中，并且是在子节点中所有关键字的最大值（或最
小）。

B+树中非叶子节点仅用于索引，不保存数据记录，跟记录有关的信息都放在叶子节点中。而 B

树中，非叶子节点既保存索引，也保存数据记录。

B+树中所有关键字都在叶子节点出现，叶子节点构成一个有序链表，而且叶子节点本身按照

关键字的大小从小到大顺序链接。

#### 009 一个 b+树中大概能存放多少条索引记录？

```
真实环境中一个页存放的记录数量是非常大的（默认 16 KB），假设指针与键值忽略不计（或看做
10 个字节），数据占 1 kb 的空间：
如果 B+树只有 1 层，也就是只有 1 个用于存放用户记录的节点，最多能存放 16 条记录。
如果 B+树有 2 层，最多能存放 1600×16=25600 条记录。
如果 B+树有 3 层，最多能存放 1600×1600×16=40960000 条记录。
如果存储千万级别的数据，只需要三层就够了
```
B+树的非叶子节点不存储用户记录，只存储目录记录，相对 B 树每个节点可以存储更多的记录，树的高
度会更矮胖，IO 次数也会更少。

#### 010 使用 B+树存储的索引 crud 执行效率如何？

c 新增

O (lognN)

N = 高度

#### 011 什么是自适应哈希索引？

自适应哈希索引是 Innodb 引擎的一个特殊功能，当它注意到某些索引值被使用的非常频繁时，会在内
存中基于 B-Tree 所有之上再创建一个哈希索引，这就让 B-Tree 索引也具有哈希索引的一些优点，比如快
速哈希查找。这是一个完全自动的内部行为，用户无法控制或配置


使用命令

查看 INSERT BUFFER AND ADAPTIVE HASH INDEX

#### 012 什么是 2-3 树 2-3-4 树？

多叉树（multiway tree）允许每个节点可以有更多的数据项和更多的子节点。2-3 树，2-3-4 树就是多
叉树，多叉树通过重新组织节点，减少节点数量，增加分叉，减少树的高度，能对二叉树进行优化。

**2-3 树**

下面 2-3 树就是一颗多叉树

2-3 树具有如下特点：

```
2-3 树的所有叶子节点都在同一层。
有两个子节点的节点叫二节点，二节点要么没有子节点，要么有两个子节点。
有三个子节点的节点叫三节点，三节点要么没有子节点，要么有三个子节点。
2-3 树是由二节点和三节点构成的树。
对于三节点的子树的值大小仍然遵守 BST 二叉排序树的规则。
```
**2-3-4 树**

```
SHOW ENGINE INNODB STATUS \G ;
```

#### 013 为什么官方建议使用自增长主键作为索引？（说一下

#### 自增主键和字符串类型主键的区别和影响）

```
自增主键能够维持底层数据顺序写入
读取可以由 b+树的二分查找定位
支持范围查找，范围数据自带顺序
```
字符串无法完成以上操作

#### 014 使用 int 自增主键后最大 id 是 10 ，删除 id 10 和 9 ，再添

#### 加一条记录，最后添加的 id 是几？删除后重启 mysql 然后

#### 添加一条记录最后 id 是几？

删除之后

```
如果重启，会从最大的 id 开始递增
如果没重启，会延续删除之前最大的 id 开始递增
```
#### 聊聊： 索引的优缺点是什么？

**优点**

聚簇（主键）索引：

```
顺序读写
范围快速查找
范围查找自带顺序
```
非聚簇索引：

```
条件查询避免全表扫描 scan
范围，排序，分组查询返回行 id，排序分组后，再回表查询完整数据，有可能利用顺序读写
覆盖索引不需要回表操作
```
**索引的代价**

索引是个好东西，可不能乱建，它在空间和时间上都会有消耗：

```
空间上的代价
```

每建立一个索引都要为它建立一棵 B+树，每一棵 B+树的每一个节点都是一个数据页，一个页默认会占
用 16 KB 的存储空间，一棵很大的 B+树由许多数据页组成，那就是很大的一片存储空间。

```
时间上的代价
```
每次对表中的数据进行增、删、改操作时，都需要去修改各个 B+树索引。而增、删、改操作可能会对
节点和记录的排序造成破坏，所以存储引擎需要额外的时间进行一些记录移位、页面分裂、页面回收等
操作来维护好节点和记录的排序。如果我们建了许多索引，每个索引对应的 B+树都要进行相关的维护
操作，会给性能拖后腿。

B 树和 B+ 树都可以作为索引的数据结构，在 MySQL 中采用的是 B+ 树。

但 B 树和 B+树各有自己的应用场景，不能说 B+树完全比 B 树好，反之亦然。

#### 聊聊：使用索引一定能提升效率吗？

不一定

```
少量数据全表扫描也很快，可以直接获取到全量数据
唯一索引会影响插入速度，但建议使用
索引过多会影响更新，插入，删除数据速度
```
#### 聊聊：如果是大段文本内容，如何创建（优化）索引？

B 树和 B+ 树都可以作为索引的数据结构， **在 MySQL 中采用的是 B+ 树。**

第一种方式是分表存储，然后创建索引

第二是使用 es 为大文本创建索引

#### 聊聊：一个表中可以有多个（非）聚簇索引吗？

聚簇索引只能有一个

非聚簇索引可以有多个

#### 聊聊： CRUD 时聚簇索引与非聚簇索引的区别是什么？

```
聚簇索引插入新值比采用非聚簇索引插入新值的速度要慢很多，因为插入要保证主键不能重复
聚簇索引范围，排序查找效率高，因为是有序的
非聚簇索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据
```
#### 聊聊：非聚簇索引为什么不存数据地址值而存储主键？

因为聚簇索引中有时会引发分页操作、重排操作数据有可能会移动

#### 聊聊：什么是回表操作？

```
id age name sex
age -> index
select * from user where age > 20 ;
```

第一次取回 id，第二次（回表）根据 id 拿到完整数据

#### 聊聊：什么是覆盖索引？

第一次取回 id，第二次（回表）根据 id 拿到完整数据

覆盖索引不会回表查询，查询效率也是比较高的

#### 聊聊：非聚集索引一定回表查询吗?

不一定，只要 b+树中包含的字段（创建索引的字段），覆盖（包含）想要 select 的字段，那么就不

会回表查询了。

#### 聊聊：为什么要回表查询？直接存储数据不可以吗？

为了控制非聚簇索引的大小

#### 聊聊：如果把一个 InnoDB 表的主键删掉，是不是就没有

#### 主键，就没办法进行回表查询了？

不是，InnoDB 会生成 rowid 辅助回表查询

#### 聊聊：什么是联合索引，组合索引，复合索引？

联合索引（也叫组合索引、复合索引、多列索引）是指对表上的多个列进行索引。

联合索引的创建方法跟单个索引的创建方法一样，不同之处仅在于有多个索引列。

开讲之前我们先弄一张学生表，表数据如下：

```
select * from user where age > 20 ;
```
```
id age name sex
age -> index
select * from user where age > 20 ;
```
```
age, name -> index
select age from user where age > 20 and name like"张%" ;
```

下面我们给出一个需求：查询表中以字母"L"开头的姓名及年龄。

**1 、常规的写法（回表查询）**

这种写法，明显效率是低下的，我们用 explain 分析一下：

由图中可以看出，在数据库中进行了全表扫描。下面我们看一下数据库中的执行过程。

**2. 优化写法（索引覆盖）**
因为我们要查询 name 和 age。所以，我们对 name 和 age 建立了联合索引，建立后的索引图如下：

从图中我们可以看出，

叶子节点中的键值都是按顺序存储的并且都包含了名字和年龄，

即 (“Ann”, 36)、(“HanMeimei”, 17)、(“Kate”, 17)、(“LiLei”, 18)、(“Lili”, 16)、(“Lisa”, 19)、(“Lucy”, 17)、
(“WeiHua”, 32)、(“ZhangWei”, 18)、(“ZhangWei”, 25)。

索引会先根据 name 排序，如果 name 相同，再根据 age 进行排序。

我们对 name 和 age 建立索引后，当我们查询 name 和 age 二个字段时，直接会从索引中查出来而不需要
回表查询，这种方式就是索引覆盖。执行步骤是这样的：

```
SELECT name, age FROM `t_user` where name like 'l%' ;
```
```
第一步：全表扫描数据，找出以“l”开头的主键 id.
第二步：将所有查询出来的数据每一个都回表，根据 id 来查询出想要的数据。
```

这种方式是不是高效多了，你要是还不信，我们用 explain 看一下，如下图：

从图中我们看的出，使用了（name, age）索引。

#### 聊聊：复合索引创建时字段顺序不一样，使用效果一样

#### 吗？

我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，

比方说我们想让 B+树按照 c 2 和 c 3 列的大小进行排序，这个包含两层含义：

```
先把各个记录和页按照 c 2 列进行排序。
在记录的 c 2 列相同的情况下，采用 c 3 列进行排序
B+树叶子节点处的记录由 c 2 列、c 3 列和主键 c 1 列组成
本质上也是二级索引
```
#### 聊聊： 什么是唯一索引？

```
随表一起创建索引：
```
```
单独建创索引：
```
```
第一步：使用联合索引（name，age）查询以“l”开头的数据
第二步：在索引中取出 name 和 age.
```
```
EXPLAIN SELECT name, age FROM `t_user` where name like 'l%' ;
```
```
create index idx_c 2_c 3 on user (c 2, c 3);
```
```
CREATE TABLE customer (
id INT UNSIGNED AUTO_INCREMENT,
customer_no VARCHAR ( 200 ),
customer_name VARCHAR ( 200 ),
PRIMARY KEY (id), -- 主键索引：列设定为主键后会自动建立索引，唯一且不能为空。
UNIQUE INDEX uk_no (customer_no), -- 唯一索引：索引列值必须唯一，允许有 NULL 值，且
NULL 可能会出现多次。
KEY idx_name (customer_name), -- 普通索引：既不是主键，列值也不需要唯一，单纯的为了提
高查询速度而创建。
KEY idx_no_name (customer_no, customer_name) -- 复合索引：即一个索引包含多个列。
);
```

#### 聊聊： 唯一索引是否影响性能？

是

#### 聊聊：什么时候使用唯一索引？

业务需求唯一字段的时候，一般不考虑性能问题

. 【强制】业务上具有唯一特性的字段，即使是多个字段的组合，也必须建成唯一索引。

说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的；

另外，即使在应用层做了非常完善的校验控制，只要没有唯一索引，根据墨菲定律，必然有脏数据产
生。

#### 聊聊： 什么时候适合创建索引，什么时候不适合创建索

#### 引？

**适合创建索引**

```
频繁作为 where 条件语句查询字段
关联字段需要建立索引
排序字段可以建立索引
分组字段可以建立索引 (因为分组前提是排序)
统计字段可以建立索引（如.count (), max ()）
```
**不适合创建索引**

```
频繁更新的字段不适合建立索引
where，分组，排序中用不到的字段不必要建立索引
可以确定表数据非常少不需要建立索引
参与 mysql 函数计算的列不适合建索引
```
**创建索引时避免有如下极端误解：**

1 ）宁滥勿缺。认为一个查询就需要建一个索引。

2 ）宁缺勿滥。认为索引会消耗空间、严重拖慢更新和新增速度。

3 ）抵制惟一索引。认为业务的惟一性一律需要在应用层通过“先查后插”方式解决。

#### 聊聊：有哪些情况会导致索引失效？

```
CREATE TABLE customer 1 (
id INT UNSIGNED,
customer_no VARCHAR ( 200 ),
customer_name VARCHAR ( 200 )
);
```
```
ALTER TABLE customer 1 ADD PRIMARY KEY customer 1 (id); -- 主键索引
CREATE UNIQUE INDEX uk_no ON customer 1 (customer_no); -- 唯一索引
CREATE INDEX idx_name ON customer 1 (customer_name); -- 普通索引
CREATE INDEX idx_no_name ON customer 1 (customer_no, customer_name); -- 复合索引
```

```
计算、函数导致索引失效
```
```
LIKE 以%，_ 开头索引失效
```
```
拓展：Alibaba《Java 开发手册》
```
```
【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。
```
```
不等于 (!= 或者<>) 索引失效
```
```
IS NOT NULL 失效和 IS NULL
```
**注意** ：当数据库中的数据的索引列的 NULL 值达到比较高的比例的时候，即使在 IS NOT NULL 的情况下
MySQL 的查询优化器会选择使用索引，此时 type 的值是 range（范围查询）

```
类型转换导致索引失效
```
复合索引未用左列字段失效

如果 mysql 觉得全表扫描更快时（数据少）;

#### 聊聊：为什么 LIKE 以%开头索引会失效？

表结构：id, name, age

使用：name 创建索引

**场景 1 ： 没有索引覆盖场景**

查询一：select * from user where name like '%明'

explain 之后，发现 type=all

```
-- 显示查询分析
EXPLAIN SELECT * FROM emp WHERE emp. name LIKE 'abc%';
EXPLAIN SELECT * FROM emp WHERE LEFT (emp. name, 3 ) = 'abc'; --索引失效
```
```
EXPLAIN SELECT * FROM emp WHERE name LIKE '%ab%'; --索引失效
```
```
EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp. name = 'abc' ;
EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp. name <> 'abc' ; --索引失效
```
```
EXPLAIN SELECT * FROM emp WHERE emp. name IS NULL;
EXPLAIN SELECT * FROM emp WHERE emp. name IS NOT NULL; --索引失效
```
```
-- 将 id>20000 的数据的 name 值改为 NULL
UPDATE emp SET `namè = NULL WHERE ìd` > 20000 ;
-- 执行查询分析，可以发现 IS NOT NULL 使用了索引
-- 具体多少条记录的值为 NULL 可以使索引在 IS NOT NULL 的情况下生效，由查询优化器的算法决定
EXPLAIN SELECT * FROM emp WHERE emp. name IS NOT NULL
```
```
EXPLAIN SELECT * FROM emp WHERE name='123';
EXPLAIN SELECT * FROM emp WHERE name= 123 ; --索引失效
```

说明，以%开头，索引其实并不会完全失效，以上查询， type=all 不是覆盖索引， **这个场景，索引失效**

覆盖索引没有生效的时，explain 会直接 type=all

**场景 2 ：覆盖索引场景**

select name, id from user where name like '%明'

type=index

explain 之后，在覆盖索引下会出现 type=index，

表示遍历了索引树，再回表查询， **这个场景，索引没失效**

没有高效使用索引是因为字符串索引会逐个转换成 accii 码，生成 b+树时按首个字符串顺序排序，

类似复合索引未用左列字段失效一样，跳过开始部分也就无法使用生成的 b+树了

#### 聊聊：一个表有多个索引的时候，能否手动选择使用哪个

#### 索引？

不可用手动直接干预，只能通过 mysql 优化器自动选择

#### 聊聊：如何查看一个表的索引？

#### 聊聊： 能否查看到索引选择的逻辑？是否使用过

#### optimizer_trace？

#### 聊聊：多个索引优先级是如何匹配的？

```
1. 主键（唯一索引）匹配
2. 全值匹配（单值匹配）
3. 最左前缀匹配
4. 范围匹配
5. 索引扫描
6. 全表扫描
```
一般性建议

```
对于单键索引，尽量选择过滤性更好的索引（例如：手机号，邮件，身份证）
在选择组合索引的时候，过滤性最好的字段在索引字段顺序中，位置越靠前越好。
选择组合索引时，尽量包含 where 中更多字段的索引
组合索引出现范围查询时，尽量把这个字段放在索引次序的最后面
尽量避免造成索引失效的情况
```
```
show index from t_emp; // 显示表上的索引
explain select * from t_emp where id= 1 ; // 显示可能会用到的索引及最终使用的索引
```
```
set session optimizer_trace="enabled=on", end_markers_in_json=on;
SELECT * FROM information_schema. OPTIMIZER_TRACE;
set session optimizer_trace="enabled=off";
```

#### 聊聊：使用 Order By 时能否通过索引排序？

没有过滤条件不走索引

#### 聊聊：通过索引排序内部流程是什么？

select name, id from user where name like '%明' order by name；

select name, id，age from user where name like '%明'

关键配置：

```
sort_buffer 可供排序的内存缓冲区大小
max_length_for_sort_data 单行所有字段总和限制，超过这个大小启动双路排序
```
```
1. 通过索引检过滤筛选条件索到需要排序的字段+其他字段（如果是符合索引）
2. 判断索引内容是否覆盖 select 的字段
i. 如果覆盖索引，select 的字段和排序都在索引上，那么在内存中进行排序，排序后输出结果
ii. 如果索引没有覆盖查询字段，接下来计算 select 的字段是否超过 max_length_for_sort_data 限
制，如果超过，启动双路排序，否则使用单路
```
#### 聊聊：什么是双路排序和单路排序

单路排序：一次取出所有字段进行排序，内存不够用的时候会使用磁盘

双路排序：取出排序字段进行排序，排序完成后再次回表查询所需要的其他字段

如果不在索引列上，filesort 有两种算法： mysql 就要启动双路排序和单路排序

**双路排序（慢）**

```
MySQL 4.1 之前是使用双路排序，字面意思就是两次扫描磁盘，最终得到数据，读取行指针和
order by 列，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对
应的数据输出
从磁盘取排序字段，在 buffer 进行排序，再从磁盘取其他字段。
取一批数据，要对磁盘进行两次扫描，众所周知，I\O 是很耗时的，所以在 mysql 4.1 之后，出现了
第二种改进的算法，就是单路排序。
```
**单路排序（快）**

从磁盘读取查询需要的所有列，按照 order by 列在 buffer 对它们进行排序，然后扫描排序后的列表进行
输出，它的效率更快一些，避免了第二次读取数据。

并且把随机 IO 变成了顺序 IO，但是它会使用更多的空间，因为它把每一行都保存在内存中了。

**结论及引申出的问题**

但是用单路有问题

在 sort_buffer 中，单路比多路要多占用很多空间，因为单路是把所有字段都取出,

所以有可能取出的数据的总大小超出了 sort_buffer 的容量，导致每次只能取 sort_buffer 容量大小的数
据，进行排序（创建 tmp 文件，多路合并），排完再取 sort_buffer 容量大小，再排...... 从而多次 I/O。

```
Select id, age, name from stu order by name;
```

单路本来想省一次 I/O 操作，反而导致了大量的 I/O 操作，反而得不偿失。

**优化策略**

```
增大 sort_buffer_size 参数的设置
增大 max_length_for_sort_data 参数的设置
减少 select 后面的查询的字段。禁止使用 select *
```
提高 Order By 的速度

```
1. Order by 时 select * 是一个大忌。只 Query 需要的字段，这点非常重要。在这里的影响是：
```
```
当 Query 的字段大小总和小于 max_length_for_sort_data 而且排序字段不是 TEXT|BLOB 类型
时，会用改进后的算法⸺单路排序，否则用老算法⸺多路排序。
两种算法的数据都有可能超出 sort_buffer 的容量，超出之后，会创建 tmp 文件进行合并排序，导
致多次 I/O，但是用单路排序算法的⻛险会更大一些，所以要提高 sort_buffer_size。
```
```
2. 尝试提高 sort_buffer_size
```
不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对
每个进程（connection）的 1 M-8 M 之间调整。 MySQL 5.7 和 8.0，InnoDB 存储引擎默认值是 1048576
字节，1 MB。

```
3. 尝试提高 max_length_for_sort_data
```
提高这个参数，会增加用改进算法的概率。

但是如果设的太高，数据总容量超出 sort_buffer_size 的概率就增大，明显症状是高的磁盘 I/O 活动和
低的处理器使用率。如果需要返回的列的总⻓度大于 max_length_for_sort_data，使用双路算法，否则
使用单路算法。1024-8192 字节之间调整

#### 聊聊：group by 分组和 order by 在索引使用上有什么区

#### 别？

group by 使用索引的原则几乎跟 order by 一致，唯一区别：

```
group by 先排序再分组，遵照索引建的最佳左前缀法则
group by 没有过滤条件，也可以用上索引。Order By 必须有过滤条件才能使用上索引。
```
#### 聊聊：如果表中有字段为 null，又被经常查询该不该给这

#### 个字段创建索引？

应该创建索引，使用的时候尽量使用 is null 判断。

IS NOT NULL 失效和 IS NULL

```
SHOW VARIABLES LIKE '%sort_buffer_size%';
```
```
SHOW VARIABLES LIKE '%max_length_for_sort_data%';
#5 . 7 默认 1024 字节
#8 . 0 默认 4096 字节
```
```
EXPLAIN SELECT * FROM emp WHERE emp. name IS NULL;
EXPLAIN SELECT * FROM emp WHERE emp. name IS NOT NULL; --索引失效
```

**注意：** 当数据库中的数据的索引列的 NULL 值达到比较高的比例的时候，即使在 IS NOT NULL 的情

况下 MySQL 的查询优化器会选择使用索引，此时 type 的值是 range（范围查询）

#### 聊聊： 有字段为 null 索引是否会失效？

不一定会失效，每一条 sql 具体有没有使用索引可以通过 trace 追踪一下

最好还是给上默认值

数字类型的给 0 ，字符串给个空串“”，

参考上一题

#### 聊聊：MySQL 索引使用的注意事项？

MySQL 索引通常是被用于提高 WHERE 条件的数据行匹配时的搜索速度，

在索引的使用过程中，存在一些使用细节和注意事项。

**1. 不要在列上使用函数和进行运算**

不要在列上使用函数，这将导致索引失效而进行全表扫描。

为了使用索引，防止执行全表扫描，可以进行改造。

还有一个建议，不要在列上进行运算，这也将导致索引失效而进行全表扫描。

为了使用索引，防止执行全表扫描，可以进行改造。

**2. 尽量避免使用 != 或 not in 或 <> 等否定操作符**

应该尽量避免在 where 子句中使用 != 或 not in 或 <> 操作符，

因为这几个操作符都会导致索引失效而进行全表扫描。

```
-- 将 id>20000 的数据的 name 值改为 NULL
UPDATE emp SET `namè = NULL WHERE ìd` > 20000 ;
-- 执行查询分析，可以发现 IS NOT NULL 使用了索引
-- 具体多少条记录的值为 NULL 可以使索引在 IS NOT NULL 的情况下生效，由查询优化器的算法决定
EXPLAIN SELECT * FROM emp WHERE emp. name IS NOT NULL
```
```
select * from news where year (publish_time) < 2017
```
```
select * from news where publish_time < '2017-01-01'
```
```
select * from news where id / 100 = 1
```
```
select * from news where id = 1 * 100
```

**尽量避免使用 or 来连接条件**

应该尽量避免在 where 子句中使用 or 来连接条件，因为这会导致索引失效而进行全表扫描。

**3. 多个单列索引并不是最佳选择、当查询条件为多个的时候，可以采用复合索引**

MySQL 只能使用一个索引，会从多个索引中选择一个限制最为严格的索引，

因此，为多个列创建单列索引，并不能提高 MySQL 的查询性能。

假设，有两个单列索引，分别为 news_year_idx (news_year) 和 news_month_idx (news_month)。

现在，有一个场景需要针对资讯的年份和月份进行查询，那么，SQL 语句可以写成：

事实上，MySQL 只能使用一个单列索引。

为了提高性能，可以使用复合索引 news_year_month_idx (news_year, news_month) 保证 news_year
和 news_month 两个列都被索引覆盖。

**4 、复合索引的最左前缀原则**

复合索引遵守“最左前缀”原则，即在查询条件中使用了复合索引的第一个字段，索引才会被使用。

因此，在复合索引中索引列的顺序至关重要。

如果不是按照索引的最左列开始查找，则无法使用索引。

假设，有一个场景只需要针对资讯的月份进行查询，那么，SQL 语句可以写成：

此时，无法使用 news_year_month_idx (news_year, news_month) 索引，因为遵守“最左前缀”原则，
在查询条件中没有使用复合索引的第一个字段，索引是不会被使用的。

**覆盖索引的好处**

如果一个索引包含所有需要的查询的字段的值，直接根据索引的查询结果返回数据，而无需读表，能够
极大的提高性能。

因此，可以定义一个让索引包含的额外的列，即使这个列对于索引而言是无用的。

**5 、范围查询对多列查询的影响**

查询中的某个列有范围查询，则其右边所有列都无法使用索引优化查找。

举个例子，假设有一个场景需要查询本周发布的资讯文章，其中的条件是必须是启用状态，且发布时间
在这周内。那么，SQL 语句可以写成：

```
select * from news where id = 1 or id = 2
```
```
select * from news where news_year = 2017 and news_month = 1
```
```
select * from news where news_month = 1
```
```
select * from news where publish_time >= '2017-01-02' and publish_time <= '2017-
01-08' and enable = 1
```

这种情况下，因为范围查询对多列查询的影响，将导致 news_publish_idx (publish_time, enable) 索引
中 publish_time 右边所有列都无法使用索引优化查找。

换句话说，news_publish_idx (publish_time, enable) 索引等价于 news_publish_idx (publish_time) 。

对于这种情况，我的建议：对于范围查询，务必要注意它带来的副作用，并且尽量少用范围查询，可以
通过曲线救国的方式满足业务场景。

例如，上面案例的需求是查询本周发布的资讯文章，因此可以创建一个 news_weekth 字段用来存储资
讯文章的周信息，使得范围查询变成普通的查询，SQL 可以改写成：

然而，并不是所有的范围查询都可以进行改造，对于必须使用范围查询但无法改造的情况，

建议：不必试图用 SQL 来解决所有问题，可以使用其他数据存储技术控制时间轴，

例如 Redis 的 SortedSet 有序集合保存时间，或者通过缓存方式缓存查询结果从而提高性能。

**6 、索引不会包含有 NULL 值的列**

只要列中包含有 NULL 值，都将不会被包含在索引中，复合索引中只要有一列含有 NULL 值，那么这一
列对于此复合索引就是无效的。

因此，在数据库设计时，除非有一个很特别的原因使用 NULL 值， **不然尽量不要让字段的默认值为
NULL** 。

**8 、隐式转换的影响**

当查询条件左右两侧类型不匹配的时候会发生隐式转换，

隐式转换带来的影响就是可能导致索引失效而进行全表扫描。

下面的案例中，date_str 是字符串，然而匹配的是整数类型，从而发生隐式转换。

因此，要谨记隐式转换的危害，时刻注意通过同类型进行比较。

**9 、like 语句的索引失效问题**

like 的方式进行查询，在 like “value%” 可以使用索引，但是对于 like “%value%” 这样的方式，执行全
表查询，

这在数据量小的表，不存在性能问题，但是对于海量数据，全表扫描是非常可怕的事情。

所以，根据业务需求，考虑使用 ElasticSearch 或 Solr 是个不错的方案。

#### 聊聊：如何创建有效的索引？

**1. 如果需要索引很长的字符串，此时需要考虑前缀索引**

前缀索引即选择所需字符串的一部分前缀作为索引，这时候，需要引入一个概念叫做索引选择性，

```
select * from news where news_weekth = 1 and enable = 1
```
```
select * from news where date_str = 201701
```

索引选择性是指不重复的索引值与数据表的记录总数的比值，可以看出索引选择性越高则查询效率越
高，

当索引选择性为 1 时，效率是最高的，

但是在这种场景下，很明显索引选择性为 1 的话我们会付出比较高的代价，索引会很大，

这时候我们就需要选择字符串的一部分前缀作为索引，通常情况下一列的前缀作为索引选择性也是很高
的

**如何选择前缀：计算该列完整列的选择性，使得前缀选择性接近于完整列的选择性**

**2 、使用多列索引**

尽量不要为多列上创建单列索引，因为这样的情况下最多只能使用一个索引，

这样的话，不如去创建一个全覆盖索引，在多列上创建单列索引大部分情况下并不能提高 MySQL 的查
询性能，

MySQL 5.0 中引入了合并索引，在一定程度上可以表内多个单列索引来定位指定的结果，

但是 5.0 以前的版本，如果 where 中的多个条件是基于多个单列索引，那么 MySQL 是无法使用这些索
引的，这种情况下，还不如使用 union。

**3 、选择合适的索引列顺序**

**经验是将选择性最高的列放到索引最前列，可以在查询的时候过滤出更少的结果集。**

但这样并不总是最好的，如果考虑到 group by 或者 order by 等情况，再比如考虑到一些特别场景下的
guest 账号等数据情况，上面的经验法则可能就不是最适用的

**4 、覆盖索引**

**所谓覆盖索引就是指索引中包含了查询中的所有字段，这种情况下就不需要再进行回表查询了**

覆盖索引对于 MyISAM 和 InnoDB 都非常有效，可以减少系统调用和数据拷贝等时间.

Tips：减少 select * 操作

**5 、使用索引扫描来做排序**

MySQL 生成有序的结果有两种方法：通过 **排序操作** ，或者 **按照索引顺序扫描** ；

使用 **排序操作需要占用大量的 CPU 和内存资源，而使用 index 性能是很好的** ，所以，当我们查询有序
结果时，尽量使用 **索引顺序扫描来生成有序结果集** 。

怎样保证使用索引顺序扫描？

```
索引列顺序和 ORDER BY 顺序一致
所有列的排序方向一致
如果关联多表，那么只有当 ORDER BY 子句引用的字段全部为第一张表时，才能使用索引做排
序，限制依然是需要满足索引的最左前缀要求
```
**6 、压缩索引**

MyISAM 中使用了前缀压缩技术，会减少索引的大小，可以在内存中存储更多的索引，这部分优化默认
也是只针对字符串的，但是可以自定义对整数做压缩。

这个优化在一定情况下性能比较好，但是对于某些情况可能会导致更慢，因为前缀压缩决定了每个关键
字都必须依赖于前面的值，所以无法使用二分查找等，只能顺序扫描，所以如果查找的是逆序那么性能
可能不佳。


**7 、减少重复、冗余以及未使用的索引**

MySQL 的唯一限制和主键限制都是通过索引实现的，所以不需要在同一列上增加主键、唯一限制再创
建索引，这样是重复索引

再举个例子，如果已经创建了索引 (A，B)，那么再创建索引 (A) 的话，就属于重复索引，因为 MySQL 索
引是最左前缀，所以索引 (A，B) 本身就可以使用索引 (A)，但是创建索引 (B) 的话不属于重复索引

尽量减少新增索引，而应该扩展已有的索引，因为新增索引可能会导致 INSERT、UPDATE、DELETE 等
操作更慢

可以考虑删除没有使用到的索引，定位未使用的索引，有两个办法，在 Percona Server 或者 MariaDB
中打开 userstates 服务器变量，然后等服务器运行一段时间后，通过查询
INFORMATION_SCHEMA. INDEX_STATISTICS 就可以查询到每个索引的使用频率

**8 、索引和锁**

InnoDB 支持行锁和表锁，默认使用行锁，而 MyISAM 使用的是表锁，

所以使用索引可以让查询锁定更少的行，这样也会提升查询的性能，

如果查询中锁定了 1000 行，但实际只是用了 100 行，

那么在 5.1 之前都需要提交事务之后才能释放这些锁，5.1 之后可以在服务器端过滤掉行之后就释放
锁，不过依然会导致一些锁冲突

**9 、减少索引和数据碎片**

首先我们需要了解一下为什么会产生碎片，比如 InnoDB 删除数据时，这一段空间就会被留空，

如果一段时间内大量删除数据，就会导致留空的空间比实际的存储空间还要大，这时候如果进行新的插
入操作时，MySQL 会尝试重新使用这部分空间，但是依然无法彻底占用，这样就会产生碎片

产生碎片带来的后果当然是，降低查询性能，因为这种情况会导致随机磁盘访问

可以通过 OPTIMIZE TABLE 或者重新导入数据表来整理数据

#### 聊聊：使用索引优化查询问题？

1 、创建单列索引还是 **多列索引** ？

如果查询语句中的 where、order by、group 涉及多个字段，一般需要创建多列索引，

比如：

2 、多列索引的顺序如何选择？

一般情况下，把 **选择性高的字段** 放在前面，

比如：查询 sql：

```
select * from user where nick_name = 'ligoudan' and job = 'dog';
```
```
select * from user where age = '20' and name = 'zh' order by nick_name;
```

这时候如果建索引的话，首字段应该是 age，因为 age 定位到的数据更少，选择性更高。

但是务必注意一点，满足了 **某个查询场景** 就可能导致 **另外一个查询场景更慢** 。

3 、避免使用 **范围查询**

很多情况下，范围查询都可能导致无法使用索引。

4 、尽量避免查询不需要的数据

同样的查询，不同的返回值，第二个就可以使用覆盖索引，第一个只能全表遍历了。

5 、查询的数据类型要正确

第一条语句就可以使用 create_date 的索引，第二个就不可以。

#### 美团一面：InndoDB 单表最多 2000 W，为什么？

在疯狂创客圈的社群面试交流中，有很多小伙伴在面大厂，经常遇到下面的问题：

**前段时间，有一个小伙伴在面美团里的时候，又遇到了此问题，关键是他说面挂了** 。

其实，这些问题，都是在考察大家对 B+树底层原理的理解

尼恩在这里，给大家做一个系统化、起底式、全方位的梳理。

```
explain select * from user where job like 'ligoudan%';
explain select job from user where job like 'ligoudan%';
```
```
explain select * from user where create_date >= now ();
explain select * from user where create_date >= '2020-05-01 00:00:00';
```
```
问题 1 ：在实际生产环境中，InnoDB 中一棵 B+ 树索引一般有多少层？
```
```
问题 2 ：在实际生产环境中，InnoDB 一棵 B+树可以存放多少行数据？
```
```
问题 3 ：MySQL 对于千万级的大表，为啥要优化？
```
```
问题 4 ：mysql 单表最好不要超过 2000 w？
```
```
问题 5 ：单表超过 2000 w 就要考虑数据迁移了，这个是为啥？
```
```
问题 6 ：你这个表数据都马上要到 2000 w 了，难怪查询速度慢，为什么？
```
```
问题 N： ... 第 100 个变种
```

按照下面的套路去答，基本可以拿到 _120_ 分。

大家收藏起来，多读几遍，然后面试的时候，一定能让面试官刮目相看，当然不会那么容易面挂。

要回答这些问题，在回答的时候，首先从 InnoDB 索引数据结构、数据组织方式说起。

###### InnoDB 索引数据结构、数据组织方式

磁盘扇区、文件系统、InnoDB 存储引擎都有各自的最小存储单元。

来看看三个重要的最小单元

```
磁盘上，存储数据最小单元是扇区，一个扇区的大小是 512 字节，
文件系统（例如 EXT 4）, 最小单元是块 （block），一个 block 块的大小是 4 k，
InnoDB 存储引擎的最小储存单元——页（Page），一个页的大小是 16 K。
```
来一个图，更清楚：

以上三个数据，非常重要，一定要背下来，并且在面试的时候，找个机会背出来。

面试官一听，感觉你的计算机底层知识是多么的扎实，好感慢慢。这是来自老架构师的尼恩衷心提
示。

由于文件系统（例如 EXT 4）的最小单元是块 （block），一个 block 块的大小是 4 k，

所以，假设一个文件大小只有 1 个字节，那么，这个文件在磁盘上，还是不得不占 4 KB 的空间。

具体如下图：


要知道，Innodb 的所有数据文件（后缀为 ibd 的文件），也是存储在磁盘的，当然也是由 block 组成，

所以，Innodb 的所有数据文件，全部都是 16384 （16 k）的整数倍。

具体如下图：

InnoDB 存储引擎的最小储存单元——页（Page），一个页的大小是 16 K，

在 MySQL 中我们的 InnoDB 页的大小当然也可以通过参数设置的，具体如下图：


通过上图，可以看到，在 MySQL 中我们的 **InnoDB 页的大小默认是 16 k**

###### InnoDB 一棵 B+树的查找流程

数据表中的数据都是存储在页中的，所以一个页中能存储多少行数据呢？

先做一个假设：

假设一行数据的大小是 1 k，

那么一个 16 K 页，可以存放 16 行这样的数据。

如果数据库只按这样的方式存储，那么如何查找数据就成为一个问题

因为我们不知道要查找的数据存在哪个页中，也不可能把所有的页遍历一遍，那样太慢了。

所以人们想了一个办法，用 B+树的方式组织这些数据。

```
B+ 树的叶子节点存储真正的记录，对应的文件系统 page 页面，可以叫做数据页
B+ 树的非叶子节点存放的是键值 + 指针，其对应的文件系统 page 页面，可以叫做索引页
```
这里用指针来描其实述不是太准确，准确来说是 **页的偏移量** ，不过借用指针一词，更好理解

如图所示：

InnoDB 中主键索引 B+树是如何组织数据、查询数据的？

我们总结一下：

1 、在 B+树中叶子节点存放数据，非叶子节点存放键值+指针。


InnoDB 存储引擎的最小存储单元是页，页可以用于存放数据也可以用于存放键值+指针，

2 、页内的记录是有序的，所以可以使用二分查找在页内到下一层的目标页面的指针

```
从根页开始，首先通过非叶子节点的二分查找法，
确定数据在下一层哪个页之后，一层一层往下找，一直到非叶子节点，
进而在非叶子节（数据页）中查找到需要的数据；
```
那么回到我们开始的问题，通常一棵 B+树可以存放多少行数据？

###### 高为 2 的 B+树可以存放多少行数据

首先，需要计算出非叶子节点能存放多少指针？

**页作为 InnoDB 磁盘管理的最小单位，不仅可以用来存放具体的行数据，还可以存放键值和指针** 。

回到文题，我们先从简单的入手，

这里我们先假设 B+树高为 2 ，即存在一个根节点和若干个叶子节点，那么 B+ 树只有两层。

如下图：

**那么对于这棵 B+ 树能够存放多少行数据，其实问的就是这棵 B+ 树的非叶子节点中存放的数据量** ，

那么，这棵简单的 B+ 树能够存放多少行数据，其实问的就是这棵 B+ 树的非叶子节点中存放的数据
量，

这棵 B+树的存放总记录数为, 是一个简单的公式：

每个叶子节点存放的行记录数就是每页存放的记录数，由于各个数据表中的字段数量都不一样，

这里我们就不深究叶子节点的存储结构了，

简单按照一行记录的数据大小为 1 k 来算的话（实际上现在很多互联网业务数据记录大小通常就是 1 K
左右），

一页（16 K）或者说一个叶子节点可以存放 16 行这样的数据。

那么，这颗 B+ 树的非叶子节点（ 唯一的）能够存储多少数据呢？

**非叶子节点里面存的是主键值 + 指针**

为了方便分析，这里我们把一个主键值 + 一个指针称为一个单元，

```
我们假设主键的类型是 BigInt，长度为 8 字节，
而指针大小在 InnoDB 中设置为 6 字节，
```
这样一个单元，一共 14 字节。

```
非叶子节点指针数 * 每个叶子节点存放的行记录数
```

这样的话，一页或者说一个非叶子节点能够存放 16384 / 14=1170 个这样的单元。

也就是说一个非叶子节点中能够存放 1170 个指针，即对应 1170 个叶子节点，

所以对于这样一棵高度为 2 的 B+ 树，能存放 1170 （一个非叶子节点中的指针数） * 16（一个叶子节
点中的行数）= 18720 行数据。

当然，这样分析其实不是很严谨，InnoDB 数据页结构，不全是主键值 + 一个指针，还有其他的一些
元数据。

按照《MySQL 技术内幕：InnoDB 存储引擎》中的定义，InnoDB 数据页结构包含如下几个部分：

###### 高为 3 的 B+树可以存放多少行数据

分析完高度为 2 的 B+ 树，同样的道理，我们来看高度为 3 的：


根页（page 10）可以存放 1170 个指针，

然后第二层的每个页（page: 11,12,13）也都分别可以存放 1170 个指针。

这样一共可以存放 1170 * 1170 个指针，

即对应的有 1170 * 1170 个非叶子节点，

所以，高为 3 的 B+树一共可以存放 1170 * 1170 * 16 = 21902400 行记录。

回到问题，InnoDB 一棵 B+ 树可以存放多少行数据？

这个问题的简单回答是： **约 2 千万** 。

###### 怎么得到 InnoDB 主键索引 B+树的高度？

在 InnoDB 引擎中，实际的情况如何呢？

在 InnoDB 的表空间文件中，约定 **page number 为 3** 的代表主键索引的根页，而在根页偏移量为 **64** 的地
方存放了该 B+树的 page level。

如果 page level 为 1 ，树高为 2 ，page level 为 2 ，则树高为 3 。

即 B+树的高度=page level+1；

下面我们将从实际环境中尝试找到这个 page level。

实验环境中，这三张表的数据量如下：


从图中可以看到，一个表 600 W，一个表 15 W，一个表 5 行数据。

在实际操作之前，你可以通过 InnoDB 元数据表确认主键索引根页的 page number 为 3 ，你也可以从
《InnoDB 存储引擎》这本书中得到确认。

说明：
information_schema 是 mysql 自带的一个信息数据库，其保存着关于 mysql 服务器所维护的所有其他数
据库的信息，如数据库名，数据库的表，表栏的数据类型与访问权限等。

```
innodb_sys_indexes：innodb 表的索引的相关信息
innodb_sys_tables：表格的格式和存储特性，包括行格式，压缩页面大小位级别的信息
```
执行结果：

```
SELECT
b.name, a.name, index_id, type, a.space, a.PAGE_NO
FROM
information_schema. INNODB_SYS_INDEXES a,
information_schema. INNODB_SYS_TABLES b
WHERE
a.table_id = b.table_id AND a.space <> 0;
```

可以看出数据库 dbt 3 下的 customer 表、lineitem 表主键索引根页的 page number 均为 3 ，而其他的二级
索引 page number 为 4 。

在进行深入分析之前，首先回顾一下基础知识

###### 回顾：主键索引和二级索引

**什么是主键索引 (Primary Key)?**

数据表的主键列使用的就是主键索引。一张数据表有只能有一个主键，并且主键不能为 null，不能重
复。

在 MySQL 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一
索引的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6 Byte 的自增主
键。

**什么是二级索引 (辅助索引)?**

二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，
可以定位主键的位置。常用的二级索引包括：

```
1. 唯一索引 (Unique Key) ：唯一索引也是一种约束。唯一索引的属性列不能出现重复的数据，但是
允许数据为 NULL，一张表允许创建多个唯一索引。建立唯一索引的目的大部分时候都是为了该
属性列的数据的唯一性，而不是为了查询效率。
2. 普通索引 (Index) ： 普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，
并允许数据重复和 NULL。
3. 前缀索引 (Prefix) ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索
引，相比普通索引建立的数据更小，因为只取前几个字符。
```
###### 回顾：表空间文件

从物理意义上来讲，InnoDB 表由共享表空间文件（ibdata 1）、独占表空间文件（ibd）、表结构文件
（. frm）、以及日志文件（redo 文件等）组成。

**1 、表结构文件**

在 MYSQL 中建立任何一张数据表，在其数据目录对应的数据库目录下都有对应表的. frm 文件

.frm 文件是用来保存每个数据表的元数据 (meta) 信息，包括表结构的定义等，

.frm 文件跟数据库存储引擎无关，也就是任何存储引擎的数据表都必须有. frm 文件，

命名方式为数据表名. frm，如 user. frm. ， .frm 文件可以用来在数据库崩溃时恢复表结构。

**2 、表空间文件**

（ 1 ）表空间结构分析

以下为 InnoDB 的表空间结构图：


数据段即 B+树的叶子节点，索引段即为 B+树的非叶子节点 InnoDB 存储引擎的管理是由引擎本身完成
的，

表空间（Tablespace）是由分散的段 (Segment) 组成。一个段 (Segment) 包含多个区（Extent）。

区（Extent）由 64 个连续的页（Page）组成，每个页大小为 16 K，即每个区大小为 1 MB，创建新表时，
先使用 32 页大小的碎片页存放数据，使用完后才是区的申请（InnoDB 最多每次申请 4 个区，保证数据的
顺序性能）
页类型有：数据页、Undo 页、系统页、事务数据页、插入缓冲位图页、以及插入缓冲空闲列表页。

（ 2 ）独占表空间文件

若将 innodb_file_per_table 设置为 on，则系统将为每一个表单独的生成一个 table_name. ibd 的文件，

在此文件中，存储与该表相关的数据、索引、表的内部数据字典信息。

（ 3 ）共享表空间文件

在 InnoDB 存储引擎中，默认表空间文件是 ibdata 1（主要存储的是共享表空间数据），初始化为 10 M，
且可以扩展，如下图所示：

实际上，InnoDB 的表空间文件是可以修改的，使用以下语句就可以修改：

使用共享表空间存储方式时，Innodb 的所有数据保存在一个单独的表空间里面，而这个表空间可以由
很多个文件组成，一个表可以跨多个文件存在，所以其大小限制不再是文件大小的限制，而是其自身的
限制。

从 Innodb 的官方文档中可以看到，其表空间的最大限制为 64 TB，也就是说，Innodb 的单表限制基本上
也在 64 TB 左右了，当然这个大小是包括这个表的所有索引等其他相关数据。

```
Innodb_data_file_path=ibdata1:370 M；ibdata2:50M:autoextend
```

而在使用单独表空间存储方式时，每个表的数据以一个单独的文件来存放，这个时候的单表限制，又变
成文件系统的大小限制了。

###### 数据库表空间文件的实操分析

了解了这些基础知识之后，下面我们对数据库表空间文件做相关的解析，主要是针对独占独占表空间文
件（ibd）

因为主键索引 B+树的根页，在整个表空间文件中的第 3 个页开始，所以可以算出它在文件中的偏移量：

另外根据《InnoDB 存储引擎》中描述在根页的 64 偏移量位置前 2 个字节，保存了 page level 的值，

因此我们想要的 page level 的值在整个文件中的偏移量为：16384*3+64=49152+64=49216，前 2 个字
节中。

接下来我们用 hexdump 工具，查看表空间文件指定偏移量上的数据：

**linetem 表的 page level 为 2 ，B+树高度为 page level+1=3；**

**region 表的 page level 为 0 ，B+树高度为 page level+1=1；**

**customer 表的 page level 为 2 ，B+树高度为 page level+1=3；**

总结，通过分析可以看到，实验环境的三个表：

```
lineitem 表的数据行数为 600 多万，B+树高度为 3 ，
customer 表数据行数只有 15 万，B+树高度也为 3 。
```
可以看出尽管数据量差异较大，这两个表树的高度都是 3 ，换句话说这两个表通过索引查询效率并没有
太大差异，因为都只需要做 3 次 IO。

所以在 InnoDB 中 B+树高度一般为 1-3 层，它就能满足千万级的数据存储。

在查找数据时一次页的查找代表一次 IO，所以通过主键索引查询通常只需要 1-3 次 IO 操作即可查找到数
据。

```
16384*3=49152（ 16384 为页大小）。
```

###### 执行一次 sql 的聚集索引/非聚集索引 io 次数

前面分析了，假设主键 ID 为 bigint 类型，长度为 8 字节，

而指针大小在 InnoDB 源码中设置为 6 字节，这样一共 14 字节。

那么一个页中能存放多少这样的组合，就代表有多少指针，即 16384 / 14 = 1170。

那么可以算出一棵高度为 2 的 B+树，能存放 1170 * 16 = 18720 条这样的数据记录。

同理：

高度为 3 的 B+树可以存放的行数 = 1170 * 1170 * 16 = 21902400

所以，千万级的数据存储只需要约 3 层 B+树，所以说，根据主键 id 索引查询约 3 次 IO 便可以找到目标结
果。

**注意：查询数据时，每加载一页（page）代表一次 IO，**

**当然 B+树的根节点是常驻内存的，这里少了一次 IO。**

但是，我们为例便于分析，在分析的时候，暂时不管吧，先看一般情况。

**对于一些复杂的查询，可能需要走二级索引，那么通过二级索引查找记录最多需要花费多少次 IO 呢？**

首先，从二级索引 B+树中，根据 name 找到对应的主键 id


然后，再根据主键 id 从聚簇索引查找到对应的记录。

如上图所示，二级索引有 3 层，聚簇索引有 3 层，那么最多花费的 IO 次数是：3+3 = 6 （这里，忽略根节
点常驻内存这件事）

聚簇索引默认是主键，如果表中没有定义主键，InnoDB 会选择一个唯一的非空索引代替。

如果连这样的索引没有，InnoDB 会隐式定义一个主键来作为聚簇索引。

```
这也是为什么 InnoDB 表必须有主键，并且推荐使用整型的自增主键！！！
InnoDB 使用的是聚簇索引，将主键组织到一棵 B+树中，而行数据就储存在叶子节点上
```
为啥磁盘 IO 的性能低？ 不多说啦，具体请参考尼恩的《葵花宝典：Java 高性能超底层原理》视频和讲
义

###### 为啥 inno DB 单表记录推荐 2 kw

通过上面的分析可以看出，如果是走非聚集索引查询，需要 6 次 IO，

走聚焦索引查询，需要 3 次磁盘 IO

当然，以上分析流程，忽略了一些性能的优化措施，比如 B+树根节点常驻内存，还有可能命中查询
缓存等等。

所以，innodb 单表推荐 2 kw 记录，超过了这个值可能会导致 B+树层级更高，影响查询性能。

当然，凡事看场景，上面只是一般的分析。

索引结构不会影响单表最大行数，2 kw 也只是推荐值，最终的单表记录数最大值，受到硬件条件，和各
种优化措施的影响。


###### 后记：超乎圆满的答案

只要按照上面的流程去作答，你的答案不是 100 分，而是 120 分

面试官一定是心满意足，五体投地。

#### 聊聊：什么是 MySQL 的 MRR 优化？

###### 什么是 MRR 优化？

MRR，全称「Multi-Range Read Optimization」。

在不使用 MRR 时，优化器需要根据二级索引返回的记录来进行“回表”，这个过程一般会有较多的随机
IO,

使用 MRR 时，SQL 语句的执行过程是这样的：

1 、先把通过二级索引取出的值缓存在缓冲区中，

这个缓冲区叫做 read_rnd_buffer ，简称 rowid buffer。

2 、再把这部分缓冲区中的数据按照 ID 进行排序。

如果二级索引扫描到索引文件的末尾或者缓冲区已满，则使用快速排序对缓冲区中的内容按照主键进行
**排序** ；

3 、然后再依次根据 ID 去聚集索引中获取整个数据行。

线程调用 MRR 接口取 rowId，然后根据 rowId 取行数据；

当根据缓冲区中的 rowId 取完数据，则继续调用过程 2) 3)，直至扫描结束；

MRR 的本质：

是在回表的过程中，把 **分散的无序回表，变成排序后有序的回表** ，从而实现随机磁盘读尽可能变成
顺序读。

通过上述过程，优化器将二级索引随机的 IO 进行排序，转化为主键的有序排列，从而实现了随机 IO 到
顺序 IO 的转化，提升性能。

可以看出，只需要通过一次排序，就使得随机 IO，变为顺序 IO，使得数据访问更加高效。

read_rnd_buffer_size 控制了数据能放入缓冲区的大小，如果一次性不够放就会分多次完成。


###### MRR 优化的本质

简单说：MRR 通过把「随机磁盘读」，转化为「顺序磁盘读」，从而提高了索引查询的性能。

MRR 的本质：

是在回表的过程中，把分散的无序回表，变成排序后有序的回表，从而实现随机磁盘读尽可能变成
顺序读。

接下来的问题是：

```
为什么要把随机读转化为顺序读？
怎么转化的？
为什么顺序读就能提升读取性能？
```
首先，从一个没有做 MRR 优化的普通回表查询说起。

###### 从一个没有做 MRR 优化的普通回表查询说起

执行一个范围查询：

当这个 sql 被执行时，MySQL 会按照下图的方式，去磁盘读取数据（假设数据不在数据缓冲池里）：

```
mysql > explain select * from stu where age between 10 and 20;
+----+-------------+-------+-------+------+---------+------+------+-------------
----------+
| id | select_type | table | type | key | key_len | ref | rows | Extra
|
+----+-------------+-------+-------+----------------+------+------+-------------
----------+
| 1 | SIMPLE | stu | range | age | 5 | NULL | 960 | Using index
condition |
+----+-------------+-------+-------+----------------+------+------+-------------
----------+
```

图中红色线就是整个的查询过程，蓝色线则是磁盘的运动路线。

为了对画图进行简化，这张图是按照 Myisam 的索引结构画的，

Innodb 涉及到二级索引、聚簇索引（cluster index）的二级结构，因为，Innodb 涉及到回表，这里的
Myisam ，没有涉及到回表。所以，画起来，会更复杂，这里，就不去耗费大量时间了。

不过上面的 Myisam 磁盘运动路线原理，对于 Innodb 也同样适用。

对于 Myisam，左边就是字段 age 的二级索引，右边是存储完整行数据的地方。

查找的过程是：

先到左边的二级索引找，找到第一条符合条件的记录（ **实际上每个节点是一个页，一个页可以有很多条
记录，这里我们假设每个页只有一条** ）

接着到右边去读取这条数据的完整记录。

读取完后，回到左边，继续找下一条符合条件的记录，

找到后，再到右边读取，

就是这么一条一条的记录，去读取的。

这时，问题来了：

在读取的过程中，会发现上一条数据的位置，和下一条数据的位置，在物理存储位置上，离的贼远！

每次读取数据，磁盘和磁头都得跑好远一段路。

咋办呢，没办法，

只能让磁盘和磁头一起做机械运动，去给你疯狂跑腿，来回跑腿，去读取下一条数据。

磁盘的简化结构可以看成这样：


可以想象一下，为了执行你这条 sql 语句，磁盘要不停的旋转，磁头要不停的移动，

这些机械运动，都是很费时的。

10,000 RPM（Revolutions Per Minute，即转每分） 的机械硬盘，每秒大概可以执行 167 次磁盘读
取，

所以在极端情况下，MySQL 每秒只能给你返回 167 条数据，这还不算上 CPU 排队时间。

对于 Innodb，也是一样的。Innodb 是聚簇索引（cluster index）：

```
如果没有涉及到回表，只需要把右边也换成一颗叶子节点带有完整数据的 B+ tree 就可以了。
如果有涉及到回表，只需要把右边换成一颗叶子节点聚簇索引（cluster index）B+ tree，和一颗
叶子节点是主键值的二级索引 B+ tree，就可以了。
```
###### 磁盘 IOPS 的计算规则

主要影响的三个参数，分别是平均寻址时间、盘片旋转速度以及最大传送速度：

**第一个寻址时间，**

考虑到被读写的数据可能在磁盘的任意一个磁道，既有可能在磁盘的最内圈 (寻址时间最短)，也可能在
磁盘的最外圈 (寻址时间最长)，

所以在计算中我们只考虑平均寻址时间，也就是磁盘参数中标明的那个平均寻址时间，这里就采用当前
最多的 10 krmp 硬盘的 5 ms。


寻道时间 Tseek 是指将读写磁头移动至正确的磁道上所需要的时间。

寻道时间越短，I/O 操作越快，目前磁盘的平均寻道时间一般在 3-15 ms。

**第二个旋转延时，**

和寻址一样，当磁头定位到磁道之后有可能正好在要读写扇区之上，这时候是不需要额外额延时就可以
立刻读写到数据，但是最坏的情况确实要磁盘旋转整整一圈之后磁头才能读取到数据，

所以这里我们也考虑的是平均旋转延时，对于 10 krpm 的磁盘就是 (60 s/10 k)*(1/2) = 2 ms。

**第三个传送时间，**

磁盘参数提供我们的最大的传输速度，当然要达到这种速度是很有难度的，

但是这个速度却是磁盘纯读写磁盘的速度，因此只要给定了单次 IO 的大小，我们就知道磁盘需要花费
多少时间在数据传送上，这个时间就是 IO Chunk Size / Max Transfer Rate。（数据传输率，单位是
Mb/s，兆每秒）。

数据传输时间 Ttransfer 是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据
大小除以数据传输率。

目前 IDE/ATA 能达到 133 MB/s，SATA II 可达到 300 MB/s 的接口数据传输率，数据传输时间通常远小于前
两部分时间。

因此，理论上可以计算出磁盘的最大 IOPS，即 IOPS = 1000 ms/ (Tseek + Troatation)，忽略数据传输时
间。

假设磁盘平均物理寻道时间为 3 ms, 磁盘转速为 7200,10 K, 15 K rpm，

则磁盘 IOPS 理论最大值分别为，

IOPS = 1000 / (3 + 60000/7200/2) = 140

IOPS = 1000 / (3 + 60000/10000/2) = 167

IOPS = 1000 / (3 + 60000/15000/2) = 200

到这里你知道了磁盘随机访问是多么奢侈的事了，所以，很明显，要把随机访问转化成顺序访问：

###### 顺序读：一场狂风暴雨般的革命

开启了 MRR 很明显，要把随机访问转化成顺序访问。

设置开启 MRR, 重新执行 sql 语句，发现 Extra 里多了一个「Using MRR」。

```
mysql > set optimizer_switch='mrr=on';
Query OK, 0 rows affected (0.06 sec)
```
```
mysql > explain select * from stu where age between 10 and 20;
+----+-------------+-------+-------+------+---------+------+------+-------------
---+
| id | select_type | table | type | key | key_len | ref | rows | Extra
|
+----+-------------+-------+-------+------+---------+------+------+-------------
---+
| 1 | SIMPLE | tbl | range | age | 5 | NULL | 960 | ...; Using
MRR |
+----+-------------+-------+-------+------+---------+------+------+-------------
---+
```

这下 MySQL 的查询过程会变成这样：

对于 Myisam，在去磁盘获取完整数据之前，会先按照 rowid 排好序，再去顺序的读取磁盘。

对于 Innodb，则会按照聚簇索引键值排好序，再顺序的读取聚簇索引。

###### 顺序读带来了几个好处：

1 、 **磁盘和磁头** 不再需要来回做机械运动；

2 、可以充分利用 **磁盘预读**

比如在客户端请求一页的数据时，可以把后面几页的数据也一起返回，放到数据缓冲池中，

这样如果下次刚好需要下一页的数据，就不再需要到磁盘读取。

这样做的理论依据是计算机科学中著名的 **局部性原理** ：当一个数据被用到时，其附近的数据也通常会马
上被使用。

**3 、在一次查询中，每一页的数据只会从磁盘读取一次**

MySQL 从磁盘读取页的数据后，会把数据放到数据缓冲池，下次如果还用到这个页，就不需要去磁盘
读取，直接从内存读。

但是如果不排序，可能你在读取了第 1 页的数据后，会去读取第 2 、 3 、 4 页数据，

接着你又要去读取第 1 页的数据，这时你发现第 1 页的数据，已经从缓存中被剔除了，于是又得再去磁
盘读取第 1 页的数据。

而转化为顺序读后，你会连续的使用第 1 页的数据，这时候按照 MySQL 的缓存剔除机制，

这一页的缓存是不会失效的，直到你利用完这一页的数据，由于是顺序读，

在这次查询的余下过程中，你确信不会再用到这一页的数据，可以和这一页数据说告辞了。

顺序读就是通过这三个方面，最大的优化了索引的读取。

别忘了，索引本身就是为了减少磁盘 IO，加快查询，而 MRR，则是把索引减少磁盘 IO 的作用，进一步
放大。


###### 拆分查询条件，进行批量查询

**此外** ，MRR 还可以将某些范围查询，拆分为键值对，以此来进行批量的数据查询。

这样做的好处是可以在拆分过程中，直接过滤一些不符合查询条件的数据。

表 t 有 (key_part 1, key_part 2) 的联合索引，因此索引根据 key_part 1, key_part 2 的位置关系进行排序。

若没有 MRR，此时查询类型为 Range，SQL 优化器会先将 key_part 1 大于 1000 且小于 2000 的数据都取出
来，即便 key_part 2 不等于 1000 。

取出后再根据 key_part 2 的条件进行过滤。这会导致无用的数据被取出。

如果启用 MRR 优化器会使性能有巨大的提升，优化器会先将查询条件拆分为 (1000,1000), (1001,1000),
(1002,1000).... (1999,1000) 最后再根据这些拆分出的条件进行数据的查询。

###### 一些关于这场革命的配置

是否启用 MRR 优化，可以通过参数 optimizer_switch 中的 flag 来控制。

1 、MRR 的开关： **mrr** =（on | off）

例如，打开 MRR 的开关:

2 、用来告诉优化器，要不要基于使用 MRR 的成本：

mrr_cost_based = （on | off)

例如，通通使用 MRR:

考虑使用 MRR 是否值得 (cost-based choice)，来决定具体的 sql 语句里要不要使用 MRR。

很明显，对于只返回一行数据的查询，是没有必要 MRR 的，而如果你把 mrr_cost_based 设为 off，那
优化器就会通通使用 MRR，

这在有些情况下是很 stupid 的，所以建议这个配置还是设为 on，毕竟优化器在绝大多数情况下都是正
确的。

3 、设置用于给 rowid 排序的内存的大小：read_rnd_buffer_size，该值默认是 256 KB

查看配置

```
SELECT * FROM t WHERE key_part 1 >= 1000 AND key_part 1 < 2000 AND key_part 2 =
1000 ;
```
```
mysql > set optimizer_switch='mrr=on';
```
```
SET GLOBAL optimizer_switch='mrr=on, mrr_cost_based=off';
```
```
show VARIABLES like 'read_rnd_buffer_size';
```

显然，MRR 在本质上是一种用空间换时间的算法。

MySQL 不可能给你无限的内存来进行排序，如果 read_rnd_buffer 满了，就会先把满了的 rowid 排好
序去磁盘读取，接着清空，然后再往里面继续放 rowid，直到 read_rnd_buffer 又达到 read_rnd_buffe
配置的上限，如此循环。

没有 MRR 的情况下， **二级索引里面得到多少行，那么就要去访问多少次主键索引** (也不能完全这样说，
因为 MySQL 实现了 BNL，是把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹
配，这样就可以大大减少重复从磁盘上加载被驱动表的代价)，而有了 MRR 的时候，次数就大约减少为
之前次数 t / buffer_size。

可以简单理解为：

MRR 把分散的回表操作，聚合成了批量的回表操作，当然，是借助空间的局部性原理和磁盘预读取
等底层机制完成的。

###### MRR 使用限制

MRR 适用于 range、ref、eq_ref 的查询

## （面试重点）Explain 分析

###### Explain 介绍

使用 EXPLAIN 关键字可以模拟优化器执行 SQL 语句，分析你的查询语句或是结构的性能瓶颈

在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询会返回执行计划
的信息，而不是执行这条 SQL

注意：如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中

Explain 官方文档：官网地址

https://dev.mysql.com/doc/refman/5.7/en/execution-plan-information.html

EXPLAIN 关键字的使用手册：

https://dev.mysql.com/doc/refman/5.7/en/explain-output.html

**Explain 的作用** ：

```
表的读取顺序
数据读取操作的操作类型
哪些索引可以使用
哪些索引被实际使用
表之间的引用
每张表有多少行被优化器查询
```
**使用 Explain** ：

```
explain + sql 语句
```

```
执行计划包含的信息（重点） ：| id | select_type | table | partitions | type |
possible_keys | key | key_len | ref | rows | filtered | Extra |
面试重点 ：id、type、key、rows、Extra
```
#### 聊聊：如何使用 EXPLAIN 关键字？

在日常工作中, 我们会记录一些执行时间比较久的 SQL 语句, 找出这些 SQL 语句并不意味着完事了,

我们常常用到 explain 这个命令来查看一个这些 SQL 语句的执行计划,

EXPLAIN 关键字可以查看该 SQL 语句有没有使用上了索引, 有没有做全表扫描,

所以，是深入了解 MySQL 基于开销的优化器.

###### 什么是 EXPLAIN 关键字

使用 EXPLAIN 关键字可以模拟优化器执行 SQL 查询语句，从而知道 MySQL 是如何处理你的 SQL 语句的。

分析你的查询语句或是表结构的性能瓶颈。

**通过 EXPLAIN，我们可以分析出以下结果：**

```
表的读取顺序
数据读取操作的操作类型
哪些索引可以使用
哪些索引被实际使用
表之间的引用
每张表有多少行被优化器查询
```
###### EXPLAIN 关键字使用方式如下：

EXPLAIN + SQL 语句

在执行 explain 命令之后, 显示的信息一共有 12 列,

执行计划包含的信息

分别是:

```
id: 选择标识符
select_type: 查询类型
table: 输出结果集的表
partitions: 匹配的分区
type: 表的连接类型
possible_keys: 查询时可能使用的索引
key: 实际使用的索引
```
```
explain select * from t_member where member_id = 1 ;
```

```
key_len: 索引字段的长度
ref: 列与索引的比较
rows: 扫描出的行数
filtered: 按表条件过滤的行百分比
extra: 执行情况描述和说明
```
###### 执行计划各字段含义

###### 1 、 id: 查询中执行 select 子句或操作表的顺序

select 查询的序列号，包含一组数字，表示查询中执行 select 子句或操作表的顺序

**id 相同时执行顺序从上到下, 在所有组中, id 值越大, 优先级越高, 越先执行**

**id 的结果共有 3 中情况**

```
id 相同，执行顺序由上至下
```
```
[总结] 加载表的顺序如上图 table 列所示：t 1 t 3 t 2
id 不同，如果是子查询，id 的序号会递增，id 值越大优先级越高，越先被执行
```
```
id 相同不同，同时存在
```

```
如上图所示，在 id 为 1 时，table 显示的是 <derived2> ,这里指的是指向 id 为 2 的表，即 t 3 表的衍生
表。
```
###### 2 、select_type：每个 select 子句的类型.

常见和常用的值有如下几种：

分别用来表示查询的类型，主要是用于区别普通查询、联合查询、子查询等的复杂查询。

```
SIMPLE 简单的 select 查询，查询中不包含子查询或者 UNION
PRIMARY 查询中若包含任何复杂的子部分，最外层查询则被标记为 PRIMARY
SUBQUERY 在 SELECT 或 WHERE 列表中包含了子查询
DERIVED 在 FROM 列表中包含的子查询被标记为 DERIVED（衍生），MySQL 会递归执行这些子查
询，把结果放在临时表中
UNION 若第二个 SELECT 出现在 UNION 之后，则被标记为 UNION：若 UNION 包含在 FROM 子句的
子查询中，外层 SELECT 将被标记为：DERIVED
UNION RESULT 从 UNION 表获取结果的 SELECT
```
###### 3 、 table 表

指的就是当前执行的表

###### 4 type 查询类型

type 所显示的是查询使用了哪种类型，type 包含的类型包括如下图所示的几种：


从最好到最差依次是：

**一般来说，得保证查询至少达到 range 级别，最好能达到 ref。**

```
system 表只有一行记录（等于系统表），这是 const 类型的特列，平时不会出现，这个也可以忽
略不计
const 表示通过索引一次就找到了，const 用于比较 primary key 或者 unique 索引。
因为只匹配一行数据，所以很快。如将主键置于 where 列表中，MySQL 就能将该查询转换为一个
常量。
```
```
首先进行子查询得到一个结果的 d 1 临时表，子查询条件为 id = 1 是常量，所以 type 是 const，id 为 1
的相当于只查询一条记录，所以 type 为 system。
eq_ref 唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。
常见于主键或唯一索引扫描
ref 非唯一性索引扫描，返回匹配某个单独值的所有行，
本质上也是一种索引访问，它返回所有匹配某个单独值的行，
然而，它可能会找到多个符合条件的行，所以他应该属于查找和扫描的混合体。
```
```
range 只检索给定范围的行，使用一个索引来选择行，key 列显示使用了哪个索引，
一般就是在你的 where 语句中出现 between、< 、>、in 等的查询，这种范围扫描索引比全表扫描
要好，
```
```
system > const > eq_ref > ref > range > index > all
```

```
因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。
```
```
index Full Index Scan，
Index 与 All 区别为 index 类型只遍历索引树。这通常比 ALL 快，因为索引文件通常比数据文件小。
（也就是说虽然 all 和 Index 都是读全表，但 index 是从索引中读取的，而 all 是从硬盘读取的）
```
```
id 是主键，所以存在主键索引
all Full Table Scan 将遍历全表以找到匹配的行
```
###### 5 possible_keys 和 key

```
possible_keys 显示可能应用在这张表中的索引，一个或多个。
```
查询涉及到的字段上若存在索引，则该索引将被列出， **但不一定被查询实际使用** 。

```
实际使用的索引，如果为 NULL，则没有使用索引。（可能原因包括没有建立索引或索引失效）
```
```
查询中若使用了覆盖索引（select 后要查询的字段刚好和创建的索引字段完全相同），则该索引
仅出现在 key 列表中
```
```
则该索引仅出现在 key 列表中
```
```
key
```

###### 6 key_len 索引中使用的字节数

表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度，在不损失精确性的情况下，长度越
短越好。key_len 显示的值为索引字段的最大可能长度，并非实际使用长度，

即 key_len 是根据表定义计算而得，不是通过表内检索出的。

###### 7 ref 那一列被使用

显示索引的那一列被使用了，如果可能的话，最好是一个常数。

哪些列或常量被用于查找索引列上的值。

###### 8 rows 所需要读取的行数

根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数，也就是说，用的越少
越好


###### 9 Extra

包含不适合在其他列中显式但十分重要的额外信息

**9.1 Using filesort**

说明 mysql 会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。

MySQL 中无法利用索引完成的排序操作, 称为“文件排序”。

**9.2 Using temporary**

使用了用临时表保存中间结果，MySQL 在对查询结果排序时使用临时表。

常见于排序 order by 和分组查询 group by。


**9.3 Using index**

表示相应的 select 操作中使用了覆盖索引（Covering Index），避免访问了表的数据行，效率不错。

如果同时出现 using where，表明索引被用来执行索引键值的查找；

如果没有同时出现 using where，表明索引用来读取数据而非执行查找动作。

如果没有同时出现 using where，表明索引用来读取数据而非执行查找动作。

理解方式一:


就是 select 的数据列只用从索引中就能够取得，不必读取数据行，MySQL 可以利用索引返回 select 列表
中的字段，而不必根据索引再次读取数据文件, 换句话说查询列要被所建的索引覆盖。

理解方式二:

索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个
行。

毕竞竟索引叶子节点存储了它们索引的数据: 当能通过速取索引就可以得到想要的数据，那就不需要速取
行了。

一个索引包含了 (或覆盖了) 满足查询结果的数据就叫做覆盖索引。

注意:
如果要使用覆盖索引，一定要注意 select 列表中只取出需要的列，不可 select *,

因为如果将所有字段一起做索引会导致索引文件过大，查询性能下降。

**9.4 Using where**

表明使用了 where 过滤

**9.5 Using join buffer**

表明使用了连接缓存, 比如说在查询的时候，多表 join 的次数非常多，那么将配置文件中的缓冲区的 join
buffer 调大一些。

**9.6 impossible where**

where 子句的值总是 false，不能用来获取任何元组

**9.7 select tables optimized away**

在没有 GROUPBY 子句的情况下，基于索引优化 MIN/MAX 操作或者对于 MyISAM 存储引擎优化
COUNT (*) 操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。

###### 9.8 distinct

优化 distinct 操作，在找到第一匹配的元组后即停止找同样值的动作

###### 实例分析

执行顺序 1 ：

```
SELECT * FROM t_user WHERE id = '1' and id = '2'
```

id 为 4 ，select_type 为 UNION，

说明第四个 select 是 UNION 里的第二个 select，最先执行【select name, id from t 2】

执行顺序 2 ：

id 为 3 ，是整个查询中第三个 select 的一部分。

因查询包含在 from 中，所以为 DERIVED【select id, name from t 1 where other_column=’’】

执行顺序 3 ：

id 为 2 ，select 列表中的子查询 select_type 为 subquery,

为整个查询中的第二个 select【select id from t 3】

执行顺序 4 ：

id 为 1 ，表示是 UNION 里的第一个 select，select_type 列的 primary 表示该查询为外层查询，table 列被
标记为<derived3>, 表示查询结果来自一个衍生表，其中 derived 3 中的 3 代表该查询衍生自第三个
select 查询，即 id 为 3 的 select。【select d 1. name ...】

执行顺序 5 ：

id 为 null，代表从 UNION 的临时表中读取行的阶段，table 列的< union1,4 >表示用第一个和第四个
select 的结果进行 UNION 操作。【两个结果 union 操作】

#### 实战：MySQL 索引优化深入实战

前言：该篇随笔通过一些案例，对索引相关的面试题进行分析。

###### 0. 准备

###### 1. 创建 test 表 (测试表)。

###### 2. 创建索引。

```
drop table if exists test;
```
```
create table test (
id int primary key auto_increment,
c 1 varchar (10),
c 2 varchar (10),
c 3 varchar (10),
c 4 varchar (10),
c 5 varchar (10)
) ENGINE=INNODB default CHARSET=utf 8;
insert into test (c 1, c 2, c 3, c 4, c 5) values ('a 1','a 2','a 3','a 4','a 5');
insert into test (c 1, c 2, c 3, c 4, c 5) values ('b 1','b 2','b 3','b 4','b 5');
insert into test (c 1, c 2, c 3, c 4, c 5) values ('c 1','c 2','c 3','c 4','c 5');
insert into test (c 1, c 2, c 3, c 4, c 5) values ('d 1','d 2','d 3','d 4','d 5');
insert into test (c 1, c 2, c 3, c 4, c 5) values ('e 1','e 2','e 3','e 4','e 5');
```

###### 3 、普通查询情况

1. 根据以下 Case 分析索引的使用情况

**Case 1：**

分析：

①创建复合索引的顺序为 c 1, c 2, c 3, c 4。

②上述四组 explain 执行的结果都一样：type=ref，key_len=132，ref=const, const, const, const。

结论：

在执行常量等值查询时，改变索引列的顺序并不会更改 explain 的执行结果，

因为 mysql 底层优化器会进行优化，但是推荐按照索引顺序列编写 sql 语句。

**Case 2：**

分析：

当出现范围的时候，type=range，key_len=99，比不用范围 key_len=66 增加了，说明使用上了索引，

但对比 Case 1 中执行结果，说明 c 4 上索引失效。

结论：范围右边索引列失效，但是范围当前位置 (c 3) 的索引是有效的，从 key_len=99 可证明。

**Case 2.1：**


分析：

与上面 explain 执行结果对比，key_len=132 说明索引用到了 4 个，

因为对此 sql 语句 mysql 底层优化器会进行优化：

范围右边索引列失效 (c 4 右边已经没有索引列了)，注意索引的顺序 (c 1, c 2, c 3, c 4)，所以 c 4 右边不会出现失
效的索引列，因此 4 个索引全部用上。

结论：

范围右边索引列失效，是有顺序的：c 1, c 2, c 3, c 4，如果 c 3 有范围，则 c 4 失效；如果 c 4 有范围，则没有失
效的索引列，从而会使用全部索引。

**Case 2.2：**

分析：

如果在 c 1 处使用范围，则 type=ALL，key=Null，索引失效，全表扫描，

这里违背了最佳左前缀法则，带头大哥已死，因为 c 1 主要用于范围，而不是查询。

解决方式使用覆盖索引。

结论：在最佳左前缀法则中，如果最左前列 (带头大哥) 的索引失效，则后面的索引都失效。

**Case 3：**

分析：

利用最佳左前缀法则：

中间兄弟不能断，因此用到了 c 1 和 c 2 索引 (查找)，从 key_len=66，ref=const, const，c 3 索引列用在排序
过程中。

**Case 3.1：**

分析：

从 explain 的执行结果来看：key_len=66，ref=const, const，从而查找只用到 c 1 和 c 2 索引，c 3 索引用于
排序。

**Case 3.2：**


分析：

从 explain 的执行结果来看：key_len=66，ref=const, const，查询使用了 c 1 和 c 2 索引，由于用了 c 4 进行
排序，跳过了 c 3，出现了 Using filesort。

**Case 4：**

分析：

查找只用到索引 c 1，c 2 和 c 3 用于排序，无 Using filesort。

**Case 4.1：**

分析：

和 Case 4 中 explain 的执行结果一样，但是出现了 Using filesort，因为索引的创建顺序为 c 1, c 2, c 3, c 4，
但是排序的时候 c 2 和 c 3 颠倒位置了。

**Case 4.2：**

分析：

在查询时增加了 c 5，但是 explain 的执行结果一样，因为 c 5 并未创建索引。

**Case 4.3：**

分析：

与 Case 4.1 对比，在 Extra 中并未出现 Using filesort，因为 c 2 为常量，在排序中被优化，所以索引未颠
倒，不会出现 Using filesort。

**Case 5：**

分析：


只用到 c 1 上的索引，因为 c 4 中间间断了，根据最佳左前缀法则，所以 key_len=33，ref=const，表示只
用到一个索引。

**Case 5.1：**

分析：

对比 Case 5，在 group by 时交换了 c 2 和 c 3 的位置，结果出现 Using temporary 和 Using filesort，极度恶
劣。原因：c 3 和 c 2 与索引创建顺序相反。

**Case 6**

**分析：**

①在 c 1, c 2, c 3, c 4 上创建了索引，直接在 c 1 上使用范围，导致了索引失效（其实这里 MySQL 底层也是有优
化的，如果 where 后的字段是索引的第一个字段使用了范围查询，如果这个范围很大，几乎已经是要扫
描所有数据了，

MySQL 就会用全表扫描，如果这个范围不是很大，那么 MySQL 底层依旧还会使用索引来进行查询），

全表扫描：type=ALL，ref=Null。因为此时 c 1 主要用于排序，并不是查询。

②使用 c 1 进行排序，但是索引失效，出现了 Using filesort。

③解决方法：使用覆盖索引。

就是将索引字段覆盖掉查询字段，实现索引覆盖，MySQL 就不会扫描全表而去使用索引了。

**Case 7：**

**分析：**

虽然排序的字段列与索引顺序一样，且 order by 默认升序，这里 c 2 desc 变成了降序，导致与索引的排
序方式不同，


因为索引的所有字段都是按照同一个方向的顺序进行排序的，如果出现了排序方向不同，那么已经排列
好的索引自然也就失效了，从而产生 Using filesort，而且 type 还是 index（index 是扫描全表索引，所以
这一个的 key_len 是 132 ，说明 4 个索引字段全部都扫描了，ALL 是扫描全表，index 比 ALL 稍微快一
点）。

**Case 8：**

**分析：**

对于排序来说，多个相等条件也是范围查询，所以索引失效，c 2, c 3 都无法使用索引，出现 Using
filesort。

并且这里 type 是 index，扫描全表索引。

###### 总结

```
MySQL 支持两种方式的排序 filesort 和 index，Using index 是指 MySQL 扫描索引本身完成排序。
index 效率高，filesort 效率低。
order by 满足两种情况会使用 Using index。
order by 语句使用索引最左前列。
使用 where 子句与 order by 子句条件列组合满足索引最左前列。
尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最左前缀法则。
如果 order by 的条件不在索引列上，就会产生 Using filesort。
group by 与 order by 很类似，其实质是先排序后分组，遵照索引创建顺序的最佳左前缀法则。注
意 where 高于 having，能写在 where 中的限定条件就不要去 having 限定了。
```
**通过以上 Case 的分析，进行如下总结：**

①最佳左前缀法则。

1. 在等值查询时，更改索引列顺序，并不会影响 explain 的执行结果，因为 mysql 底层会进行优化。

2. 在使用 order by 时，注意索引顺序、常量，以及可能会导致 Using filesort 的情况。

②group by 容易产生 Using temporary。

③通俗理解口诀：

全值匹配我最爱，最左前缀要遵守；

带头大哥不能死，中间兄弟不能断；

索引列上少计算，范围之后全失效；

LIKE 百分写最右，覆盖索引不写星；

不等空值还有 or，索引失效要少用。

```
EXPLAIN extended select c 1 from test where c 1 in ('a 1','b 1') ORDER BY c 2, c 3;
```

#### 聊聊：产生临时表的原因有哪些？

你在对 sql 分析语句的执行计划 (explain) 的时候，发现 **「extra」** 中有 **Using temporary,** 那就说明使用
了临时表保存中间结果，Mysql 在对查询结果排序时使用临时表。出现这种情况我们就需要看下能不能
优化一下了。

那首先知道什么情况下会产生临时表

1 ）order by 子句和 group by 子句不同，例如：ordery by price group by name；

2 ）order by 中使用了 distinct 关键字 ordery by distinct (price)

3 ）直接使用磁盘临时表的场景

```
表包含 text 或者 blob 列；
group by 或者 distinct 子句中包含长度大于 512 字节的列；
使用 union 或者 union all 时，select 子句中包含大于 512 字节的列；
```
#### 聊聊：如何解决临时表问题呢？

使用临时表一般都意味着性能比较低，特别是使用磁盘临时表，性能更慢，因此我们在实际应用中应该
尽量避免临时表的使用。常见的避免临时表的方法有：

```
创建索引：在 ORDER BY 或者 GROUP BY 的列上创建索引；
如果你的 varvhar 2，字节数是否超过 512 字节，看能否修改。
分拆很长的列：一般情况下，TEXT、BLOB，大于 512 字节的字符串，基本上都是为了显示信息，
而不会用于查询条件，因此表设计的时候，应该将这些列独立到另外一张表。
```
#### 聊聊：日常开发中你是怎么优化 SQL 的？

这个问题问的挺大的，那么我们可以也先从几个大的纬度来回答。

```
1. 添加合适索引
2. 优化表结构
3. 优化查询语句
```
然后针对每个大的纬度稍微讲几句。

**「 1 、添加合适索引」**

```
对作为查询条件和 order by 的字段建立索引。
对于多个查询字段的考虑建立组合索引，同时注意组合索引字段的顺序, 将最常用作限制条件的列
放在最左边，依次递减。
索引不宜太多，一般 5 个以内。
```
**「 2 、优化表结构」**

选择正确的数据类型，对于提高性能也是至关重要。下面给出几种原则:

```
「数字型字段优于字符串类型」
若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。
「数据类型更小通常更好」
使用最小的数据类型, 会减少磁盘的空间，内存和 CPU 缓存。好比时间类型尽量使用 TIMESTAMP 类
型，因为其存储空间只需要 DATETIME 类型的一半。对于只需要精确到某一天的数据类型，建议
使用 DATE 类型，因为他的存储空间只需要 3 个字节，比 TIMESTAMP 还少。
```

```
「尽量使用 NOT NULL」
NULL 类型比较特殊，SQL 难优化。如果是一个组合索引，那么这个 NULL 类型的字段会极大影响
整个索引的效率。此外，NULL 在索引中的处理也是特殊的，也会占用额外的存放空间。
```
**「 3 、优化查询语句」**

```
分析语句，是否加载了不必要的字段/数据。
分析 SQl 执行计划，是否命中索引等。
如果 SQL 很复杂，优化 SQL 结构
如果表数据量太大，考虑分表
```
#### 聊聊：什么是小表驱动大表？要注意那些？

**对于程序而言，外层循环越大，性能越低** ，对于数据库而言，永远是小的数据集放在最外层

1 、首先优化原则，小表驱动大表，即小的数据集驱动大的数据集。

2 、当 B 表的数据集必须小于 A 表的数据集时，用 in 优于 exists

3 、当 A 表的数据集小于 B 表的数据集时，用 exists 优于 in。

4 、exists (subquery) 只返回 true 或 false，因此子查询中的 select * 也可以是 select 1 或 select ‘x’, 官方说
法是实际执行时会忽略掉 select 清单，因此没有区别。

5 、exists 子查询的实际执行过程可能经过优化而不是理解上的逐条对比，如果担忧效率问题，可进行实
际建立以确定是否有效率问题。

6 、exists 子查询往往也可以用条件表达式，其他子查询或者 join 来替代，何种最优需要具体问题具体分
析。

#### 聊聊：关心过业务系统里面的 sql 耗时吗? 统计过慢查询吗?

#### 对慢查询都怎么优化过?（explain）？

在业务系统中, 除了使用主键进行的查询, 其他的我都会在测试库上测试其耗时,

慢查询的统计主要由运维在做, 会定期将业务中的慢查询反馈给我们。

慢查询的优化首先要搞明白慢的原因是什么?

是查询条件没有命中索引? 是 load 了不需要的数据列? 还是数据量太大?

所以优化也是针对这三个方向来的,

```
首先分析语句, 看看是否 load 了额外的数据, 可能是查询了多余的行并且抛弃掉了, 可能是加载了许多
结果中并不需要的列, 对语句进行分析以及重写。
```
```
select * from A where id in (select id from b)
```
```
select * from A where exists (select 1 from B where b.id=a.id)
```

```
分析语句的执行计划 (explain), 然后获得其使用索引的情况, 之后修改语句或者修改索引, 使得语句可
以尽可能的命中索引。
如果对语句的优化已经无法进行, 可以考虑表中的数据量是否太大, 如果是的话可以进行横向或者纵
向的分表。
```
#### 聊聊：一条 sql 执行过长的时间，你如何优化，从哪些方面

#### 入手？

```
查看是否涉及多表和子查询，优化 Sql 结构，如去除冗余字段，是否可拆表等
优化索引结构，看是否可以适当添加索引
数量大的表，可以考虑进行分离/分表（如交易流水表）
数据库主从分离，读写分离
explain 分析 sql 语句，查看执行计划，优化 sql
查看 Mysql 执行日志，分析是否有其他方面的问题
```
上面这 3 道面试题也是差不多的，只是问的方式不一样，可以总结出自己的话术来表达。

#### 网易二面：CPU 飙升 900%，该怎么处理？

社群一位小伙伴面试了网易，遇到了一个性能类的面试题：

**CPU 飙升 900%，该怎么处理？**

可惜的是，以上的问题，这个小伙没有回答理想。

**最终，导致他网易之路，终止在二面，非常可惜**

现在把这个题目，以及参考答案，收入咱们的《尼恩 Java 面试宝典》，供后面的小伙伴参考，前车之
鉴啊

###### 首先，说明一下问题：CPU 飙升 200% 以上是生产容易发生的场景

**场景:1：MySQL 进程飙升 900%**

大家在使用 MySQL 过程，想必都有遇到过 CPU 突然过高，或者达到 200%以上的情况。

数据库执行查询或数据修改操作时，系统需要消耗大量的 CPU 资源维护从存储系统、内存数据中的一致
性。

并发量大并且大量 SQL 性能低的情况下，比如字段是没有建立索引，则会导致快速 CPU 飙升，如果还开
启了慢日志记录，会导致性能更加恶化。生产上有 MYSQL 飙升 900% 的恶劣情况。

**场景 2 ：Java 进程飙升 900%**

一般来说 Java 进程不做大量 CPU 运算，正常情况下，CPU 应该在 100~200% 之间，

但是，一旦高并发场景，要么走到了死循环，要么就是在做大量的 GC, 容易出现这种 CPU 飙升的情
况，CPU 飙升 900%，是完全有可能的。

**其他场景：其他的类似进程飙升 900%的场景**

比如 Redis、Nginx 等等。

尼恩提示： _**_ 大家介绍场景的时候，就说自己主要涉及了两个场景， _Java_ 进程飙升 _900%_ 、 _MySQL_ 进程
飙升 _900%_ 两种场景，其实，这两个场景就足够讲半天了，其他的，使用规避技巧规避一下就行。


###### 场景一：MySQL 进程 CPU 飙升到 900%，怎么处理？

**定位过程：**

```
使用 top 命令观察，确定是 mysqld 导致还是其他原因。
如果是 mysqld 导致的，show processlist，查看 session 情况，确定是不是有消耗资源的 sql 在运
行。
找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。
```
**处理过程：**

```
kill 掉这些线程 (同时观察 cpu 使用率是否下降)，一般来说，肯定要 kill 掉这些线程 (同时观察 cpu
使用率是否下降)，等进行相应的调整 (比如说加索引、改 sql、改内存参数) 之后，再重新跑这些
SQL。
进行相应的调整 (比如说加索引、改 sql、改内存参数)
index 是否缺失，如果是，则建立索引。也有可能是每个 sql 消耗资源并不多，但是突然之间，
有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，
再做出相应的调整，比如说限制连接数等
优化的过程，往往不是一步完成的，而是一步一步，执行一项优化措辞，再观察，再优化。
```
#### 场景 1 的真实案例：MySQL 数据库优化的真实案例

尼恩提示：以下案例，来自互联网。大家参考一下，准备一个自己的案例。

本问题亲身经历过。

之前开发同事编写的 SQL 语句，就导致过线上 CPU 过高，MySQL 的 CPU 使用率达到 900%+，通过优化最
后降低到 70%~80%。下面说说个人在这个过程中的排查思路。

首先，我们要对问题定位而不是盲目的开启什么慢日志，在并发量大并且大量 SQL 性能低的情况下，开
启慢日志无意是将 MySQL 推向崩溃的边缘。

当时遇到这个情况，分析了当前的数据量、索引情况、缓存使用情况。目测数据量不大，也就几百万条
而已。接下来就去定位索引、缓存问题。

```
1. 经过询问，发现很多查询都是走 MySQL，没有用到缓存。
2. 既然没有用到缓存，则是大量请求全部查询 MySQL 导致。通过下面的命令查看:
```
发现类似很多相同的 SQL 语句，一直处于 query 状态中。

初步分析可能是 user_code 字段没有索引导致。接着查询 user 表的索引情况：

发现这个字段是没有建立索引。增加索引之后，该条 SQL 查询能够正常执行。

3 、没隔一会，又发生大量的请求超时问题。接着进行分析，发现是开启了慢日志查询。大量的 SQL 查
询语句超过慢日志设置的阀值，于是将慢日志关闭之后，速度瞬间提升。CPU 的使用率基本保持在
300%左右。但还不是理想状态。

```
show processlist;
```
```
select id form user where user_code = 'xxxxx';
```
```
show index form user;
```

4 、紧接着将部分实时查询数据的 SQL 语句，都通过缓存 (redis) 读写实现。观察一段时间后，基本维持在
了 70%~80%。

总结：其实本次事故的解决很简单，就是添加索引与缓存结合使用。

```
1. 不推荐在这种 CPU 使用过高的情况下进行慢日志的开启。因为大量的请求，如果真是慢日志问题会
发生日志磁盘写入，性能贼低。
2. 直接通过 MySQL show processlist 命令查看，基本能清晰的定位出部分查询问题严重的 SQL 语
句，在针对该 SQL 语句进行分析。一般可能就是索引、锁、查询大量字段、大表等问题导致。
3. 再则一定要使用缓存系统，降低对 MySQL 的查询频次。
4. 对于内存调优，也是一种解决方案。
```
**场景 2 展开：Java 进程 CPU 飙升到 900%，怎么处理？**

**定位过程：**

CPU 飙升问题定位的一般步骤是：

```
1. 首先通过 top 指令查看当前占用 CPU 较高的进程 PID；
2. 查看当前进程消耗资源的线程 PID：top -Hp PID
3. 通过 print 命令将线程 PID 转为 16 进制，根据该 16 进制值去打印的堆栈日志内查询，查看该线程所
驻留的方法位置。
4. 通过 jstack 命令，查看栈信息，定位到线程对应的具体代码。
5. 分析代码解决问题。
```
**处理过程：**

```
1. 如果是空循环，或者空自旋。
处理方式：可以使用 Thread. sleep 或者加锁，让线程适当的阻塞。
2. 在循环的代码逻辑中，创建大量的新对象导致频繁 GC。比如，从 mysql 查出了大量的数据，比如
100 W 以上等等。
处理方式：可以减少对象的创建数量，或者，可以考虑使用对象池。
3. 其他的一些造成 CPU 飙升的场景，比如 selector 空轮训导致 CPU 飙升。
处理方式：参考 Netty 源码，无效的事件查询到了一定的次数，进行 selector 重建。
```
###### Java 的 CPU 飙升 700%优化的真实案例

尼恩提示：以下案例，来自互联网。大家参考一下，准备一个自己的案例。

最近负责的一个项目上线，运行一段时间后发现对应的进程竟然占用了 700%的 CPU，导致公司的物理
服务器都不堪重负，频繁宕机。

那么, 针对这类 java 进程 CPU 飙升的问题，我们一般要怎么去定位解决呢？、

**采用 top 命令定位进程**

登录服务器，执行 top 命令，查看 CPU 占用情况，找到进程的 pid


很容易发现，PID 为 29706 的 java 进程的 CPU 飙升到 700%多，且一直降不下来，很显然出现了问题。

**使用 top -Hp 命令定位线程**

使用 top -Hp <pid> 命令（为 Java 进程的 id 号）查看该 Java 进程内所有线程的资源占用情况（按
shft+p 按照 cpu 占用进行排序，按 shift+m 按照内存占用进行排序）

此处按照 cpu 排序：

很容易发现，多个线程的 CPU 占用达到了 90%多。我们挑选线程号为 30309 的线程继续分析。

**使用 jstack 命令定位代码**

**1. 线程号转换 5 为 16 进制**

```
printf “%x\n” 命令（tid 指线程的 id 号）将以上 10 进制的线程号转换为 16 进制：
```
```
top
```
```
top -Hp 23602
```
```
printf "%x\n" 30309
```

转换后的结果分别为 7665 ，由于导出的线程快照中线程的 nid 是 16 进制的，而 16 进制以 0 x 开头，所以对
应的 16 进制的线程号 nid 为 0 x 7665

**2. 采用 jstack 命令导出线程快照**

通过使用 dk 自带命令 jstack 获取该 java 进程的线程快照并输入到文件中：

命令（为 Java 进程的 id 号）来获取线程快照结果并输入到指定文件。

**3. 根据线程号定位具体代码**

在 jstack_result. txt 文件中根据线程好 nid 搜索对应的线程描述

根据搜索结果，判断应该是 ImageConverter.run () 方法中的代码出现问题

当然，这里也可以直接采用

来定位具体代码

**分析代码解决问题**

```
jstack -l 进程 ID > ./jstack_result. txt
```
```
jstack -l 29706 > ./jstack_result. txt
```
```
cat jstack_result. txt |grep -A 100  7665
```
```
jstack <pid> |grep -A 200 <nid>
```
```
$jstack 44529 |grep -A 200 ae 24
"System Clock" #28 daemon prio=5 os_prio=0 tid=0 x 00007 efc 19 e 8 e 800 nid=0 xae 24
waiting on condition [0 x 00007 efbe 0 d 91000]
java. lang. Thread. State: TIMED_WAITING (sleeping)
at java.lang.Thread.sleep (Native Method)
at java.lang.Thread.sleep (Thread. java:340)
at java.util.concurrentC.TimeUnit.sleep (TimeUnit. java:386)
at com.*. order.Controller.OrderController.detail (OrderController. java:37)
//业务代码阻塞点
```

下面是 ImageConverter.run () 方法中的部分核心代码。

逻辑说明：

在 while 循环中，不断读取堵塞队列 dataQueue 中的数据，如果数据为空，则执行 continue 进行下一次
循环。

如果不为空，则通过 poll () 方法读取数据，做相关逻辑处理。

初看这段代码好像每什么问题，但是如果 dataQueue 对象长期为空的话，这里就会一直空循环，导致
CPU 飙升。

那么如果解决呢？

分析 LinkedBlockingQueue 阻塞队列的 API 发现：

这两种取值的 API，显然 take 方法更时候这里的场景。

代码修改为：

```
/存储 minicap 的 socket 连接返回的数据 (改用消息队列存储读到的流数据) ，设置阻塞队列长度，防止
出现内存溢出
//全局变量
private BlockingQueue<byte[]> dataQueue = new LinkedBlockingQueue<byte[]>
( 100000 );
//消费线程
@Override
public void run () {
//long start = System.currentTimeMillis ();
while (isRunning) {
//分析这里从 LinkedBlockingQueue
if (dataQueue.isEmpty ()) {
continue;
}
byte[] buffer = device.getMinicap (). dataQueue.poll ();
int len = buffer. length;
}
```
```
//取出队列中的头部元素，如果队列为空则调用此方法的线程被阻塞等待，直到有元素能被取出，如果等待过
程被中断则抛出 InterruptedException
E take () throws InterruptedException;
//取出队列中的头部元素，如果队列为空返回 null
E poll ();
```
```
while (isRunning) {
/* if (device.getMinicap (). dataQueue.isEmpty ()) {
continue;
}*/
byte[] buffer = new byte[ 0 ];
try {
buffer = device.getMinicap (). dataQueue.take ();
} catch (InterruptedException e) {
e.printStackTrace ();
}
......
}
```

重启项目后，测试发现项目运行稳定，对应项目进程的 CPU 消耗占比不到 10%。

###### 参考文献：

https://developer.aliyun.com/article/1053255

https://www.zhihu.com/question/22002813/answer/2662962349

#### 聊聊：MySQL innodb 的事务与日志的实现方式

**有多少种日志**

```
错误日志：记录出错信息，也记录一些警告信息或者正确的信息。
查询日志：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。
慢查询日志：设置一个阈值，将运行时间超过该值的所有 SQL 语句都记录到慢查询的日志文件
中。
二进制日志：记录对数据库执行更改的所有操作。
中继日志：中继日志也是二进制日志，用来给 slave 库恢复
事务日志：重做日志 redo 和回滚日志 undo
```
**事物的 4 种隔离级别**

```
读未提交 (RU)
读已提交 (RC)
可重复读 (RR)
串行
```
**事务是如何通过日志来实现的，说得越深入越好**

事务日志是通过 redo 和 innodb 的存储引擎日志缓冲（Innodb log buffer）来实现的，当开始一个事
务的时候，会记录该事务的 lsn (log sequence number) 号;

当事务执行时，会往 InnoDB 存储引擎的日志的日志缓存里面插入事务日志；

当事务提交时，必须将存储引擎的日志缓冲写入磁盘（通过 innodb_flush_log_at_trx_commit 来控
制），也就是写数据前，需要先写日志。这种方式称为“预写日志方式”

#### 聊聊：MySQL 的 binlog 有有几种录入格式？分别有什么区

#### 别？

logbin 格式：


```
binlog_format=STATEMENT（默认）：数据操作的时间，同步时不一致每一条会修改数据的 sql
语句会记录到 binlog 中。优点是并不需要记录每一条 sql 语句和每一行的数据变化，减少了 binlog
日志量，节约 IO，提高性能。缺点是在某些情况下会导致 master-slave 中的数据不一致 ( 如
sleep () 函数， last_insert_id ()，以及 user-defined functions (udf) 等会出现问题)
binlog_format=ROW：批量数据操作时，效率低不记录每条 sql 语句的上下文信息，仅需记录哪
条数据被修改了，修改成什么样了。而且不会出现某些特定情况下的存储过程、或 function、或
trigger 的调用和触发无法被正确复制的问题。缺点是会产生大量的日志，尤其是 alter table 的时
候会让日志暴涨。
binlog_format=MIXED：是以上两种 level 的混合使用，有函数用 ROW，没函数用 STATEMENT，
但是无法识别系统变量
```
#### 聊聊：MySQL binlog 的几种日志录入格式以及区别

**一：Statement：每一条会修改数据的 sql 都会记录在 binlog 中。**

**优点：**

不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO，提高性能。

相比 row 能节约多少性能与日志量，这个取决于应用的 SQL 情况，正常同一条记录修改或者插入 row
格式所产生的日志量还小于 Statement 产生的日志量，但是考虑到如果带条件的 update 操作，以及整
表删除，alter 表等操作，ROW 格式会产生大量日志，因此在考虑是否使用 ROW 格式日志时应该根据
应用的实际情况，其所产生的日志量会增加多少，以及带来的 IO 性能问题。

**缺点：**

由于记录的只是执行语句，为了这些语句能在 slave 上正确运行，因此还必须记录每条语句在执行的时
候的一些相关信息，以保证所有语句能在 slave 得到和在 master 端执行时候相同的结果。

另外 mysql 的复制, 像一些特定函数功能，slave 可与 master 上要保持一致会有很多相关问题 (如
sleep () 函数， last_insert_id ()，以及 user-defined functions (udf) 会出现问题).

使用以下函数的语句也无法被复制：

```
LOAD_FILE ()
```
```
UUID ()
```
```
USER ()
```
```
FOUND_ROWS ()
SYSDATE () (除非启动时启用了 --sysdate-is-now 选项)
```
同时在 INSERT ... SELECT 会产生比 RBR 更多的行级锁

**二：Row: 不记录 sql 语句上下文相关信息，仅保存哪条记录被修改。**

**优点：** binlog 中可以不记录执行的 sql 语句的上下文相关的信息，仅需要记录那一条记录被修改成什么
了。

所以 rowlevel 的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下
的存储过程，或 function，以及 trigger 的调用和触发无法被正确复制的问题

**缺点：** 所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量
的日志内容。


比如一条 update 语句，修改多条记录，则 binlog 中每一条修改都会有记录，这样造成 binlog 日志量
会很大，特别是当执行 alter table 之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该
表每一条记录都会记录到日志中。

**三：Mixedlevel: 以上两种 level 的混合使用。**

一般的语句修改使用 statment 格式保存 binlog，

如一些函数，statement 无法完成主从复制的操作，则采用 row 格式保存 binlog, MySQL 会根据执行的
每一条具体的 sql 语句来区分对待记录的日志形式，也就是在 Statement 和 Row 之间选择一种。

新版本的 MySQL 中对 row level 模式也被做了优化，并不是所有的修改都会以 row level 来记录，像遇
到表结构变更的时候就会以 statement 模式来记录。

至于 update 或者 delete 等修改数据的语句，还是会记录所有行的变更。

#### 聊聊：Mysql 集群同步时为什么使用 binlog？优缺点是什

#### 么？

```
binlog 是 mysql 提供的日志，所有存储引擎都可用。
支持增量同步
binlog 还可以供其他中间件读取，比如同步到 hdfs 中
如果复制表数据：
不支持某个阶段回放
直接复制数据过程中一旦中断复制（比如断网），很难确定复制的 offset
```
#### 聊聊：Innodb 是如何实现事务的

Innodb 通过 Buffer Pool，LogBuffer，Redo Log，Undo Log 来实现事务，

以一个 update 语句为例：

```
1. Innodb 在收到一个 update 语句后，会先根据条件找到数据所在的⻚，并将该⻚缓存在 Buffer Pool
中
2. 执行 update 语句，修改 Buffer Pool 中的数据，也就是内存中的数据
3. 针对 update 语句生成一个 Redo Log 对象，并存入 LogBuffer 中
4. 针对 update 语句生成 undolog 日志，用于事务回滚
5. 如果事务提交，那么则把 RedoLog 对象进行持久化，后续还有其他机制将 Buffer Pool 中所修改的
数据⻚持久化到磁盘中
6. 如果事务回滚，则利用 undolog 日志进行回滚
```
#### 聊聊：bin log/redo log/undo log 是什么？

bin log、redo log、undo log 三种日志属于不同级别的日志，

按照 Mysql 的划分可以分为服务层和引擎层两大层，bin log 是在服务层实现的；

redo log、undo log 是在引擎层实现的，且是 innodb 引擎独有的，主要和事务相关。

**「 1 、bin log」**


bin log 是 Mysql 数据库级别的文件，记录对 Mysql 数据库执行修改的所有操作，不会记录 select 和 show
语句。

使用任何存储引擎的 Mysql 数据库都会记录 binlog 日志。

在实际应用中， binlog 的主要使用场景有两个，分别是 **主从复制** 和 **数据恢复** 。

主从复制 ：在 Master 端开启 binlog ，然后将 binlog 发送到各个 Slave 端，从而达到主从数据一致。

数据恢复 ：通过使用 Mysqlbinlog 工具来恢复数据。

**「 2 、redo log」**

redo log 中记录的是要更新的数据，比如一条数据已提交成功，并不会立即同步到磁盘，而是先记录到
redo log 中，等待合适的时机再刷盘，为了实现事务的持久性。

如果没有 redo log, 那么每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。

但是这么做会有严重的性能问题，主要体现在两个方面：

因为 Innodb 是以 **页** 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，
这个时候将完整的数据页刷到磁盘的话，太浪费资源了！

一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机 IO 写入性能太差！

因此 Mysql 设计了 redo log ，具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决
性能问题了 (相对而言文件更小并且是顺序 IO)。

**「 3 、undo log」**

除了记录 redo log 外，当进行数据修改时还会记录 undo log，undo log 用于数据的撤回操作，它保留了
记录修改前的内容。通过 undo log 可以实现事务回滚，并且可以根据 undo log 回溯到某个特定的版本的
数据，实现 MVCC。

#### 聊聊：bin log 和 redo log 有什么区别？

```
bin log 会记录所有日志记录，包括 InnoDB、MyISAM 等存储引擎的日志；redo log 只记录 innoDB
自身的事务日志。
bin log 只在事务提交前写入到磁盘，一个事务只写一次；而在事务进行过程，会有 redo log 不断写
入磁盘。
bin log 是逻辑日志，记录的是 SQL 语句的原始逻辑；redo log 是物理日志，记录的是在某个数据页
上做了什么修改。
```
#### 大厂面试题：MySQL 普通索引、唯一索引到底什么区别？

尼恩说明：这个题目，是社群小伙伴面试阿里，遇到的真题哈。

大家好好研究，掌握。比较有难度。


###### 1 概念区分

**普通索引 V.S 唯一索引**

普通索引可重复，唯一索引和主键一样不能重复。

唯一索引可作为数据的一个合法验证手段，例如学生表的身份证号码字段，人为规定该字段不得重复，
那么就使用唯一索引。（一般设置学号字段为主键）

**主键 V.S 唯一索引**

主键保证 DB 的每一行都是唯一、不重复，比如身份证，学号等，不重复。

唯一索引的作用跟主键一样。

但在一张表里面只能有一个主键，不能为空，唯一索引可有多个。唯一索引可有一条记录为 null。

比如学生表：

```
在学校，一般用学号做主键，身份证号作为唯一索引
在教育局，就把身份证号弄成主键，学号作为唯一索引
```
所以选谁做主键，取决于业务需求。

###### 2 案例

某居民系统，每人有唯一身份证号。

若系统要按身份证号查姓名：

```
id_card 字段较大，不推荐做主键。
```
现有如下选择：

```
1. 在 id_card 创建唯一索引
2. 创建一个普通索引
```
假定业务代码已确保不会写入重复身份证号，这两个选择逻辑上都正确。

但性能角度考虑，选择哪个呢？

假设字段 k 上的值都不重复。

InnoDB 索引结构：

```
select name from CUser where id_card = 'ooxx';
```

###### 3 查询性能

通过 B+树从 root 开始层序遍历到叶节点，数据页内部通过二分搜索：

```
普通索引查找到满足条件的第一个记录 (4,400) 后，继续查找下个记录，直到碰到第一个不满足
k=4 的记录
唯一索引查到第一个满足条件的，就停止搜索
```
看起来性能差距很小。

InnoDB 数据按数据页单位读写。

即读一条记录时，并非将该一个记录从磁盘读出，而以页为单位，将其整体读入内存。

所以普通索引，多了一次“查找和判断下一条记录”的操作，即一次指针寻找和一次计算。

若 k=4 记录恰为该数据页的最后一个记录，则此时要取下个记录，还得读取下个数据页。

对整型字段，一个数据页可存近千个 key，因此这种情况概率其实也很低。

因此计算平均性能差异时，可认为该操作成本对 CPU 开销忽略不计。

###### 4 更新性能

往表中插入一个新记录 (4,400)，InnoDB 会有什么反应？

这要看该记录要更新的目标页是否在内存：

**在内存**

```
普通索引找到 3 和 5 之间的位置，插入值，结束。
唯一索引找到 3 和 5 之间的位置，判断到没有冲突，插入值，结束。
```
只是一个判断的差别，耗费微小 CPU 时间。

**不在内存**

```
唯一索引将数据页读入内存，判断到没有冲突，插入值，结束。
普通索引将更新记录在 change buffer，结束。
```
将数据从磁盘读入内存涉及随机 I/O 访问，是 DB 里成本最高的操作之一。

而 change buffer 可以减少随机磁盘访问，所以更新性能提升明显。

###### 5 索引选择最佳实践

普通索引、唯一索引在查询性能上无差别，主要考虑更新性能。

所以，推荐 **尽量选择普通索引** 。

若所有更新后面，都紧跟对该记录的查询，就该关闭 change buffer。

其它情况下，change buffer 都能提升更新性能。

```
select id from T where k= 4
```

普通索引和 change buffer 的配合使用，对数据量大的表的更新优化还是明显的。

在使用机械硬盘时，change buffer 收益也很大。

所以，当你有“历史数据”库，且出于成本考虑用机械硬盘，应该关注这些表里的索引，尽量用普通索
引，把 change buffer 开大，确保“历史数据”表的数据写性能。

###### 6 普通索引带 change buffer 的读过程和写过程

**6.1 插入流程**

假设当前 k 索引树的状态，查找到位置后：

```
k 1 所在数据页在内存 (buffer pool)
k 2 数据页不在内存
```
看如下流程：

**带 change buffer 的更新流程**

```
图中箭头都是后台操作，不影响更新请求的响应。
```
该更新做了如下操作：

```
1. Page 1 在内存，直接更新内存
2. Page 2 不在内存，就往 change buffer 区，缓存一个“往 Page 2 插一行记录”的信息
3. 将前两个动作记入 redo log
```
至此，事务完成。执行该更新语句成本很低，只是写两处内存，然后写一处磁盘（前两次操作合在一起
写了一次磁盘），还是顺序写。

**6.2 处理之后的读请求**

```
insert into t (id, k) values (id 1, k 1), (id 2, k 2);
```

读语句紧随更新语句之后，这时内存中的数据都还在，所以此时这俩读操作就与系统表空间和 redo log
无关。

**带 change buffer 的读过程**

读 Page 1 时，直接从内存返回。

WAL 之后若读数据，是否一定要读盘？一定要从 redo log 将数据更新后才能返回？
其实不用。看上图状态，虽然磁盘上还是之前的数据，但这里直接从内存返回结果，结果是正确的。

读 Page 2 时，需将 Page 2 从磁盘读入内存，然后应用 change buffer 里的操作日志，生成一个正确版本并
返回结果。所以一直到需要读 Page 2 时，该数据页才会被从磁盘读入内存。

综上，这俩机制的更新性能：

```
redo log 主要节省随机写磁盘的 I/O 消耗（转成顺序写）
change buffer 主要节省随机读磁盘的 I/O 消耗
```
###### 7 change buffer 和 redo log 底层原理

更新流程中涉及到：重做日志（redo log）和归档日志 (bin log)

重做日志（redo log）是在引擎层中，采用“黑板-账本”模式（即 WAL 技术）。

**为什么需要 redo log？**

每一次的更新操作的具体数据变更，都是需要写入到磁盘中去的，因为写入的是一条确定的数据，

所以我们还需要在磁盘中找到那条相对应的记录，然后才能完成更新。

而对于磁盘的 IO 操作，众所周知是最消耗的操作，因此为了解决这个问题，我们使用 redo log。

redo log 中记录的是要更新的数据，

```
select * from t
where k
in (k 1, k 2);
```

比如一条数据已提交成功，并不会立即同步到磁盘，而是先记录到 redo log 中，等待合适的时机再刷
盘，为了实现事务的持久性。

如果没有 redo log, 那么每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。

但是这么做会有严重的性能问题，主要体现在两个方面：

因为 Innodb 是以 **页** 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，
这个时候将完整的数据页刷到磁盘的话，太浪费资源了！

一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机 IO 写入性能太差！

因此 Mysql 设计了 redo log ，具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决
性能问题了 (相对而言文件更小并且是顺序 IO)。

**redo log 的具体过程**

在 redo log 中采用是“黑板”+“账本”的过程。

通俗的想法是，一旦由需要写入的内容时，我们先写在黑板上，当黑板上写不下的时候或者有空闲的时
候，再将黑板上的内容都拷贝到账本中。

```
黑板，在 Mysql 中对应日志（redo log）;
账本，对应 Mysql 中的磁盘。
```
举例-----------当有一条记录需要更新，InnoDB 引擎会先把记录写到 redo log（黑板）中，并更新内存。

到这一步，该引擎认为这个更新步骤已经完成了。

当 InnoDB 遇到了系统空闲的时候，才会将这个操作记录更新到磁盘中去。

**update 图解执行过程**


**bin log**

归档日志 (bin log)-------在 server 层，记录 MySQL 功能层面的事情

redo log 跟 bin log 的区别：

redo log 是存储引擎层产生的，而 bin log 是数据库层产生的。


假设一个事务，对表做了 10 万行的记录插入，在这个过程中，一直不断的往 redo log 顺序记录，而 bin
log 不会记录，直到这个事务提交，才会一次写入 bin log 文件中。

**两阶段提交**

从流程图中，我们可以看到，

redo log 的完成过程是被分成了两个阶段的，分别是：prepare 阶段和 commit 状态。

这就是所谓的两阶段提交。

目的：为了保证数据库的目前状态和它通过日志恢复出来的库的状态是一致的。

**redo log 的好处**

相较于在事务提交时将所有修改过的页刷新到磁盘中，只将该事务执行过程中产生的 redo 日志刷到磁
盘，有下面的好处：

**(1) redo 日志降低了刷盘频率** 。

**(2) redo 日志占用的空间非常小** 。

**(3) redo 日志是顺序写入磁盘的** 。

在执行事务的过程中，每执行一条语句，就可能产生若干条 redo 日志，这些日志是按照产生的顺序写入
磁盘的，也就是顺序 IO。

InnoDB 存储引擎的事务采用了 WAL 技术（Write-Ahead Logging）, 这种技术就是先写日志，再写磁
盘，只有日志写入成功，才算事务提交成功，这里的日志就是 redo log。

当发生宕机且数据未刷新到磁盘的时候，可以通过 redo log 恢复过来，保证事务的持久性。

**事务的原子性、一致性和持久性由事务的 redo 日志和 undo 日志来保证** 。

redo log 称为重做日志，提供再写入操作，恢复提交事务修改的页操作，用来保证事务的持久性。

undo log 称为回滚日志，回滚行记录到某个特定版本，用来保证事务的原子性、一致性。

redo log 是存储引擎层生成的日志，记录的是物理级别上的页修改操作，比如页号 xxx，偏移量 yyy，写
入了 zzz 数据，主要是为了保证数据的可靠性。

undo log 是存储引擎层生成的日志，记录的是逻辑操作的日志，比如对某一行数据进行了 insert 语句操
作，那么 undo log 就记录一条与之相反的 delete 操作。主要用于事务的回滚和一致性非锁定读 (mvcc)。

**什么是 Buffer Pool**

InnoDB 存储引擎在处理客户端的请求时，如果需要访问某个页的数据，就会把完整的页中的数据全部
加载到内存中，即使只访问页中的一条记录，也需要先把 **整个页** 的数据加载到内存中。

将整个页加载到内存后就可以进行读写访问了，而且在读写访问之后并不着急把该页对应的内存空间释
放掉，而是将其缓存起来，这样将来有请求再次访问该页面时，就可以剩下磁盘 IO 的开销了。

为了缓存磁盘中的页，Innodb 在 MySQL 服务器启动时就像操作系统申请了一片连续的内存，即 Buffer
Pool（缓冲池）。

默认情况下，Buffer Pool 的大小为 128 M。

Buffer Pool 对应的一片连续的内存被划分为若干个页面，页面大小与 Innodb 表空间用的页面大小一
致，默认都是 16 kb，为了与磁盘中的页面区分开来，我们把这些 Buffer Pool 中的页面称为缓冲页。


当我们修改了 Buffer Pool 中某个缓冲页的数据，它就与磁盘上的页不一致了，这样的缓冲页称为脏页。

当然，我们可以每当修改完某个数据页时，就立即将其刷新到磁盘中对应的页上，但是频繁的往磁盘中
写数据会严重的影响程序的性能，所以每次修改缓冲页后，我们并不着急立即将修改刷新到磁盘上，而
是在某个时间点进行刷新。

后台有专门的线程负责每隔一段时间就把脏页刷新到磁盘，这样就可以不影响用户线程处理正常的请
求。

总之：

InnoDB 存储引擎是以页为单位来管理存储空间的，在真正访问页面之前，需要先把磁盘中的页加载到
内存中的 Buffer Pool 中，

之后才可以访问，所有的变更都必须先更新缓冲池中的数据，然后缓冲池中的脏页以一定的频率刷新到
磁盘（checkpoint 机制），通过缓冲池来优化 CPU 和磁盘之间的鸿沟，这样就能保证整体的性能不会下
降的太快。

Buffer Pool 缓存表数据与索引数据，把磁盘上的数据加载到缓冲池，避免每次访问都进行磁盘 IO，起
到加速访问的作用。

速度快，那 **为啥不把所有数据都放到缓冲池里** ？

凡事都具备两面性，抛开数据易失性不说，访问快速的反面是存储容量小：

（ 1 ）缓存访问快，但容量小，数据库存储了 200 G 数据，缓存容量可能只有 64 G；

（ 2 ）内存访问快，但容量小，买一台笔记本磁盘有 2 T，内存可能只有 16 G；

因此，只能把“最热”的数据放到“最近”的地方，以“最大限度”的降低磁盘访问。

**什么是 change buffer**

change buffer 是 buffer pool 中的一部分内存；

它既在内存中有拷贝，也可以持久化到磁盘；

其大小通过参数 innodb_change_buffer_max_size 控制, 表示最多占用 buffer pool 的百分比；

当需要更新一个数据页时，如果数据页在内存中，则直接更新；

否则，在不影响数据一致性的前提下，InnoDB 将这些操作缓存在 change buffer 中，这样就不必从磁
盘中读取数据，当下次查询需要访问这个数据页时，再将数据页读入内存，然后执行 change buffer 中
与这个页有关的操作，最后将查询结果返回。

merge：将 change buffer 的操作应用到数据页的过程称为 merge。

除了访问数据页会触发 merge 外；系统后台有线程会定期 merge；数据库正常关闭的过程中也会触发
merge 操作。

更新操作记录到 change buffer ，可以减少读磁盘，提高执行效率；而且读入数据会占用 buffer pool
，还可以提高内存使用率。

change buffer 使用条件

```
对于唯一索引，所有更新操作都需要做唯一性约束的判断，必须将数据页读入内存，直接在内存中
更新，不使用 change buffer 。
对于普通索引，当数据页在内存中时，直接进行更新操作即可；当数据页不在内存中时，直接将更
新操作写入 change buffer 即可。
```
**change buffer 使用场景**


change buffer 的主要作用就是将记录的变更操作缓存下来，

在 merge 之前， change buffer 记录的越多，收益就越大。

change buffer 适合页面变更完之后被马上访问的 **概率较小** 的场景。就是修改之后，就访问的概率小。

如果页面变更之后又要被访问，此时会立即触发 merge 过程，这样反而增加了 change buffer 的维护
代价，多了一个写 change buffer 的操作，此时关闭 change buffer 反而能提高效率；

**redo log 和 change buffer 的区别**

redo log：

```
节省了随机写磁盘的 IO 消耗
```
chagne buffer:

```
节省了随机读磁盘的 IO 消耗
```
更新普通索引时：

如果数据不在内存中，可以直接将变更操作缓存到 change buffer，而不需要将数据读入内存；如果没
有 change buffer，则必须将数据读入内存，然后更新数据。此时节省了随机读磁盘的 IO 消耗；

redo log 将数据变更操作记录在日志中，不用写入磁盘即可返回执行结果；如果没有 redo log，则更新
完数据必须先将数据写入磁盘，再返回执行结果。此时节省了随机写磁盘的 IO 消耗；

查询普通索引时：

如果数据不在内存中，必须先将数据读入磁盘，再执行 change buffer 中的相关操作，然后返回查询结
果。只有查询数据时，才需要将数据读磁盘到内存

###### 8 总结

因为唯一索引用不了 change buffer，若业务可以接受，从性能角度，优先考虑非唯一索引。

**到底何时使用唯一索引**

问题就在于“业务可能无法确保”，而本文前提是“业务代码已保证不会写入重复数据”，才讨论的性能问
题。

```
若业务无法保证或业务就是要求数据库来做约束
没有撤退可言，必须创建唯一索引。那本文意义就在于，若碰上大量插入数据慢、内存命中率低
时，多提供了一个排查思路
“归档库”场景，可考虑使用唯一索引
比如线上数据只需保留半年，然后历史数据存在归档库。此时，归档数据已是确保没有唯一键冲
突。要提高归档效率，可考虑把表的唯一索引改为普通索引。
```
**若某次写入使用了 change buffer，之后主机异常重启，是否会丢失 change buffer 数据**

不会！虽然是只更新内存，但在事务提交时，change buffer 的操作也被记录到了 redo log。所以崩溃
恢复时，change buffer 也能找回。


**merge 时是否会把数据直接写回磁盘**

**merge 流程**

```
1. 从磁盘读入数据页到内存（老版本数据页）
2. 从 change buffer 找出该数据页的 change buffer 记录 (可能多个），依次应用，得到新版数据页
3. 写 redo log
该 redo log 包含数据的变更和 change buffer 的变更
```
至此 merge 结束。

这时，数据页和内存中 change buffer 对应磁盘位置都尚未修改，是脏页，之后各自刷回自己物理数
据，就是另外一过程。

在构造第一个例子的过程，通过 session A 的配合，让 session B 删除数据后又重新插入一遍数据，然后
就发现 explain 结果中，rows 字段从 10001 变成 37000 多。

而如果没有 session A 的配合，只是单独执行 delete from t 、call idata ()、explain 这三句话，会看到
rows 字段其实还是 10000 左右。这是什么原因呢？

如果没有复现，检查

```
隔离级别是不是 RR（Repeatable Read，可重复读）
创建的表 t 是不是 InnoDB 引擎
```
为什么经过这个操作序列，explain 的结果就不对了？

delete 语句删掉了所有的数据，然后再通过 call idata () 插入了 10 万行数据，看上去是覆盖了原来 10 万
行。

但 session A 开启了事务并没有提交，所以之前插入的 10 万行数据是不能删除的。这样，之前的数据每
行数据都有两个版本，旧版本是 delete 之前数据，新版本是标记 deleted 的数据。

这样，索引 a 上的数据其实有两份。

不对啊，主键上的数据也不能删，那没有使用 force index 的语句，使用 explain 命令看到的扫描行数为
什么还是 100000 左右？（潜台词，如果这个也翻倍，也许优化器还会认为选字段 a 作为索引更合适）

是的，不过这个是主键，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是 show
table status 的值。

大家的机器如果 IO 能力比较差的话，做这个验证的时候，可以把 innodb_flush_log_at_trx_commit 和
sync_binlog 都设成 0 。

#### 携程二面：聊聊 MySQL 中的 WAL 策略和 CheckPoint 技

#### 术

###### InnoDB 体系架构

在说 WAL 之前，有必要简单介绍下 InnoDB 存储引擎的体系架构，下面是官方的 InnoDB 引擎结构图，
主要分为内存结构和磁盘结构两大部分。


**内存区域**

**Buffer Pool:** 在 InnoDB 访问表记录和索引时会在 Buffer Pool 的页中缓存，以后使用可以减少磁盘 IO 操
作，提升效率。主要用来缓存热的 **数据页和索引页** 。

**Log Buffer** ：用来缓存 redo log

**Adaptive Hash Index** ：自适应哈希索引

**Change Buffer** ：针对非唯一普通索引场景，它是一种应用在非唯一普通索引页（non-unique
secondary index page）不在缓冲池中，对页进行了写操作，并不会立刻将磁盘页加载到缓冲池，而仅
仅记录缓冲变更（Buffer Changes），等未来数据被读取时，再将数据合并（Merge）恢复到缓冲池中
的技术。写缓冲的目的是降低写操作的磁盘 IO，提升数据库性能。

从宏观架构的维度，可以简单理解，InnoDB 存储引擎由 **内存池** 和一些 **后台线程** 组成：


**内存池**

为啥需要内存池？ 主要是为了性能。

InnoDB 存储引擎是基于磁盘存储的，并将其中的记录按照 **页** 的方式进行管理。由于 CPU 速度与磁盘速
度之间的不匹配，通常会使用 **缓冲池** 技术来提高数据库的整体性能。

内存池其实就是一块内存区域，所以这里的内存池也被称为缓冲池（简单理解为缓存就好了）。

拥有了缓冲池后，“读取页” 操作的类似于 Cache Aside（旁路缓冲）策略：

```
首先将从磁盘读到的页存放在缓冲池中
下一次再读相同的页时，首先判断该页是否在缓冲池中。若在缓冲池中，称该页在缓冲池中被命
中，直接读取该页。否则，读取磁盘上的页。
```
“修改页” 操作的具体步骤就是这样的：

```
首先修改在缓冲池中的页；
然后再以一定的频率刷新到磁盘上。
```
所谓 ” **脏页** “ 就发生在修改这个操作中，如果缓冲池中的页已经被修改了，但是还没有刷新到磁盘上，那
么我们就称缓冲池中的这页是 ”脏页“，即缓冲池中的页的版本要比磁盘的新。

所以 WAL 技术修改数据需要写两次写入

```
内存写入：第一次，修改在缓冲池中的页，随机 IO
磁盘写入：第二次，再以一定的频率刷新到磁盘上，顺序 IO
```
当然： **缓冲池的大小直接影响着数据库的整体性能** 。

**后台线程**

后台线程其实最大的作用就是用来完成 “将从磁盘读到的页存放在缓冲池中” 以及 “将缓冲池中的数据以
一定的频率刷新到磁盘上” 这俩个操作的，当然了，还有其他的作用。

以下是《MySQL 技术内幕：InnoDB 存储引擎 - 第 2 版》对于后台线程的描述：


```
后台线程的主要作用就是刷新内存池中的数据，保证内存池中缓存的是最近的数据；此外将已修
改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下 InnoDB 能恢复到正常运行
状态。
```
另外， **InnoDB 存储引擎是多线程的模型** ，也就是说它拥有多个不同的后台线程，负责处理不同的任
务。这里简单列举下几种不同的后台线程：

```
Master Thread ：主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性
IO Thread ：在 InnoDB 存储引擎中大量使用了 AIO（Async IO）来处理写 IO 请求，这样可以极
大提高数据库的性能。IO Thread 的工作主要是负责这些 IO 请求的回调（call back）处理
Purge Thread ：回收已经使用并分配的 undo 页
Page Cleaner Thread ：将之前版本中脏页的刷新操作都放入到单独的线程中来完成。其目的是
为了减轻原 Master Thread 的工作及对于用户查询线程的阻塞，进一步提高 InnoDB 存储引擎的
性能
```
###### redo log 与 WAL 策略

上文我们提到，当缓冲池中的某页数据被修改后，该页就被标记为 ”脏页“，脏页的数据会被定期刷新到
磁盘上。

倘若每次一个页发生变化，就将新页的版本刷新到磁盘，那么这个开销是非常大的。

并且，如果热点数据都集中在某几个页中，那么数据库的性能将变得非常差。另外，如果在从缓冲池将
页的新版本刷新到磁盘时发生了宕机，那么这个数据就不能恢复了。

所以，为了避免发生数据丢失的问题，当前事务数据库系统（并非 MySQL 所独有）普遍都采用了
WAL（Write Ahead Log， **预写日志** ）策略：

即 **当事务提交时，先写重做日志（redo log），再修改页（先修改缓冲池，再刷新到磁盘）；当由于发
生宕机而导致数据丢失时，通过 redo log 来完成数据的恢复** 。

这也是事务 ACID 中 D（Durability 持久性）的要求。

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力
称为 **crash-safe** 。

在 redo log 中采用是“黑板”+“账本”的过程。

通俗的想法是，一旦由需要写入的内容时，我们先写在黑板上，当黑板上写不下的时候或者有空闲的时
候，再将黑板上的内容都拷贝到账本中。

```
黑板，在 Mysql 中对应日志（redo log）;
账本，对应 Mysql 中的磁盘。
```
举例-----------当有一条记录需要更新，InnoDB 引擎会先把记录写到 redo log（黑板）中，并更新内存。

到这一步，该引擎认为这个更新步骤已经完成了。

当 InnoDB 遇到了系统空闲的时候，才会将这个操作记录更新到磁盘中去。

这就是 WAL。黑板就是 redo log，账本就是磁盘。

当然了，redo log 可不是黑板这么简单，一张用完了换一张就行了，这里有必要详细解释下。

每个 InnoDB 存储引擎 **至少** 有 1 个重做日志文件组（ redo log group），每个文件组下至少有 2 个重做
日志文件（redo log file），默认的话是一个 redo log group，其中包含 2 个 redo log file：
ib_logfile 0 和 ib_logfile 1 。

一般来说，为了得到更高的可靠性，用户可以设置多个镜像日志组（mirrored log groups），将不同
的文件组放在不同的磁盘上，以此提高 redo log 的高可用性。在日志组中每个 redo log file 的大小一
致，并以 **循环写入** 的方式运行。


所谓循环写入，举个例子，如下图，一个 redo log group，包含 3 个 redo log file：

InnoDB 存储引擎会先写 redo log file 0，当 file 0 被写满的时候，会切换至 redo log file 1，当 file 1
也被写满时，会切换到 redo log file 2 中，而当 file 2 也被写满时，会再切换到 file 0 中。

可以看出，redo log file 的大小设置对于 InnoDB 存储引擎的性能有着非常大的影响：

```
redo log file 不能设置得太大，如果设置得很大，在恢复时可能需要很长的时间
redo log file 又不能设置得太小了，否则可能导致一个事务的日志需要多次切换重做日志文件
```
###### CheckPoint 技术

有了 redo log 就可以高枕无忧了吗？显然不是这么简单，我们仍然面临这样 3 个问题：

1 ）缓冲池不是无限大的，也就是说不能没完没了的存储我们的数据等待一起刷新到磁盘

2 ）redo log 是循环使用而不是无限大的（也许可以，但是成本太高，同时不便于运维），那么当所有
的 redo log file 都写满了怎么办？

3 ）当数据库运行了几个月甚至几年时，这时如果发生宕机，重新应用 redo log 的时间会非常久，此时
恢复的代价将会非常大。

因此 Checkpoint 技术的目的就是解决上述问题：

```
缓冲池不够用时，将脏页刷新到磁盘
redo log 不可用时，将脏页刷新到磁盘
缩短数据库的恢复时间
```
所谓 CheckPoint 技术简单来说其实就是在 redo log file 中找到一个位置，将这个位置前的页都刷新到
磁盘中去，这个位置就称为 CheckPoint（检查点）。

针对上面这三点我们依次来解释下：

1 ） **缩短数据库的恢复时间** ：当数据库发生宕机时，数据库不需要重做所有的日志，因为 Checkpoint
之前的页都已经刷新回磁盘。故数据库只需对 Checkpoint 后的 redo log 进行恢复就行了。这显然大大
缩短了恢复的时间。

2 ） **缓冲池不够用时，将脏页刷新到磁盘** ：所谓缓冲池不够用的意思就是缓冲池的空间无法存放新读取
到的页，这个时候 InnoDB 引擎会怎么办呢？LRU 算法。 InnoDB 存储引擎对传统的 LRU 算法做了一
些优化，用其来管理缓冲池这块空间。


总的思路还是传统 LRU 那套，具体的优化细节这里就不再赘述了：即最频繁使用的页在 LRU 列表
（LRU List）的前端，最少使用的页在 LRU 列表的尾端；当缓冲池的空间无法存放新读取到的页时，将
首先释放 LRU 列表中尾端的页。这个被释放出来（溢出）的页，如果是脏页，那么就需要强制执行
CheckPoint，将脏页刷新到磁盘中去。

3 ） **redo log 不可用时，将脏页刷新到磁盘** ：

所谓 redo log 不可用就是所有的 redo log file 都写满了。但事实上，其实 redo log 中的数据并不是时
时刻刻都是有用的，那些已经不再需要的部分就称为 ”可以被重用的部分“，即当数据库发生宕机时，数
据库恢复操作不需要这部分的 redo log，因此这部分就可以被覆盖重用（或者说被擦除）。

举个例子来具体解释下：一组 4 个文件，每个文件的大小是 1 GB，那么总共就有 4 GB 的 redo log file
空间。write pos 是当前 redo log 记录的位置，随着不断地写入磁盘，write pos 也不断地往后移，就
像我们上文说的，写到 file 3 末尾后就回到 file 0 开头。CheckPoint 是当前要擦除的位置（将
Checkpoint 之前的页刷新回磁盘），也是往后推移并且循环的：

**write pos 和 CheckPoint 之间的就是 redo log file 上还空着的部分，可以用来记录新的操作** 。如果
write pos 追上 CheckPoint，就表示 redo log file 满了，这时候不能再执行新的更新，得停下来先覆盖
（擦掉）一些 redo log，把 CheckPoint 推进一下。

综上所述，Checkpoint 所做的事情无外乎是将缓冲池中的脏页刷新到磁盘。不同之处在于每次刷新多
少页到磁盘，每次从哪里取脏页，以及什么时间触发 Checkpoint。在 InnoDB 存储引擎内部，有两种
Checkpoint，分别为：

```
Sharp Checkpoint ：发生在数据库关闭时将所有的脏页都刷新回磁盘，这是默认的工作方式，参
数 innodb_fast_shutdown=1
Fuzzy Checkpoin ：InnoDB 存储引擎内部使用这种模式，只刷新一部分脏页，而不是刷新所有的
脏页回磁盘。关于 Fuzzy CheckPoint 具体的情况这里就不再赘述了。
```

###### 有了 bin log 为什么还需要 redo log？

前文我们讲过，MySQL 架构可以分成俩：

```
一层是 Server 层，它主要做的是 MySQL 功能层面的事情；
另一层就是存储引擎，负责存储与提取相关的具体事宜。
```
redo log 是 InnoDB 引擎特有的日志，

而 Server 层也有自己的日志，包括：错误日志（error log）、二进制日志（binlog）、慢查询日志
（slow query log）、查询日志（log）。

其他三个日志顾明思意都挺好理解的，需要解释的就是 binlog（二进制日志，binary log），它记录了
对 MySQL 数据库执行更改的所有操作，但是不包括 SELECT 和 SHOW 这类操作，因为这类操作对数据
本身并没有修改。也就是说，binlog 是 **逻辑日志** ，记录的是这个语句的原始逻辑，比如 “给 ID=1 这一
行的 a 字段加 1”。

可以看出来，binlog 日志只能用于归档，因此 binlog 也被称为 **归档日志** ，显然如果 MySQL 只依靠
binlog 等这四种日志是没有 crash-safe 能力的，所以为了弥补这种先天的不足，得益于 MySQL 可插拔
的存储引擎架构，InnoDB 开发了另外一套日志系统 — 也就是 redo log 来实现 crash-safe 能力。

这就是为什么有了 bin log 为什么还需要 redo log 的答案。

回顾下 redo log 存储的东西，可以发现 redo log 是 **物理日志** ，记录的是 “在某个数据页上做了什么修
改”。

另外，还有一点不同的是：binlog 是追加写入的，就是说 binlog 文件写到一定大小后会切换到下一
个，并不会覆盖以前的日志；而 redo log 是循环写入的。

#### 美团一面：知道 MySQL 的 WAL、LSN、Checkpoint 吗？

###### 1. WAL （预写式日志）技术

WAL 的全称是 Write-Ahead Logging。

修改的数据要持久化到磁盘，会先写入磁盘的文件系统缓存，然后可以由后台线程异步慢慢地刷回到磁
盘。

所以 WAL 技术修改数据需要写两次写入。

**1.1 两次写入**

内存写入：第一次，修改在缓冲池中的页，随机 IO

磁盘写入：第二次，再以一定的频率刷新到磁盘上，顺序 IO

**1.2 WAL 的好处**

节省了随机写磁盘的 IO 消耗（转成顺序写）。

而且顺序 IO 性能比较高。

具体请参考（尼恩的葵花宝典视频）

###### 2 LSN（日志序列号）


LSN 是 Log Sequence Number 的缩写，即日志序列号，表示 Redo Log 的序号。

**2.1 特性**

LSN 占用 8 字节，LSN 的值会随着日志的写入而逐渐增大，每写入一个 Redo Log 时，LSN 就会递增该
Redo Log 写入的字节数。

**2.2 LSN 的不同含义**

重做日志写入的总量，单位字节。

通过 LSN 开始号码和结束号码可以计算出写入的日志量。

```
checkpoint 的位置
```
最近一次刷盘的页，即最近一次检查点 _(checkpoint)_ ，也是通过 _LSN_ 来记录的，它也会被写入 _redo log_
里。

```
数据页的版本号。
```
在每个页的头部，有一个 FIL_PAGE_LSN，记录的该页的 LSN。表示该页最后刷新时 LSN 的大小。

其可以用来标记数据页的“版本号”。因此页中的 LSN 用来判断页是否需要进行恢复操作。

```
通过数据页中的 LSN 值和 redo log 中的 LSN 值比较，如果页中的 LSN 值小于 redo log 中 LSN
值，则表示数据丢失了一部分，这时候可以通过 redo log 的记录来恢复到 redo log 中记录的 LSN
值时的状态。
```
**2.3 查看 LSN**

redo log 的 LSN 信息可以通过 show engine innodb status 命令来查看。

其中：

```
log sequence number 就是当前的 redo log (in buffer) 中的 lsn；
log flushed up to 是刷到 redo log file on disk 中的 lsn；
pages flushed up to 是已经刷到磁盘数据页上的 LSN；
last checkpoint at 是上一次检查点所在位置的 LSN。
```
###### 3. Checkpoint (检查点)

**3.1 背景**

缓冲池的容量和重做日志（redo log）容量是有限的。

**3.2 目的**

```
---
LOG
---
Log sequence number 15114138
Log flushed up to 15114138
Pages flushed up to 15114138
Last checkpoint at 15114129
0 pending log flushes, 0 pending chkp writes
10 log i/o's done, 0.00 log i/o's/second
```

Checkpoint 所做的事就是把脏页给刷新回磁盘。

**3.3 定义**

一个时间点，由一个 LSN 值（Checkpoint LSN）表示的整型值，在 checkpoint LSN 之前的每个数据页
(buffer pool 中的脏页) 的更改都已经落盘 (刷新到数据文件中)，checkpoint 完成后，在 checkpoint LSN
之前的 Redo Log 就不再需要了。

所以：checkpoint 是通过 LSN 实现的。

**3.4 分类**

Sharp Checkpont

该机制下，在数据库发生关闭时将所有的脏页都刷新回磁盘。

Fuzzy Checkpoint

在该机制下，只刷新一部分脏页，而不是刷新所有脏页回磁盘。

**3.5 检查点触发时机**

```
Master Thread Checkpoint
```
后台异步线程以每秒或每十秒的速度从缓冲池的脏页列表中刷新一定比例的页回磁盘。

```
FLUSH_LRU_LIST Checkpoint
```
为了保证 LRU 列表中可用页的数量（通过参数 innodb_lru_scan_depth 控制，默认值 1024 ），后台线程
定期检测 LRU 列表中空闲列表的数量，若不满足，就会将移除 LRU 列表尾端的页，若移除的页为脏页，
则需要进行 Checkpoint。

```
Async/sync Flush Checkpoint
```
当重做日志不可用（即 redo log 写满）时，需要强制将一些页刷新回磁盘，此时脏页从脏页列表中获
取。

```
Dirty Page too much Checkpoint
```
即脏页数量太多，会强制推进 CheckPoint。目的是保证缓冲区有足够的空闲页。
innodb_max_dirty_pages_pct 的默认值为 75 ，表示当缓冲池脏页比例达到该值时，就会强制进行
Checkpoint，刷新一部分脏页到磁盘。

```
数据库关闭时，使用 Sharp Checkpont 机制刷新脏页。
数据库运行时，使用 Fuzzy Checkpoint 机制刷新脏页。
```
```
show VARIABLES like 'innodb_lru_scan_depth'
```
```
show VARIABLES like 'innodb_max_dirty_pages_pct'
```

**3.6 解决的问题**

```
缩短数据库的恢复时间。
缓冲池不够用时，刷新脏页到磁盘。
重做日志满时，刷新脏页。
```
###### 4. LSN 与 checkpoint 的联系

LSN 号串联起一个事务开始到恢复的过程。

重启 innodb 时，Redo log 完不完整，采用 Redo log 相关知识。用 Redo log 恢复，启动数据库时，
InnoDB 会扫描数据磁盘的数据页 data disk lsn 和日志磁盘中的 checkpoint lsn。

两者相等则从 checkpoint lsn 点开始恢复，恢复过程是利用 redo log 到 buffer pool，直到
checkpoint lsn 等于 redo log file lsn，则恢复完成。如果 checkpoint lsn 小于 data disk lsn，说明在
检查点触发后还没结束刷盘时数据库宕机了。

因为 checkpoint lsn 最新值是在数据刷盘结束后才记录的，检查点之后有一部分数据已经刷入数据磁
盘，这个时候数据磁盘已经写入部分的部分恢复将不会重做，直接跳到没有恢复的 lsn 值开始恢复。

###### 5. 总结

日志空间中的每条日志对应一个 LSN 值，而在数据页的头部也记录了当前页最后一次修改的 LSN 号，每
次当数据页刷新到磁盘后，会去更新日志文件中 checkpoint，以减少需要恢复执行的日志记录。

极端情况下，数据页刷新到磁盘成功后，去更新 checkpoint 时如果宕机，则在恢复过程中，由于
checkpoint 还未更新，则数据页中的记录相当于被重复执行，不过由于在日志文件中的操作记录具有幂
等性，所以同一条 redo log 执行多次，不影响数据的恢复。

#### 聊聊：sharding-jdbc 和 mycat 比较？

sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，
性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合
sharding-jdbc 的依赖；

mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于
各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。

通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 sharding-jdbc，client 层方
案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么
多；但是中大型公司最好还是选用 mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团
队很大，人员充足，那么最好是专门弄个人来研究和维护 mycat，然后大量项目直接透明使用即可。

#### 聊聊：现在有一个未分库分表的系统，未来要分库分表，

#### 如何设计才可以让系统从未分库分表动态切换到分库分表

#### 上？

**「 1 ）停机迁移方案」**

我先给你说一个最 low 的方案，就是很简单，大家伙儿凌晨 12 点开始运维，网站或者 app 挂个公告，
说 0 点到早上 6 点进行运维，无法访问。


接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一
个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。

导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就
用最新的代码，然后直接启动连到新的分库分表上去。

验证一下，ok 了，完美，大家伸个懒腰，看看看凌晨 4 点钟的北京夜景，打个滴滴回家吧。

但是这个方案比较 low，谁都能干，我们来看看高大上一点的方案。

**「 2 ）双写迁移方案」**

这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨 4 点的风景。

简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，除了对老库增删改，都加上对新
库的增删改，这就是所谓的双写，同时写俩库，老库和新库。

然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根
据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是
比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。

导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数
据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表
的数据都完全一致为止。

接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于
分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这
么干的。

#### 聊聊：如何设计可以动态扩容缩容的分库分表方案？

**「 1 ）停机扩容（不推荐）」**

这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽
出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据
量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。

从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。那么你写个工具，多
弄几台机器并行跑， 1 小时数据就导完了。这没有问题。

如果 3 个库 + 12 个表，跑了一段时间了，数据量都 1~2 亿了。光是导 2 亿数据，都要导个几个小时，
6 点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证， 10 点才可以搞完。所以不能这么
搞。

**「 2 ）优化后的方案」**

一开始上来就是 32 个库，每个库 32 个表，那么总共是 1024 张表。

我可以告诉各位同学，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑
还是数据量支撑都没问题。

每个库正常承载的写入并发量是 1000 ，那么 32 个库就可以承载 32 * 1000 = 32000 的写并发，如果每
个库承载 1500 的写并发，32 * 1500 = 48000 的写并发，接近 5 万/s 的写入并发，前面再加一个 MQ，
削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。

有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的
这么一个规模， 128 个库， 256 个库， 512 个库。

1024 张表，假设每个表放 500 万数据，在 Mysql 里可以放 50 亿条数据。


每秒的 5 万写并发，总共 50 亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。

谈分库分表的扩容，第一次分库分表，就一次性给他分个够， 32 个库， 1024 张表，可能对大部分的中
小型互联网公司来说，已经可以支撑好几年了。

一个实践是利用 32 * 32 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024
张表。根据某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。

#### 聊聊：Mysql 主从复制原理的是啥？

这里先放一张图，这张图很好的诠释的主从复制的原理

上面主要分成了三步，下面会详细说明。

(1) Master 的更新事件 (update、insert、delete) 会按照顺序写入 bin-log 中。当 Slave 连接到 Master 的
后, Master 机器会为 Slave 开启

```
binlog dump 线程, 该线程会去读取 bin-log 日志
```
(2) Slave 连接到 Master 后, Slave 库有一个 I/O 线程通过请求 binlog dump thread 读取 bin-log 日志, 然后
写入从库的 relay log 日志中。

(3) Slave 还有一个 SQL 线程, 实时监控 relay-log 日志内容是否有更新, 解析文件中的 SQL 语句, 在 Slave 数据
库中去执行。

**总结**

(1) 既然是要把事件记录到 bin-log 日志，那么对于 Master 就必须 **开启 bin-log 功能** 。

(2) 整个 Mysql 主从复制一共开启了 3 个线程。 **Master 开启 IO 线程，Slave 开启 IO 线程和 SQL 线程** 。

(3) 这点也很重要那就是 Master 和 Slave 交互的时候，记住这里是 Slave 去请求 Master, 而不是 Master 主动
推给 Slave。Slave 通过 IO 线程


连接 Master 后发起请求, Master 服务器收到 Slave IO 线程发来的日志请求信息，io 线程去将 bin-log 内容
返回给 slave IO 线程。

#### 聊聊：Mysql 主从复制同步方式有哪些？

**(1) 异步复制**

Mysql 主从同步默认是异步复制的。就是上面三步中,只有第一步是同步的 (也就是 Mater 写入 bin log 日
志), 就是主库写入 binlog 日志后即可成功返回客户端，无须等待 binlog 日志传递给从库的过程。

Master 不关心 Slave 的数据有没有写入成功。因此如果 Master 和 Slave 之间有网络延迟，就会造成暂时
的数据不一致的现象；

如果 Master 出故障，而数据还没有复制过去，则会造成数据丢失；但也有好处，效率较其他两种复制方
式最高。

**(2) 同步复制**

对于同步复制而言，Master 主机将事件发送给 Slave 主机后会触发一个等待，直到所有 Slave 节点（如果
有多个 Slave）返回数据复制成功的信息给 Master。

这种复制方式最安全，但是同时，效率也是最差的。

**(3) 半同步复制**

对于半同步复制而言，Master 主机将事件发送给 Slave 主机后会触发一个等待，直到其中一个 Slave 节点
（如果有多个 Slave）返回数据复制成功的信息给 Master。

由此增强了数据的一致性，但是因为 Master 主机的确认开销，会损耗一部分的性能；

另外，半同步复制除了不需要等待所有 Slave 主机确认事件的接收外，半同步数据复制并不要求那些事
件完全地执行，因此，仍有可能看到在 Slave 主机上数据复制延迟的发生，如果因为网络延迟等原因造
成 Slave 迟迟没有返回复制成功的信息，超过了 Master 设置的超时时长，半同步复制就降级为异步复制
方式，而后继续数据复制。

#### 聊聊：Mysql 主从同步延时产生原因? 怎么优化？

上面也说了， **「Mysql 默认采用的异步操作」** ，因为它的效率明显是最高的。因为只要写入 bin log 后事
物就结束返回成功了。但由于 **从库从主库异步拷贝日志** 以及 **串行执行 SQL 的特点** ，所以从库的数据一
定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，
甚至几百毫秒才能读取到。这就是主从同步延时问题。

**「1)、影响延迟因素」**

这里整理了影响主从复制延迟大致有以下几个原因：

```
主节点如果执行一个很大的事务，那么就会对主从延迟产生较大的影响
网络延迟，日志较大，slave 数量过多
主上多线程写入，从节点只有单线程同步
机器性能问题，从节点是否使用了“烂机器”
锁冲突问题也可能导致从机的 SQL 线程执行慢
```
**「 2 ）、优化主从复制延迟」**

这个没有说去完全解决，要想解决那么就只能采用同步复制策略。不过，一般不建议使用这种同步模
式。显而易见，如果写操作必须等待更新同步完成，肯定会极大地影响性能，除非你不在乎性能。


1 ）大事务：将大事务分为小事务，分批更新数据

2 ）减少 Slave 的数量，不要超过 5 个，减少单次事务的大小

3 ）Mysql 5.7 之后，可以使用多线程复制，使用 MGR 复制架构

4 ）在磁盘、raid 卡、调度策略有问题的情况下可能会出现单个 IO 延迟很高的情况，可用 iostat 命令查看
DB 数据盘的 IO 情况，再进一步判断

5 ）针对锁问题可以通过抓去 processlist 以及查看 information_schema 下面和锁以及事务相关的表来查
看

**总结**

主机与从机之间的物理延迟是无法避免的，既然无法避免就可以考虑尝试通过缓存等方式，降低新修改
数据被立即读取的概率。

#### 聊聊：Mysql 集群的高可用方案？

```
1. 主从或主主 + Keepalived
2. MHA（Master-Mater replication manager for MySQL），MMM（MySQL Master High
Available）
3. pxc
4. MGR/InnoDB Cluster
5. Xenon
6. Orchestrator
```
###### 1 主从或主主 + Keepalived

主从或主主 + Keepalived 算是历史比较悠久的 MySQL 高可用方案，常见架构如下：


其大致原理是：

在主实例的 Keepalived 中，增加监测本机 MySQL 是否存活的脚本，

如果监测 MySQL 挂了，就会重启 Keepalived，从而使 VIP 飘到从实例。

优点

```
部署简单。
只有两个节点，没有主实例宕机后选主的问题。
```
缺点：

```
级联复制或者一主多从在切换之后，其他从实例需要重新配置连接新主。
云环境使用不了。
```
###### 2 MMM

MMM（Master-Master replication manager for MvSQL，MySQL 主主复制管理器）

MMM 是一套灵活的脚本程序，基于 perl 实现，用来对 mysql replication 进行监控和故障迁移，并能管
理 MySQL Master-Master 复制的配置。

是一套支持双主故障切换和双主日常管理的脚本程序。

MMM 使用 Perl 语言开发，主要用来监控和管理 MySQL Master-Master （双主）复制，

虽然叫做双主复制，但是业务上同一时刻只允许对一个主进行写入，另一台备选主上提供部分读服务，
以加速在主主切换时备选主的预热，


可以说 MMM 这套脚本程序一方面实现了故障切换的功能，另一方面其内部附加的工具脚本也可以实现
多个 Slave 的 read 负载均衡。

MMM 提供了自动和手动两种方式移除一组服务器中复制延迟较高的服务器的虚拟 ip，同时它还可以备
份数据，实现两节点之间的数据同步等。

由于 MMM 无法完全保证数据的一致性，所以 MMM 适用于对数据的一致性要求不是很高，但是又想最
大程度地保证业务可用性的场景。

VIP 是基于 ARP 协议，因此所有节点必须处于同一局域网。

###### 3 MHA

MHA，Master High Availability，目前在 MySQL 高可用方面是一个相对成熟的解决方案，

它由日本 DeNA 公司的 youshimaton（现就职于 Facebook）开发，是一套优秀的作为 MySQL 高可用
性环境下故障切换和主从提升的高可用软件。

MHA（Master High Avaliable） 是一款 MySQL 开源高可用程序，

MHA 在监测到主实例无响应后，可以自动将同步最靠前的 Slave 提升为 Master，

然后将其他所有的 Slave 重新指向新的 Master。

常见架构如下：


优点

```
可以根据需要扩展 MySQL 的节点数量。
只要复制没有延迟，MHA 通常可以在几秒内实现故障切换。
可以使用任何存储引擎。
```
缺点

```
仅监视主数据库。
需要做 SSH 互信
使用 Perl 开发，二次开发困难。
跟不上 MySQL 新版本，最近一次发版是 2018 年。
```
###### 4 PXC

PXC（Percona XtraDB Cluster）是一个完全开源的 MySQL 高可用解决方案。

它将 Percona Server、Percona XtraBackup 与 Galera 库集成在一起，以实现多主复制的 MySQL 集
群。

常见架构图如下：


优点

```
去中心：任何节点宕机，集群都可以继续运行而不会丢失任何数据。
每个节点都可以写入数据并自动同步到其他节点。
数据写入需要集群所有节点验证通过才会提交，保证了数据的强一致性。
```
缺点

```
只支持 InnoDB 引擎。
木桶效应：集群吞吐量取决于性能最差的节点。
增加节点时，必须从现有节点之一复制完整的数据集。
```
###### 5 MGR/InnoDB Cluster

MySQL 5.7 推出了 MGR（MySQL Group Replication），与 PXC 类似，也实现了多节点数据写入和强
一致性的特点。

不借助外部力量，只使用 MySQL 本身。

如果主节点挂掉，将自动选择某个从改成主；无需人工干预，基于组复制，保证数据一致性。

MGR 采用 GCS（Group Communication System）协议同步数据，GCS 可保证消息的原子性。

在使用 MGR 时，如果要实现完整的高可用方案，就需要用到 InnoDB Cluster。

架构图如下：


**MySQL InnoDB Cluster** 是一个高可用的框架，构成组件：

```
MySQL Group Replication
提供 DB 的扩展、自动故障转移
MySQL Router
轻量级中间件，提供应用程序连接目标的故障转移。MySQL Router 是一个轻量级的中间件，可以
提供负载均衡和应用连接的故障转移。它是 MySQL 团队为 MGR 量身打造的，通过使用 Router 和
Shell, 用户可以利用 MGR 实现完整的数据库层的解决方案。如果您在使用 MGR，请一定配合使用
Router 和 Shell，可以理解为它们是为 MGR 而生的，会配合 MySQl 的开发路线图发展的工具。
MySQL Shell
新的 MySQL 客户端，多种接口模式。可以设置群组复制及 Router。MySQL Shell 是 MySQL 团队打
造的一个统一的客户端，它可以对 MySQL 执行数据操作和管理。它支持通过 JavaScript，
Python，SQL 对关系型数据模式和文档型数据模式进行操作。使用它可以轻松配置管理 InnoDB
Cluster。
```
优点

```
支持多节点写入，有冲突检测机制。
强一致性。
```
缺点

```
只支持 InnoDB 表，每张表都要有主键。
最多只支持 9 个节点。
```

###### 6 Xenon

Xenon 是一个使用 Raft 协议的 MySQL 高可用和复制管理工具，使用 Go 语音编写。

架构图如下：

Xenon 基于 Raft 协议进行无中心化选主，并能实现秒级切换。

优点

```
不需要管理节点。
无数据丢失的快速故障转移。
```
缺点

```
只适用于 GTID。
默认情况下，Xenon 和 MySQL 跑在同一个账号下。
```
###### 7 Orchestrator

Orchestrator 是 Go 语音编写的 MySQL 高可用性和复制管理工具，作为服务运行并提供命令行访问、
HTTP API 和 Web 界面。

架构图如下：


优点

```
自动发现 MySQL 拓扑，可以拖动拓扑图进行复制关系的变更。
提供命令行和 API 接口，方便运维管理。
快速故障转移。
多种故障等级，应对不同故障等级可配置不同处理办法。
```
缺点

```
相对于其他高可用组件，参数多很多。
在某些场景可能出现丢数据的情况，数据补偿机制需要优化。
```
## 美团一面：聊聊 MySQL 的七种日志

最近，一位小伙伴面试了美团，遇到了一个 MySQL 日志的面试题：

**聊聊，MySQL 七种日志。**

可惜，这个小伙只能说出其中的 4 中，其他的 3 种没有答上来，

**所以，导致他一面挂了，非常可惜**

现在把这个题目，以及参考答案，收入咱们的《尼恩 Java 面试宝典》，供后面的小伙伴参考，前车之
鉴啊


#### 面试题的出现形式：

聊聊，MySQL 的七种日志

聊聊，MySQL 的日志有哪些，具体的功能和作用如何

#### 参考答案

```
错误日志（error log）
```
error log 主要记录 MySQL 在启动、关闭或者运行过程中的错误信息，在 MySQL 的配置文件 my. cnf 中，

可以通过 log-error=/var/log/mysqld. log 执行 mysql 错误日志的位置。

```
慢查询日志（slow query log）
MySQL 的慢查询日志是 MySQL 提供的一种日志记录，它用来记录在 MySQL 中响应时间超过阀值的
语句，具体指运行时间超过 long_query_time 值的 SQL，则会被记录到慢查询日志中。
具体指运行时间超过 long_query_time 值的 SQL，则会被记录到慢查询日志中。long_query_time
的默认值为 10 ，意思是运行 10 秒以上的语句。
由他来查看哪些 SQL 超出了我们的最大忍耐时间值，比如一条 sql 执行超过 5 秒钟，我们就算慢
SQL，希望能收集超过 5 秒的 sql，结合之前 explain 进行全面分析
在生产环境中，如果要手工分析日志，查找、分析 SQL，显然是个体力活，MySQL 提供了日志分
析工具 mysqldumpslow。
一般查询日志（general log）
```
general log 记录了客户端连接信息以及执行的 SQL 语句信息，包括客户端何时连接了服务器、客户端发
送的所有 SQL 以及其他事件，比如 MySQL 服务启动和关闭等等。

```
重写日志（redo log）
redo log 属于 MySQL 存储引擎 InnoDB 的事务日志。
回滚日志（undo log）
undo log 属于逻辑日志，如其名主要起到回滚的作用，它是保证事务原子性的关键。
二进制日志（bin log）
bin log 是一种数据库 Server 层（和什么引擎无关），以二进制形式存储在磁盘中的逻辑日志。
```
#### 七种日志的详细分析

进入正题前，可以先简单介绍一下，MySQL 的逻辑架构，


MySQL 的逻辑架构大致可以分为三层：

```
第一层：处理客户端连接、授权认证，安全校验等。
第二层：服务器 server 层，负责对 SQL 解释、分析、优化、执行操作引擎等。
第三层：存储引擎，负责 MySQL 中数据的存储和提取。
```
我们要知道 MySQL 的服务器层是不管理事务的，事务是由存储引擎实现的，

而 MySQL 中支持事务的存储引擎又属 InnoDB 使用的最为广泛，

所以后续文中提到的存储引擎都以 InnoDB 为主。

而且，可以再简单介绍一下，MySQL 数据更新流程，作为铺垫，具体如下图：


#### 1 、redo log（重做日志）

**redo log** 属于 MySQL 存储引擎 **InnoDB** 的事务日志。

MySQL 的数据是存放在磁盘中的，每次读写数据都需做磁盘 IO 操作，如果并发场景下性能就会很差。

为此 MySQL 提供了一个优化手段，引入缓存 **Buffer Pool** 。

这个缓存中包含了磁盘中 **部分** 数据页（ **page** ）的映射，以此来缓解数据库的磁盘压力。

当从数据库读数据时，首先从缓存中读取，如果缓存中没有，则从磁盘读取后放入缓存；

当向数据库写入数据时，先向缓存写入，此时缓存中的数据页数据变更，这个数据页称为 **脏页** ， **Buffer
Pool** 中修改完数据后会按照设定的更新策略，定期刷到磁盘中，这个过程称为 **刷脏页** 。

**如何保证数据不丢失，实现高可靠，实现事务持久性 ？**

如果刷脏页还未完成，可 MySQL 由于某些原因宕机重启，此时 **Buffer Pool** 中修改的数据还没有及时的
刷到磁盘中，就会导致数据丢失，无法保证事务的持久性。

为了解决这个问题引入了 **redo log** ，redo Log 如其名侧重于重做！

它记录的是数据库中 **每个页** 的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数
据页，且只能恢复到最后一次提交的位置。

**redo log** 用到了 **WAL** （Write-Ahead Logging）技术，这个技术的核心就在于修改记录前，一定要先写
日志，并保证日志先落盘，才能算事务提交完成。

有了 **redo log** 再修改数据时，InnoDB 引擎会把更新记录先写在 redo log 中，再修改 **Buffer Pool** 中的数
据，

当提交事务时，调用 **fsync** 把 redo log 刷入磁盘。

至于缓存中更新的数据文件何时刷入磁盘，则由后台线程异步处理。


**注意** ：此时 redo log 的事务状态是 **prepare** ，还未真正提交成功，要等 **bin log** 日志写入磁盘完成才会变
更为 **commit** ，事务才算真正提交完成。

这样一来即使刷脏页之前 MySQL 意外宕机也没关系，只要在重启时解析 redo log 中的更改记录进行重
放，重新刷盘即可。

**redo log 大小固定**

redo log 采用固定大小，循环写入的格式，当 redo log 写满之后，重新从头开始如此循环写，形成一个
环状。

那为什么要如此设计呢？

因为 redo log 记录的是数据页上的修改，如果 **Buffer Pool** 中数据页已经刷磁盘后，那这些记录就失效
了，新日志会将这些失效的记录进行覆盖擦除。

上图中的 **write pos** 表示 redo log 当前记录的日志序列号 **LSN** (log sequence number)，写入还未刷盘，
循环往后递增；

**check point** 表示 redo log 中的修改记录已刷入磁盘后的 LSN，循环往后递增，这个 LSN 之前的数据已经
全落盘。

**write pos** 到 **check point** 之间的部分是 redo log 空余的部分（绿色），用来记录新的日志；

**check point** 到 **write pos** 之间是 redo log 已经记录的数据页修改数据，此时数据页还未刷回磁盘的部
分。

当 **write pos** 追上 **check point** 时，会先推动 **check point** 向前移动，空出位置（刷盘）再记录新的日
志。

```
注意 ：redo log 日志满了，在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已
经刷到磁盘中了。擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求的，此刻 MySQL
的性能会下降。所以在并发量大的情况下，合理调整 redo log 的文件大小非常重要。
```

**crash-safe**

因为 redo log 的存在使得 **Innodb** 引擎具有了 **crash-safe** 的能力，即 MySQL 宕机重启，系统会自动去检
查 redo log，将修改还未写入磁盘的数据从 redo log 恢复到 MySQL 中。

MySQL 启动时，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。

会先检查数据页中的 **LSN** ，如果这个 LSN 小于 redo log 中的 LSN，即 **write pos** 位置，说明在 **redo log**
上记录着数据页上尚未完成的操作，接着就会从最近的一个 **check point** 出发，开始同步数据。

简单理解，比如：redo log 的 **LSN** 是 500 ，数据页的 LSN 是 300 ，表明重启前有部分数据未完全刷入到磁
盘中，那么系统则将 redo log 中 **LSN** 序号 300 到 500 的记录进行重放刷盘。

###### 2. undo log（回滚日志）

**undo log** 也是属于 MySQL 存储引擎 InnoDB 的事务日志。

**undo log** 属于逻辑日志，如其名主要起到回滚的作用，它是保证事务原子性的关键。

记录的是数据修改前的状态，在数据修改的流程中，同时会记录一条与当前操作相反的逻辑日志到
**undo log** 中。

我们举个栗子：

假如更新 ID=1 记录的 name 字段，name 原始数据为小富，现改 name 为程序员内点事

事务执行 **update X set name = 程序员内点事 where id =1** 语句时，先会在 **undo log** 中记录一条相反
逻辑的 **update X set name = 小富 where id =1** 记录，这样当某些原因导致服务异常事务失败，就可
以借助 **undo log** 将数据回滚到事务执行前的状态，保证事务的完整性。


那可能有人会问：

同一个事物内的一条记录被多次修改，那是不是每次都要把数据修改前的状态都写入 **undo log** 呢？

答案是不会的！

**undo log** 只负责记录事务开始前要修改数据的原始版本，当我们再次对这行数据进行修改，所产生的
修改记录会写入到 **redo log** ， **undo log** 负责完成回滚， **redo log** 负责完成前滚。

**2.1 回滚**

未提交的事务，即事务未执行 commit。但该事务内修改的脏页中，可能有一部分脏块已经刷盘。如果
此时数据库实例宕机重启，就需要用回滚来将先前那部分已经刷盘的脏块从磁盘上撤销。

**2.2 前滚**

未完全提交的事务，即事务已经执行 **commit** ，但该事务内修改的脏页中只有一部分数据被刷盘，另外
一部分还在 **buffer pool** 缓存上，如果此时数据库实例宕机重启，就需要用前滚来完成未完全提交的事
务。

将先前那部分由于宕机在内存上的未来得及刷盘数据，从 **redo log** 中恢复出来并刷入磁盘。

```
数据库实例恢复时，先做前滚，后做回滚。
```
如果你仔细看过了上边的 **MySQL 数据更新流程图** 就会发现， **undo log、redo log、bin log** 三种日志
都是在刷脏页之前就已经刷到磁盘了的，相互协作最大限度保证了用户提交的数据不丢失。

###### 3. bin log（归档日志）

**bin log** 是一种数据库 Server 层（和什么引擎无关），以二进制形式存储在磁盘中的逻辑日志。

**bin log** 记录了数据库所有 DDL 和 DML 操作（不包含 SELECT 和 SHOW 等命令，因为这类操作对数据本
身并没有修改）。


默认情况下，二进制日志功能是关闭的。

可以通过以下命令查看二进制日志是否开启：

**bin log** 也被叫做 **归档日志** ，

因为它不会像 **redo log** 那样循环写擦除之前的记录，而是会一直记录日志。

一个 **bin log** 日志文件默认最大容量 1 G（也可以通过 **max_binlog_size** 参数修改），单个日志超过最大
值，则会新创建一个文件继续写。

**bin log** 日志的内容格式其实就是执行 SQL 命令的反向逻辑，这点和 **undo log** 有点类似。一般来说开启
**bin log** 都会给日志文件设置过期时间（ **expire_logs_days** 参数，默认永久保存），要不然日志的体量
会非常庞大。

**bin log** 主要应用于 MySQL 主从模式（ **master-slave** ）中，主从节点间的数据同步；以及基于时间点的
数据还原。

**3.1 主从同步**

通过下图 MySQL 的主从复制过程，来了解下 **bin log** 在主从模式下的应用。

```
mysql> SHOW VARIABLES LIKE 'log_bin';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| log_bin | OFF |
+---------------+-------+
```
```
mysql> show binary logs;
+-----------------+-----------+
| Log_name | File_size |
+-----------------+-----------+
| mysq-bin. 000001 | 8687 |
| mysq-bin. 000002 | 1445 |
| mysq-bin. 000003 | 3966 |
| mysq-bin. 000004 | 177 |
| mysq-bin. 000005 | 6405 |
| mysq-bin. 000006 | 177 |
| mysq-bin. 000007 | 154 |
| mysq-bin. 000008 | 154 |
```
```
mysql> show variables like 'expire_logs_days';
+------------------+-------+
| Variable_name | Value |
+------------------+-------+
| expire_logs_days | 0 |
+------------------+-------+
1 row in set
```
```
mysql> SET GLOBAL expire_logs_days= 30 ;
Query OK, 0 rows affected
```

```
用户在主库 master 执行 DDL 和 DML 操作，修改记录顺序写入 bin log ;
从库 slave 的 I/O 线程连接上 Master，并请求读取指定位置 position 的日志内容;
Master 收到从库 slave 请求后，将指定位置 position 之后的日志内容，和主库 bin log 文件的名称
以及在日志中的位置推送给从库;
slave 的 I/O 线程接收到数据后，将接收到的日志内容依次写入到 relay log 文件最末端，并将读取
到的主库 bin log 文件名和位置 position 记录到 master-info 文件中，以便在下一次读取用;
slave 的 SQL 线程检测到 relay log 中内容更新后，读取日志并解析成可执行的 SQL 语句，这样就实
现了主从库的数据一致;
```
**3.2 基于时间点还原**

我们看到 **bin log** 也可以做数据的恢复，而 **redo log** 也可以，那它们有什么区别？

```
层次不同：redo log 是 InnoDB 存储引擎实现的，bin log 是 MySQL 的服务器层实现的，但 MySQL
数据库中的任何存储引擎对于数据库的更改都会产生 bin log。
作用不同：redo log 用于碰撞恢复（ crash recovery ），保证 MySQL 宕机也不会影响持久性；
bin log 用于时间点恢复（ point-in-time recovery ），保证服务器可以基于时间点恢复数据和主
从复制。
内容不同：redo log 是物理日志，内容基于磁盘的页 Page ；bin log 的内容是二进制，可以根据
binlog_format 参数自行设置。
写入方式不同：redo log 采用循环写的方式记录；binlog 通过追加的方式记录，当文件大小大于
给定值后，后续的日志会记录到新的文件上。
刷盘时机不同：bin log 在事务提交时写入；redo log 在事务开始时即开始写入。
```
bin log 与 redo log 功能并不冲突而是起到相辅相成的作用，需要二者同时记录，才能保证当数据库发
生宕机重启时，数据不会丢失。

###### 4. relay log（中继日志）

**relay log** 日志文件具有与 **bin log** 日志文件相同的格式，从上边 MySQL 主从复制的流程可以看出，

**relay log** 起到一个中转的作用， **slave** 先从主库 **master** 读取二进制日志数据，写入从库本地，后续再
异步由 **SQL 线程** 读取解析 **relay log** 为对应的 SQL 命令执行。

###### 5. slow query log


慢查询日志（ **slow query log** ）: 用来记录在 MySQL 中执行时间超过指定时间的查询语句，在 SQL 优
化过程中会经常使用到。通过慢查询日志，我们可以查找出哪些查询语句的执行效率低，耗时严重。

出于性能方面的考虑，一般只有在排查慢 SQL、调试参数时才会开启，默认情况下，慢查询日志功能是
关闭的。可以通过以下命令查看是否开启慢查询日志：

通过如下命令开启慢查询日志后，我发现 **iZ 2 zebfzaequ 90 bdlz 820 sZ-slow. log** 日志文件里并没有内
容啊，可能因为我执行的 SQL 都比较简单没有超过指定时间。

上边提到超过 **指定时间** 的查询语句才算是慢查询，那么这个时间阈值又是多少嘞？我们通过
**long_query_time** 参数来查看一下，发现默认是 10 秒。

这里我们将 **long_query_time** 参数改小为 0.001 秒再次执行查询 SQL，看看慢查询日志里是否有变
化。

果然再执行 SQL 的时，执行时间大于 0.001 秒，发现慢查询日志开始记录了。

###### 6. general query log

一般查询日志（general query log）：用来记录用户的 **所有** 操作，包括客户端何时连接了服务器、
客户端发送的所有 SQL 以及其他事件，比如 MySQL 服务启动和关闭等等。

```
MySQL 服务器会按照它接收到语句的先后顺序写入日志文件。
```
由于一般查询日志记录的内容过于详细，开启后 Log 文件的体量会非常庞大，所以出于对性能的考虑，
默认情况下，该日志功能是关闭的，通常会在排查故障需获得详细日志的时候才会临时开启。

我们可以通过以下命令查看一般查询日志是否开启，命令如下：

```
mysql> SHOW VARIABLES LIKE 'slow_query%';
+---------------------+--------------------------------------------------------+
| Variable_name | Value |
+---------------------+--------------------------------------------------------+
| slow_query_log | OFF |
| slow_query_log_file | /usr/local/mysql/data/iZ 2 zebfzaequ 90 bdlz 820 sZ-slow. log |
+---------------------+--------------------------------------------------------+
```
```
mysql>  SET GLOBAL slow_query_log=ON; Query OK, 0 rows affected
```
```
mysql>  SET GLOBAL slow_query_log=ON;
Query OK, 0 rows affected
```
```
mysql> SET GLOBAL long_query_time=0.001; Query OK, 0 rows affected
```

下边开启一般查询日志并查看日志存放的位置。

执行一条查询 SQL 看看日志内容的变化。

我们看到日志内容详细的记录了所有执行的命令、SQL、SQL 的解析过程、数据库设置等等。

###### 7. error log

错误日志（ **error log** ）: 应该是 MySQL 中最好理解的一种日志，主要记录 MySQL 服务器每次启动和
停止的时间以及诊断和出错信息。

默认情况下，该日志功能是开启的，通过如下命令查找错误日志文件的存放路径。

```
mysql> show variables like 'general_log';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| general_log | OFF |
+---------------+-------+
```
```
mysql> SET GLOBAL general_log=on;
Query OK, 0 rows affected
```
```
mysql> show variables like 'general_log_file';
+------------------+---------------------------------------------------+
| Variable_name | Value |
+------------------+---------------------------------------------------+
| general_log_file | /usr/local/mysql/data/iZ 2 zebfzaequ 90 bdlz 820 sZ. log |
+------------------+---------------------------------------------------+
```
```
mysql> select * from t_config;
+---------------------+------------+---------------------+---------------------+
| id | remark | create_time | last_modify_time |
+---------------------+------------+---------------------+---------------------+
| 1325741604307734530 | 我是广播表 | 2020 - 11 - 09 18 :06:44 | 2020 - 11 - 09 18 :06:44 |
+---------------------+------------+---------------------+---------------------+
```

**注意** ：错误日志中记录的可并非全是错误信息，像 MySQL 如何启动 InnoDB 的表空间文件、如何初始
化自己的存储引擎，初始化 buffer pool 等等，这些也记录在错误日志文件中。

#### 聊聊：预编译 sql 是什么？

完整解释：

https://dev.mysql.com/doc/refman/8.0/en/prepare.html

PreparedStatement

#### 聊聊： 预编译 sql 有什么好处？

```
预编译 sql 会被 mysql 缓存下来
作用域是每个 session，对其他 session 无效，重新连接也会失效
提高安全性防止 sql 注入
select * from user where id =?
"1; delete from user where id = 1";
编译语句有可能被重复调用，也就是说 sql 相同参数不同在同一 session 中重复查询执行效率明显比
较高
mysql 5,8 支持服务器端的预编译
```
#### 聊聊：子查询与 join 哪个效率高？

子查询虽然很灵活，但是执行效率并不高。

#### 聊聊：为什么子查询效率低？

在执行子查询的时候，MYSQL 创建了临时表，查询完毕后再删除这些临时表

```
mysql> SHOW VARIABLES LIKE 'log_error';
+---------------+---------------------------------------------------------------
-+
| Variable_name | Value
|
+---------------+---------------------------------------------------------------
-+
| log_error | /usr/local/mysql/data/LAPTOP-UHQ 6 V 8 KP. err |
+---------------+---------------------------------------------------------------
-+
```

子查询的速度慢的原因是多了一个创建和销毁临时表的过程。

而 join 则不需要创建临时表所以会比子查询快一点

#### 聊聊：join 查询算法了解吗？

```
Simple Nested-Loop Join：SNLJ，简单嵌套循环连接
Index Nested-Loop Join：INLJ，索引嵌套循环连接
Block Nested-Loop Join：BNLJ，缓存块嵌套循环连接
```
#### 聊聊： join 查询可以无限叠加吗？Mysql 对 join 查询有什

#### 么限制吗？

建议 join 不超过 3 张表关联，mysql 对内存敏感，关联过多会占用更多内存空间，使性能下降

Too many tables; MySQL can only use 61 tables in a join；

系统限制最多关联 61 个表

#### 聊聊： 如何优化过多 join 查询关联？

```
适当使用冗余字段减少多表关联查询
驱动表和被驱动表（小表 join 大表）
业务允许的话尽量使用 inner join 让系统帮忙自动选择驱动表
关联字段一定创建索引
调整 JOIN BUFFER 大小
```
#### 聊聊：是否有过 mysql 调优经验？

调优：

```
1. sql 调优
2. 表（结构）设计调优
3. 索引调优
4. 慢查询调优
5. 操作系统调优
6. 数据库参数调优
```
#### 聊聊：开发中使用过哪些调优工具？

官方自带：

```
EXPLAIN
mysqldumpslow
show profiles 时间
optimizer_trace
```
第三方：性能诊断工具，参数扫描提供建议，参数辅助优化


#### 聊聊：死锁是如何产生的，如何预防？

###### 发生死锁的必要条件

发生死锁的 **必要条件** 有 4 个，分别为互斥条件、不可剥夺条件、请求与保持条件和循环等待条件，如下图
所示：

**互斥条件**

在一段时间内，计算机中的某个资源只能被一个进程占用。此时，如果其他进程请求该资源，则只能等
待。

**不可剥夺条件**

某个进程获得的资源在使用完毕之前，不能被其他进程强行夺走，只能由获得资源的进程主动释放。

**请求与保持条件**

进程已经获得了至少一个资源，又要请求其他资源，但请求的资源已经被其他进程占有，此时请求的进
程就会被阻塞，并且不会释放自己已获得的资源。

**循环等待条件**

系统中的进程之间相互等待，同时各自占用的资源又会被下一个进程所请求。例如有进程 A、进程 B 和进
程 C 三个进程，进程 A 请求的资源被进程 B 占用，进程 B 请求的资源被进程 C 占用，进程 C 请求的资源被进
程 A 占用，于是形成了循环等待条件，如下图所示：

需要注意的是，只有 4 个必要条件都满足时，才会发生死锁。


处理死锁有 4 种方法，分别为预防死锁、避免死锁、检测死锁和解除死锁，如下图所示：

```
预防死锁 ：处理死锁最直接的方法就是破坏造成死锁的 4 个必要条件中的一个或多个，以防止死锁
的发生。
避免死锁 ：在系统资源的分配过程中，使用某种策略或者方法防止系统进入不安全状态，从而避免
死锁的发生。
检测死锁 ：这种方法允许系统在运行过程中发生死锁，但是能够检测死锁的发生，并采取适当的措
施清除死锁。
解除死锁 ：当检测出死锁后，采用适当的策略和方法将进程从死锁状态解脱出来。
```
在实际工作中，通常采用有序资源分配法和银行家算法这两种方式来避免死锁，大家可自行了解。

###### MySQL 中的死锁问题

在 MySQL 5.5.5 及以上版本中，MySQL 的默认存储引擎是 InnoDB。

该存储引擎使用的是行级锁，在某种情况下会产生死锁问题，所以 InnoDB 存储引擎采用了一种叫作 **等
待图** （wait-for graph）的方法来自动检测死锁，如果发现死锁，就会自动回滚一个事务。

###### 死锁案例

接下来，我们看一个 MySQL 中的死锁案例。

**第一步** ：打开终端 A，登录 MySQL，将事务隔离级别设置为可重复读，开启事务后为 account 数据表中
id 为 1 的数据添加排他锁，如下所示：

**第二步** ：打开终端 B，登录 MySQL，将事务隔离级别设置为可重复读，开启事务后为 account 数据表中
id 为 2 的数据添加排他锁，如下所示：

```
mysql> set session transaction isolation level repeatable read;
Query OK, 0 rows affected (0.00 sec)
```
```
mysql> start transaction;
Query OK, 0 rows affected (0.00 sec)
```
```
mysql> select * from account where id = 1 for update;
+----+--------+---------+
| id | name | balance |
+----+--------+---------+
| 1 | 张三 | 300 |
+----+--------+---------+
1 row in set (0.00 sec)
```
```
mysql> set session transaction isolation level repeatable read;
Query OK, 0 rows affected (0.00 sec)
```
```
mysql> start transaction;
Query OK, 0 rows affected (0.00 sec)
```

**第三步** ：在终端 A 为 account 数据表中 id 为 2 的数据添加排他锁，如下所示：

此时，线程会一直卡住，因为在等待终端 B 中 id 为 2 的数据释放排他锁。

**第四步** ：在终端 B 中为 account 数据表中 id 为 1 的数据添加排他锁，如下所示：

此时发生了死锁。通过如下命令可以查看死锁的日志信息。

通过命令行查看 LATEST DETECTED DEADLOCK 选项相关的信息，可以发现死锁的相关信息，或者通过
配置 innodb_print_all_deadlocks（MySQL 5.6.2 版本开始提供）参数为 ON，将死锁相关信息打印到
MySQL 错误日志中。

###### 实际开发中如何尽量避免死锁发生

一般来讲在实际开发中，很少会发生死锁的情况，尤其是在业务并发量不是很大的情况下。在并发很大
的情况下可能会存在偶尔产生死锁。

不过呢，在自己实际开发中，有遇到过请求一个接口出现 100%概率死锁的情况。

当时的场景其实很简单。一段业务代码中，有去走 Dubbo 调其它接口服务, 这就存在了两个事务, 结果各
自事务提交的时候，都需要等待对方的锁释放，就导致每次都发生死锁超时。

这其实是一种代码不规范而导致死锁的发生。这里也总结下如何尽量避免死锁发生。

**在 MySQL 中，通常通过以下几种方式来避免死锁。**

```
尽量让数据表中的数据检索都通过索引来完成，避免无效索引导致行锁升级为表锁。
合理设计索引，尽量缩小锁的范围。
尽量减少查询条件的范围，尽量避免间隙锁或缩小间隙锁的范围。
尽量控制事务的大小，减少一次事务锁定的资源数量，缩短锁定资源的时间。
```
```
mysql> select * from account where id = 2 for update;
+----+--------+---------+
| id | name | balance |
+----+--------+---------+
| 2 | 李四 | 350 |
+----+--------+---------+
1 row in set (0.00 sec)
```
```
mysql> select * from account where id = 2 for update;
```
```
mysql> select * from account where id = 1 for update;
ERROR 1213 ( 40001 ): Deadlock found when trying to get lock; try restarting
transaction
```
```
show engine innodb status\G
```

```
如果一条 SQL 语句涉及事务加锁操作，则尽量将其放在整个事务的最后执行。
尽可能使用低级别的事务隔离机制。
尽量使用主键更新数据, 因为主键是唯一索引，在等值查询能查到数据的情况下只会产生行锁，不
会产生间隙锁，这样产生死锁的概率就减少了。当然如果是范围查询，一样会产生间隙锁。
在主键等值更新的时候，尽量先查询看数据库中有没有满足条件的数据，如果不存在就不用更新，
存在才更新。为什么要这么做呢，因为如果去更新一条数据库不存在的数据，一样会产生间隙锁。
举例：如果表中只有 id=1 和 id=5 的数据，那么如果你更新 id=3 的 sql，因为这条记录表中不存在，
那就会产生一个 (1,5) 的间隙锁，但其实这个锁就是多余的，因为去更新一个数据都不存在的数据没
有任何意义。
```
```
不同的应用访问同一组表时，应尽量约定以相同的顺序访问各表。对一个表而言，应尽量以固定的
顺序存取表中的行。这点真的很重要, 它可以明显的减少死锁的发生。
举例：好比有 a, b 两张表，如果事务 1 先 a 后 b, 事务 2 先 b 后 a, 那就可能存在相互等待产生死锁。那如
果事务 1 和事务 2 都先 a 后 b，那事务 1 先拿到 a 的锁，事务 2 再去拿 a 的锁，如果锁冲突那就会等待事
务 1 释放锁，那自然事务 2 就不会拿到 b 的锁，那就不会堵塞事务 1 拿到 b 的锁，这样就避免死锁了。
在允许幻读和不可重复度的情况下，尽量使用 RC 的隔离级别，避免 gap lock 造成的死锁，因为产
生死锁经常都跟间隙锁有关，间隙锁的存在本身也是在 RR 隔离级别来
```
## 附 1 ：MySQL 开发的三十六条军规

#### 一、核心军规 (5)

###### 1.1 尽量不在数据库做运算

```
别让脚趾头想事情，那是脑瓜子的职责
让数据库多做她擅长的事:
尽量不在数据库做运算
复杂运算秱到程序端 CPU
尽可能简单应用 MySQL
举例: md 5 () / Order by Rand ()
```
###### 1.2 控制单表数据量

```
一年内的单表数据量预估
纯 INT 不超 1000 W
含 CHAR 不超 500 W
合理分表不超载
USERID
DATE
AREA
......
```

```
建议单库不超过 300-400 个表
```
###### 1.3 保持表身段苗条

```
表字段数少而精
IO 高效
全表遍历
表修复快
提高幵发
alter table 快
单表多少字段合适?
单表 1 G 体积 500 W 行评估
顺序读 1 G 文件需 N 秒
单行不超过 200 Byte
单表不超过 50 个纯 INT 字段
单表不超过 20 个 CHAR (10) 字段
单表字段数上限控制在 20~50 个
```
###### 1.4 平衡范式不冗余

```
严格遵循三大范式?
效率优先、提升性能
没有绝对的对不错
适当时牺牲范式、加入冗余
但会增加代码复杂度
```
###### 1.5 拒绝 3 B

```
数据库幵发像城市交通
非线性增长
拒绝 3 B
大 SQL (BIG SQL)
大事务 (BIG Transaction)
大批量 (BIG Batch)
详细解析见后
```
###### 1.6 核心军规小结

```
尽量不在数据库做运算
控制单表数据量
保持表身段苗条
平衡范式不冗余
拒绝 3 B
```
#### 二、字段类军规 (6)

###### 2.1 用好数值字段类型

```
三类数值类型:
TINYINT (1 Byte)
SMALLINT (2 B)
```

```
MEDIUMINT (3 B)
INT (4 B)、BIGINT (8 B)
FLOAT (4 B)、DOUBLE (8 B)
DECIMAL (M, D)
BAD CASE:
INT (1) VS INT (11)
BIGINT AUTO_INCREMENT
DECIMAL (18,0)
```
###### 2.2 将字符转化为数字

```
数字型 VS 字符串型索引
更高效
查询更快
占用空间更小
举例: 用无符号 INT 存储 IP，而非 CHAR (15)
INT UNSIGNED
INET_ATON ()
INET_NTOA ()
```
###### 2.3 优先使用 ENUM 或 SET

```
优先使用 ENUM 或 SET
字符串
可能值已知且有限
存储
ENUM 占用 1 字节，转为数值运算
SET 视节点定，最多占用 8 字节
比较时需要加' 单引号 (即使是数值)
举例
sex enum ('F','M') COMMENT '性别'
c 1 enum ('0','1','2','3') COMMENT '职介审核'
```
###### 2.4 避免使用 NULL 字段

```
避免使用 NULL 字段
很难进行查询优化
NULL 列加索引，需要额外空间
含 NULL 复合索引无效
举例
a char (32) DEFAULT NULL
b int (10) NOT NULL
c int (10) NOT NULL DEFAULT 0
```
###### 2.5 少用并拆分 TEXT/BLOB

```
TEXT 类型处理性能远低亍 VARCHAR
强制生成硬盘临时表
浪费更多空间
VARCHAR (65535)==>64 K (注意 UTF-8)
```

```
尽量不用 TEXT/BLOB 数据类型
若必须使用则拆分到单独的表
举例:
```
###### 2.6 不在数据库里存图片

###### 2.7 字段类军规小结

```
用好数值字段类型
将字符转化为数字
优先使用枚举 ENUM/SET
避免使用 NULL 字段
少用幵拆分 TEXT/BLOB
不在数据库里存图片
```
#### 三、索引类军规 (5)

###### 3.1 谨慎合理添加索引

```
谨慎合理添加索引
改善查询
减慢更新
索引不是赹多赹好
能不加的索引尽量不加
综合评估数据密度和数据分布
最好不赸过字段数 20%
结合核心 SQL 优先考虑覆盖索引
举例
不要给“性别”列创建索引
```
###### 3.2 字符字段必须建前缀索引

```
区分度
单字母区分度:26
4 字母区分度: 26 26 26*26=456,976
5 字母区分度: 26 26 26 26 26=11,881,376
6 字母区分度: 26 26 26 26 26*26=308,915,776
字符字段必须建前缀索引:
```
###### 3.3 不在索引列做运算

```
CREATE TABLE t 1 (
id INT NOT NULL AUTO_INCREMENT, data text NOT NULL,
```
- PRIMARY KEY id
) ENGINE=InnoDB;

```
(
`pinyin` varchar (100) DEFAULT NULL COMMENT '小区拼音', KEY `idx_pinyin`
(`pinyin`(8)),
) ENGINE=InnoDB
```

```
不在索引列进行数学运算或凼数运算
无法使用索引
导致全表扫描
举例:
```
###### 3.4 自增列或全局 ID 做 INNODB 主键

```
对主键建立聚簇索引
二级索引存储主键值
主键不应更新修改
按自增顺序揑入值
忌用字符串做主键
聚簇索引分裂
推荐用独立亍业务的 AUTO_INCREMENT 列或全局 ID 生成器做代理主键
若不指定主键，InnoDB 会用唯一且非空值索引代替
```
###### 3.5 尽量不用外键

```
线上 OLTP 系统 (线下系统另论)
外键可节省开发量
有额外开销
逐行操作
可'到达'其它表，意味着锁
高并发时容易死锁
由程序保证约束
```
###### 3.6 索引类军规小结

```
谨慎合理添加索引
字符字段必须建前缀索引
不在索引列做运算
自增列或全局 ID 做 INNODB 主键
尽量不用外键
```
#### 四、SQL 类军规 (15)

###### 4.1 SQL 语句尽可能简单

```
大 SQL VS 多个简单 SQL
传统设计思想
BUT MySQL NOT
一条 SQL 叧能在一个 CPU 运算
5000+ QPS 的高幵发中， 1 秒大 SQL 意味着?
可能一条大 SQL 就把整个数据库堵死
拒绝大 SQL，拆解成多条简单 SQL
简单 SQL 缓存命中率更高
减少锁表时间，特别是 MyISAM
```
```
BAD: SELECT * from table WHERE to_days (current_date) – to_days (date_col) <= 10
GOOD: SELECT * from table WHERE date_col >= DATE_SUB ('2011-10- 22', INTERVAL 10
DAY);
```

```
用上多 CPU
```
###### 4.2 保持事务 (连接) 短小

```
保持事务/DB 连接短小精悍
事务/连接使用原则: 即开即用，用完即关
与事务无关操作放到事务外面, 减少锁资源的占用
不破坏一致性前提下，使用多个短事务代替长事务
举例
发贴时的图片上传等待
大量的 sleep 连接
```
###### 4.3 尽可能避免使用 SP/TRIG/FUNC

```
线上 OLTP 系统 (线下库另论)
尽可能少用存储过程
尽可能少用触发器
减用使用 MySQL 凼数对结果进行处理
由客户端程序负责
```
###### 4.4 尽量不用 SELECT

```
用 SELECT * 时
更多消耗 CPU、内存、IO、网络带宽
先向数据库请求所有列，然后丢掉不需要列?
尽量不用 SELECT * ，叧取需要数据列 • 更安全的设计: 减少表变化带来的影响
为使用 covering index 提供可能性
SELECT/JOIN 减少硬盘临时表生成，特别是有 TEXT/BLOB 时
举例:
```
###### 4.5 改写 OR 为 IN ()

```
同一字段，将 or 改写为 in ()
OR 效率: O (n)
IN 效率: O (Log n)
当 n 很大时，OR 会慢很多
注意控制 IN 的个数，建议 n 小亍 200
举例:
```
###### 4.6 改写 OR 为 UNION

```
不同字段，将 or 改为 union
减少对不同字段进行 "or" 查询
Merge index 往往很弱智
如果有足够信心: set global optimizer_switch='index_merge=off';
举例:
```
```
SELECT * FROM tag WHERE id = 999184;
SELECT keyword FROM tag WHERE id = 999184;
```
```
SELECT * from opp WHERE phone='12347856' or phone='42242233' \G;
SELECT * from opp WHERE phone in ('12347856' , '42242233');
```

###### 4.7 避免负向查询和% 前缀模糊查询

```
避免负向查询
NOT、!=、<>、!<、!>、NOT EXISTS、NOT IN、 NOT LIKE 等
避免 % 前缀模糊查询
B+ Tree
使用不了索引
导致全表扫描
举例:
```
###### 4.8 COUNT (*) 的几个例子

```
几个有趣的例子:
COUNT (COL) VS COUNT (*)
COUNT (*) VS COUNT (1)
COUNT (1) VS COUNT (0) VS COUNT (100)
示例:
```
```
结论
COUNT (*)=count (1) *COUNT (0)=count (1)
COUNT (1)=count (100)
COUNT (*)!=count (col)
WHY?
```
###### 4.9 减少 COUNT (*)

```
MyISAM VS INNODB
不带 WHERE COUNT ()
带 WHERE COUNT ()
COUNT (*) 的资源开销大，尽量不用少用
计数统计
实时统计: 用 memcache，双向更新，凌晨跑基准
非实时统计: 尽量用单独统计表，定期重算
```
###### 4.10 LIMIT 高效分页

```
传统分页:
SELECT * from table limit 10000,10;
LIMIT 原理:
```
```
SELECT * from opp WHERE phone='010-88886666' or cellPhone='13800138000';
SELECT * from opp WHERE phone='010-88886666' union SELECT * from opp WHERE
cellPhone='13800138000';
```
```
SELECT * from post WHERE title like '北京%'; -- 298 rows in set (0.01 sec)
SELECT * from post WHERE title like '%北京%'; -- 572 rows in set (3.27 sec)
```
```
`id` int (10) NOT NULL AUTO_INCREMENT COMMENT '公司的 id', `sale_id` int (10)
unsigned DEFAULT NULL,
```

```
Limit 10000,10  偏秱量赹大则赹慢
推荐分页:
SELECT * from table WHERE id>=23423 limit 11; *SELECT * from table WHERE
id>=23434 limit 11;
分页方式二:
SELECT * from table WHERE id >= ( SELECT id from table limit 10000,1 ) limit 10;
分页方式三:
SELECT * FROM table INNER JOIN (SELECT id FROM table LIMIT 10000,10) USING (id);
分页方式四:
程序取 ID: SELECT id from table limit 10000,10;
SELECT * from table WHERE id in (123,456...);
可能需按场景分析幵重组索引
示例:
```
###### 4.11 用 UNION ALL 而非 UNION

```
若无需对结果进行去重，则用 UNION ALL
UNION 有去重开销
举例:
```
###### 4.12 分解联接保证高并发

```
高幵发 DB 不建议进行两个表以上的 JOIN
适当分解联接保证高幵发
可缓存大量早期数据
使用了多个 MyISAM 表
对大表的小 ID IN ()
联接引用同一个表多次
举例:
```
```
MySQL> SELECT sql_no_cache * from post limit 10,10; 10 row in set (0.01 sec)
MySQL> SELECT sql_no_cache * from post limit 20000,10; 10 row in set (0.13 sec)
MySQL> SELECT sql_no_cache * from post limit 80000,10; 10 rows in set (0.58 sec)
MySQL> SELECT sql_no_cache id from post limit 80000,10; 10 rows in set (0.02
sec)
MySQL> SELECT sql_no_cache * from post WHERE id>=323423 limit 10; 10 rows in set
(0.01 sec)
MySQL> SELECT * from post WHERE id >= ( SELECT sql_no_cache id from post limit
80000,1 ) limit 10; 10 rows in set (0.02 sec)
```
```
SELECT * FROM detail 20091128 UNION ALL SELECT * FROM detail 20110427 UNION ALL
SELECT * FROM detail 20110426 UNION ALL SELECT * FROM detail 20110425 UNION ALL
SELECT * FROM detail 20110424 UNION ALL SELECT * FROM detail 20110423;
```
```
MySQL> SELECT * from tag JOIN post on tag_post. post_id=post. id WHERE tag. tag='二
手玩具';
```
```
MySQL> SELECT * from tag WHERE tag='二手玩具';
MySQL> SELECT * from tag_post WHERE tag_id=1321;
MySQL> SELECT * from post WHERE post. id in (123,456,314,141);
```

###### 4.13 GROUP BY 去除排序

```
GROUP BY 实现
分组
自劢排序
无需排序: Order by NULL
特定排序: Group by DESC/ASC
举例:
```
###### 4.14 同数据类型的列值比较

```
原则: 数字对数字，字符对字符
数值列不字符类型比较
同时转换为双精度
进行比对
字符列不数值类型比较
字符列整列转数值
不会使用索引查询
举例: 字符列不数值类型比较
```
###### 4.15 Load data 导数据

```
批量数据快导入:
成批装载比单行装载更快，不需要每次刷新缓存
无索引时装载比索引装载更快
Insert values ,values，values 减少索引刷新
Load data 比 insert 快约 20 倍
尽量不用 INSERT ... SELECT
延迟
同步出错
```
###### 4.16 打散大批量更新

```
大批量更新凌晨操作，避开高峰
凌晨不限制
白天上限默认为 100 条/秒 (特殊再议)
举例:
```
```
MySQL> SELECT phone,count (*) from post group by phone limit 1 ; 1 row in set
(2.19 sec)
MySQL> SELECT phone,count (*) from post group by phone order by null limit 1; 1
row in set (2.02 sec)
```
```
字段:`remark` varchar (50) NOT NULL COMMENT '备注, 默认为空',
```
```
MySQL>SELECT `id`, `gift_code` FROM gift WHERE `deal_id` = 640 AND
remark=115127; 1 row in set (0.14 sec)
MySQL>SELECT `id`, `gift_code` FROM pool_gift WHERE `deal_id` = 640 AND
remark='115127'; 1 row in set (0.005 sec)
```

###### 4.17 Know Every SQL

```
SHOW PROFILE
MySQLdumpslow
EXPLAIN
Show Slow Log
SHOW QUERY_RESPONSE_TIME (Percona)
MySQLsla
Show Processlist
```
###### 4.18 SQL 类军规小结

```
SQL 语句尽可能简单
保持事务 (连接) 短小
尽可能避免使用 SP/TRIG/FUNC
尽量不用 SELECT *
改写 OR 语句
避免负向查询和% 前缀模糊查询
减少 COUNT (*)
LIMIT 的高效分页
用 UNION ALL 而非 UNION
分解联接保证高幵发
GROUP BY 去除排序
同数据类型的列值比较
Load data 导数据
打散大批量更新
Know Every SQL!
```
#### 五、约定类军规 (5)

###### 5.1 隔离线上线下

```
构建数据库的生态环境
开发无线上库操作权限
原则: 线上连线上，线下连线下
实时数据用 real 库
模拟环境用 sim 库
测试用 qa 库
开发用 dev 库
```
###### 5.2 禁止未经 DBA 确认的子查询

```
MySQL 子查询
大部分情况优化较差
特别 WHERE 中使用 IN id 的子查询一般可用 JOIN 改写
举例:
```
```
update post set tag=1 WHERE id in (1,2,3); sleep 0.01;
update post set tag=1 WHERE id in (4,5,6); sleep 0.01;
......
```

**MySQL 系统关键字**

###### 5.3 永远不在程序端显式加锁

```
永远不在程序端对数据库显式加锁
外部锁对数据库不可控
高并发发时是灾难
极难调试和排查
并发扣款等一致性问题
采用事务
相对值修改
Commit 前二次较验冲突
```
###### 5.4 统一字符集为 UTF 8

```
字符集:
MySQL 4.1 以前叧有 latin 1
为多语言支持增加多字符集
也带来了 N 多问题
保持简单
统一字符集: UTF 8
校对规则: utf 8_general_ci
乱码: SET NAMES UTF 8
```
###### 5.5 统一命名规范

```
库表等名称统一用小写
Linux VS Windows
MySQL 库表大小写敏感
字段名的大小写不敏感
索引命名默认为“idx_字段名”
库名用缩写，尽量在 2~7 个字母
DataSharing ==> ds
注意避免用保留字命名
......
```
###### 5.6 注意避免用保留字命名

```
举例:
```
###### 5.7 约定类军规小结

```
隔离线上线下
```
```
SELECT * from table 1 where id id from table 2) in (SELECT insert into table 1
(SELECT * from table 2); -- 可能导致复制异常
```
```
SELECT * from return;
SELECT * from `return`;
```

```
禁止未经 DBA 确认的子查询上线
永远不在程序端显式加锁
统一字符集为 UTF 8
统一命名规范
```
## 淘宝一面：mysql 和 es 的 5 个一致性方案，

## 你知道吗？

###### 说在前面

在 40 岁老架构师尼恩的 **读者交流群** (50+) 中，最近有小伙伴拿到了一线互联网企业如拼多多、极兔、有
赞、希音的面试资格，遇到一几个很重要的面试题：

```
说 5 种 mysql 和 elasticsearch 数据一致性方案
```
与之类似的、其他小伙伴遇到过的问题还有：

```
Mysql 和 ES 数据一致性问题及方案？
Mysql 和 redis 数据一致性问题及方案？
如果保证 Mysql 和 redis 数据一致性？
如果保证 Mysql 和 HBase 数据一致性？
等等等等.....
```
这里尼恩给大家做一下系统化、体系化的线程池梳理，使得大家可以充分展示一下大家雄厚的 “技术肌
肉”， **让面试官爱到 “不能自已、口水直流”** 。

也一并把这个题目以及参考答案，收入咱们的《尼恩 Java 面试宝典》V 70 版本，供后面的小伙伴参考，
提升大家的 3 高架构、设计、开发水平。

```
最新《尼恩架构笔记》《尼恩高并发三部曲》、《尼恩 Java 面试宝典》的 PDF 文件，请通过公众
号（技术自由圈）获取。
```
###### 问题场景分析

咱们的生产需求上，为了便于商品的聚合搜索，高速搜索，采用两大优化方案：

```
把商品数据冗余存储在 Elasticsearch 中，实现高速搜索
把商品数据冗余存储在 redis 中，实现高速缓存
```

很多的时候，要求保持很高的数据一致性。

比如：

```
要求 mysql 与 es 做到秒级别的数据同步。
要求 mysql 与 redis 做到秒级别的数据同步。
要求 mysql 与 hbase 做到秒级别的数据同步。
```
接下来，以 mysql 与 es 的数据一致，作为业务场景进行分析，其他的场景比如 mysql 与 redis 的数据
一致性方案，都是差不多的。

只要大家能把下面的 5 大数据一致性方案，滔滔不绝的说出来，面试官一定会爱到 “不能自已、口水直
流”。

###### 方案一：同步双写

同步双写是一种最为简单的方式，在将数据写到 MySQL 时，同时将数据写到 ES。


**同步双写优点：**

这种方式简单粗暴，实时写入能做到秒级。

**同步双写缺点：**

```
业务耦合，这种方式代码侵入性强，商品的管理中耦合大量数据同步代码，要在之前写 mysql 的
地方加写 es 的代码。以后写 mysql 的地方也要加写 es 的代码。
影响性能，写入两个存储，响应时间变长，本来 MySQL 的性能不是很高，再加一个 ES，系统的
性能必然会下降。
不便扩展：搜索可能有一些个性化需求，需要对数据进行聚合，这种方式不便实现
高风险：存在双写失败丢数据风险
```
###### 方案二：异步双写

同步操作性能低，异步性能高。

异步双写，分为两种：

```
使用内存队列（如阻塞队列）异步
使用消息队列进行异步
```

**方案 2.1 使用内存队列（如阻塞队列）异步**

先把商品数据写入 DB 后，然后把数据写入 BlockingQueue 阻塞队列

消费线程异步从 drain 数据，batch 写入 ElasticSearch, 保证数据一致性

**方案 2.2 使用消息队列（如阻塞队列）异步**

如果内存队列里边数据丢失，那么 es 当中的数据和 DB 就不一致了

如果解决呢？

```
方式 1 ：定期同步 db 数据到 es ，同步周期一般比较长，这里有比较长时间的不一致
方式 2 ： 保证队列的可靠性，使用高可靠消息队列
```
生产场景中，一般会有一个搜索服务，由搜索服务去订阅商品变动的消息，来完成同步。


**异步双写优点：**

```
性能高；
不易出现数据丢失问题，主要基于 MQ 消息的消费保障机制，比如 ES 宕机或者写入失败，还能重
新消费 MQ 消息；
多源写入之间相互隔离，便于扩展更多的数据源写入。
```
**异步双写缺点：**

```
硬编码问题，接入新的数据源需要实现新的消费者代码；
系统复杂度增加，引入了消息中间件；
MQ 是异步消费模型，用户写入的数据不一定可以马上看到，造成延时。
```
###### 方案三：定期同步

为了保证 DB 和 ES /HBase 数据一致性，包括两个方面：

```
增量数据一致性
全量数据一致性
```

为了保证 DB 和 ES /HBase 的全量数据一致性，往往需要进行定期的全量数据同步


数据增量数据，很少，并且，一致性要求不高，那么可以把增量数据一致性行的同步双写、异步双写去
掉。

**定期同步优点：**

实现比较简单

**定期同步缺点：**

```
实时性难以保证
对存储压力较大
```
当然，增量数据，可以考虑用定时任务来处理：

```
1. 数据库的相关表中增加一个字段为 timestamp 的字段，任何 CURD 操作都会导致该字段的时间发
生变化；
2. 原来程序中的 CURD 操作不做任何变化；
3. 增加一个定时器程序，让该程序按一定的时间周期扫描指定的表，把该时间段内发生变化的数据提
取出来；
4. 逐条写入到 ES 中。
```
###### 方案四：数据订阅


```
Cancal Maxwell Python-Mysql-
Rplication
```
```
开源方阿里巴巴 Zendesk 社区
```
```
开发语言 Java Java Python
```
```
活跃度活跃活跃活跃
```
```
高可用支持支持不支持
```
如果要提高实时性，又要低入侵, 可以利用 MySQL 的 Binlog 来进行同步。

MySQL 通过 binlog 订阅实现主从同步，canal Server 是一个伪装的 slave 节点，接收到 binlog 日志后，
发送到 MQ, 其他的存储消费 MQ 里边的 binlog 日志，实现数据订阅。

架构图如下

这种方式和异步双写比较像，但是有两个优点：

```
第一降低了商品服务的入侵性，
第二数据的实时性更好。
```
所以使用数据订阅：

```
优点：
业务入侵较少
实时性较好
```
至于数据订阅框架的选型，主流的大体上是这些：


```
Cancal Maxwell Python-Mysql-
Rplication
```
```
客户端 Java/Go/PHP/Python/Rust 无 Python
```
```
消息落地 Kafka/RocketMQ 等 Kafka/RabbitNQ/Redis
等
```
```
自定义
```
```
消息格式自定义 JSON 自定义
```
```
文档详略详细详细详细
```
```
Boostrap 不支持支持不支持
```
注意，尼恩的 100 Wqps 三级缓存组件架构实操中，也介绍了，这种架构，存在秒级延迟。

如果不允许有秒级延迟的场景，不能使用这种架构。

具体请参见尼恩的 100 Wqps 三级缓存组件架构实操。

###### 方案五： etl 工具

MySQL 同步到 Redis、MySQL 同步到 hbase、MySQL 同步到 es、或机房同步、主从同步等，都可以考虑
使用 elt 工具。

什么是 etl 工具呢？

ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（extract）、转换
（transform）、加载（load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据
仓库。

ETL 是构建数据仓库的重要一环，用户从数据源抽取出所需的数据，经过数据清洗, 最终按照预先定义好
的数据仓库模型，将数据加载到数据仓库中去。

常用的 etl 工具有： databus、canal （方案四用了这个组件，有 etl 的部分功能）、otter 、kettle 等

下面以 databus 为例，介绍一下。

Databus 是一个低延迟、可靠的、支持事务的、保持一致性的数据变更抓取系统。由 LinkedIn 于 2013
年开源。

Databus 通过挖掘数据库日志的方式，将数据库变更实时、可靠的从数据库拉取出来，业务可以通过定
制化 client 实时获取变更并进行其他业务逻辑。

特点：

```
多数据源：Databus 支持多种数据来源的变更抓取，包括 Oracle 和 MySQL。
可扩展、高度可用：Databus 能扩展到支持数千消费者和事务数据来源，同时保持高度可用性。
事务按序提交：Databus 能保持来源数据库中的事务完整性，并按照事务分组和来源的提交顺寻
交付变更事件。
低延迟、支持多种订阅机制：数据源变更完成后，Databus 能在毫秒级内将事务提交给消费者。
同时，消费者使用 D atabus 中的服务器端过滤功能，可以只获取自己需要的特定数据。
无限回溯：对消费者支持无限回溯能力，例如当消费者需要产生数据的完整拷贝时，它不会对数据
库产生任何额外负担。当消费者的数据大大落后于来源数据库时，也可以使用该功能。
```
再看看 Databus 的系统架构。

Databus 由 Relays、bootstrap 服务和 Client lib 等组成，Bootstrap 服务中包括 Bootstrap Producer
和 Bootstrap Server。


```
快速变化的消费者直接从 Relay 中取事件；
如果一个消费者的数据更新大幅落后，它要的数据就不在 Relay 的日志中，而是需要请求
Bootstrap 服务，返回的将会是自消费者上次处理变更之后的所有数据变更快照。
```
开源地址：https://github.com/linkedin/databus

###### 说在最后

数据一致性的方案，是非常常见的面试题。

以上的 5 大方案，如果大家能对答如流，如数家珍，基本上面试官会被你震惊到、吸引到。

最终， **让面试官爱到 “不能自已、口水直流”** 。 offer，也就来了。

学习过程中，如果有啥问题，大家可以来找 40 岁老架构师尼恩交流。


## 美团一面： 100 亿级分库分表，如何不停机迁

## 移？

#### 说在前面

在 40 岁老架构师尼恩的 **读者交流群** (50+) 中，最近有小伙伴拿到了一线互联网企业如腾讯、美团、阿
里、拼多多、极兔、有赞、希音的面试资格，遇到一几个很重要的面试题：

```
数据库如何不停机迁移?
100 亿级库表，如何不停机迁移？
等等等等.....
```
其实，答案不是一成不变的。

参考答案其实千千万万，在这里，尼恩一直结合行业案例，梳理一个最为全面、最为系统化的答案，

这里有一个新的行业案例《菜鸟积分系统稳定性建设 - 分库分表&百亿级数据迁移》，尼恩从面试维
度，对这个方案，进行二次重构和梳理，现在把其做为参考答案，收入咱们的《尼恩 Java 面试宝典
PDF》 V 65 版本

供后面的小伙伴参考，大家一定好好看看这个生产级别的答案。

本文原文：《菜鸟积分系统稳定性建设 - 分库分表&百亿级数据迁移》原始方案的作者是星花，原文
来自于公号，阿里开发者。

下面的内容，是尼恩是结合自己的 3 高架构笔记，以及尼恩的 3 高架构知识体系（ 3 高架构宇宙），在原
文的基础上，做的二次架构分析和创作。

#### 分库分表&数据迁移的技术重要性

分库分表&数据迁移，不仅仅是一个技术难题，考验的一个人的技术功底。

分库分表&数据迁移，也是一个协调难题，需要协调开发、运维，而是一个人就能搞定的。考验的一个
人的协调功底。

**所以，分库分表&数据迁移是大厂面试、架构面试的核心问题。**

#### 积分系统的巨大价值

伟大的系统，都是迭代出来的。

在早期，为了快速试错，菜鸟积分系统单体系统，数据库是单库多表。

随着业务快速增长，菜鸟 C 端用户已经过 3 亿+，消费者从查、取、寄快件开始慢慢扩展到玩、购。


玩、购主要是 **裹酱商城** 以及各个业务线多种多样的活动，这些活动包含了停留、任务、开奖、分享、签
到、助力、兑换、抽奖等多种多样的互动手段。

菜鸟的积分代表用户虚拟权益，在其中起到了钩子的作用。为互动产品本身提供了某些“价值”，使得消
费者在平台停留时间增加。

从这个维度来说，菜鸟积分系统一直扮演着一个底层核心角色，承载用户核心资产。

###### 积分系统的功能架构如下：

#### 积分系统面临的巨大挑战

在大促期间需要支持大流量的活动，因此整个积分系统在大促期间面临挑战是非常大的。

**菜鸟 C 端用户已经过 3 亿+，单体应用不可能支撑，存储层的单库多表也是不可能支撑的。**


为了支持菜鸟 C 端营销业务的持续爆炸式增长，菜鸟积分系统需要升级。

**问题是：如何在业务不暂停的情况下，完成积分从单库单表到分库分表的数据架构升级？**

同样的问题：这一高风险操作是如何逐步分阶段完成的？

在开始设计迁移方案之前，菜鸟积分团队认真进行了问题分析，发现了本项目面临以下几个主要的挑
战：

```
从 1 到 N 在系统架构上差异较大 ：原来的是单库单表的系统，之前的设计开发都是基于单库单表
的，例如 SQL 查询，在单表上建了多个索引来支持这些查询业务，切换到分库分表之后，不带分表
键的查询就不能支持了。
业务不可暂停 ：整个迁移过程，营销业务不允许暂停，类似于开着飞机换引擎，比一些银行系统的
暂停服务来做迁移的难度大。
数据量大 ：单表数据量超级大，对数据同步链路的稳定性提出了很大的挑战。
系统架构旧 ：积分系统建立时间很早，所选用的技术框架老旧，有些甚至已经不再维护了，导致整
个改造成本加大，改动过程风险很大。
接口版本多 ：由于历史原因，积分的发放和消耗接口版本非常多，有些历史包袱在里面，使得迁移
的难度和风险都很大。
可监控、可灰度、可回滚 ：为了降低整个项目的风险，按照阿里稳定性三板斧，要求整个迁移过程
可监控、可灰度、可回滚。这三个要求对服务重构没有大问题，但是对于这个同时涉及数据迁移的
项目，难度反而是加大了，如果不要求可灰度、可回滚，只用做数据的从老的单库单表到新的分库
分表的单向迁移就可以了，而如果要求可灰度、可回滚，则必须要求数据的双向同步，加大了数据
同步链路的风险。
时间紧迫 ：需要在特定时间前完成拆库和数据源迁移 (近 1 个多月的时间)，封网期间加大了操作流
程的复杂性。
```
###### 重构风险极大，但不得不做

由于涉及到数据的重构，项目风险极大，没处理好可能就要背包走人了。

为啥呢？

```
第一：数据一旦错乱或者丢失，有可能会造成部分数据不可恢复。
第二：或者即使能恢复，恢复时间通常也很长，按小时，甚至按天计算。业务很难承受这个代价。
```
然而，既然风险这么大，那能不干这个分库分表迁移项目吗？

不能！为啥呢：

```
因为菜鸟积分系统在 2020 年双十一期压测期间，已经成为瓶颈了。
已经到了不得不迁移重构的地步，而且这个重构越晚做，风险越大。
```
正所谓：箭在弦上，不得不发。

#### 100 亿级库表重构和迁移的方案设计

###### 存量数据分析

表的情况：积分系统主要 2 张核心的表，积分总表和积分明细表。

数据的情况： 2 个库中的数据达百亿，积分明细的增量数据规模大，日新增数据也在千万级。

###### 服务接口分析

积分系统对外提供的服务接口主要有：


```
读积分
加积分
扣减 (冻结积分)
退还积分。
```
为了临时解决大促期间系统性能瓶颈问题，读积分会通过缓存 tair（类似 redis）做查询，击穿缓存才会
到达数据库，写积分会直接更新数据库 (缓存是)。

数据库的压力，在写不在读。

通过观察，大促期间，数据库瓶颈是在写积分对数据库的压力上。

如果把数据库纵向扩展，升级最高配置，也不能彻底解决问题：

```
一方面预算上花费太高，
另一方面也不一定能支撑预估流量，即使这次侥幸度过，将来也无法再做水平扩展。
```
###### 解决 100 亿级库表性能瓶颈的可选方案

```
40 岁老架构师提升：作为架构师，一定要提供多个候选方案，可选方案。
```
解决 100 亿级库表性能瓶颈的可选方案，主要有三个方案：

```
1. 分库分表，数据迁移：考虑数据库性能，决定将积分系统由单库多表升级为分库分表
2. 对数据库操作进行优化，减少加积分数据库操作
3. 对业务优先级进行排序，部分场景降级，不写积分明细
```
方案二、方案三指标不治本，只是延迟了问题，并没有解决问题。 **延迟的结果是，越拖成本越高。**

通过综合考虑， **选择方案一进行优化** ，彻底解决解决 100 亿级库表性能瓶颈。

#### 分库分表+数据迁移方案设计

###### 分库分表方案设计

（ 1 ） 分库分表的分片键设计：

由于是积分总表和明细表，所以从用户维度进行分表，

（ 2 ） 分库分表的规模设计：

分库分表是 **分 8 个库， 1024 张表**

```
具体的分库分表的方法论，请参见尼恩秒杀视频。或者参考尼恩的推送中台视频。
```
（ 3 ）主键 ID 的方案设计

在迁移数据的过程中特别要注意的是主键 ID，主键 ID 需要程序生产的分布式主键.

也就是说，需要在程序中，手动的去指定主键。


而不能使用 DB 自增主键，防止 ID 生成顺序错误

```
具体的主键 ID 的方案设计，请参见尼恩秒杀视频。或者参考尼恩的推送中台视频。
```
###### 数据迁移方案设计

分库分表肯定要涉及数据迁移，数据迁移必定涉及一个全量和增量的数据，

数据迁移的关键是：如何保证数据不会重复，不会丢失，而且保证唯一性

数据迁移方案包括： 全量迁移，增量迁移。

使用阿里数据迁移工具精卫，这个工具既可以进行全量迁移，也可以进行增量迁移。

精卫工具会在全量任务的时候相当于 **记录数据库的一个副本** ，然后增量的时候可以进行任务回溯，

如何进行数据唯一性保证？该迁移工具可以保证数据的唯一性，对于已经存在的记录可以进行更新操
作。

尼恩提示：

如果不是阿里，可以使用开源的数据同步工具。

比如阿里开源的 DTS，就是是一件非常不错的工具。

DTS 工具可以实现不同数据源之间的数据迁移、数据同步，只需要配置好两端的数据源就可以自动实
现，不在需要人为的操作，非常的方便。

当然，如果熟悉 kettle 的，也可以使用 kettle 进行数据同步。

总之，有很多类似的中间件，或者自己编写迁移脚本

###### 动态读写切换改造

由于是动态迁移，不停服迁移，肯定不能通过发布程序来切换数据源，

所以，在程序层面进行改造，进行动态读写切换改造。

大致的思路： 在程序里面手动去指定 **双数据源** ，预先在程序里面应该加载两个 db 的数据源，通过配置
预先设定开关，然后 **动态进行读写切换** 。

为了实现 **可监控、可灰度、可回滚** ，还需要进行灰度。

从配置的维度来说，不仅仅是切换的开关需要配置化，而且需要保证灰度过程中切换比例，可以通过
配置进行调控

###### 不停服、 100 亿级数据迁移执行流程

数据迁移执行流程，大概有 10 个步骤：


1. 环境准备：线上库配置完成
2. 全量同步：数据迁移工具上新建 2 张表（积分表、明细表）的全量任务
3. 增量同步：全量迁移完成后开启增量 (自动回溯全量开始时间，消息多次消费会进行幂等)
4. 数据校验：全量数据校验，查看数据是否一致
5. 切流测试：改造代码预发测试（采集线上流量进行回放，多种 case 跑一下，切流开关等校验），
没问题发布上线
6. 二次校验：再次全量进行校验&订正（数据追平）
7. 开启双写：打开双写（保证数据实时性）
8. 开启读灰度：低流量节点 (凌晨过后) 进行灰度切流 userId%x，进行验证，逐步流量打开，持续观察
9. 只写新库：双写开关切到新库，保证只写新库，完成数据迁移方案
10. 迁移完成： 系统稳定运行一段时间，迁移&双写代码下线，老库进行资源释放


###### step 1 环境准备：线上库配置完成

线上库配置完成，并且找到业务低峰时段，进行迁移操作

1 ）配置工作

```
申请 db 资源 8 个库
创建逻辑库，配置逻辑表
配置逻辑表路由算法
应用中配置好分库分表规则
多数据源配置
```
有关多数据源配置，可以参考尼恩推送中台中的多数据源配置。

2 ）通过监控查看业务低峰时段

```
从监控找出业务低峰期，预估操作时段，由图我们可以看出业务低峰时间在2:00-5:00
```
###### step 2: 开始全量数据迁移


全量数据迁移周期很长。

**数据迁移** 的时间，有可能是一周，有可能是 1 个月

每一次数据迁移都需要一段漫长的时间，不是一天两天就搞定了的。

通常来说我们迁移数据的过程基本如下图：

开启全量同步

```
使用精卫（阿里数据同步工具），它可以根据配置好的分库分表规则，自动将数据同步到相应的物
理分表中
全量开始时间，选择 gmt_modified 作为条件字段
全量迁移的时候，源数据默认走备库，目标端写入的是主库。
无论是全量还是校验，其实都会对源端备库，目标端的主库造成压力。所以这里需要注意一下，
以免对线上服务造成影响。
```
```
在迁移数据过程中，当写入发生冲突时，转换为 update 执行
注意一下数据迁移完成的时间，假设我们以 80 亿数据上，同时开启 8 个任务，每个任务上线 tps 为
1 w，计算公式如下：
```

```
数据迁移看板，实时查看迁移进度（ps：本工具是阿里内部的，外部也有很多类似开源系统）
```
###### step 3： 开启增量同步

增量同步：全量迁移完成后开启增量 (自动回溯全量开始时间，消息多次消费会进行幂等)

完成全量的迁移后，然后需要处理新增的这部分数据。

需要实时的增量数据在写完原本的数据库之后然后写到我们的新的存储，在这一过程中我们需要不断的
进行数据校验。

当我们校验基本问题不大的时候，然后进行切流操作，直到完全切流之后，我们就可以不用再进行数据
校验和增量数据迁移。

在数据迁移工具（精卫）中，开启增量任务，增量同步需要注意的地方：

```
在 INSERT 时，出现主键冲突，精卫会将 INSERT 改成 UPDATE 事件。用户无需担心主键冲突产生异
常。
异常处理机制：增量任务产生异常后，迁移工具默认会在同一机器上重试三次，若三次都失败后，
会给出报警信息，并稍后换一台机器继续重试。
```
增量同步涉及到同步位点说明：增量同步主要是参照原库的 binlog 位点开始的，这个同步位点，也叫做
消费位点。

具体来说，消费位点是指当前已经成功消费的 Binlog 队列的位置，位点是一个 'yyyy-MM-dd
HH:mm: ss' 格式的时间戳。


###### step 4：数据校验

全量数据校验，查看数据是否一致

全量同步之后，我们需要不断的进行数据校验。

增量同步之后，我们也需要不断的进行数据校验。

当我们校验基本问题不大的时候，然后进行切流操作，直到完全切流之后，我们就可以不用再进行数据
校验和增量数据迁移。

数据校验包括：

```
全量校验服务
全量订正服务
积分对账（业务校验）
```
**全量校验服务**

下面使用的工具是阿里内部的，原理都类似，可以参考阿里对外开源的数据传输服务 DTS。

```
全量校验服务和全量迁移服务类似，配置流程同上。校验服务执行完成，会在页面展示缺失和差异
的数量。
验证源端和目标端的数据是否一致，也是全量订正服务必须的前置操作。
```

通过校验，找到数据的差异，进行数据的修正，或者叫做订正

**全量订正服务**

```
通过订正服务可以将不一致的源端 DB 和目标端 DB 进行数据订正，保证一致性。
使用订正服务前必须进行校验服务。
```

**积分对账（业务校验）**

业务校验主要是进行数据的一致性校验。

需要注意：业务校验任务注意不要影响线上运行的服务，通常校验任务会写很多批查询的语句，会出现
批量扫表的情况，如果代码没有写好很容易导致数据库挂掉。

**对账标准** ：target 数据库和 source 数据库中数据保持一致（所有字段）

**对账梳理** ：可以从积分总表和积分明细两个方面来处理

**对账流程** ：通过定时任务轮询执行已经完成迁移的用户在新老库的数据一致性。需要注意的是由于读取
新老库有先后顺序，所以产生 **瞬时的数据不一致** ，对于这种问题可以采用对账重试，只要保证最终一致
即可。

1 ）抽样数据校验


按业务类型或者用户 id，对最新增量数据进行抽样校验（下面校验工具为阿里内部工具，外部可以参考
阿里数据传输服务 DTS），然后需要对有问题数据进行订正

2 ）Odps 离线数据校验

使用 odps 的小时表来进行对账。思路很简单，利用 odps 数据同步能力，离线数据的处理能力，加上动
态脚本的编写快速实现多系统间对账。

不需要进行应用的改造，稳定性上也有保障。对于实时性要求不高的场景，可以推荐尝试使用。

###### step 5：切流测试：

改造代码预发测试（采集线上流量进行回放，多种 case 跑一下，切流开关等校验），没问题发布上线

###### step 6：二次校验：

确保安全，在正式切流前，再次全量进行校验&订正（数据追平）


###### step 7: 开启双写 (增，删，改) 切换

整体上分为下面 4 个步骤，通过配置动态进行切换，切换期间需要注意的问题如下：

```
对写新库操作需要进行日志埋点
新库不要求一定写成功（不影响服务，后期不一致数据通过增量任务兜底）
如果老库和新库都写，最终返回结果是老数据源
数据源回滚：开启了双写，新数据库中总积分值不对（新数据源回滚不了）
开启双写时机：由于已经开启增量，所以对于还没切流前，不需要开启双写。在准备进行切库时，
开启双写。为什么这里还需要开启双写？
考虑极限情况下，增量同步任务会出现延迟（理论上是秒级）
实时同步数据到新数据库中
```

###### step 8 开启灰度读切流：

找准时间阶段操作，在低流量阶段 (凌晨过后) 进行操作

操作的时候，进行灰度处理，逐步推进。

进行灰度切流 userId%x，进行验证，逐步流量打开，持续观察

读切流的流程：

```
对所有查询接口进行整理
对 DAO 层编写代理层 xxProxyDAO. class
对读接口在代理层进行开关控制
根据 userId 后 4 位取模进行灰度，动态获取查询时用的数据源
```
```
注意：对于没有路由字段 userId，需要进行代码改造
```

当我们数据校验基本没有报错了之后，说明我们的迁移程序是比较稳定的了，那么我们就可以直接使用
我们新的数据了吗？当然是不可以的，如果我们一把切换了，顺利的话当然是很好的，如果出现问题
了，那么就会影响所有的用户。所以我们接下来就需要进行灰度，也就是切流。

本次切流方案是基于用户 id 取模的方式去进行切流，这个切流需要制定好一个切流计划，在什么时间
段，放出多少的流量，并且切流的时候一定要选择流量比较少的时候进行切流，每一次切流都需要对日
志做详细的观察，出现问题尽早修复，流量的一个放出过程是一个由慢到快的过程，比如最开始是以
1%的量去不断叠加的，到后面的时候我们直接以 10%，20%的量去快速放量。因为如果出现问题的话
往往在小流量的时候就会发现，如果小流量没有问题那么后续就可以快速放量。

切换过程采用逐步放量的形式，灰度方式很多我们采用的是先白名单验证，然后用户 ID 取模 10000 逐步
放量的方式。

灰度切流验证：万分之 1-1%-5%-10%-50%-100%切流

灰度读切流完成之后，读流量，全部落入到新库。

###### step 9 只写新库：

双写开关切到新库，保证只写新库，完成数据迁移方案

前面的第 7 步的双写，已经变成了单写。

###### step 10： 完成迁移

直到切流到 100%，迁移已经完成。

但是，老库还不能立即下线，还得备用一段时间，以防万一出问题还能切回老库

并且，需要保证老库的数据，能是最新的。

所以，开启精卫新库到老库同步任务。

然后观察各个业务后续工单反馈情况和各个系统预警&日志；对新库进行性能压测，确保新库的稳定性

###### 100 亿级数据分库分表后，如何迁移：

最后简单来总结下这个套路，总结起来，其实就是四个步骤：


第 1 步：存量同步，

第 2 步：增量同步，

第 3 步：数据校验，

第 4 步：灰度切流

更细致一点，就是 10 个步骤，具体如上。

#### 数据迁移的总结

周期很长：整个迁移过程从方案制定到最终的迁移完成持续约一个多月时间，最终完成迁移。

方案可靠：无论是利用数据库工具还是利用服务对数据进行迁移，目标都是一致的那就是数据无差异，
用户无感知，异常可监控，方案可回滚。

充分演练：在迁移前尽可能进行演练，通过一些测试编写的自动化脚本能否高效发现一些潜在的问题。

尽早迁移：同时存储是有状态的，迁移难度比较大，开发者需要具备前瞻性，尽量在选型的时候慎重，
选择合适的数据库，避免进行数据库迁移。发现数据库选型有潜在的问题时，需要当机立断，尽早迁
移。不要以为出现问题的概率不大，就拖延了。否则一旦出现问题，就是重大故障，造成的损失难以估
量。

#### 结合闲鱼的方案，回顾前面的面试题：

```
千万级数据，如何做系统架构？
亿级数据，如何做做系统架构？
千万级流量，如何做系统架构？
亿级流量，如何做做系统架构？
高并发系统，如何架构？
```
以上的方案，可以作为大家的一个参考答案。后续尼恩会给大家结合行业案例，分析出更多，更加劲爆
的答案。

当然，如果大家遇到这类高并发的面试难题，可以找来尼恩的社群交流。

## 网易一面：25 Wqps 高吞吐写 Mysql，100 W

## 数据 4 秒写完，如何实现？

#### 说在前面

在尼恩的（50+）读者社群中，经常遇到一个非常、非常高频的一个面试题，但是很不好回答，类似如
下：

```
Java 怎么实现几十万条数据插入？
JDBC 添加几万条数据，要求保证效率，请写出代码？
```
**最近，有个小伙伴网易一面，又遇到了这个问题。**


咱们一直心心念念的 “Java 怎么实现几十万条数据插入？” 的教科书式的答案，

接下来，尼恩为大家梳理一个教科书式的答案，

超高并发写 mysql，一定要用到批处理方案。

其实，很多小伙伴都用过， **但是却会少说一些核心参数，少说一些核心步骤** ，导致面试严重丢分。

这里，尼恩也一并把这个题目以及参考答案，收入咱们的《尼恩 Java 面试宝典 PDF》V 98 版本，供后面
的小伙伴参考，提升大家的 3 高架构、设计、开发水平。

```
《尼恩架构笔记》《尼恩高并发三部曲》《尼恩 Java 面试宝典》的 PDF，请关注本公众号【技术
自由圈】获取，暗号：领电子书
```
#### 什么是 JDBC 的批量处理语句？

当需要成批插入或者更新记录时，可以采用 Java 的批量 **更新** 机制，

这一机制允许多条语句一次性提交给数据库 **批量处理** 。通常情况下， **批量处理** 比单独提交处理更有效率

JDBC 的批量处理语句包括下面三个方法：

```
addBatch (String)：添加需要批量处理的 SQL 语句或是参数；
executeBatch ()：执行批量处理语句；
clearBatch (): 清空缓存的数据
```
通常我们会遇到两种批量执行 SQL 语句的情况：

```
多条 SQL 语句的批量处理；
一个 SQL 语句的批量传参；
```
#### 方式一：普通插入

没有对比，没有伤害

没有数据，不是高手

看看普通插入 10000 W 条记录的性能数据

```
/**
* 方式一
* 普通批量插入，直接将插入语句执行多次即可
*/
@Test
public void bulkSubmissionTest 1 () {
long start = System.currentTimeMillis ();//开始计时【单位：毫秒】
Connection conn = jdbcUtils.getConnection ();//获取数据库连接
String sql = "insert into a (id, name) VALUES (?, null)";
PreparedStatement ps = null;
try {
ps = conn.prepareStatement (sql);
for (int i = 1 ; i <= 1000000 ; i++) {
ps.setObject ( 1 , i);//填充 sql 语句种得占位符
ps.execute ();//执行 sql 语句
```

用时

折算：3736/60= 62 分钟多

#### 方式二：使用批处理插入

使用 PreparedStatement

JDBC 的批量处理语句包括下面三个方法：

```
addBatch (String)： 将 sql 语句打包到一个 Batch 容器中, 添加需要批量处理的 SQL 语句或是参
数；
executeBatch ()：将 Batch 容器中的 sql 语句提交, 执行批量处理语句；
clearBatch (): 清空 Batch 容器，为下一次打包做准备
```
注意这三个方法实现 sql 语句打包，累计到一定数量一次提交

```
}
} catch (SQLException e) {
e.printStackTrace ();
} finally {
jdbcUtils.close (conn, ps, null);
}
//打印耗时【单位：毫秒】
System.out.println ("百万条数据插入用时：" + (System.currentTimeMillis () -
start)+"【单位：毫秒】");
}
```
```
@Test
public void bulkSubmissionTest 2 () {
long start = System.currentTimeMillis ();
Connection conn = jdbcUtils.getConnection ();//获取数据库连接
String sql = "insert into a (id, name) VALUES (?, null)";
PreparedStatement ps = null;
try {
ps = conn.prepareStatement (sql);
for (int i = 1 ; i <= 1000000 ; i++) {
ps.setObject ( 1 , i);
ps.addBatch ();//将 sql 语句打包到一个容器中
if (i % 500 == 0 ) {
ps.executeBatch ();//将容器中的 sql 语句提交
ps.clearBatch ();//清空容器，为下一次打包做准备
}
}
//为防止有 sql 语句漏提交【如 i 结束时%500！=0 的情况】，需再次提交 sql 语句
ps.executeBatch ();//将容器中的 sql 语句提交
ps.clearBatch ();//清空容器
} catch (SQLException e) {
e.printStackTrace ();
} finally {
jdbcUtils.close (conn, ps, null);
```

用时

折算：3685/60= 61 分钟多

方式一、二总结：到此可以看出其实批处理程序是没有起作用的

#### 方式三：设置数据源的批处理重写标志

通过连接配置 url 设置&rewriteBatchedStatements=true，打开驱动的 rewriteBatchedStatements 开
关

在方式二的基础上，允许 jdbc 驱动重写批量提交语句，在数据源的 url 需加上
&rewriteBatchedStatements=true ，表示 (重写批处理语句=是)

驱动的 url 设置参考如下：

再执行 jdbc 的批处理

```
}
System.out.println ("百万条数据插入用时：" + (System.currentTimeMillis () -
start)+"【单位：毫秒】");
}
```
```
spring. datasource. url = jdbc:mysql://localhost: 3306/seckill?
useUnicode=true&characterEncoding=utf 8&useSSL=false&serverTimezone=GMT%2 B 8&zeroD
ateTimeBehavior=convertToNull&allowMultiQueries=true&rewriteBatchedStatements=tr
ue
spring. datasource. username = root
spring. datasource. password = 123456
```
```
/**
* 方式三
*/
@Test
public void bulkSubmissionTest 3 () {
long start = System.currentTimeMillis ();
Connection conn = jdbcUtils.getConnection ();//获取数据库连接
String sql = "insert into a (id, name) VALUES (?, null)";
PreparedStatement ps = null;
try {
ps = conn.prepareStatement (sql);
for (int i = 1 ; i <= 1000000 ; i++) {
ps.setObject ( 1 , i);
ps.addBatch ();
if (i % 500 == 0 ) {
ps.executeBatch ();
```

用时：

折算：10 s

从 60 分钟到 10 s，提升了多少倍？

360 倍

到此批处理语句才正是生效

###### 注意

数据库连接的 url 设置了【&rewriteBatchedStatements=true】时，java 代码种的 sql 语句不能有分号
【;】号，

否则批处理语句打包就会出现错误，导致后面的 sql 语句提交出现【BatchUpdateException】异常

批量更新异常：BatchUpdateException

```
ps.clearBatch ();
}
}
ps.executeBatch ();
ps.clearBatch ();
} catch (SQLException e) {
e.printStackTrace ();
} finally {
jdbcUtils.close (conn, ps, null);
}
System.out.println ("百万条数据插入用时：" + (System.currentTimeMillis () -
start)+"【单位：毫秒】");
}
```

#### rewriteBatchedStatements 底层原理

从 Java 1.2 开始，该 Statement 接口一直提供 addBatch 我们可以用来批处理多个语句的接口，以便在
调用 executeBatch 方法时通过单个请求发送它们，如下面的示例所示：

通过分析源码，会发现以下代码块：

```
String INSERT = "insert into post (id, title) values (%1$d, 'Post no. %1$d')";
```
```
try (Statement statement = connection.createStatement ()) {
for (long id = 1 ; id <= 10 ; id++) {
statement.addBatch (
String.format (INSERT, id)
);
}
statement.executeBatch ();
}
```
```
if (this.rewriteBatchedStatements.getValue () && nbrCommands > 4 ) {
return executeBatchUsingMultiQueries (
multiQueriesEnabled,
nbrCommands,
individualStatementTimeout
);
}
```
```
updateCounts = new long[nbrCommands];
```
```
for (int i = 0 ; i < nbrCommands; i++) {
updateCounts[i] = - 3 ;
}
```
```
int commandIndex = 0 ;
```

因为 rewriteBatchedStatements 标志 is flase ，每个 INSERT 语句还是单独执行
executeUpdateInternal，并没有走 executeBatchUsingMultiQueries 批处理逻辑。

因此，即使我们使用 addBatch ，默认情况下，MySQL 在使用普通 JDBC 对象 executeBatch 时仍会单
独执行 INSERT 语句。

但是，如果我们启用 rewriteBatchedStatementsJDBC 配置属性

方式一：在 springboot 应用中，调整 dataSource 的 url 参数

方式二：通过 dataSource 的方法设置

设置完了之后，调试 executeBatch 方法执行，你会看到：

这一次，executeBatchUsingMultiQueries 被调用了

```
for (commandIndex = 0 ; commandIndex < nbrCommands; commandIndex++) {
try {
String sql = (String) batchedArgs.get (commandIndex);
updateCounts[commandIndex] = executeUpdateInternal (sql, true, true);
```
```
...
} catch (SQLException ex) {
updateCounts[commandIndex] = EXECUTE_FAILED;
```
```
...
}
}
```
```
spring. datasource. url = jdbc:mysql://localhost: 3306/seckill?
useUnicode=true&characterEncoding=utf 8&useSSL=false&serverTimezone=GMT%2 B 8&zeroD
ateTimeBehavior=convertToNull&allowMultiQueries=true&rewriteBatchedStatements=tr
ue
```
```
MysqlDataSource dataSource = new MysqlDataSource ();
```
```
String url = "jdbc:mysql://localhost/high_performance_java_persistence?
useSSL=false";
```
```
dataSource.setURL (url);
dataSource.setUser (username ());
dataSource.setPassword (password ());
```
```
dataSource.setRewriteBatchedStatements (true);
```

并且该 executeBatchUsingMultiQueries 方法会将各个 INSERT 语句拼接到 queryBuf （一个
StringBuilder），

拼接后，运行单个 execute 调用：

因此，对于普通的 JDBCStatement 批处理，MySQLrewriteBatchedStatements 配置属性将附加当
前批处理的语句并在单个数据库往返中执行它们。

###### 将 rewriteBatchedStatements 与 PreparedStatement 一起使

###### 用

```
if (this.rewriteBatchedStatements.getValue () && nbrCommands > 4 ) {
return executeBatchUsingMultiQueries (
multiQueriesEnabled,
nbrCommands,
individualStatementTimeout
);
}
```
```
StringBuilder queryBuf = new StringBuilder ();
```
```
batchStmt = locallyScopedConn.createStatement ();
JdbcStatement jdbcBatchedStmt = (JdbcStatement) batchStmt;
```
```
...
```
```
int argumentSetsInBatchSoFar = 0 ;
```
```
for (commandIndex = 0 ; commandIndex < nbrCommands; commandIndex++) {
String nextQuery = (String) this.query.getBatchedArgs (). get (commandIndex);
```
```
...
```
```
queryBuf.append (nextQuery);
queryBuf.append (";");
argumentSetsInBatchSoFar++;
}
```
```
if (queryBuf.length () > 0 ) {
try {
batchStmt.execute (queryBuf.toString (),
java. sql. Statement. RETURN_GENERATED_KEYS);
} catch (SQLException ex) {
sqlEx = handleExceptionForBatch (
commandIndex - 1 , argumentSetsInBatchSoFar, updateCounts, ex
);
}
```
```
...
}
```

使用 JPA 和 Hibernate 时， SQL 语句都将使用 JDBC 执行 PreparedStatement，而不是 Statement，
这是有充分理由的：

```
PreparedStatement 准备好的语句允许你增加语句缓存的可能性
PreparedStatement 准备好的语句允许你避免 SQL 注入攻击，因为你绑定参数值而不是像我们在
之前的 String. format 调用中那样注入它们。
```
但是，由于 Hibernate 默认不启用 JDBC 批处理，我们需要提供以下配置属性来激活自动批处理机制：

因此，当持久化 10 个 Post 实体时：

Hibernate 将执行单个 JDBC INSERT，如 datasource-proxy 日志条目所示：

```
注意：如果使用 IDENTITY 实体标识符策略，Hibernate 将无法自动批处理插入语句。看看这篇
文章。
```
因此，使用默认的 MySQL JDBC 驱动程序设置，一条语句被发送到 MySQL 数据库服务器。

但是，如果你检查数据库服务器日志，我们可以看到语句到达后，MySQL 执行每个语句，就好像它们
在 for 循环中运行一样：

```
spring. jpa. properties. hibernate. jdbc. batch_size= 10
spring. jpa. properties. hibernate. order_inserts=true
spring. jpa. properties. hibernate. order_updates=true
```
```
for (long i = 1 ; i <= 10 ; i++) {
entityManager.persist (
new Post ()
.setId (i)
.setTitle (String.format ("Post no. %d", i))
);
}
```
```
Type: Prepared, Batch: True, QuerySize: 1 , BatchSize: 10 ,
Query:["
insert into post (title, id) values (?, ?)
"],
Params:[
(Post no. 1 , 1 ), (Post no. 2 , 2 ), (Post no. 3 , 3 ),
(Post no. 4 , 4 ), (Post no. 5 , 5 ), (Post no. 6 , 6 ),
(Post no. 7 , 7 ), (Post no. 8 , 8 ), (Post no. 9 , 9 ),
(Post no. 10 , 10 )
]
```

因此，启用 rewriteBatchedStatementsMySQL JDBC Driver 设置后：

当我们重新运行之前插入 10 个实体的测试用例时 Post，我们可以看到在数据库端执行了以下 INSERT
语句：

语句更改的原因是 MySQL JDBC 驱动程序现在调用将 executeBatchWithMultiValuesClause 批处理
的 INSERT 语句重写为单个多值 INSERT 的方法。

#### 方式四：通过数据库连接取消自动提交，手动提交数据

在方式三的基础上，取消自动提交 sql 语句，当 sql 语句都提交了才手动提交 sql 语句

```
Query insert into post (title, id) values ('Post no. 1', 1 )
Query insert into post (title, id) values ('Post no. 2', 2 )
Query insert into post (title, id) values ('Post no. 3', 3 )
Query insert into post (title, id) values ('Post no. 4', 4 )
Query insert into post (title, id) values ('Post no. 5', 5 )
Query insert into post (title, id) values ('Post no. 6', 6 )
Query insert into post (title, id) values ('Post no. 7', 7 )
Query insert into post (title, id) values ('Post no. 8', 8 )
Query insert into post (title, id) values ('Post no. 9', 9 )
Query insert into post (title, id) values ('Post no. 10', 10 )
Query commit
```
```
dataSource.setRewriteBatchedStatements (true);
```
```
Query insert into post (title, id)
values ('Post no. 1', 1 ), ('Post no. 2', 2 ), ('Post no. 3', 3 ),
('Post no. 4', 4 ), ('Post no. 5', 5 ), ('Post no. 6', 6 ),
('Post no. 7', 7 ), ('Post no. 8', 8 ), ('Post no. 9', 9 ),
('Post no. 10', 10 )
Query commit
```
```
if (! this. batchHasPlainStatements &&
this.rewriteBatchedStatements.getValue ()) {
```
```
if (getQueryInfo (). isRewritableWithMultiValuesClause ()) {
return executeBatchWithMultiValuesClause (batchTimeout);
}
```
```
...
}
```

需将 Connection conn; 连接的【conn.setAutoCommit (false)】(设置自动提交=否)

用时：【 4 秒左右】

#### 汇总一下，批处理操作的详细步骤

上述示例代码中，我们通过 JDBC 连接 MySQL 数据库，并执行批处理操作插入数据。

具体实现步骤如下：

```
1. 获取数据库连接。
2. 创建 Statement 对象。
3. 定义 SQL 语句，使用 PreparedStatement 对象预编译 SQL 语句并设置参数。
4. 取消自动提交
5. 将 sql 语句打包到一个 Batch 容器中, 添加需要批量处理的 SQL 语句或是参数
6. 执行批处理操作。
7. 清空 Batch 容器，为下一次打包做准备
```
```
/**
* 方式四
*
*/
@Test
public void bulkSubmissionTest 4 () {
long start = System.currentTimeMillis ();
Connection conn = jdbcUtils.getConnection ();//获取数据库连接
String sql = "insert into a (id, name) VALUES (?, null)";
PreparedStatement ps = null;
try {
ps = conn.prepareStatement (sql);
conn.setAutoCommit (false);//取消自动提交
for (int i = 1 ; i <= 1000000 ; i++) {
ps.setObject ( 1 , i);
ps.addBatch ();
if (i % 500 == 0 ) {
ps.executeBatch ();
ps.clearBatch ();
}
}
ps.executeBatch ();
ps.clearBatch ();
conn.commit ();//所有语句都执行完毕后才手动提交 sql 语句
} catch (SQLException e) {
e.printStackTrace ();
} finally {
jdbcUtils.close (conn, ps, null);
}
System.out.println ("百万条数据插入用时：" + (System.currentTimeMillis () -
start)+"【单位：毫秒】");
}
```

```
8. 不断迭代第 5-7 步，直到数据处理完成。
9. 关闭 Statement 和 Connection 对象。
```
使用 setAutoCommit (false) 来禁止自动提交事务，然后在每次批量插入之后手动提交事务。

每次插入数据时都新建一个 PreparedStatement 对象以避免状态不一致问题。

在插入数据的循环中，每累计到一定量的数据如 10000 条数据就执行一次 executeBatch () 插入数
据。

###### 另外注意：

```
1. 使用批量提交数据，url 一定要设置允许重写批量提交 rewriteBatchedStatements=true，
2. sql 语句一定不能有分号，否则有 BatchUpdateException 异常，
3. 在循环插入时带有适当的等待时间和批处理大小，从而避免内存占用过高等问题：
```
```
设置适当的批处理大小：批处理大小指在一次插入操作中插入多少行数据。如果批处理大小太小，
插入操作的频率将很高，而如果批处理大小太大，可能会导致内存占用过高。通常，建议将批处理
大小设置为 1000-5000 行，这将减少插入操作的频率并降低内存占用。
采用适当的等待时间：等待时间指在批处理操作之间等待的时间量。等待时间过短可能会导致内存
占用过高，而等待时间过长则可能会延迟插入操作的速度。通常，建议将等待时间设置为几秒钟到
几十秒钟之间，这将使操作变得平滑且避免出现内存占用过高等问题。
可以考虑使用一些内存优化的技巧，例如使用内存数据库或使用游标方式插入数据，以减少内存占
用。
总的来说，选择适当的批处理大小和等待时间可以帮助您平稳地进行插入操作，避免出现内存占用
过高等问题。
5. 索引: 在大量数据插入前暂时去掉索引，最后再打上，这样可以大大减少写入时候的更新索引的时
间。
6. 数据库连接池：使用数据库连接池可以减少数据库连接建立和关闭的开销，提高性能。在没有使用
数据库连接池的情况，记得在 finally 中关闭相关连接。
7. 数据库参数调整：增加 MySQL 数据库缓冲区大小、配置高性能的磁盘和 I/O 等。
8. 需要根据实际情况优化连接池和数据库的相关配置，以防止连接超时等问题。
```
#### 所以，以上才是“教科书式” 答案：

结合 B 站的方案，大家回到前面的面试题：

```
Java 怎么实现几十万条数据插入？
JDBC 添加几万条数据，要求保证效率，请写出代码？
```
以上的方案，才是完美的答案，才是“教科书式” 答案。后续，尼恩会给大家结合行业案例，分析出更
多，更加劲爆的答案。

当然，如果遇到这类问题，可以找尼恩求助。


## 滴滴一面，痛失 40 K：因 MVCC 没说明白

#### 说在前面

在 40 岁老架构师尼恩的 **读者交流群** (50+) 中，最近有小伙伴拿到了一线互联网企业如阿里、网易、有
赞、希音、百度、网易、滴滴的面试资格，遇到很多次遇到 MVCC 相关的面试题：

```
说一下 MVCC 的实现原理？
请你讲下 MVCC 是什么？
```
前几天，小伙伴给尼恩反馈，在滴滴的面试中，遇到这个问题，没有说清楚，导致面试失败。

在 MySQL 中， **MVCC （多版本并发控制）** 主要解决并发访问数据库带来的一系列问题。 **例如，** 读写之
间阻塞的问题、减少死锁的发生、解决一致性读（快照读）的问题。 **MVCC** 可以在尽量减少锁使用的情
况下，用更高效、更好的方式去处理读写冲突， **极大提高了数据库并发性能** 。

**MVCC 是面试的核心问题。** 这里尼恩给大家做一下系统化、体系化的梳理，使得大家可以充分展示一下
大家雄厚的 “技术肌肉”， **让面试官爱到 “不能自已、口水直流”** 。

**本篇，我们深入理解 MVCC（多版本并发控制）原理。**

也一并把这个题目以及参考答案，收入咱们的《尼恩 Java 面试宝典 PDF》V 106 版本，供后面的小伙伴参
考，提升大家的 3 高架构、设计、开发水平。

```
最新《尼恩架构笔记》《尼恩高并发三部曲》《尼恩 Java 面试宝典》的 PDF，请关注本公众号
【技术自由圈】获取，回复：领电子书
```
#### 1 、什么是 MVCC

**MVCC ，即多版本并发控制，全拼 Version Concurrency Control** 。

**MVCC** 为每个事务创建多个数据版本，每个版本对应一个特定时间点的数据库状态，不同事务可以基于
各自的时间点来进行读取和写入操作，而不会相互干扰。

#### 2 、什么是当前读、快照读？

在深入了解 MVCC 之前，我们先来探讨一下 MySQL InnoDB 的当前读和快照读。

当前读和快照读是 MVCC 机制下的两种数据读取方法，各自适用于各种不同的应用场景。

**当前读（Current Read）**

```
当前读是指事务在读取数据时，总是读取最新提交的数据版本。
当前读能够读取其他事务已经提交的数据，同时在当前事务有未提交的修改时，也会读取自己所做
的修改，可能会读取到未提交的数据。
当前读适用于需要获取最新数据状态的场景，比如，实时查询账户余额。然而，需要注意的是，在
并发环境下，当前读可能会引发一致性问题。
```
**快照读（Snapshot Read）**

```
快照读，也称为一致性读，是指事务在读取数据时，会读取一个事务开始时的数据版本，即创建事
务时的快照。
快照读仅会读取已提交的数据版本，不会读取其他事务未提交的数据。
快照读适用于需要事务隔离和数据一致性的场景。比如，在事务内部进行多次读取操作。
```

```
隔离界别脏读不可重复读幻读
```
```
READUNCOMMITTED：未提交读可能发生可能发生可能发生
```
```
READ COMMITTED：已提交读解决可能发生可能发生
```
```
REPEATABLE READ：可重复读解决解决可能发生
```
```
SERIALIZABLE：可串行化解决解决解决
```
```
快照读能够提供事务开始时的数据一致性视图，避免了并发冲突和未提交数据的影响，但可能不够
实时。
```
根据事务隔离级别和应用需求的不同，我们可以选择适合的读取方式。

#### 3 、MVCC 的作用

数据库的三种并发场景是 **读 - 读** 、 **读 - 写** 、 **写 - 写** 。

```
读 - 读 ：不存在任何问题，也不需要并发控制；
读 - 写 ：有线程安全问题，事务可能出现隔离性问题，例如脏读、幻读、不可重复读；
写 - 写 ：有线程安全问题，可能存在更新丢失问题。
```
在 MySQL InnoDB 中， **MVCC 主要解决并发访问数据库带来的一系列问题** ：

```
读-写之间阻塞的问题；
减少死锁的发生；
解决一致性读（快照读）的问题。
```
如果没有 MVCC，读-写之间，就必须加锁。

锁，是一种性能低下的组件。

MVCC 就是一种不使用锁，去解决读写冲突问题，可以理解为是一种类似的写时复制（copy on
write）、或者读时复制（copy on ready）机制。

本质上，MVCC 是通过无锁的方式，去解决高并发场景下，读写、和写写冲突的问题。在尼恩的 3 高架构
知识宇宙体系中，属于一种无锁编程的架构。

在多个事务同时读取和修改数据库时， **MVCC** 可以在尽量减少锁使用的情况下，用更高效、更好的方式
去处理读写冲突，

使用 MVCC，即便出现了读写冲突，也可以做到不加锁、非阻塞并发读， **极大提高了数据库并发性能** 。

数据库的四种隔离级别：

以上四个级别，都没有脏写。

为啥呢？ 脏写最为严重，四种隔离级别都不允许出现脏写，因此没有脏写。

**MVCC** 支持数据库的 **不同事务隔离级别** ，例如 **读未提交、读已提交、可重复读和串行化** 。

如何做到的呢？

#### 4 、MVCC 的实现原理

MVCC 的实现原理是依靠表记录中的 3 个隐含字段、undo log 日志、ReadView 来实现的。


```
列名
```
```
是否必
须描述
```
```
row_id 否行 ID，唯一标识一条记录 ( 如果定义主键，它就没有啦 )
```
```
transaction_id 是事务 ID
```
```
roll_pointer 是 DB_ROLL_PTR 是一个回滚指针，^ 用于配合 undo 日志，指向上一
个旧版本
```
**MVCC 的实现主要依赖于这三个隐藏字段、Undo log 及 ReadView。**

**首先，看看第一个部分：三个隐藏字段**

在 InnoDB 存储引擎为每行数据添加了三个隐藏字段：trx_id、roll_pointer、row_id。

对应到表隐式定义的 DB_TRX_ID, DB_ROLL_PTR, DB_ROW_ID 字段如下

```
DB_TRX_ID： 6 字节，最近修改事务 id，记录创建这条记录或者最后一次修改该记录的事务 id
DB_ROLL_PTR： 7 字节，回滚指针，指向这条记录的上一个版本，用于配合 undolog，指向上一个
旧版本
DB_ROW_JD： 6 字节，隐藏的主键，如果数据表没有主键，那么 innodb 会自动生成一个 6 字节的
row_id
```
记录如图所示：


在上图中，DB_ROW_ID 是数据库默认为该行记录生成的唯一隐式主键，由于已经存在 id，这个字段就
不用了。

在上图中，DB_TRX_ID 是当前操作该记录的事务 ID，DB_ROLL_PTR 是一个回滚指针，用于配合 undo 日
志，指向上一个旧版本

**undo log**

undolog 被称之为回滚日志，表示在进行 insert，delete，update 操作的时候产生的方便回滚的日志

当进行 insert 操作的时候，产生的 undolog 只在事务回滚的时候需要，并且在事务提交之后可以被立刻
丢弃

当进行 update 和 delete 操作的时候，产生的 undolog 不仅仅在事务回滚的时候需要，在快照读的时候也
需要，所以不能随便删除，

只有在快照读或事务回滚不涉及该日志时，对应的日志才会被 purge 线程统一清除（当数据发生更新和
删除操作的时候，都只是设置一下老记录的 deleted_bit，并不是真正的将过时的记录删除，因为为了节
省磁盘空间，innodb 有专门的 purge 线程来清除 deleted_bit 为 true 的记录，如果某个记录的
deleted_bit 为 true，并且 DB_TRX_ID 相对于 purge 线程的 ReadView 可见，那么这条记录一定是可以被
清除的）。

下面我们来看一下 undolog 生成的记录链

roll_pointer 的作用是，可以构成 undo log 数据的版本链

**版本链** 在每次进行 update 或者 delete 操作时，会将每次的操作细节，详细记录在 undo log 中。

每条 undo log 中，都记录了 rol_pointer 信息，通过 roll_pointer 进行关联，可以构成数据的版本链。

（ 1 ）假设有一个事务编号为 10 的事务向表中插入一条记录，那么此时行数据的状态为：


（ 2 ）假设有第二个事务编号为 2 对该记录的 name 做出修改，改为校长

首先，在事务 20 修改该行记录数据时，数据库会对该行加排他锁。

然后把该行老数据拷贝到 undolog 中，作为旧记录，即在 undolog 中有当前行的拷贝副本。拷贝完毕
后，真正开始干活：修改该行 name 为校长，并且修改隐藏字段的事务 id 为 20 ，回滚指针指向拷贝到
undolog 的副本记录的地址。

最后，事务提交后，释放锁。

（ 3 ）假设有第三个事务编号为 30 对该记录的 name 做了修改，改为李四

首先，在事务 30 修改该行记录数据时，数据库会对该行加排他锁。

然后把该行老数据拷贝到 undolog 中，作为旧记录，即在 undolog 中有当前行的拷贝副本。拷贝完毕
后，真正开始干活：修改该行 name 为李四，并且修改隐藏字段的事务 id 为 30 ，回滚指针指向拷贝到
undolog 的副本记录的地址。

最后，事务提交后，释放锁。


（ 4 ）假设有第三个事务编号为 40 对该记录的 name 做了修改，改为王五

首先，在事务 40 修改该行记录数据时，数据库会对该行加排他锁。

然后把该行老数据拷贝到 undolog 中，作为旧记录，即在 undolog 中有当前行的拷贝副本。

拷贝完毕后，真正开始干活：修改该行 name 为王五，并且修改隐藏字段的事务 id 为 40 ，回滚指针指向
拷贝到 undolog 的副本记录的地址。

最后，事务提交后，释放锁。


从上述的一系列图中，大家可以发现，不同事务或者相同事务的对同一记录的修改，会导致该记录的
undolog 生成一条记录版本线性表，即链表，undolog 的链首就是最新的旧记录，链尾就是最早的旧记
录。

所以，一个记录会被一堆事务进行修改，一个记录中就会存在很多 Undo log。

那对某个事务来说， **这么多 Undo log，到底应该选择哪些 Undo log 执行回滚呢？**

即，哪个版本可以被事务看到呢？

**ReadView 机制** 就是用来为事务做可见性判断的，它可以判断版本链中的哪个版本是当前事务可见
的。

上面的流程如果看明白了，那么大家需要再深入理解下 ReadView 的概念了。

#### 5 、ReadView 机制

###### 5.1 什么是 ReadView

**ReadView** （读视图）是多版本并发控制（MVCC）中的一个重要概念。

**ReadView** 用于控制事务读取数据的逻辑视图，确保事务在整个过程中看到一致的数据状态。它是如何
判断的呢？

ReadView 是事务进行快照读操作的时候生产的读视图，

ReadView 是在该事务执行快照读的那一刻，会生成一个数据系统当前的快照，记录并维护系统当前活
跃事务的 id，事务的 id 值是递增的。

ReadView 的最大作用是用来做可见性判断的，

当某个事务在执行快照读的时候，对该记录创建一个 ReadView 的视图，把它当作条件，去判断当前事
务能够看到哪个版本的数据，

比如说，有可能读取到的版本，是最新的数据，

再比如说，也有可能读取的是数据版本，是当前行记录的 undolog 中某个版本的数据。

首先，看看 **ReadView 最重要的 4 个部分** ：


```
注意：请点击图像以查看清晰的视图！
```
###### 5.2 ReadView 读取规则

ReadView 仅仅记录一个事务开始的时候，系统的事务 id 列表，和相关的事务信息。

如何通过 ReadView ，去判断当前事务，应该去读取哪个记录的数据版本？

围绕 ReadView，有一套可见性算法。

将要被修改的数据的最新记录中的 DB_TRX_ID（即当前事务 ID）取出来，与系统当前其他活跃事务的
ID 去对比（由 ReadView 维护）。


**可见性算法大致流程如下**

将要被修改的数据的最新记录中的 DB_TRX_ID（当前事务 id）取出来，与系统当前其他活跃事务的 id 去
对比，如果 DB_TRX_ID 跟 ReadView 的属性做了比较，不符合可见性，那么就通过 DB_ROLL_PTR 回滚指
针去取出 undolog 中的 DB_TRX_ID 做比较，即遍历链表中的 DB_TRX_ID，直到找到满足条件的
DB_TRX_ID, 这个 DB_TRX_ID 所在的旧记录，就是当前事务能看到的最新老版本数据。

具体如下图：

首先要知道 ReadView 中的三个全局属性：

trx_list：一个数值列表，用来维护 ReadView 生成时刻系统正活跃的事务 ID（1,2,3）

up_limit_id (up-id)：记录 trx_list 列表中事务 ID 最小的 ID（ 1 ）

low_limit_id：ReadView 生成时刻系统尚未分配的下一个事务 ID，（ 4 ）

具体的比较规则如下：

首先比较 DB_TRX_ID < up_limit_id, 如果小于，则当前事务能看到 DB_TRX_ID 所在的记录，如果大于等
于进入下一个判断

接下来判断 DB_TRX_ID >= low_limit_id, 如果大于等于则代表 DB_TRX_ID 所在的记录在 ReadView 生成后
才出现的，那么对于当前事务肯定不可见，如果小于，则进入下一步判断

判断 DB_TRX_ID 是否在活跃事务中，如果在，则代表在 ReadView 生成时刻，这个事务还是活跃状态，
还没有 commit，修改的数据，当前事务也是看不到，


如果不在，则说明这个事务在 ReadView 生成之前就已经开始 commit，那么修改的结果是能够看见的

下面，进行场景细致梳理，当被访问版本的 trx_id 属性值：

```
如果 trx_id = creator_trx_id ，当前事务在访问自己修改过的记录，则该版本可以被当前事
务访问。
如果 trx_id < min trx_id，表明生成该版本的事务在当前事务生成 ReadView 前已经提交，故
该版本可以被当前事务访问。
如果 trx_id > = max _trx_id，表明生成该版本的事务在当前事务生成 ReadView 后才开启，
故该版本不可以被当前事务访问。
如果 min_trx_id <= trx _id<= max_trx_id，那就需要判断一下 trx_id 属性值是不是在 m_ids
列表中，如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问；
如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。
```
###### 5.3 ReadView 生成规则

但是， **读取已提交** 和 **可重复读** 这两种隔离级别所产生的 ReadView 是不同的。

```
在读已提交（READ COMMITTED）的隔离级别下： 事务中每次对数据进行 SELECT ，都会生成一
个 ReadView。
在可重复读（ REPEATABLE READ）的隔离级别下 ：在一个事务中对一行数据第一次进行 SELECT
查询，会生成一个 ReadView，之后事务都将使用该 ReadView 进行数据的读取。
```
```
注意：请点击图像以查看清晰的视图！
```
总的来说，ReadView 读视图就是在进行快照读时会产生一个 ReadView 视图、在该事务执行的快照读的
那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的 ID (当每个事务开启时，都
会被分配一个 ID, 这个 ID 是递增的，所以最新的事务，ID 值越大)。


ReadView 是用来记录发生快照读那一刻所有的记录，当你下次就算有执行新的事务记录改变了，
ReadView 没变，读出来的数据依然是不变的。

而隔离级别中的 RR（可重复读）、和 RC（提交读）不同就是差在快照读时

RR 创建一个快照和 ReadView，并且下次快照读时使用的还是同一个 ReadView，所以其他事务修改数据
对他是不可见的、解决了不可重复读问题。

RC 则是每次快照读时都会产生新的快照和 ReadView、所以就会产生不可重复读问题。

###### 5.4 ReadView 如何解决幻读

接下来，说明 InnoDB 是如何解决幻读的。注意是在可重复读（ REPEATABLE READ）的隔离级别下。

假设现在表 user 中只有一条数据，数据内容中，主键 id=1，隐藏的 trx_id=10，它的 undo log 如下图
所示。

假设现在有事务 A 和事务 B 并发执行，事务 A 的事务 id 为 20 ，事务 B 的事务 id 为 30 。

步骤 1 ：事务 A 开始第一次查询数据，查询的 SQL 语句如下。

在开始查询之前，MySQL 会为事务 A 产生一个 ReadView，此时 ReadView 的内容如下：

由于此时表中只有一条数据，且符合 where id>=1 条件，因此会查询出来。

然后根据 ReadView 机制，发现该行数据的 trx_id=10，小于事务 A 的 ReadView 里 up_limit_id，这表
示这条数据是事务 A 开启之前，其他事务就已经提交了的数据，因此事务 A 可以读取到。

结论：事务 A 的第一次查询，能读取到一条数据，id=1。

```
select * from user where id >= 1 ;
```
```
trx_ids=[ 20 , 30 ] ，up_limit_id= 20 ，
```
```
low_limit_id= 31 ， creator_trx_id= 20
```

步骤 2 ：接着事务 B (trx_id=30)，往表中新插入两条数据，并提交事务。

此时表中就有三条数据了，对应的 undo 如下图所示：

步骤 3 ：接着事务 A 开启第二次查询，根据可重复读隔离级别的规则，此时事务 A 并不会再重新生成
ReadView。此时表 student 中的 3 条数据都满足 where id>=1 的条件，因此会先查出来。

然后根据 ReadView 机制，判断每条数据是不是都可以被事务 A 看到。

1 ）首先 id=1 的这条数据，前面已经说过了，可以被事务 A 看到。

2 ）然后是 id=2 的数据，它的 trx_id=30，此时事务 A 发现，这个值处于 up_limit_id 和 low_limit_id
之间，因此还需要再判断 30 是否处于 trx_ids 数组内。由于事务 A 的 trx_ids=[20,30]，因此在数组
内，这表示 id=2 的这条数据是与事务 A 在同一时刻启动的其他事务提交的，所以这条数据不能让事务
A 看到。

3 ）同理，id=3 的这条数据，trx_id 也为 30 ，因此也不能被事务 A 看见。

结论：最终事务 A 的第二次查询，只能查询出 id=1 的这条数据。这和事务 A 的第一次查询的结果是一
样的，因此没有出现幻读现象，所以说在 MySQL 的可重复读隔离级别下，不存在幻读问题。

#### 6 、总结

通过本文，我们了解并掌握了 MVCC 的概念、作用、工作原理等。

**MVCC** 为每个事务创建多个数据版本，每个版本对应一个特定时间点的数据库状态，不同事务可以基于
各自的时间点来进行读取和写入操作，而不会相互干扰， **极大提高了数据库并发性能** 。

**MVCC** 依赖于 InnoDB 下的三个隐藏字段、Undo log 及 ReadView 来实现，在一定程度上实现了 **读写
并发** 。

```
insert into user (id, name) values ( 2 ,'李四');
```
```
insert into user (id, name) values ( 3 ,'王五');
```

#### 说在最后

MVCC 相关面试题，是非常常见的面试题。

以上的内容，如果大家能对答如流，如数家珍，基本上面试官会被你震惊到、吸引到。

最终， **让面试官爱到 “不能自已、口水直流”** 。offer，也就来了。

学习过程中，如果有啥问题，大家可以来找 40 岁老架构师尼恩交流。

## 参考文献：

清华大学出版社《尼恩 Java 高并发核心编程卷 2 加强版》

4000 页《尼恩 Java 面试宝典》中专题 29 多线程面试专题

[1]. https://www.infoq.cn/article/1afyz3b6hnhprrg12833

[2].https://www.iamle.com/archives/2900.html

[3].https://blog.51cto.com/lianghecai/4755693

[4].https://qinyuanpei.github.io/posts/1333693167/

[5].https://github.com/alibaba/canal/wiki/ClientAdapter

[1] TDDL 开源 Github 地址
https://github.com/alibaba/tb_tddl

[2] 阿里愚公开源 Github 地址

https://github.com/alibaba/yugong

[3] 阿里云 DTS 产品官网
https://www.aliyun.com/product/dts

分布式事务（ 图解 + 秒懂 + 史上最全 ） - 疯狂创客圈 - 博客园 (cnblogs. com)

https://cloud.tencent.com/developer/article/1478535

https://www.cnblogs.com/CnFallTime/p/16297502.html

https://www.cnblogs.com/zh770/p/15194297.html

https://blog.csdn.net/qq_38645103/article/details/125891108

https://blog.csdn.net/weixin_39559282/article/details/119210641

https://blog.csdn.net/qq_39487835/article/details/123787952

https://www.cnblogs.com/aspirant/p/6920987.html

https://blog.csdn.net/qq_44969643/article/details/125000906


https://blog.csdn.net/qq_27828675/article/details/115955740

https://mp.weixin.qq.com/s/IIyimk50T0mV4PJXjaM1wQ

https://zhuanlan.zhihu.com/p/436733017

https://blog.csdn.net/qq_39708228/article/details/118692397

https://zhuanlan.zhihu.com/p/401198674

https://cloud.tencent.com/developer/article/1774781

https://blog.csdn.net/sufu1065/article/details/123343482

https://www.cnblogs.com/xiatc/p/16363312.html

https://blog.csdn.net/a303549861/article/details/96117063

https://segmentfault.com/a/1190000021086051

https://blog.csdn.net/CSDN_WYL2016/article/details/120500830

https://blog.csdn.net/xiao__jia__jia/article/details/117408114

https://blog.csdn.net/why15732625998/article/details/80388236

https://blog.csdn.net/weixin_39928017/article/details/113217272

https://blog.csdn.net/Chad_it/article/details/127089851

https://blog.csdn.net/AlbenXie/article/details/127851306

Meituan-Dianping/DBProxy (github. com)

美团点评 MySQL 数据库高可用架构 (tencent. com)

MyFlash——美团点评的开源 MySQL 闪回工具)

https://blog.csdn.net/yuan2019035055/article/details/120301159

索引结构 https://blog.csdn.net/LXYDSF/article/details/125873790

https://blog.csdn.net/m0_49039508/article/details/127368330

https://developer.aliyun.com/article/1053255

https://blog.csdn.net/weixin_36586564/article/details/104000328/

https://www.cnblogs.com/leefreeman/p/8315844.html

https://www.cnblogs.com/fangdadada/p/13055589.html

https://www.jianshu.com/p/cf5d381ef637

https://www.modb.pro/db/139052

《MYSQL 内核：INNODB 存储引擎卷 1 》

https://blog.csdn.net/LJFPHP/article/details/97133701)


## 推荐阅读

《百亿级访问量，如何做缓存架构设计》

《多级缓存架构设计》

《消息推送架构设计》

《阿里 2 面：你们部署多少节点？1000 W 并发，当如何部署？》

《美团 2 面： 5 个 9 高可用 99.999%，如何实现？》

《网易一面：单节点 2000 Wtps，Kafka 怎么做的？》

《字节一面：事务补偿和事务重试，关系是什么？》

《网易一面：25 Wqps 高吞吐写 Mysql，100 W 数据 4 秒写完，如何实现？》

《亿级短视频，如何架构？》

《炸裂，靠“吹牛”过京东一面，月薪 40 K》

《太猛了，靠“吹牛”过顺丰一面，月薪 30 K》

《炸裂了... 京东一面索命 40 问，过了就 50 W+》

《问麻了... 阿里一面索命 27 问，过了就 60 W+》

《百度狂问 3 小时，大厂 offer 到手，小伙真狠！》

《饿了么太狠：面个高级 Java，抖这多硬活、狠活》

《字节狂问一小时，小伙 offer 到手，太狠了！》

《收个滴滴 Offer：从小伙三面经历，看看需要学点啥？》



```
技术自由圈
```
## 未来职业，如何突围：三栖架构师


```
技术自由圈
```
### 成功案例： 2 年翻 3 倍， 35 岁卷王成功转型为架构师

详情：http://topcoder.cloud/forum.php?mod=forumdisplay&fid=43&page=1


技术自由圈


技术自由圈


技术自由圈


```
技术自由圈
```
### 硬核推荐：尼恩 Java 硬核架构班

详情：https://www.cnblogs.com/crazymakercircle/p/9904544.html


技术自由圈


```
技术自由圈
```
##### 架构班（社群 VIP）的起源：

最初的视频，主要是给读者加餐。很多的读者，需要一些高质量的实操、理论视频，所以，我就围绕书，和底层，做了几个
实操、理论视频，然后效果还不错，后面就做成迭代模式了。

##### 架构班（社群 VIP）的功能：^

提供高质量实操项目整刀真枪的架构指导、快速提升大家的:
⚫ 开发水平
⚫ 设计水平
⚫ 架构水平
弥补业务中 CRUD 开发短板，帮助大家尽早脱离具备 3 高能力，掌握：
⚫ 高性能
⚫ 高并发
⚫ 高可用
作为一个高质量的架构师成长、人脉社群，把所有的卷王聚焦起来，一起卷：
⚫ 卷高并发实操
⚫ 卷底层原理
⚫ 卷架构理论、架构哲学
⚫ 最终成为顶级架构师，实现人生理想，走向人生巅峰

##### 架构班（社群 VIP）的目的：^

⚫ 高质量的实操，大大提升简历的含金量，吸引力，增强面试的召唤率
⚫ 为大家提供九阳真经、葵花宝典，快速提升水平
⚫ 进大厂、拿高薪
⚫ 一路陪伴，提供助学视频和指导，辅导大家成为架构师
⚫ 自学为主，和其他卷王一起，卷高并发实操，卷底层原理、卷大厂面试题，争取狠卷 3 月成高手，狠卷 3 年成为顶级架
构师


```
技术自由圈
```
##### N 个超高并发实操项目：简历压轴、个顶个精彩


```
技术自由圈
```
【样章】第 17 章：横扫全网 Rocketmq 视频第 2 部曲: 工业级 rocketmq 高可用（HA）底层原
理和实操

工业级 rocketmq 高可用底层原理，包含：消息消费、同步消息、异步消息、单向消息等不同消息的底层原理和源码实现；
消息队列非常底层的主从复制、高可用、同步刷盘、异步刷盘等底层原理。
工业级 rocketmq 高可用底层原理和搭建实操，包含：高可用集群的搭建。
解决以下难题：
1 、技术难题：RocketMQ 如何最大限度的保证消息不丢失的呢？RocketMQ 消息如何做到高可靠投递？
2 、技术难题：基于消息的分布式事务，核心原理不理解
3 、选型难题： kafka or rocketmq ，该娶谁？
下图链接：https://www.processon.com/view/6178e8ae0e3e7416bde9da19


```
技术自由圈
```
### 简历优化后的成功涨薪案例（ VIP 含免费简历优化）


技术自由圈


技术自由圈


技术自由圈


技术自由圈


技术自由圈


技术自由圈


技术自由圈


```
技术自由圈
```
### 修改简历找尼恩（资深简历优化专家）

⚫ 如果面试表达不好，尼恩会提供简历优化指导

⚫ 如果项目没有亮点，尼恩会提供项目亮点指导

⚫ 如果面试表达不好，尼恩会提供面试表达指导

作为 40 岁老架构师，尼恩长期承担技术面试官的角色：

⚫ 从业以来，“阅历”无数，对简历有着点石成金、改头换面、脱胎换骨的指导能力。

⚫ 尼恩指导过刚刚就业的小白，也指导过 P 8 级的老专家，都指导他们上岸。

如何联系尼恩。尼恩微信，请参考下面的地址：

语雀：https://www.yuque.com/crazymakercircle/gkkw8s/khigna
码云：https://gitee.com/crazymaker/SimpleCrayIM/blob/master/疯狂创客圈总目录.md


